<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>cross entropy on WFUing&#39;s Blog</title>
    <link>https://WFUing.github.io/tags/cross-entropy/</link>
    <description>Recent content in cross entropy on WFUing&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>© 2024 WFUing</copyright>
    <lastBuildDate>Fri, 13 Oct 2023 09:25:45 +0800</lastBuildDate><atom:link href="https://WFUing.github.io/tags/cross-entropy/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>交叉熵</title>
      <link>https://WFUing.github.io/posts/algorithm/ai/cross-entropy/</link>
      <pubDate>Fri, 13 Oct 2023 09:25:45 +0800</pubDate>
      
      <guid>https://WFUing.github.io/posts/algorithm/ai/cross-entropy/</guid>
      <description>案例驱动 # 通过几个简单的例子来解释和总结什么是交叉熵（Cross Entropy）以及机器学习分类问题中为什么使用交叉熵。
第一个例子 # 假设随机从一个口袋里取硬币，口袋里有一个蓝色的，一个红色的，一个绿色的，一个橘色的。取出一个硬币之后，每次问一个问题，然后做出判断，目标是，问最少的问题，得到正确答案。其中一个最好的设计问题的策略如下：
每一个硬币有 $\frac{1}{4}$ 的概率被选中，$\frac{1}{4}机率 * 2道题目 * 4颗球 = 2$，平均需要问两道题目才能找出不同颜色的球，也就是说期望值为 $2$，就是熵（entropy）。
第二个例子 # 例子变了，变成了袋子中 $\frac{1}{8}$ 的硬币是绿色的，$\frac{1}{8}$ 的是橘色的，$\frac{1}{4}$ 是红色的，$\frac{1}{2}$ 是蓝色的，这时最优的问题的策略如下:
$\frac{1}{2}$ 的概率是蓝色，只需要 $1$ 个问题就可以知道是或者不是，$\frac{1}{4}$ 的概率是红色，需要2个问题，按照这个逻辑，猜中硬币需要的问题的期望是
$$ \frac{1}{2}*1+\frac{1}{4}*2+\frac{1}{8}*3+\frac{1}{8}*3=1.75 $$
第三个例子 # 假设袋子中全部是蓝色的硬币，那么这时候需要 $0$ 个问题就可以猜到硬币，即 $\log_{2}{1}=0$。 需要注意的是，只有当知道袋子中全部是蓝色的硬币的时候需要的问题是 $0$ 个。
总结上面的例子，假设一种硬币出现的概率是 $p$，那么猜中该硬币的所需要的问题数是 $\log_2{\frac1{P_i}}$。例如 $p=\frac{1}{4}，\log_{2}{4}$ 。
在这个问题中，问题个数的期望是
$$ \sum_i{p_i}*log_2{\frac{1}{p_i}} $$</description>
      
    </item>
    
  </channel>
</rss>
