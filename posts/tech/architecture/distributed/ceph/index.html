<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    
    <meta property="og:site_name" content="Waiting For You">
    <meta property="og:type" content="article">

    
    <meta property="og:image" content="https://WFUing.github.io//">
    <meta property="twitter:image" content="https://WFUing.github.io//" />
    

    
    <meta name="title" content="Ceph集群部署" />
    <meta property="og:title" content="Ceph集群部署" />
    <meta property="twitter:title" content="Ceph集群部署" />
    

    
    <meta name="description" content="这是一个纯粹的博客......">
    <meta property="og:description" content="这是一个纯粹的博客......" />
    <meta property="twitter:description" content="这是一个纯粹的博客......" />
    

    
    <meta property="twitter:card" content="Ceph是一种开源分布式存储系统，为大规模数据提供可扩展性和高性能。它使用分布式对象存储、块存储和文件系统，通过智能数据复制和动态数据分布，确保高可用性和容错性。Ceph的设计使其适用于云计算和大数据环境，提供灵活、可靠的存储解决方案，同时支持自动负载平衡和故障恢复。" />
    
    

    <meta name="keyword"  content="">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>Ceph集群部署 | </title>

    <link rel="canonical" href="/posts/tech/architecture/distributed/ceph/">

    
    
    
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    
    <link rel="stylesheet" href="/css/hugo-theme-cleanwhite.min.css">

    
    <link rel="stylesheet" href="/css/zanshang.css">

    
    <link rel="stylesheet" href="/css/font-awesome.all.min.css">

    
    

    
    <script src="/js/jquery.min.js"></script>

    
    <script src="/js/bootstrap.min.js"></script>

    
    <script src="/js/hux-blog.min.js"></script>

    
    <script src="/js/lazysizes.min.js"></script>

    
    

</head>






<nav class="navbar navbar-default navbar-custom navbar-fixed-top">

    <div class="container-fluid">
        
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Waiting For You</a>
        </div>

        
        
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">All Posts</a>
                    </li>
                    
                        
                    
                    
		    
		            <li>
                        <a href="/search"><i class="fa fa-search"></i></a>
		           </li>
                </ul>
            </div>
        </div>
        
    </div>
    
</nav>
<script>
    
    
    
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        
            $navbar.className = " ";
            
            setTimeout(function(){
                
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>




<style type="text/css">
    header.intro-header {
        background-image: url('/')
    }
</style>

<header class="intro-header" >

    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/tags/ceph" title="ceph">
                            ceph
                        </a>
                        
                    </div>
                    <h1>Ceph集群部署</h1>
                    <h2 class="subheading"></h2>
                    <span class="meta">
                        
                            Posted by 
                            
                                    Waiting For You
                             
                            on 
                            Tuesday, December 5, 2023
                            
                            
                            
                            
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>




<article>
    <div class="container">
        <div class="row">

            
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                
                <p>研一云计算的课程作业之一是用ceph和flink实现一个实时数据分析工具，我们是四个人一个小组，我负责部署ceph。互联网时代的网络资源层次不齐，记录几个比较好的博客，信息熵比较大。</p>
<h2 id="resources">Resources</h2>
<ul>
<li>官网：
<ul>
<li><a href="https://docs.ceph.com/en/latest/start/intro/">https://docs.ceph.com/en/latest/start/intro/</a></li>
<li><a href="https://docs.ceph.com/en/mimic/start/quick-ceph-deploy/">https://docs.ceph.com/en/mimic/start/quick-ceph-deploy/</a></li>
</ul>
</li>
<li>非官方资源：
<ul>
<li><a href="https://huaweicloud.csdn.net/638db510dacf622b8df8cd1a.html">手把手带你用docker部署Ceph集群</a></li>
<li><a href="https://jingyan.baidu.com/article/cbf0e500a9731e2eab289371.html">如何使用fdisk进行分区</a></li>
<li><a href="https://juejin.cn/post/7086381284733222948">使用ceph-deploy部署ceph集群</a></li>
<li><a href="https://www.jianshu.com/p/90c0e8d6eb39">ceph部署常见错误</a></li>
</ul>
</li>
</ul>
<h1 id="架构">架构</h1>
<p>
  <img src="ans4.png" alt="">

</p>
<h1 id="ceph部分">Ceph部分</h1>
<p>1、关闭防火墙和selinux</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sed -i  <span class="s2">&#34;s/SELINUX=enforcing/SELINUX=permissive/g&#34;</span> /etc/selinux/config
</span></span><span class="line"><span class="cl">setenforce <span class="m">0</span>
</span></span><span class="line"><span class="cl">systemctl stop firewalld
</span></span><span class="line"><span class="cl">systemctl disable firewalld
</span></span></code></pre></td></tr></table>
</div>
</div><p>2、配置hosts文件</p>
<p>保证集群内主机名与ip解析正常（每个节点都需要配置）</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># cat /etc/hosts</span>
</span></span><span class="line"><span class="cl">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
</span></span><span class="line"><span class="cl">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
</span></span><span class="line"><span class="cl">192.168.56.125  ceph-node1
</span></span><span class="line"><span class="cl">192.168.56.126  ceph-node2
</span></span><span class="line"><span class="cl">192.168.56.127  ceph-node3
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># ping ceph-node2</span>
</span></span><span class="line"><span class="cl">PING ceph-node2 <span class="o">(</span>192.168.56.126<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
</span></span><span class="line"><span class="cl"><span class="m">64</span> bytes from ceph-node2 <span class="o">(</span>192.168.56.126<span class="o">)</span>: <span class="nv">icmp_seq</span><span class="o">=</span><span class="m">1</span> <span class="nv">ttl</span><span class="o">=</span><span class="m">64</span> <span class="nv">time</span><span class="o">=</span>0.616 ms
</span></span><span class="line"><span class="cl">…………
</span></span></code></pre></td></tr></table>
</div>
</div><p>3、创建部署用户及配置sudo权限（所有节点都执行）</p>
<p>a.考虑到使用root用户的安全性问题，所以这里创建一个 ceph-admin 普通用户做为部署及运维使用
b.再加上ceph-deploy会在节点安装软件包，所以创建的用户需要无密码 sudo 权限</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># useradd ceph-admin</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># echo &#34;123456&#34; | passwd --stdin ceph-admin</span>
</span></span><span class="line"><span class="cl">Changing password <span class="k">for</span> user ceph-admin.
</span></span><span class="line"><span class="cl">passwd: all authentication tokens updated successfully.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># echo &#34;ceph-admin ALL = NOPASSWD:ALL&#34; | tee /etc/sudoers.d/ceph-admin</span>
</span></span><span class="line"><span class="cl">ceph-admin <span class="nv">ALL</span> <span class="o">=</span> NOPASSWD:ALL
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># chmod 0440 /etc/sudoers.d/ceph-admin</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># ll /etc/sudoers.d/ceph-admin</span>
</span></span><span class="line"><span class="cl">-r--r-----. <span class="m">1</span> root root <span class="m">30</span> Oct <span class="m">19</span> 16:06 /etc/sudoers.d/ceph-admin
</span></span></code></pre></td></tr></table>
</div>
</div><p>测试</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># su - ceph-admin</span>
</span></span><span class="line"><span class="cl">Last login: Mon Oct <span class="m">19</span> 16:11:51 CST <span class="m">2020</span> on pts/0
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-admin@ceph-node1 ~<span class="o">]</span>$ sudo su -
</span></span><span class="line"><span class="cl">Last login: Mon Oct <span class="m">19</span> 16:12:04 CST <span class="m">2020</span> on pts/0
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># exit</span>
</span></span><span class="line"><span class="cl"><span class="nb">logout</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-admin@ceph-node1 ~<span class="o">]</span>$ <span class="nb">exit</span>
</span></span><span class="line"><span class="cl"><span class="nb">logout</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>4、配置ssh无密码访问（在主节点node1上执行）</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">[root@ceph-node1 ~]# su - ceph-admin
</span></span><span class="line"><span class="cl">[ceph-admin@ceph-node1 ~]$ ssh-keygen          （每一步都按回车，口令密码留空）
</span></span><span class="line"><span class="cl">[ceph-admin@ceph-node1 ~]$ ssh-copy-id ceph-admin@ceph-node1
</span></span><span class="line"><span class="cl">[ceph-admin@ceph-node1 ~]$ ssh-copy-id ceph-admin@ceph-node2
</span></span><span class="line"><span class="cl">[ceph-admin@ceph-node1 ~]$ ssh-copy-id ceph-admin@ceph-node3
</span></span></code></pre></td></tr></table>
</div>
</div><p>5、配置ntp时间同步</p>
<p>配置时间同步目的：因在时间一致的情况下，才可保证集群正常运行
配置时间同步方式：node1连接网络上的ntp服务器同步时间，node2,3连接node1同步时间（即node1既为ntp服务端，也为客户端）
注：ntpd启动后需要等待几分钟去同步</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">yum -y intall ntp（安装ntp，全部节点都需要执行）
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">node1节点操作：
</span></span><span class="line"><span class="cl">vim /etc/ntp.conf
</span></span><span class="line"><span class="cl">注释掉默认的配置项：
</span></span><span class="line"><span class="cl">    <span class="c1">#server 0.centos.pool.ntp.org iburst</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#server 1.centos.pool.ntp.org iburst</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#server 2.centos.pool.ntp.org iburst</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#server 3.centos.pool.ntp.org iburst</span>
</span></span><span class="line"><span class="cl">添加配置项：
</span></span><span class="line"><span class="cl">server  ntp1.aliyun.com     <span class="c1">#阿里云ntp服务器</span>
</span></span><span class="line"><span class="cl">server 127.127.1.0     <span class="c1">#本地ntp服务器，配置此项是为了在外网ntp连接异常的情况下还能保证ntp正常，维护集群稳定</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">node2/node3节点操作：
</span></span><span class="line"><span class="cl">vim /etc/ntp.conf
</span></span><span class="line"><span class="cl">同样注释掉默认的server配置项：
</span></span><span class="line"><span class="cl">添加配置项：
</span></span><span class="line"><span class="cl">server 192.168.56.125     <span class="c1">#node1-ntp服务器</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">全部节点都执行：
</span></span><span class="line"><span class="cl">systemctl restart ntpd
</span></span><span class="line"><span class="cl">systemctl <span class="nb">enable</span> ntpd
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">查看ntp连接情况和状态
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># ntpq -p</span>
</span></span><span class="line"><span class="cl">     remote           refid      st t when poll reach   delay   offset  <span class="nv">jitter</span>
</span></span><span class="line"><span class="cl"><span class="o">==============================================================================</span>
</span></span><span class="line"><span class="cl">*120.25.115.20   10.137.53.7      <span class="m">2</span> u   <span class="m">41</span>  <span class="m">128</span>  <span class="m">377</span>   30.382   -1.019   1.001
</span></span><span class="line"><span class="cl"> LOCAL<span class="o">(</span>0<span class="o">)</span>        .LOCL.           <span class="m">5</span> l  <span class="m">806</span>   <span class="m">64</span>    <span class="m">0</span>    0.000    0.000   0.000
</span></span><span class="line"><span class="cl"> 
</span></span><span class="line"><span class="cl"> <span class="o">[</span>root@ceph-node2 ~<span class="o">]</span><span class="c1"># ntpq -p</span>
</span></span><span class="line"><span class="cl">     remote           refid      st t when poll reach   delay   offset  <span class="nv">jitter</span>
</span></span><span class="line"><span class="cl"><span class="o">==============================================================================</span>
</span></span><span class="line"><span class="cl">*ceph-node1      120.25.115.20    <span class="m">3</span> u   <span class="m">20</span>   <span class="m">64</span>  <span class="m">377</span>    2.143   33.254  10.350
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># ntpstat</span>
</span></span><span class="line"><span class="cl">synchronised to NTP server <span class="o">(</span>120.25.115.20<span class="o">)</span> at stratum <span class="m">3</span>
</span></span><span class="line"><span class="cl">   <span class="nb">time</span> correct to within <span class="m">27</span> ms
</span></span><span class="line"><span class="cl">   polling server every <span class="m">128</span> s
</span></span></code></pre></td></tr></table>
</div>
</div><p>二、开始部署Ceph集群</p>
<p>1、添加阿里云的base源和epel源（所有节点都执行）</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">备份系统原本的源
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># mkdir /mnt/repo_bak</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># mv /etc/yum.repos.d/* /mnt/repo_bak</span>
</span></span><span class="line"><span class="cl">添加新源
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>2、添加ceph的yum源（所有节点都执行）</p>
<p>注意事项：
这里的yum源是确定了ceph的版本，在源中的baseurl项中rpm-nautilus即代表着是ceph的nautilus版本的rpm包（nautilus是ceph的14.x版本）如果需要安装其他版本，还需要替换为其他版本号，12.x版本是luminous，13.x版本是rpm-mimic。
详情可以去ceph官方源中查看：download.ceph.com/</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">vim /etc/yum.repos.d/ceph.repo
</span></span><span class="line"><span class="cl"><span class="o">[</span>Ceph<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">name</span><span class="o">=</span>Ceph
</span></span><span class="line"><span class="cl"><span class="nv">baseurl</span><span class="o">=</span>http://download.ceph.com/rpm-nautilus/el7/x86_64
</span></span><span class="line"><span class="cl"><span class="nv">enabled</span><span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="nv">gpgcheck</span><span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="nv">type</span><span class="o">=</span>rpm-md
</span></span><span class="line"><span class="cl"><span class="nv">gpgkey</span><span class="o">=</span>https://download.ceph.com/keys/release.asc
</span></span><span class="line"><span class="cl"><span class="nv">priority</span><span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">[</span>Ceph-noarch<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">name</span><span class="o">=</span>Ceph noarch packages
</span></span><span class="line"><span class="cl"><span class="nv">baseurl</span><span class="o">=</span>http://download.ceph.com/rpm-nautilus/el7/noarch
</span></span><span class="line"><span class="cl"><span class="nv">enabled</span><span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="nv">gpgcheck</span><span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="nv">type</span><span class="o">=</span>rpm-md
</span></span><span class="line"><span class="cl"><span class="nv">gpgkey</span><span class="o">=</span>https://download.ceph.com/keys/release.asc
</span></span><span class="line"><span class="cl"><span class="nv">priority</span><span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-source<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">name</span><span class="o">=</span>Ceph <span class="nb">source</span> packages
</span></span><span class="line"><span class="cl"><span class="nv">baseurl</span><span class="o">=</span>http://download.ceph.com/rpm-nautilus/el7/SRPMS
</span></span><span class="line"><span class="cl"><span class="nv">enabled</span><span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="nv">gpgcheck</span><span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="nv">type</span><span class="o">=</span>rpm-md
</span></span><span class="line"><span class="cl"><span class="nv">gpgkey</span><span class="o">=</span>https://download.ceph.com/keys/release.asc
</span></span><span class="line"><span class="cl"><span class="nv">priority</span><span class="o">=</span><span class="m">1</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>更新yum缓存及系统软件</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">yum makecache
</span></span><span class="line"><span class="cl">yum -y update
</span></span></code></pre></td></tr></table>
</div>
</div><p>可查看ceph版本，判断yum是否配置正确</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="o">[</span>root@ceph-node1 yum.repos.d<span class="o">]</span><span class="c1"># yum list ceph --showduplicates |sort -r</span>
</span></span><span class="line"><span class="cl"> * updates: mirrors.cn99.com
</span></span><span class="line"><span class="cl">Loading mirror speeds from cached hostfile
</span></span><span class="line"><span class="cl">Loaded plugins: fastestmirror
</span></span><span class="line"><span class="cl"> * extras: mirrors.163.com
</span></span><span class="line"><span class="cl">ceph.x86_64                         2:14.2.9-0.el7                          Ceph
</span></span><span class="line"><span class="cl">ceph.x86_64                         2:14.2.8-0.el7                          Ceph
</span></span><span class="line"><span class="cl">ceph.x86_64                         2:14.2.7-0.el7                          Ceph
</span></span><span class="line"><span class="cl">ceph.x86_64                         2:14.2.6-0.el7                          Ceph
</span></span><span class="line"><span class="cl">ceph.x86_64                         2:14.2.5-0.el7                          Ceph
</span></span><span class="line"><span class="cl">ceph.x86_64                         2:14.2.4-0.el7                          Ceph
</span></span><span class="line"><span class="cl">ceph.x86_64                         2:14.2.3-0.el7                          Ceph
</span></span><span class="line"><span class="cl">ceph.x86_64                         2:14.2.2-0.el7                          Ceph
</span></span><span class="line"><span class="cl">ceph.x86_64                         2:14.2.11-0.el7                         Ceph
</span></span><span class="line"><span class="cl">ceph.x86_64                         2:14.2.1-0.el7                          Ceph
</span></span><span class="line"><span class="cl">ceph.x86_64                         2:14.2.10-0.el7                         Ceph
</span></span><span class="line"><span class="cl">ceph.x86_64                         2:14.2.0-0.el7                          Ceph
</span></span><span class="line"><span class="cl">ceph.x86_64                         2:14.1.1-0.el7                          Ceph
</span></span><span class="line"><span class="cl">ceph.x86_64                         2:14.1.0-0.el7                          Ceph
</span></span><span class="line"><span class="cl"> * base: mirrors.163.com
</span></span><span class="line"><span class="cl">Available Packages
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@ceph-node1 yum.repos.d<span class="o">]</span><span class="c1"># yum list ceph-deploy --showduplicates |sort -r</span>
</span></span><span class="line"><span class="cl"> * updates: mirrors.cn99.com
</span></span><span class="line"><span class="cl">Loading mirror speeds from cached hostfile
</span></span><span class="line"><span class="cl">Loaded plugins: fastestmirror
</span></span><span class="line"><span class="cl"> * extras: mirrors.163.com
</span></span><span class="line"><span class="cl">ceph-deploy.noarch                     2.0.1-0                       Ceph-noarch
</span></span><span class="line"><span class="cl">ceph-deploy.noarch                     2.0.0-0                       Ceph-noarch
</span></span><span class="line"><span class="cl">ceph-deploy.noarch                     1.5.39-0                      Ceph-noarch
</span></span><span class="line"><span class="cl">ceph-deploy.noarch                     1.5.38-0                      Ceph-noarch
</span></span><span class="line"><span class="cl">ceph-deploy.noarch                     1.5.37-0                      Ceph-noarch
</span></span><span class="line"><span class="cl">ceph-deploy.noarch                     1.5.36-0                      Ceph-noarch
</span></span><span class="line"><span class="cl">ceph-deploy.noarch                     1.5.35-0                      Ceph-noarch
</span></span><span class="line"><span class="cl">ceph-deploy.noarch                     1.5.34-0                      Ceph-noarch
</span></span><span class="line"><span class="cl">ceph-deploy.noarch                     1.5.33-0                      Ceph-noarch
</span></span><span class="line"><span class="cl">ceph-deploy.noarch                     1.5.32-0                      Ceph-noarch
</span></span><span class="line"><span class="cl">ceph-deploy.noarch                     1.5.31-0                      Ceph-noarch
</span></span><span class="line"><span class="cl">ceph-deploy.noarch                     1.5.30-0                      Ceph-noarch
</span></span><span class="line"><span class="cl">ceph-deploy.noarch                     1.5.29-0                      Ceph-noarch
</span></span><span class="line"><span class="cl"> * base: mirrors.163.com
</span></span><span class="line"><span class="cl">Available Packages
</span></span></code></pre></td></tr></table>
</div>
</div><p>3、安装ceph-deploy（在主节点node1上执行）</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># su - ceph-admin</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-admin@ceph-node1 ~<span class="o">]</span>$ sudo yum -y install python-setuptools   <span class="c1">#安装ceph依赖包</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-admin@ceph-node1 ~<span class="o">]</span>$ sudo yum install ceph-deploy  （默认会选择安装2.0最新版本）
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">查看ceph-deploy安装版本
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># ceph-deploy --version</span>
</span></span><span class="line"><span class="cl">2.0.1
</span></span></code></pre></td></tr></table>
</div>
</div><p>4、初始化集群（在主节点node1上执行）
创建集群安装目录（ceph-deploy部署程序会将文件输出到当前目录）</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="o">[</span>ceph-admin@ceph-node1 ~<span class="o">]</span>$ mkdir cluster
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-admin@ceph-node1 ~<span class="o">]</span>$ <span class="nb">cd</span> cluster/
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">创建集群（后边是指定哪些节点做为mon监视器使用，所以选择规划中部署mon的节点-node1）
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-admin@ceph-node1 cluster<span class="o">]</span>$ ceph-deploy new ceph-node1
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.conf<span class="o">][</span>DEBUG <span class="o">]</span> found configuration file at: /home/ceph-admin/.cephdeploy.conf
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span> Invoked <span class="o">(</span>2.0.1<span class="o">)</span>: /bin/ceph-deploy new ceph-node1
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span> ceph-deploy options:
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span>  username                      : None
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span>  func                          : &lt;<span class="k">function</span> new at 0x7f14c44c9de8&gt;
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span>  verbose                       : False
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span>  overwrite_conf                : False
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span>  quiet                         : False
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span>  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f14c3c424d0&gt;
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span>  cluster                       : ceph
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span>  ssh_copykey                   : True
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span>  mon                           : <span class="o">[</span><span class="s1">&#39;ceph-node1&#39;</span><span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span>  public_network                : None
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span>  ceph_conf                     : None
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span>  cluster_network               : None
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span>  default_release               : False
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span>  fsid                          : None
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.new<span class="o">][</span>DEBUG <span class="o">]</span> Creating new cluster named ceph
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.new<span class="o">][</span>INFO  <span class="o">]</span> making sure passwordless SSH succeeds
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-node1<span class="o">][</span>DEBUG <span class="o">]</span> connection detected need <span class="k">for</span> sudo
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-node1<span class="o">][</span>DEBUG <span class="o">]</span> connected to host: ceph-node1
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-node1<span class="o">][</span>DEBUG <span class="o">]</span> detect platform information from remote host
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-node1<span class="o">][</span>DEBUG <span class="o">]</span> detect machine <span class="nb">type</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-node1<span class="o">][</span>DEBUG <span class="o">]</span> find the location of an executable
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-node1<span class="o">][</span>INFO  <span class="o">]</span> Running command: sudo /usr/sbin/ip link show
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-node1<span class="o">][</span>INFO  <span class="o">]</span> Running command: sudo /usr/sbin/ip addr show
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-node1<span class="o">][</span>DEBUG <span class="o">]</span> IP addresses found: <span class="o">[</span>u<span class="s1">&#39;192.168.56.125&#39;</span><span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.new<span class="o">][</span>DEBUG <span class="o">]</span> Resolving host ceph-node1
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.new<span class="o">][</span>DEBUG <span class="o">]</span> Monitor ceph-node1 at 192.168.56.125
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.new<span class="o">][</span>DEBUG <span class="o">]</span> Monitor initial members are <span class="o">[</span><span class="s1">&#39;ceph-node1&#39;</span><span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.new<span class="o">][</span>DEBUG <span class="o">]</span> Monitor addrs are <span class="o">[</span><span class="s1">&#39;192.168.56.125&#39;</span><span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.new<span class="o">][</span>DEBUG <span class="o">]</span> Creating a random mon key...
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.new<span class="o">][</span>DEBUG <span class="o">]</span> Writing monitor keyring to ceph.mon.keyring...
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.new<span class="o">][</span>DEBUG <span class="o">]</span> Writing initial config to ceph.conf...
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-admin@ceph-node1 cluster<span class="o">]</span>$ ls
</span></span><span class="line"><span class="cl">ceph.conf  ceph-deploy-ceph.log  ceph.mon.keyring
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">在当前目录下的ceph.conf中添加以下两行内容
</span></span><span class="line"><span class="cl"><span class="nv">public_network</span> <span class="o">=</span> 192.168.56.0/24
</span></span><span class="line"><span class="cl"><span class="nv">cluster_network</span> <span class="o">=</span> 192.168.56.0/24
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">安装Ceph包至其他节点
</span></span><span class="line"><span class="cl">（其中 --no-adjust-repos 参数含义：使用本地配置的源，不更改源。以防出现问题）
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-admin@ceph-node1 cluster<span class="o">]</span>$ ceph-deploy install --no-adjust-repos ceph-node1 ceph-node2 ceph-node3
</span></span></code></pre></td></tr></table>
</div>
</div><p>如果出现“RuntimeError: Failed to execute command: ceph &ndash;version”报错，是因为服务器网络问题导致，下载ceph安装包速度太慢，达到5分钟导致超时，可以重复执行，或者单独在所有节点执行yum -y install ceph即可</p>
<p>初始化mon节点</p>
<p>在2.0.1版本的ceph-deploy中在该初始化的时候就会做收集密钥的动作，无需再执行 ceph-deploy gatherkeys
{monitor-host}  这个命令</p>
<p>[ceph-admin@ceph-node1 cluster]$ ceph-deploy mon create-initial</p>
<p>5、添加OSD</p>
<p>如果是里边有数据的磁盘，还需先清除数据：（详细可查看 ceph-depoy disk zap &ndash;help）</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">列出所有节点上所有可用的磁盘
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-admin@ceph-node1 cluster<span class="o">]</span>$ ceph-deploy disk list ceph-node1 ceph-node2 ceph-node3
</span></span><span class="line"><span class="cl">清除数据
</span></span><span class="line"><span class="cl">sudo ceph-deploy disk zap <span class="o">{</span>osd-server-name<span class="o">}</span> <span class="o">{</span>disk-name<span class="o">}</span>
</span></span><span class="line"><span class="cl">    eg：sudo ceph-deploy disk zap ceph-node2 /dev/sdb
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">如果是干净的磁盘，可忽略上边清除数据的操作，直接添加OSD即可
</span></span><span class="line"><span class="cl">（我这里是新添加的/dev/sdb磁盘）
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-admin@ceph-node1 cluster<span class="o">]</span>$ ceph-deploy osd create --data /dev/sdb ceph-node1
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-admin@ceph-node1 cluster<span class="o">]</span>$ ceph-deploy osd create --data /dev/sdb ceph-node2
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-admin@ceph-node1 cluster<span class="o">]</span>$ ceph-deploy osd create --data /dev/sdb ceph-node3
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">可以看到cpeh将新增OSD创建为LVM格式加入ceph集群中
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-admin@ceph-node1 cluster<span class="o">]</span>$ sudo pvs
</span></span><span class="line"><span class="cl">  PV         VG                                        Fmt  Attr PSize   PFree
</span></span><span class="line"><span class="cl">  /dev/sdb   ceph-ab1b8533-018e-4924-8520-fdbefbb7d184 lvm2 a--  &lt;10.00g    <span class="m">0</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>6、允许主机以管理员权限执行 Ceph 命令
将ceph-deploy命令将配置文件和 admin key复制到各个ceph节点，其他节点主机也能管理ceph集群
[ceph-admin@ceph-node1 cluster]$ ceph-deploy admin ceph-node1 ceph-node2 ceph-node3</p>
<p>7、部署MGR用于获取集群信息
[ceph-admin@ceph-node1 cluster]$ ceph-deploy mgr create ceph-node1</p>
<p>查看集群状态</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="o">[</span>ceph-admin@ceph-node1 cluster<span class="o">]</span>$ sudo ceph health detail
</span></span><span class="line"><span class="cl">HEALTH_OK
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-admin@ceph-node1 cluster<span class="o">]</span>$ sudo ceph -s
</span></span><span class="line"><span class="cl">  cluster:
</span></span><span class="line"><span class="cl">    id:     e9290965-40d4-4c65-93ed-e534ae389b9c
</span></span><span class="line"><span class="cl">    health: HEALTH_OK
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  services:
</span></span><span class="line"><span class="cl">    mon: <span class="m">1</span> daemons, quorum ceph-node1 <span class="o">(</span>age 62m<span class="o">)</span>
</span></span><span class="line"><span class="cl">    mgr: ceph-node1<span class="o">(</span>active, since 5m<span class="o">)</span>
</span></span><span class="line"><span class="cl">    osd: <span class="m">3</span> osds: <span class="m">3</span> up <span class="o">(</span>since 12m<span class="o">)</span>, <span class="m">3</span> in <span class="o">(</span>since 12m<span class="o">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  data:
</span></span><span class="line"><span class="cl">    pools:   <span class="m">0</span> pools, <span class="m">0</span> pgs
</span></span><span class="line"><span class="cl">    objects: <span class="m">0</span> objects, <span class="m">0</span> B
</span></span><span class="line"><span class="cl">    usage:   3.0 GiB used, <span class="m">27</span> GiB / <span class="m">30</span> GiB avail
</span></span><span class="line"><span class="cl">    pgs:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">如果查看集群状态为“HEALTH_WARN mon is allowing insecure global_id reclaim”，是因为开启了不安全的模式，将之禁用掉即可：
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> <span class="o">[</span>ceph-admin@ceph-node1 cluster<span class="o">]</span>$ sudo ceph config <span class="nb">set</span> mon auth_allow_insecure_global_id_reclaim <span class="nb">false</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">因/etc/ceph/下key文件普通用户没有读权限，所以普通用户无权直接执行ceph命令
</span></span><span class="line"><span class="cl">如果需要ceph-admin普通用户也可直接调用集群，增加对ceph配置文件的读权限即可
</span></span><span class="line"><span class="cl">（想要每个节点普通用户都可以执行ceph相关命令，那就所有节点都修改权限）
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-admin@ceph-node1 ~<span class="o">]</span>$ ll /etc/ceph/
</span></span><span class="line"><span class="cl">total <span class="m">12</span>
</span></span><span class="line"><span class="cl">-rw-------. <span class="m">1</span> root root <span class="m">151</span> Oct <span class="m">21</span> 17:33 ceph.client.admin.keyring
</span></span><span class="line"><span class="cl">-rw-r--r--. <span class="m">1</span> root root <span class="m">268</span> Oct <span class="m">21</span> 17:35 ceph.conf
</span></span><span class="line"><span class="cl">-rw-r--r--. <span class="m">1</span> root root  <span class="m">92</span> Oct <span class="m">20</span> 04:48 rbdmap
</span></span><span class="line"><span class="cl">-rw-------. <span class="m">1</span> root root   <span class="m">0</span> Oct <span class="m">21</span> 17:30 tmpcmU035
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-admin@ceph-node1 ~<span class="o">]</span>$ sudo chmod +r /etc/ceph/ceph.client.admin.keyring
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-admin@ceph-node1 ~<span class="o">]</span>$ ll /etc/ceph/
</span></span><span class="line"><span class="cl">total <span class="m">12</span>
</span></span><span class="line"><span class="cl">-rw-r--r--. <span class="m">1</span> root root <span class="m">151</span> Oct <span class="m">21</span> 17:33 ceph.client.admin.keyring
</span></span><span class="line"><span class="cl">-rw-r--r--. <span class="m">1</span> root root <span class="m">268</span> Oct <span class="m">21</span> 17:35 ceph.conf
</span></span><span class="line"><span class="cl">-rw-r--r--. <span class="m">1</span> root root  <span class="m">92</span> Oct <span class="m">20</span> 04:48 rbdmap
</span></span><span class="line"><span class="cl">-rw-------. <span class="m">1</span> root root   <span class="m">0</span> Oct <span class="m">21</span> 17:30 tmpcmU035
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-admin@ceph-node1 ~<span class="o">]</span>$ ceph -s
</span></span><span class="line"><span class="cl">  cluster:
</span></span><span class="line"><span class="cl">    id:     130b5ac0-938a-4fd2-ba6f-3d37e1a4e908
</span></span><span class="line"><span class="cl">    health: HEALTH_OK
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  services:
</span></span><span class="line"><span class="cl">    mon: <span class="m">1</span> daemons, quorum ceph-node1 <span class="o">(</span>age 20h<span class="o">)</span>
</span></span><span class="line"><span class="cl">    mgr: ceph-node1<span class="o">(</span>active, since 20h<span class="o">)</span>
</span></span><span class="line"><span class="cl">    osd: <span class="m">3</span> osds: <span class="m">3</span> up <span class="o">(</span>since 20h<span class="o">)</span>, <span class="m">3</span> in <span class="o">(</span>since 20h<span class="o">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  data:
</span></span><span class="line"><span class="cl">    pools:   <span class="m">0</span> pools, <span class="m">0</span> pgs
</span></span><span class="line"><span class="cl">    objects: <span class="m">0</span> objects, <span class="m">0</span> B
</span></span><span class="line"><span class="cl">    usage:   3.0 GiB used, <span class="m">27</span> GiB / <span class="m">30</span> GiB avail
</span></span><span class="line"><span class="cl">    pgs:
</span></span></code></pre></td></tr></table>
</div>
</div><p>三、配置Mgr-Dashboard模块
开启dashboard模块</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="o">[</span>ceph-admin@ceph-node1 ~<span class="o">]</span>$ sudo ceph mgr module <span class="nb">enable</span> dashboard
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">如果报错如下：
</span></span><span class="line"><span class="cl">Error ENOENT: all mgr daemons <span class="k">do</span> not support module <span class="s1">&#39;dashboard&#39;</span>, pass --force to force enablement
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">那是因为没有安装ceph-mgr-dashboard，在mgr节点上安装即可
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-admin@ceph-node1 ~<span class="o">]</span>$ sudo yum -y install ceph-mgr-dashboard
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">默认情况下，仪表板的所有HTTP连接均使用SSL/TLS进行保护。
</span></span><span class="line"><span class="cl">要快速启动并运行仪表板，可以使用以下命令生成并安装自签名证书
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-admin@ceph-node1 ~<span class="o">]</span>$ sudo ceph dashboard create-self-signed-cert
</span></span><span class="line"><span class="cl">Self-signed certificate created
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">创建具有管理员角色的用户:
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-admin@ceph-node1 ~<span class="o">]</span>$ sudo ceph dashboard set-login-credentials admin admin
</span></span><span class="line"><span class="cl">******************************************************************
</span></span><span class="line"><span class="cl">***          WARNING: this <span class="nb">command</span> is deprecated.              ***
</span></span><span class="line"><span class="cl">*** Please use the ac-user-* related commands to manage users. ***
</span></span><span class="line"><span class="cl">******************************************************************
</span></span><span class="line"><span class="cl">Username and password updated
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">之前用的“admin admin”，现在好像不能直接这样写了，需要将密码写在一个文件中读取，不然会报错
</span></span><span class="line"><span class="cl">“dashboard set-login-credentials &lt;username&gt; : Set the login credentials. Password <span class="nb">read</span> from -i &lt;file&gt;”
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">那就加上-i参数来创建也是一样
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-admin@ceph-node1 cluster<span class="o">]</span>$ <span class="nb">echo</span> admin &gt; userpass
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-admin@ceph-node1 cluster<span class="o">]</span>$ sudo ceph dashboard set-login-credentials admin -i userpass
</span></span><span class="line"><span class="cl">******************************************************************
</span></span><span class="line"><span class="cl">***          WARNING: this <span class="nb">command</span> is deprecated.              ***
</span></span><span class="line"><span class="cl">*** Please use the ac-user-* related commands to manage users. ***
</span></span><span class="line"><span class="cl">******************************************************************
</span></span><span class="line"><span class="cl">Username and password updated
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">查看ceph-mgr服务:
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph-admin@ceph-node1 ~<span class="o">]</span>$ sudo ceph mgr services
</span></span><span class="line"><span class="cl"><span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;dashboard&#34;</span>: <span class="s2">&#34;https://ceph-node1:8443/&#34;</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>浏览器访问测试:</p>
<p>
  <img src="ans3.png" alt="">

</p>
<h1 id="flink部分">Flink部分</h1>
<p>所有jar包可在对应module的target目录中获取。</p>
<h2 id="filerwatcher">FilerWatcher</h2>
<p>jar包，用于监听是否有新数据被爬取，将新数据输入Kafka的某个topic。
arg0:kafka topic名
arg1:监听路径</p>
<p><code>java -jar FileWatcher-1.0-SNAPSHOT.jar arg0 arg1</code></p>
<h2 id="filerwriter">FilerWriter</h2>
<p>jar包，用于监听Kafka的某个topic是否有新消息，并将新消息写入文件。
arg0:kafka topic名
arg1:写入文件全路径名</p>
<p><code>java -jar FileWriter-1.0-SNAPSHOT.jar arg0 arg1</code></p>
<h2 id="flink">Flink</h2>
<p>依赖：</p>
<p>zookeeper3.4.13  配置在2181端口<br>
kafka2.1.1 配置在9092端口<br></p>
<p>Flink集群配置了三个节点master，worker1，worker2，每个节点中有一个slot。在启动集群后，浏览器打开 master:8081 进入flink dashboard提交任务。</p>
<p>任务jar包：<br>
ciyun-1.0-SNAPSHOT.jar<br>
任务入口：</p>
<pre><code># 计算北京地区Python岗位的描述关键词词频
com.zmy.CiyunJob 
</code></pre>
<p>flink_python-1.0-SNAPSHOT.jar<br>
任务入口：</p>
<pre><code># 计算北京各地区Python岗位的数量
com.zmy.PythonAreaJob 
# 计算北京地区Python岗位的学历要求
com.zmy.PythonDegreeJob 
</code></pre>
<p>salary-1.0-SNAPSHOT.jar<br>
任务入口：</p>
<pre><code># 计算北京地区Python岗位按地区分组计算平均工资
com.zmy.SalaryJob 
</code></pre>
<h1 id="结果">结果</h1>
<p>
  <img src="ans2.png" alt="">

</p>
<p>
  <img src="ans1.png" alt="">

</p>


                

                
                <hr>
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/posts/tech/architecture/distributed/overview/" data-toggle="tooltip" data-placement="top" title="分布式系统概述">&larr;
                            Previous Post</a>
                    </li>
                    
                    
                </ul>
                

                



            </div>

            
            
            <div class="
                col-lg-2 col-lg-offset-0
                visible-lg-block
                sidebar-container
                catalog-container">
                <div class="side-catalog">
                    <hr class="hidden-sm hidden-xs">
                    <h5>
                        <a class="catalog-toggle" href="#">CATALOG</a>
                    </h5>
                    <ul class="catalog-body"></ul>
                </div>
            </div>
            

            
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                
                

                
                
            </div>
        </div>
    </div>
</article>




<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">                  
                    
                    
                    
                    
                    

		            
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
            
            
            
           
             </ul>
		<p class="copyright text-muted">
                    Copyright &copy; Waiting For You 2023
                    <br>
                    <a href="https://themes.gohugo.io/hugo-theme-cleanwhite">CleanWhite Hugo Theme</a> by <a href="https://zhaohuabing.com">Huabing</a> |
                    <iframe
                        style="margin-left: 2px; margin-bottom:-5px;"
                        frameborder="0" scrolling="0" width="100px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=zhaohuabing&repo=hugo-theme-cleanwhite&type=star&count=true" >
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>




<script>
    function loadAsync(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>






<script>
    
    if($('#tag_cloud').length !== 0){
        loadAsync("/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>


<script>
    loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>






<script type="text/javascript">
    function generateCatalog(selector) {

        
        
        
        
            _containerSelector = 'div.post-container'
        

        
        var P = $(_containerSelector), a, n, t, l, i, c;
        a = P.find('h1,h2,h3,h4,h5,h6');

        
        $(selector).html('')

        
        a.each(function () {
            n = $(this).prop('tagName').toLowerCase();
            i = "#" + $(this).prop('id');
            t = $(this).text();
            c = $('<a href="' + i + '" rel="nofollow">' + t + '</a>');
            l = $('<li class="' + n + '_nav"></li>').append(c);
            $(selector).append(l);
        });
        return true;
    }

    generateCatalog(".catalog-body");

    
    $(".catalog-toggle").click((function (e) {
        e.preventDefault();
        $('.side-catalog').toggleClass("fold")
    }))

    


    loadAsync("\/js\/jquery.nav.js", function () {
        $('.catalog-body').onePageNav({
            currentClass: "active",
            changeHash: !1,
            easing: "swing",
            filter: "",
            scrollSpeed: 700,
            scrollOffset: 0,
            scrollThreshold: .2,
            begin: null,
            end: null,
            scrollChange: null,
            padding: 80
        });
    });
</script>






</body>
</html>
