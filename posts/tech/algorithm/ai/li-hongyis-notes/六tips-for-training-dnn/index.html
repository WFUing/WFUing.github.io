<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    
    <meta property="og:site_name" content="Waiting For You">
    <meta property="og:type" content="article">

    
    <meta property="og:image" content="https://WFUing.github.io//">
    <meta property="twitter:image" content="https://WFUing.github.io//" />
    

    
    <meta name="title" content="六、Tips for Training DNN.md" />
    <meta property="og:title" content="六、Tips for Training DNN.md" />
    <meta property="twitter:title" content="六、Tips for Training DNN.md" />
    

    
    <meta name="description" content="这是一个纯粹的博客......">
    <meta property="og:description" content="这是一个纯粹的博客......" />
    <meta property="twitter:description" content="这是一个纯粹的博客......" />
    

    
    <meta property="twitter:card" content="summary" />
    
    

    <meta name="keyword"  content="">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>六、Tips for Training DNN.md | </title>

    <link rel="canonical" href="/posts/tech/algorithm/ai/li-hongyis-notes/%E5%85%ADtips-for-training-dnn/">

    
    
    
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    
    <link rel="stylesheet" href="/css/hugo-theme-cleanwhite.min.css">

    
    <link rel="stylesheet" href="/css/zanshang.css">

    
    <link rel="stylesheet" href="/css/font-awesome.all.min.css">

    
    

    
    <script src="/js/jquery.min.js"></script>

    
    <script src="/js/bootstrap.min.js"></script>

    
    <script src="/js/hux-blog.min.js"></script>

    
    <script src="/js/lazysizes.min.js"></script>

    
    

</head>






<nav class="navbar navbar-default navbar-custom navbar-fixed-top">

    <div class="container-fluid">
        
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Waiting For You</a>
        </div>

        
        
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">All Posts</a>
                    </li>
                    
                        
                    
                    
		    
		            <li>
                        <a href="/search"><i class="fa fa-search"></i></a>
		           </li>
                </ul>
            </div>
        </div>
        
    </div>
    
</nav>
<script>
    
    
    
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        
            $navbar.className = " ";
            
            setTimeout(function(){
                
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>




<style type="text/css">
    header.intro-header {
        background-image: url('/')
    }
</style>

<header class="intro-header" >

    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                    </div>
                    <h1>六、Tips for Training DNN.md</h1>
                    <h2 class="subheading"></h2>
                    <span class="meta">
                        
                            Posted by 
                            
                                    Waiting For You
                             
                            on 
                            Monday, October 23, 2023
                            
                            
                            
                            
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>




<article>
    <div class="container">
        <div class="row">

            
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                
                <h2 id="tips-for-training-dnn">Tips for Training DNN</h2>
<p>[toc]</p>
<h3 id="61-神经网络训练问题与解决方案">6.1 神经网络训练问题与解决方案</h3>
<h4 id="明确问题类型及其对应方法">明确问题类型及其对应方法</h4>
<p>在深度学习中，一般有两种问题：</p>
<ol>
<li>在训练集上性能不好</li>
<li>在测试集上性能不好。</li>
</ol>
<p>当一个方法被提出时，它往往是针对这两个问题其中之一的，比如dropout方法是用来处理在测试集上性能不好的情况。</p>
<h4 id="处理神经网络在训练集上性能不好的情况和方法">处理神经网络在训练集上性能不好的情况和方法</h4>
<ul>
<li>修改神经网络架构，比如换成更好的激活函数： sigmoid函数会导致梯度消失，可以换成ReLU、Leaky ReLU、Parametric ReLU、Maxout</li>
<li>调整学习率： 比如RMSProp、Momentum、Adam</li>
</ul>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220121181842946.png" alt="image-20220121181842946">

</p>
<h4 id="处理神经网络在测试集上性能不好的情况和方法">处理神经网络在测试集上性能不好的情况和方法</h4>
<ul>
<li>Early Stopping、Regularization，这两个是比较传统的方法，不只适用于深度学习</li>
<li>Dropout，比较有深度学习的特色</li>
</ul>
<h4 id="一些性能优化方法的简介">一些性能优化方法的简介</h4>
<p>下面3点都是在增加<strong>模型的随机性</strong>，鼓励模型做更多的exploration。</p>
<ul>
<li>Shuffling： 输入数据的顺序不要固定，mini-batch每次要重新生成</li>
<li>Dropout： 鼓励每个神经元都学到东西，也可以广义地理解为增加随机性</li>
<li>Gradient noise： 2015年提出，计算完梯度后，加上Gaussian noise。 随着迭代次数增加，noise应该逐渐变小。</li>
</ul>
<p>下面3点是关于<strong>学习率调整</strong>的技巧</p>
<ul>
<li>warm up： 开始时学习率较小，等稳定之后学习率变大</li>
<li>Curriculum learning： 2009年提出，先使用简单的数据训练模型（一方面此时模型比较弱，另一方面在clean data中更容易提取到核心特征），然后再用难的数据训练模型。 这样可以提高模型的鲁棒性。</li>
<li>Fine-tuning</li>
</ul>
<p>下面3点是关于<strong>数据预处理</strong>的技巧，避免模型学习到太极端的参数</p>
<ul>
<li>Normalization：   有Batch Normalization、Instance Normalization、Group Normalization、Layer Normalization、Positional Normalization</li>
<li>Regularization</li>
</ul>
<h3 id="62-神经网络精度低不一定是因为过拟合">6.2 神经网络精度低不一定是因为过拟合</h3>
<ul>
<li>相比于决策树等方法，神经网络更不容易过拟合：K近邻、决策树等方法在训练集上更容易得到100%等很高的正确率，神经网络一般不能，训练神经网络首先遇到的问题一般是在训练集上的精度不高。</li>
<li>不要总是把精度低归咎于过拟合：如果模型在训练集上精度高，对于K近邻、决策树等方法我们可以直接判断为过拟合，但对于神经网络来说我们还需要检查神经网络在测试集上的精度。<strong>如果神经网络在训练集上精度高但在测试集上精度低，这才说明神经网络过拟合了</strong>。 如果56层的神经网络和20层的神经网络相比，56层网络在测试集上的精度低于20层网络，这还不能判断为56层网络包含了过多参数导致过拟合。一般来讲，56层网络优于20层网络，但如果我们发现56层网络在训练集上的精度本来就低于20层网络，那原因可能有很多而非过拟合，比如56层网络没训练好导致一个不好的局部最优、虽然56层网络的参数多但结构有问题等等。</li>
<li>感兴趣可以看看ResNet论文**<a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a>**，这篇论文可能与该问题有关。</li>
</ul>
<h3 id="63-常用激活函数训练集">6.3 常用激活函数（训练集）</h3>
<h4 id="梯度消失vanishing-gradient-problem">梯度消失（Vanishing Gradient Problem）</h4>
<p>定义：1980年代常用的激活函数是sigmoid函数。以MNIST手写数字识别为例，在使用sigmoid函数时会发现随着神经网络层数增加，识别准确率逐渐下降，这个现象的原因并不是过拟合（原因见上文），而是梯度消失。</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220121002804394.png" alt="image-20220121002804394">

</p>
<p>如上图所示，当神经网络层数很多时，靠近输入层的参数的梯度会很小，靠近输出层的参数的梯度会很大。当每个参数的学习率相同时，靠近输入层的参数会更新得很慢，靠近输出层的几层参数会更新得很快。所以，当靠近输入层的参数几乎还是随机数时，靠近输出层的参数已经收敛了。</p>
<ul>
<li>原因： 按照反向传播的式子，这确实是会发生的。直观感觉上，sigmoid函数输入的范围是无穷大，但输出的范围是[0,1]，也就是说sigmoid函数减弱了输入变化导致输出变化的幅度。那为什么靠近输出层的参数的梯度更大呢？sigmoid函数是一层层叠起来的，不断地减弱靠近输入层的参数的变化导致输出变化的幅度，所以更靠后的参数的梯度越大。</li>
<li>解决方法： Hinton提出无监督逐层训练方法以解决这个问题，其基本思想是每次训练一层隐节点。 后来Hinton等人<strong>提出修改激活函数</strong>，比如换成ReLU。</li>
</ul>
<h4 id="relurectified-linear-unit">ReLU（Rectified Linear Unit）</h4>
<p>定义： 当输入小于等于0时，输出为0；当输入大于0时，输出等于输入，如下图所示。</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220121093655977.png" alt="image-20220121093655977">

</p>
<p>优点： 相比于sigmoid函数，它有以下优点</p>
<ol>
<li>运算更快</li>
<li>更符合生物学</li>
<li>等同于无穷多个bias不同的sigmoid函数叠加起来</li>
<li>可以解决梯度消失问题</li>
</ol>
<ul>
<li>如何解决梯度消失问题？ 当ReLU输出为0时该激活函数对神经网络不起作用，所以在神经网络中生效的激活函数都是输出等于输入，所以就不会出现sigmoid函数导致的减弱输入变化导致输出变化的幅度的情况。</li>
<li>ReLU会使整个神经网络变成线性的吗？ 可知有效的激活函数都是线性的，但整个神经网络还是非线性的。当输入改变很小、不改变每个激活函数的Operation  Region（操作区域，大概意思就是输入范围）时，整个神经网络是线性的；当输入改变很大、改变了Operation  Region时，整个神经网络就是非线性的。==目前我是凭直觉理解这一点，还未细究==</li>
<li>ReLU可以做微分吗？ 不用处理输入为0的情况，当输入小于0时，微分就是0，当输入大于0时微分就是1。</li>
</ul>
<h4 id="leaky-relu">Leaky ReLU</h4>
<p>当输入小于等于0时，输出为输入的0.01倍；当输入大于0时，输出等于输入。</p>
<h4 id="parametric-relu">Parametric ReLU</h4>
<p>当输入小于等于0时，输出为输入的 $\alpha$ 倍；当输入大于0时，输出等于输入。</p>
<p>其中 $\alpha$ 是通过梯度下降学习到的参数</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220121093800890.png" alt="image-20220121093800890">

</p>
<h4 id="maxout">Maxout</h4>
<p>通过学习得到一个激活函数，人为将每层输出的多个值分组，然后输出每组值中的最大值。（跟maxpooling一模一样），其实ReLU是Maxout的一个特例。</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220121094059002.png" alt="image-20220121094059002">

</p>
<p>Maxout比ReLU包含了更多的函数</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220121094851221.png" alt="image-20220121094851221">

</p>
<p>Maxout 可以得到任意的分段线性凸函数（piecewise linear convex），有几个分段取决于每组里有几个值</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220121095033371.png" alt="image-20220121095033371">

</p>
<h5 id="如何训练maxout">如何训练Maxout</h5>
<p>Maxout只是选择输出哪一个线性函数的值而已，因此Maxout激活函数还是线性的。 因为在多个值中只选择最大值进行输出，所以会形成一个比较瘦长/窄深的神经网络。 在多个值中只选择最大值进行输出，这并不会导致一些参数无法被训练：<strong>因为输入不同导致一组值中的最大值不同，所以各个参数都可能被训练到</strong>。 当输入不同时，形成的也是不同结构的神经网络。</p>
<h3 id="64-学习率调整方法训练集">6.4 学习率调整方法（训练集）</h3>
<h4 id="adagrad">Adagrad</h4>
<p>Adaptive Gradient Descent，自适应梯度下降，解决不同参数应该使用不同的更新速率的问题。Adagrad自适应地为各个参数分配不同学习率的算法。2011年提出，核心是每个参数（parameter）有不同的学习率。每次迭代中，学习率要除以它对应参数的之前梯度的均方根（RMS），即 $w_{t+1} = w_t-\frac{\eta}{\sqrt{\sum_{i=0}^{t}{(g_t)^2}}}g_t$ 。</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220121114218225.png" alt="image-20220121114218225">

</p>
<h4 id="rmsprop">RMSProp</h4>
<h5 id="背景">背景</h5>
<p>RMSProp是Adagrad的升级版，在2013年Hinton在Coursera提出。 在训练神经网络时，损失函数不一定是凸函数（局部最小值即为全局最小值），可能是各种各样的函数，有时需要较大的学习率，有时需要较小的学习率，而Adagrad并不能实现这种效果，因此产生了RMSProp。</p>
<h5 id="定义">定义</h5>
<p>$w_{t+1}=w_{t}-\frac{\eta}{\sigma_t}g_t$ （$\sigma_0=g_0 , \sigma_t=\sqrt{\alpha(\sigma_{t+1})^2+(1-\alpha)(g_t)^2}$），其中 $w$ 是某个参数， $\eta$ 是学习率，$g$ 是梯度， $\alpha$ 代表旧的梯度的重要性，值越小则旧的梯度越不重要。</p>
<h5 id="神经网络中很难找到最优的参数吗">神经网络中很难找到最优的参数吗？</h5>
<p>面临的问题有plateau、saddle point和local minima。</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220121115827871.png" alt="image-20220121115827871">

</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220121120226050.png" alt="image-20220121120226050">

</p>
<p>2007年有人（名字读音好像是young la ken）指出神经网络的error surface是很平滑的，没有很多局部最优。假设有1000个参数，一个参数处于局部最优的概率是 $p$ ，则整个神经网络处于局部最优的概率是 $p^{1000}$ ，这个值是很小的。</p>
<h4 id="momentum">Momentum</h4>
<h5 id="如何处理停滞期鞍点局部最小值等问题">如何处理停滞期、鞍点、局部最小值等问题？</h5>
<p>考虑现实世界中物体具有惯性、动量（Momentum）的特点，尽可能避免“小球”陷入error surface上的这几种位置。</p>
<h5 id="定义-1">定义</h5>
<p>1986年提出。如下图所示，不仅考虑当前的梯度，还考虑上一次的移动方向：$v_t = \lambda v_{t-1}-\eta g_t$ ， $v_0=0$，其中 $t$ 是迭代次数，$v$ 指移动方向（movement），类似物理里的速度，$g$ 是梯度（gradient），$\lambda$ 用来控制惯性的重要性，值越大代表惯性越重要，$\eta$ 是学习率</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220121131251914.png" alt="image-20220121131251914">

</p>
<h4 id="adam">Adam</h4>
<p>RMSProp+Momentum+Bias Correction，2015年提出</p>
<h4 id="adam-vs-sgdm">Adam VS SGDM</h4>
<p>目前常用的就是Adam和SGDM。</p>
<ul>
<li>Adam训练速度快，large generalization gap（在训练集和验证集上的性能差异大），但不稳定；</li>
<li>SGDM更稳定，little generalization gap，更加converge（收敛）。</li>
<li>SGDM适用于计算机视觉，Adam适用于NLP、Speech Synthesis、GAN、Reinforcement Learning。</li>
</ul>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220121131446379.png" alt="image-20220121131446379">

</p>
<p>####　SWATS</p>
<p>2017年提出，尝试把Adam和SGDM结合，其实就是前一段时间用Adam，后一段时间用SGDM，但在切换时需要解决一些问题。</p>
<h4 id="尝试改进adam">尝试改进Adam</h4>
<ul>
<li>AMSGrad
<ul>
<li>Adam的问题:  Non-informative gradients contribute more than informative gradients. 在Adam中，之前所有的梯度都会对第 $t$ 步的movement产生影响。然而较早阶段(比如第1、2步)的梯度信息是相对无效的，较晚阶段（比如 $t-1$ 、$t-2$ 步)的梯度信息是相对有效的。在Adam中，可能发生较早阶段梯度相对于较晚阶段梯度比重更大的问题。</li>
<li>提出AMSGrad:  2018年提出</li>
</ul>
</li>
<li>AdaBound 2019年提出，目的也是改进Adam。</li>
<li>Adam需要warm up吗？需要。 warm up：开始时学习率小，后面学习率大。 因为实验结果说明在刚开始的几次（大概是10次）迭代中，参数值的分布比较散乱（distort），因此梯度值就比较散乱，导致梯度下降不稳定。</li>
<li>RAdam    2020年提出</li>
<li>Lookahead    2019年提出，像一个wrapper一样套在优化器外面，适用于Adam、SGDM等任何优化器。 迭代几次后会回头检查一下。</li>
<li>Nadam    2016年提出，把NAG的概念应用到Adam上。</li>
<li>AdamW    2017年提出，这个优化器还是有重要应用的（训练出了某个BERT模型）。</li>
</ul>
<h4 id="尝试改进sgdm">尝试改进SGDM</h4>
<ul>
<li>LR range test    2017年提出</li>
<li>Cyclical LR    2017年提出</li>
<li>SGDR    2017年提出，模拟Cosine但并不是Cosine</li>
<li>One-cycle LR     2017年提出，warm-up+annealing+fine-tuning</li>
<li>SGDW    2017年提出</li>
</ul>
<h4 id="改进momentum">改进Momentum</h4>
<h5 id="背景-1">背景</h5>
<p>如果梯度指出要停下来，但动量说要继续走，这样可能导致坏的结果。</p>
<ul>
<li>NAG（Nesterov accelerated gradient） 1983年提出，会预测下一步。</li>
</ul>
<h3 id="65-处理测试集性能不好的方法">6.5 处理测试集性能不好的方法</h3>
<h4 id="651-early-stopping">6.5.1 Early Stopping</h4>
<p>如果学习率调整得较好，随着迭代次数增加，神经网络在训练集上的loss会越来越小，但因为验证集（Validation set）和训练集不完全一样，所以神经网络在验证集上的loss可能不降反升，所以我们应该在神经网络在验证集上loss最小时停止训练。</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220121175626937.png" alt="image-20220121175626937">

</p>
<h4 id="652-regularization">6.5.2 Regularization</h4>
<p>正则化，L2 regularization如下图所示（$||\theta||^2$ ，L2范式的平方），一般不考虑bias项。</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220121180116795.png" alt="image-20220121180116795">

</p>
<p>L2 regularization 的偏微分，每次都会使 $w_t$ 接近0，因为 $1-\eta\lambda$ 接近1但是小于1，比如0.99，每次成0.99，都会向0接近。</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220121181149440.png" alt="image-20220121181149440">

</p>
<p>当然，regularization 也可以有其他形式，比如是 $w_i$ 绝对值的集合，成为L1 regularization，具体如下图。但是，这个方法每次使得梯度减少的速度都一样，都是 1 或 -1 乘上 $\eta\lambda$ 。</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220121181641152.png" alt="image-20220121181641152">

</p>
<h4 id="653-dropout">6.5.3 Dropout</h4>
<p>dropout是指在<a href="https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;spm=1001.2101.3001.7020">深度学习</a>网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。注意是暂时，对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220121182241382.png" alt="image-20220121182241382">

</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220121182318836.png" alt="image-20220121182318836">

</p>
<p>dropout 在test集合上不使用，只用在Validation set上，并且在test集合上的时候，需要把weight 乘上 $(1-p)%$ 。</p>
<p>直观的解释：一个团队中，每个人都希望自己的同伴会做这个工作，最后什么都没有做；但是如果你知道你的同伴会dropdout，你就会做的更好，去carry他；不过当testing的时候，没有人会去dropout，所以最后的结果会更好。</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220121182629058.png" alt="image-20220121182629058">

</p>
<p>为什么要乘 $(1-p)%$ ？直观的解释如下图。</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220121183338383.png" alt="image-20220121183338383">

</p>
<p>dropout是一个整体</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220121190304915.png" alt="image-20220121190304915">

</p>
<p>做dropout 的时候是训练了一堆 neural structual</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220121190446601.png" alt="image-20220121190446601">

</p>
<p>那这些值的 average 与一开始算的 $\hat y$ 是否相等呢？</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220121190759249.png" alt="image-20220121190759249">

</p>
<p>其实是一样的。</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220121191028004.png" alt="image-20220121191028004">

</p>


                

                
                <hr>
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/posts/tech/algorithm/ai/li-hongyis-notes/%E4%B8%83convolutional-neural-network/" data-toggle="tooltip" data-placement="top" title="七、Convolutional Neural Network.md">&larr;
                            Previous Post</a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/posts/tech/algorithm/ai/li-hongyis-notes/%E4%BA%8C%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/" data-toggle="tooltip" data-placement="top" title="二、回归模型.md">Next
                            Post &rarr;</a>
                    </li>
                    
                </ul>
                

                



            </div>

            
            
            <div class="
                col-lg-2 col-lg-offset-0
                visible-lg-block
                sidebar-container
                catalog-container">
                <div class="side-catalog">
                    <hr class="hidden-sm hidden-xs">
                    <h5>
                        <a class="catalog-toggle" href="#">CATALOG</a>
                    </h5>
                    <ul class="catalog-body"></ul>
                </div>
            </div>
            

            
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                
                

                
                
            </div>
        </div>
    </div>
</article>




<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">                  
                    
                    
                    
                    
                    

		            
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
            
            
            
           
             </ul>
		<p class="copyright text-muted">
                    Copyright &copy; Waiting For You 2023
                    <br>
                    <a href="https://themes.gohugo.io/hugo-theme-cleanwhite">CleanWhite Hugo Theme</a> by <a href="https://zhaohuabing.com">Huabing</a> |
                    <iframe
                        style="margin-left: 2px; margin-bottom:-5px;"
                        frameborder="0" scrolling="0" width="100px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=zhaohuabing&repo=hugo-theme-cleanwhite&type=star&count=true" >
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>




<script>
    function loadAsync(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>






<script>
    
    if($('#tag_cloud').length !== 0){
        loadAsync("/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>


<script>
    loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>






<script type="text/javascript">
    function generateCatalog(selector) {

        
        
        
        
            _containerSelector = 'div.post-container'
        

        
        var P = $(_containerSelector), a, n, t, l, i, c;
        a = P.find('h1,h2,h3,h4,h5,h6');

        
        $(selector).html('')

        
        a.each(function () {
            n = $(this).prop('tagName').toLowerCase();
            i = "#" + $(this).prop('id');
            t = $(this).text();
            c = $('<a href="' + i + '" rel="nofollow">' + t + '</a>');
            l = $('<li class="' + n + '_nav"></li>').append(c);
            $(selector).append(l);
        });
        return true;
    }

    generateCatalog(".catalog-body");

    
    $(".catalog-toggle").click((function (e) {
        e.preventDefault();
        $('.side-catalog').toggleClass("fold")
    }))

    


    loadAsync("\/js\/jquery.nav.js", function () {
        $('.catalog-body').onePageNav({
            currentClass: "active",
            changeHash: !1,
            easing: "swing",
            filter: "",
            scrollSpeed: 700,
            scrollOffset: 0,
            scrollThreshold: .2,
            begin: null,
            end: null,
            scrollChange: null,
            padding: 80
        });
    });
</script>






</body>
</html>
