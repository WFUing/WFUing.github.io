<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    
    <meta property="og:site_name" content="Waiting For You">
    <meta property="og:type" content="article">

    
    <meta property="og:image" content="https://WFUing.github.io//">
    <meta property="twitter:image" content="https://WFUing.github.io//" />
    

    
    <meta name="title" content="四、分类模型.md" />
    <meta property="og:title" content="四、分类模型.md" />
    <meta property="twitter:title" content="四、分类模型.md" />
    

    
    <meta name="description" content="这是一个纯粹的博客......">
    <meta property="og:description" content="这是一个纯粹的博客......" />
    <meta property="twitter:description" content="这是一个纯粹的博客......" />
    

    
    <meta property="twitter:card" content="summary" />
    
    

    <meta name="keyword"  content="">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>四、分类模型.md | </title>

    <link rel="canonical" href="/posts/tech/algorithm/ai/li-hongyis-notes/%E5%9B%9B%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/">

    
    
    
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    
    <link rel="stylesheet" href="/css/hugo-theme-cleanwhite.min.css">

    
    <link rel="stylesheet" href="/css/zanshang.css">

    
    <link rel="stylesheet" href="/css/font-awesome.all.min.css">

    
    

    
    <script src="/js/jquery.min.js"></script>

    
    <script src="/js/bootstrap.min.js"></script>

    
    <script src="/js/hux-blog.min.js"></script>

    
    <script src="/js/lazysizes.min.js"></script>

    
    

</head>






<nav class="navbar navbar-default navbar-custom navbar-fixed-top">

    <div class="container-fluid">
        
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Waiting For You</a>
        </div>

        
        
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">All Posts</a>
                    </li>
                    
                        
                    
                    
		    
		            <li>
                        <a href="/search"><i class="fa fa-search"></i></a>
		           </li>
                </ul>
            </div>
        </div>
        
    </div>
    
</nav>
<script>
    
    
    
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        
            $navbar.className = " ";
            
            setTimeout(function(){
                
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>




<style type="text/css">
    header.intro-header {
        background-image: url('/')
    }
</style>

<header class="intro-header" >

    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                    </div>
                    <h1>四、分类模型.md</h1>
                    <h2 class="subheading"></h2>
                    <span class="meta">
                        
                            Posted by 
                            
                                    Waiting For You
                             
                            on 
                            Monday, October 23, 2023
                            
                            
                            
                            
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>




<article>
    <div class="container">
        <div class="row">

            
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                
                <h2 id="四分类模型">四、分类模型</h2>
<p>[toc]</p>
<h3 id="41-分类简介及其与回归的区别">4.1 分类简介及其与回归的区别</h3>
<h3 id="分类模型应用案例classification-cases">分类模型应用案例（Classification Cases）</h3>
<ul>
<li>信用评分（Credit Scoring）
<ul>
<li>输入：收入、储蓄、职业、年龄、信用历史等等</li>
<li>输出：是否贷款</li>
</ul>
</li>
<li>医疗诊断（Medical Diagnosis）
<ul>
<li>输入：现在症状、年龄、性别、病史</li>
<li>输出：哪种疾病</li>
</ul>
</li>
<li>手写文字识别（Handwritten Character Recognition）
<ul>
<li>输入：文字图片</li>
<li>输出：是哪一个汉字</li>
</ul>
</li>
<li>人脸识别（Face Recognition）
<ul>
<li>输入：面部图片</li>
<li>输出：是哪个人</li>
</ul>
</li>
</ul>
<h3 id="把分类当成回归去做">把分类当成回归去做</h3>
<p>不行</p>
<ul>
<li>假设有两个类别，其中类别1的标签为1，类别2的标签为-1，那0就是分界线，大于0就是类别1，小于0就是类别2。但是回归模型会惩罚哪些太正确的样本，如果结果远远大于1，它的分类应该是类别1还是类别2？这时为了降低整体误差，需要调整已经找到的回归函数，就会导致结果的不准确。
<ul>
<li>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220110213105315.png" alt="image-20220110213105315">

</li>
</ul>
</li>
<li>假设有多个类别，类别1的标签是1，类别2的标签是2，类别3的标签是3。这样的话，标签间具有2和3相近、3大于2这种本来不存在的数字关系。</li>
</ul>
<h3 id="理想替代方案ideal-alternatives">理想替代方案（Ideal Alternatives）</h3>
<ul>
<li>模型：模型可以根据特征判断类型，输入是特征，输出是类别</li>
<li>损失函数：预测错误的次数，即$L(f)=\sum_n{\sigma(f(x_n) \neq \hat{y_n} }$ 。这个函数不可微。</li>
<li>如何找到最好的函数，比如感知机（Perceptron）、支持向量机（SVM）</li>
</ul>
<h3 id="42-分类模型指概率生成模型">4.2 分类模型指概率生成模型</h3>
<h4 id="贝叶斯公式">贝叶斯公式</h4>
<ul>
<li>$P(A \cap B) = P(A)P(B|A) =  P(B)P(A|B)$</li>
<li>$P(A|B) = \frac{P(A)P(B|A)}{P(B)}$</li>
</ul>
<h4 id="全概率公式">全概率公式</h4>
<p>$P(B)=\sum_{i=1}^{n}{P(A_i)P(B|A_i)}$</p>
<h4 id="概率生成模型probalitity-genetative-model">概率生成模型（Probalitity Genetative Model）</h4>
<h5 id="理论与定义">理论与定义</h5>
<p>假设有两个类别的$C_1和C_2$，要判断对象$x$属于哪个类别，这样把分类问题变成了概率计算问题。</p>
<ol>
<li>根据贝叶斯公式（Bayes&rsquo; theorem）和全概率公式（Total Probability Theorem）可以知道，$x$属于类别$C_1$的概率为$P(C_1|x)= \frac{P(x|C_1)P(C_1)}{P(x)}=\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}$ ，如果$P(C_1|x)&gt;0.5$ 则类别为$C_1$ ，否则类别为$C_2$。</li>
<li>概率生成模型的意思就是可以通过这个模型生成一个$$x$$。具体来讲就是，根据$P(x)=P(x|C_1)P(C_1)+P(x|C_2)P(C_2)$ 计算出$P(x)$，就可以知道 $x$ 的分布进而生成 $x$ 。如果想要计算出$P(x)$，就要根据训练集估计出$P(C_1)$、$P(x|C_1)$、$P(C_2)$、$P(x|C_2)$这四个值。更直观一点地讲，每个类别就是一个多元正态分布，其中多元是因为每个样本有多个维度的特征。</li>
<li>可以根据数据集中属于两个类别的对象的数量计算 $P(C_1)$ 和 $P(C_2)$ 这两个先验概率（Prior Probability）。如果有2个样本属于类别$C_1$ ，4个样本属于类别$C_2$ ，那$P(C_1)= \frac{1}{3}$、$P(C_2)= \frac{2}{3}$。</li>
<li>要计算后验概率（Posterior Probability）$P(x|C_1)$ 和 $P(x|C_2)$，可以假设训练集中的各类别样本的特征分别是从某个多元正太分布（多元对应特征的多维）中取样得到的，或者说是假设训练集中各类别样本的特征分别符合某多元正态分布。<strong>该正太分布的输入是一个样本的特征 $x$，输出为样本 $x$ 是从这个正太分布取样得到（或者说该样本属于某类别）的概率密度，然后通过积分就可以求得 $P(x|C_1)$ 和 $P(x|C_2)$ 。</strong></li>
<li>正太分布公式为 $f_{\mu,\sum{(x)}}=\frac{1}{(2\pi)^\frac{D}{2}} \frac{1}{|\sum|^\frac{1}{2}} {e^{-\frac{1}{2}(x-\mu)^T\sum^{-1}{x-\mu}}}$ 。正太分布有2个参数，即均值 $\mu$ （代表正太分布的中心位置）和协方差矩阵（Covariance Matrix）$\sum$ （代表正态分布的离散程度），计算出均值 $\mu$ 和协方差 $\sum$ 即可得到该正态分布。公式中的 $D$ 为多维特征的维度。</li>
<li>实际上从任何一个正态分布中取样都有可能得到训练集中的特征，只是概率不同而已。通过极大值似然估计（Maximum Likelihood Estimate，MLE），我们可以<strong>找到取样得到训练集特征的概率最大的那个正态分布</strong>，假设其均值和协方差矩阵为  $ \mu^* $  和  $ \sum^* $  。
<ol>
<li>根据某正态分布的均值 $\mu$ 和协方差 $\sum$ ，可以计算出从该正态分布取样得到训练集的概率。 $ L(\mu,\sum) = f_{\mu,\sum}{x_1} f_{\mu,\sum}{x_2}f_{\mu,\sum}{x_3}&hellip;f_{\mu,\sum}{x_N} $  ，这就是似然函数（Likelihood Function），其中$N$ 是训练集中某个类别样本的数量。</li>
<li>$\mu^<em>,\sum^</em>=\arg\max_{\mu,\sum}{L(\mu,\sum)}$，当然可以求导。直觉：$\mu^<em>=\frac{1}{N}\sum_{i=1}^{N}{x_i}$，$\sum^</em> = \frac{1}{N}\sum_{i=1}^{N}(x_i-\mu^*)^2T$</li>
</ol>
</li>
</ol>
<h4 id="协方差矩阵共享">协方差矩阵共享</h4>
<p>每个类别的特征符合一个多元正态分布，每个多元正态分布也有不同的均值和协方差矩阵。让每个类别对应的多元正态分布共享一个协方差矩阵（各个协方差矩阵的加权平均和），公式为 $\sum = \frac{N_1}{N_1+N_2}\sum_1+\frac{N_2}{N_1+N_2}\sum_2$，可以减少模型参数，缓解过拟合</p>
<h4 id="极大似然估计">极大似然估计</h4>
<p>极大似然估计指已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，然后通过若干次试验，观察其结果，利用结果推出参数的大概值。一般说来，在一次试验中如果事件A发生了，则认为此时的参数值会使得 $P(A|\theta)$ 最大，极大似然估计法就是要这样估计出的参数值，使所选取的样本在被选的总体中出现的可能性为最大。</p>
<p>求极大似然函数估计值的一般步骤：</p>
<ol>
<li>写出似然函数</li>
<li>对似然函数取对数，并整理</li>
<li>求导数</li>
<li>解似然函数</li>
</ol>
<p>当共享协方差矩阵时，此时似然函数是$L(\mu_1,\mu_2,\sum)=f_{\mu_1,\sum}(x_1)f_{\mu_1,\sum}(x_2)&hellip;f_{\mu_1,\sum}{(x_{N1}) \times f_{\mu_2,\sum}(x_{N1+1})f_{\mu_2,\sum}(x_{N1+2})&hellip;f_{\mu_2,\sum}(x_{N1+N2})}$ ，其中 $N_1$ 为训练集中类别 $C_1$ 的样本数、$N_2$ 为训练集中类别 $C_2$ 的样本数。当只有两个类别、两个特征时，如果共享协方差矩阵，那最终得到的两个类别的分界线是直线（横纵轴是两个特征），这一点可以在下文解释。</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/v2-97436b02d3c5c7702959cc388e71f217_r.jpg" alt="preview">

</p>
<ul>
<li>除了正态分布，还可以用其它的概率模型。 比如对于二值特征，可以使用伯努利分布（Bernouli Distribution）。</li>
<li>朴素贝叶斯分类：如果假设样本各个维度的数据是互相独立的，那这就是朴素贝叶斯分类器（Naive Bayes Classfier）。</li>
</ul>
<h4 id="sigmoid-函数">Sigmoid 函数</h4>
<p>由上我们可知，$P(C_1|x)= \frac{P(x|C_1)P(C_1)}{P(x)}=\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}=\frac{1} {1+\frac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1)}}$ ，令 $z=\ln{\frac{P(x|C_1)P(C_1)}{P(x|C_2)P(C_2)}}$ ，则 $P(C_1|x) =\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}=\frac{1} {1+\frac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1)}}=\frac{1}{1+e^{-z}} = \delta(z)$ ，这就是Sigmoid函数。</p>
<p>如果共享协方差矩阵，经过运算可以得到 $z=w_T+b$ 的形式，其中常量 $w_T = (\mu_1-\mu_2)^T\sum^{-1}$，常量 $b=-\frac{1}{2}(\mu_1)^T(\sum_1)^{-1}\mu_1+\frac{1}{2}(\mu_2)^T(\sum_2)^{-1}\mu_2+\ln{\frac{N_1}{N_2}}$ ，即形如 $P(C_1|x) = \delta(w\cdot x+b)$ 。</p>
<h3 id="43-分类模型之逻辑回归">4.3 分类模型之逻辑回归</h3>
<h4 id="逻辑回归">逻辑回归</h4>
<p>假设训练集如下图所示，有2个类别 $C_1$ 和 $C_2$ ，下图表格中的每列为一个样本。</p>
<table>
<thead>
<tr>
<th>$x_1$</th>
<th>$x_2$</th>
<th>$x_3$</th>
<th>&hellip;</th>
<th>$x_N$</th>
</tr>
</thead>
<tbody>
<tr>
<td>$C_1$</td>
<td>$C_1$</td>
<td>$C_2$</td>
<td>&hellip;</td>
<td>$C_1$</td>
</tr>
<tr>
<td>$\hat{y_1} =1$</td>
<td>$\hat{y_2} =1$</td>
<td>$\hat{y_3} = 0$</td>
<td>&hellip;</td>
<td>$\hat{y_n} =1$</td>
</tr>
</tbody>
</table>
<p>例如，第一列表示样本 $x_1$ 的类别为 $C_1$ ，所以它标签是 $\hat{y_1}$ 是1。</p>
<h4 id="模型定义">模型定义</h4>
<p>在分类（Classification）一节中，我们要找到一个模型 $P_{w,b}(C_1|x)$ ，如果 $P_{w,b}(C_1|x)\geq0.5$ ，则 $x$ 属于类别 $C_1$ ，否则属于类别 $C_2$ 。可知 $P_{w,b}(C_1|x) = \sigma(z)$ ，其中 $\sigma(z)=\frac{1}{1+e^{-z}}$ （Sigmoid Fuction），$z=w \cdot x+b=\sum^{N}<em>{i=1}{w_ix_i+b} $ 。最终我们找到了模型 $f</em>{w,b(x)}=\sigma(\sum^N_{i=1}{w_ix_i+b})$。 最终我们找到了模型  $f_{w,b(x)=\sigma(\sum^N_{i=1}{w_ix_i+b})}$ ，这其实就是逻辑回归（Logistic Regression）。</p>
<h3 id="损失函数">损失函数</h3>
<p>从模型 $ f_{w,b}(x)=P_{w,b}(C_1|x) $ 中取样得到训练集的概率为： $ L(w,b)=f_{w,b}(x_1)f_{w,b}(x_2)(1-f_{w,b}(x_3))&hellip;f_{w,b}(x_N) $ （似然函数）。</p>
<p>我们要求出  $w^<em>,b^</em>=\arg\max_{w,b}L(w,b)$，等同于  $w^<em>,b^</em>=\arg\min_{w,b}-\ln{L(w,b)}$ （对数似然方程，Log-likelihood Equation）。</p>
<p>而 $ -\ln{L(w,b)=-\ln{f_{w,b}{x_1}} -\ln{f_{w,b}{x_2}}} -\ln{(1-f_{w,b}{x_3})}&hellip;$ ，其中 $ \ln{f_{w,b}(x_N)=\hat{y^N}\ln{f_{w,b}{(x^N)}} + (1-\hat{y^N})\ln{(1-f_{w,b}{(x^N)})}} $ ，所以 $ -\ln{L(w,b)=\sum^N_{n=1}{-[\hat y^N\ln{f_{w,b}(x^N)}+(1-\hat y^n)\ln(1-f_{w,b}(x^N))]}} $ ，式中N用来选择某个样本。</p>
<p>假设有两个伯努利分布 $p$ 和 $q$ ，在 $p$ 中有 $p(x=1)=\hat {y^N}$ ，$p(x=0)=1-\hat{y^N}$ ，在  $q$ 中有  $q(x=1)=f(x_N)$ ，$q(x=0)=1-f(x_N)$ ，则 $p$ 和 $q$ 的交叉熵（Cross Entropy，代表两个分布有多接近，两个分布一摸一样时交叉熵为0），为 $H(p,q)=-\sum_x{p(x)\ln(q(x))}$ 。所以损失函数 $L(f)=\sum^N_{n=1}{C(f(x_n),\hat{y^N})}$ ，其中 $C(f(x_N),\hat{y^N})=-[\hat y^N\ln{f_{w,b}(x^N)}+(1-\hat y^n)\ln(1-f_{w,b}(x^N))]$ ，即损失函数为所有样本的 $f(x_N)$ 与 $\hat{y_N}$ 的交叉熵之和，式中 $N$ 用来选择某个样本。</p>
<h3 id="梯度">梯度</h3>
<p>$ \frac{-\ln{L(w,b)}}{\sigma_{w_i}} = \sum^N_{n=1}{-(\hat{y^n}-f_{w,b}{(x^n)})x_i^n} $ ，其中 $i$ 用来选择数据的某个维度， $n$ 用来选择某个样本， $ N $ 为数据集中样本个数。该式表明，预测值与label相差越大时，参数更新的步幅越大，这符合常理。</p>
<h3 id="逻辑回归-vs-线性回归">逻辑回归 VS 线性回归</h3>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220114095456941.png" alt="image-20220114095456941">

</p>
<h4 id="模型">模型</h4>
<p>逻辑函数模型比线性回归模型多了一个sigmoid函数。逻辑函数输出是[0,1]，而线性回归的输出是任意值。</p>
<h4 id="损失函数-1">损失函数</h4>
<p>逻辑回归模型使用的数据集中label的值必须是0或1，而线性回归模型训练集中label的值是真实值。</p>
<p>图中的 $\frac{1}{2}$ 是为了方便求导 。这里有一个问题，为什么逻辑回归模型中不适用Square Error呢？这个问题的答案见下文</p>
<h4 id="梯度-1">梯度</h4>
<p>逻辑回归模型和线性回归模型的梯度公式一样</p>
<h4 id="为什么逻辑回归模型中不使用square-error">为什么逻辑回归模型中不使用Square Error</h4>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220114100921433.png" alt="image-20220114100921433">

</p>
<p>由上图可知，当label的值为1时，不管预测值是0还是1，梯度都为0，当label值为0时也是这样。</p>
<p>如下图所示，如果在逻辑回归中使用Square Error，当梯度接近0时，我们无法判断目前与最优解的距离，也就无法调节学习率；并且在大多数时候梯度都是接近0的，收敛速度会很慢。</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220114100950925.png" alt="image-20220114100950925">

</p>
<h3 id="判别模型-vs-生成模型">判别模型 VS 生成模型</h3>
<h4 id="形式对比">形式对比</h4>
<p>逻辑回归是一个判别模型（Discriminative Model），用正态分布描述后验概率（Posterior Probability）则是生成模型（Generative Model）。如果生成模型中公用协方差矩阵，那两个模型/函数集其实是一样的，都是 $ P(C_1|x)=\sigma(w \cdot x+b) $ 。因为做了不同的假，即使是使用同一个数据集、同一个模型，找到的函数是不一样的。</p>
<h4 id="优劣对比">优劣对比</h4>
<ul>
<li>如果现在数据很少，当假设了概率分布以后，就可以需要更少的数据用于训练，受数据影响较小；而判别模型就只根据数据来学习，易受数据影响，需要更多数据</li>
<li>当假设了概率分布后，生成模型受数据影响小，对噪声的鲁棒性更强</li>
<li>对于生成模型来讲，先验的和基于类别的概率（Prors and class-dependent probabilities），即 $ P(C_1) $ 和 $ P(C_2) $ ，可以从不同的来源估计得到。以语音识别为例，如果用生成模型，可能并不需要声音的数据，网上的文本也可以用来估计某段文本出现的概率</li>
</ul>
<h3 id="多分类multi-class-classification">多分类（Multi-class Classification）</h3>
<p>以3个类别 $C_1、C2和 C3$ 为例，分别对应参数$w_1、b_1、W_2、b_2、W_3、b_3$，即$z_1=w_1 \cdot x+b_1、z_2=w_2 \cdot x+b_2、z_3=w_3 \cdot x+b_3$</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220119141011956.png" alt="image-20220119141011956">

</p>
<h4 id="softmax">Softmax</h4>
<p>使用softmax（$y_i=\frac{e_{z_i}}{\sum^c_{j=1}{e_{z_j}}}$）</p>
<p>softmax公式中为什么要用$e$？这是由原因的/可解释的，可以看下PRML，也可以搜一下最大熵</p>
<p>最大熵（Maximum Entropy）其实也是一种分类器，和逻辑回归一样，只是从信息论的角度来看</p>
<img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220119141353415.png" alt="image-20220119141353415"  />
<h4 id="损失函数-2">损失函数</h4>
<p>计算预测值$y$和$\hat y$都是一个向量，即$-\sum^3_{i=1}{{\hat y}_i\ln{y_i}}$</p>
<p>这时需要使用one-hot编码：如果$x\in C_1$，则$y=\begin{bmatrix}
1\
0\
0\
\end{bmatrix}$；如果$x\in C_1$，则$y=\begin{bmatrix}
0\
1\
0\
\end{bmatrix}$；如果$x\in C_1$，则$y=\begin{bmatrix}
0\
0\
1\
\end{bmatrix}$。</p>
<h4 id="梯度-2">梯度</h4>
<p>和逻辑回归的思路一样。</p>
<h3 id="逻辑回归的局限性">逻辑回归的局限性</h3>
<p>如下图所示，假如有2个类别，数据集中有4个样本，每个样本有2维特征，将这4个样本画在图上。</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220119143941226.png" alt="image-20220119143941226">

</p>
<p>如下图所示，假如用逻辑回归做分类，即$y=\sigma(z)=\sigma(w_1\cdot x_1+w_2\cdot x_2+b)$，我们找不到一个可以把“蓝色”样本和“红色”样本间隔开的函数。</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220119144059137.png" alt="image-20220119144059137">

</p>
<p>假如一定要用逻辑回归，那我们可以怎么办呢？我们可以尝试特征变换（Feature Transformation）。</p>
<h4 id="特征变换feature-transformation">特征变换（Feature Transformation）</h4>
<p>在上面的例子中，我们并不能找到一个能将蓝色样本和红色样本间隔开的函数。如下图所示，我们可以把原始的数据/特征转换到另外一个空间，在这个新的特征空间中，找到一个函数将“蓝色”样本和“红色”样本间隔开。比如把原始的两维特征变换为$\begin{bmatrix}
0\
0\
\end{bmatrix} 和 \begin{bmatrix}
1\
1\
\end{bmatrix}$ 的距离，在这个新的特征空间，“蓝色”样本和“红色”样本是可分的。</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220119144323062.png" alt="image-20220119144323062">

</p>
<p>但有一个问题是，我们并不一定知道怎么进行特征变换。或者说我们想让机器自己学会特征变换，这可以通过级联逻辑回归模型实现，即把多个逻辑回归模型连接起来，如下图所示。下图中有3个逻辑回归模型，根据颜色称它们为小蓝、小绿和小红。小蓝和小绿的作用是分别将原始的2维特征变换为新的特征$x_1&rsquo;和x_2&rsquo;$，小红的作用是在新的特征空间$\begin{bmatrix}
x_1&rsquo;\
x_2&rsquo;\
\end{bmatrix}$上将样本分类。</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220119144455564.png" alt="image-20220119144455564">

</p>
<p>如下图所示，举一个例子。小蓝的功能是（下图左上角），离$(1,0)$越远、离$(0,1)$越近，则$x_1&rsquo;$越大；小蓝的功能是（下图左上角），离$(1,0)$越远、离$(0,1)$越近，则$x_1&rsquo;$越小。小蓝和小绿将特征映射到新的特征空间$\begin{bmatrix}
x_1&rsquo;\
x_2&rsquo;\
\end{bmatrix}$中，结果见下图右下角，然后小红就能找到一个函数将“蓝色”样本和“红色”样本间隔开。</p>
<p>
  <img src="https://gitee.com/jyj3621/image/raw/master/image/image-20220119144927053.png" alt="image-20220119144927053">

</p>
<h3 id="神经网络neural-network">神经网络（Neural Network）</h3>
<p>假如把上例中的一个逻辑回归叫做神经元（Neuron），那我们就形成了一个神经网络。</p>
<h3 id="roc">ROC</h3>
<p>在信号检测理论中，接收者操作特征曲线(receiver operating characteristic curve，或者叫ROC曲线)是<a href="https://www.zhihu.com/search?q=%E5%9D%90%E6%A0%87%E5%9B%BE%E5%BC%8F&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A603394642%7D">坐标图式</a>的分析工具，用于 (1) 选择最佳的<a href="https://www.zhihu.com/search?q=%E4%BF%A1%E5%8F%B7%E4%BE%A6%E6%B5%8B%E6%A8%A1%E5%9E%8B&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A603394642%7D">信号侦测模型</a>、舍弃次佳的模型。 (2) 在同一模型中设定最佳阈值。在做决策时，ROC分析能不受成本／效益的影响，给出客观中立的建议。</p>
<p><a href="https://www.zhihu.com/search?q=ROC%E6%9B%B2%E7%BA%BF&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A603394642%7D">ROC曲线</a>首先是由二战中的电子工程师和雷达工程师发明的，用来侦测战场上的敌军载具(飞机、船舰)，也就是信号检测理论。之后很快就被引入了心理学来进行信号的知觉检测。数十年来，ROC分析被用于医学、无线电、生物学、<a href="https://www.zhihu.com/search?q=%E7%8A%AF%E7%BD%AA%E5%BF%83%E7%90%86%E5%AD%A6&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A603394642%7D">犯罪心理学</a>领域中，而且最近在机器学习(machine  learning)和数据挖掘(data mining)领域也得到了很好的发展。</p>
<p>术语</p>
<ul>
<li>阳性(P, positive)</li>
<li>阴性(N, Negative)</li>
<li>真阳性 (TP, true positive) 正确的肯定。又称：命中 (hit)</li>
<li>真阴性 (TN, true negative) 正确的否定。又称：正确拒绝 (correct rejection)</li>
<li>伪阳性 (FP, false positive) 错误的肯定，又称：假警报 (false alarm)，第一型错误伪阴性 (FN, false negative) 错误的否定，又称：未命中 (miss)，第二型错误</li>
<li>真阳性率 (TPR, true positive rate) 又称：命中率 (hit rate)、<a href="https://www.zhihu.com/search?q=%E6%95%8F%E6%84%9F%E5%BA%A6&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A603394642%7D">敏感度</a>(sensitivity)
TPR = TP / P = TP / (TP+FN)</li>
<li>伪阳性率(FPR, false positive rate) 又称：错误命中率，假警报率 (false alarm rate) FPR = FP / N = FP / (FP + TN)</li>
<li>准确度 (ACC, accuracy) ACC = (TP + TN) / (P + N) 即：(真阳性+真阴性) / 总样本数真阴性率 (TNR) 又称：特异度 (SPC, specificity) SPC = TN / N = TN / (FP + TN) = 1 - FPR</li>
<li><a href="https://www.zhihu.com/search?q=%E9%98%B3%E6%80%A7%E9%A2%84%E6%B5%8B%E5%80%BC&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A603394642%7D">阳性预测值</a> (PPV) PPV = TP / (TP + FP)</li>
<li><a href="https://www.zhihu.com/search?q=%E9%98%B4%E6%80%A7%E9%A2%84%E6%B5%8B%E5%80%BC&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A603394642%7D">阴性预测值</a> (NPV) NPV = TN / (TN + FN)</li>
<li>假发现率 (FDR) FDR = FP / (FP + TP)</li>
</ul>


                

                
                <hr>
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/posts/tech/algorithm/ai/li-hongyis-notes/%E4%BA%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" data-toggle="tooltip" data-placement="top" title="五、深度学习.md">&larr;
                            Previous Post</a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/posts/tech/algorithm/ai/li-hongyis-notes/%E4%B8%89%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" data-toggle="tooltip" data-placement="top" title="三、梯度下降.md">Next
                            Post &rarr;</a>
                    </li>
                    
                </ul>
                

                



            </div>

            
            
            <div class="
                col-lg-2 col-lg-offset-0
                visible-lg-block
                sidebar-container
                catalog-container">
                <div class="side-catalog">
                    <hr class="hidden-sm hidden-xs">
                    <h5>
                        <a class="catalog-toggle" href="#">CATALOG</a>
                    </h5>
                    <ul class="catalog-body"></ul>
                </div>
            </div>
            

            
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                
                

                
                
            </div>
        </div>
    </div>
</article>




<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">                  
                    
                    
                    
                    
                    

		            
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
            
            
            
           
             </ul>
		<p class="copyright text-muted">
                    Copyright &copy; Waiting For You 2023
                    <br>
                    <a href="https://themes.gohugo.io/hugo-theme-cleanwhite">CleanWhite Hugo Theme</a> by <a href="https://zhaohuabing.com">Huabing</a> |
                    <iframe
                        style="margin-left: 2px; margin-bottom:-5px;"
                        frameborder="0" scrolling="0" width="100px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=zhaohuabing&repo=hugo-theme-cleanwhite&type=star&count=true" >
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>




<script>
    function loadAsync(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>






<script>
    
    if($('#tag_cloud').length !== 0){
        loadAsync("/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>


<script>
    loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>






<script type="text/javascript">
    function generateCatalog(selector) {

        
        
        
        
            _containerSelector = 'div.post-container'
        

        
        var P = $(_containerSelector), a, n, t, l, i, c;
        a = P.find('h1,h2,h3,h4,h5,h6');

        
        $(selector).html('')

        
        a.each(function () {
            n = $(this).prop('tagName').toLowerCase();
            i = "#" + $(this).prop('id');
            t = $(this).text();
            c = $('<a href="' + i + '" rel="nofollow">' + t + '</a>');
            l = $('<li class="' + n + '_nav"></li>').append(c);
            $(selector).append(l);
        });
        return true;
    }

    generateCatalog(".catalog-body");

    
    $(".catalog-toggle").click((function (e) {
        e.preventDefault();
        $('.side-catalog').toggleClass("fold")
    }))

    


    loadAsync("\/js\/jquery.nav.js", function () {
        $('.catalog-body').onePageNav({
            currentClass: "active",
            changeHash: !1,
            easing: "swing",
            filter: "",
            scrollSpeed: 700,
            scrollOffset: 0,
            scrollThreshold: .2,
            begin: null,
            end: null,
            scrollChange: null,
            padding: 80
        });
    });
</script>






</body>
</html>
