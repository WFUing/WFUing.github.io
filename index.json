[{"content":"","date":"12 June 2024","permalink":"/posts/reviews/%E7%A0%94%E7%A9%B6%E7%94%9F%E5%A4%8D%E4%B9%A0/","section":"博客","summary":"南京大学软件学院研究生期末复习","title":"南软研究生复习"},{"content":"","date":"12 June 2024","permalink":"/posts/","section":"博客","summary":"时间就像海绵里的水，只要你愿意挤，总还有的。\u0026ndash;鲁迅","title":"博客"},{"content":"","date":"12 June 2024","permalink":"/posts/reviews/","section":"博客","summary":"不打没有准备的仗！！在面对挑战时，充分的准备是成功的关键。只有通过认真的复习和准备，才能有效地解决问题并取得胜利。","title":"复习准备"},{"content":"总纲 # 课程的教学目标：掌握蛮力、分治、减治、边治和时空权衡等算法设计思想的特点。\n能够熟练使用蛮力法解决问题，作为优化的基线方法，例如找到数组中大小最接近的两个元素的差、找到凸包极点等问题 分治思想在具体实现的时候，可以使用递归的方法，也可以结合特殊的数据结构使用循环的方法，例如合并排序、快速排序等 能够采用分治思想解决的两个经典问题是最近对问题和凸包问题，请掌握具体的代码实现 时空权衡思想在算法设计中应用非常广泛，例如字符串匹配、B树，乃至动态规划，都和时间权衡有关，需要熟练掌握 贪心法的思想和特点，用贪心法解决经典问题，最小生成树、活动相容等。 动态规划的思想和特点，针对以下经典问题能写出子问题的递归式，描述算法思想和伪代码：最长公共子序列（最长回文子序列）和矩阵连乘问题 了解平滑函数性质，能用主定理求解递归方程，判断当前方程适用主定理哪一类型，进而解出递归式。 复杂度的表示式：$O(f)$、$\\Omega (f)$、$\\Theta (f)$的含义，基于三种表示的一系列性质 介绍NPC理论的含义和实际应用价值，能列举NPC问题，证明一些主要的NPC问题（如装箱问题，0-1背包）既是NP的也是NP-hard的 近似算法的定义，如何衡量近似算法的好坏，能掌握对于装箱问题的几种贪心算法（First Fit Algorithm - FF，Next Fit Algorithm - NF，First Fit Decreasing - FFD）。在线算法的定义，以上哪个算法是在线算法，复杂度如何。 上面10个点，每个点是一道题，每题10分，包括题型：\n给模板填空题（比如使用 Prim、Kruskal 构造最小生成树的过程中，给出前几步，让你补完过程 讨论题和问答题（比如大O(f)的含义 写伪代码的题 调分\n不管出什么题，你写点相关的定义都有保底分； 不会也要写，好调分； 答疑\n邮箱联系 可以约办公室 office hour 上图的‘例如’ 就是对应题型的考试范围；\n蛮力法：找数组中大小最接近的两个元素、凸包极点\u0026hellip; 就从这些里面选考 前半部分是李传艺老师授课、出题，后半部分是李言辉老师授课、出题。李言辉后面着重复习了自己的部分。\n课本： 算法设计与分析基础.第3版 往年卷： 可参考题型 1. 蛮力法 # 蛮力法（Brute Force）是一种通过遍历所有可能的解决方案来解决问题的算法。尽管其时间复杂度通常较高，但它简单直接，适用于规模较小的问题或作为其他优化算法的基线方法。以下是对蛮力法及其在两个具体问题中的应用介绍：\n1. 数组中大小最接近的两个元素的差\n问题描述： 给定一个包含 $n$ 个整数的数组，找到数组中差值最小的两个元素。\n蛮力法：\n初始化一个最小差值变量 minDiff，并设置为一个足够大的值。 使用两层嵌套循环遍历数组中的每一对元素。 对于每一对元素，计算它们的差的绝对值。 如果计算得到的差小于 minDiff，则更新 minDiff。 最后，返回 minDiff。 public class MinDiff { public static int findMinDiff(int[] arr) { int n = arr.length; int minDiff = Integer.MAX_VALUE; for (int i = 0; i \u0026lt; n; i++) { for (int j = i + 1; j \u0026lt; n; j++) { int diff = Math.abs(arr[i] - arr[j]); if (diff \u0026lt; minDiff) { minDiff = diff; } } } return minDiff; } public static void main(String[] args) { int[] arr = {3, 8, 15, 17}; System.out.println(\u0026#34;The minimum difference is: \u0026#34; + findMinDiff(arr)); } } 2. 找到凸包极点\n问题描述： 给定一个二维平面上的点集，找到这些点的凸包极点（形成凸包的最外层点）。\n蛮力法：\n初始化一个空列表 hullPoints 用于存储凸包极点。 使用两层嵌套循环，遍历每一对点。 对于每一对点，假设这两个点是凸包的一条边，检查其他所有点是否都在这条边的同一侧。 如果某条边的假设成立，则将构成这条边的两个点加入 hullPoints 去除重复点，最终得到凸包极点。 import java.util.ArrayList; import java.util.List; public class ConvexHull { public static class Point { int x, y; public Point(int x, int y) { this.x = x; this.y = y; } } // 计算向量 (o -\u0026gt; a) 和 (o -\u0026gt; b) 的叉积 public static int crossProduct(Point o, Point a, Point b) { return (a.x - o.x) * (b.y - o.y) - (a.y - o.y) * (b.x - o.x); } public static List\u0026lt;Point\u0026gt; convexHull(Point[] points) { int n = points.length; if (n \u0026lt; 3) return List.of(points); List\u0026lt;Point\u0026gt; hullPoints = new ArrayList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; n; i++) { for (int j = i + 1; j \u0026lt; n; j++) { boolean allLeft = true; boolean allRight = true; for (int k = 0; k \u0026lt; n; k++) { if (k == i || k == j) continue; int cp = crossProduct(points[i], points[j], points[k]); if (cp \u0026gt; 0) { allRight = false; } else if (cp \u0026lt; 0) { allLeft = false; } if (!(allLeft || allRight)) break; } if (allLeft || allRight) { if (!hullPoints.contains(points[i])) hullPoints.add(points[i]); if (!hullPoints.contains(points[j])) hullPoints.add(points[j]); } } } return hullPoints; } public static void main(String[] args) { Point[] points = { new Point(0, 0), new Point(1, 1), new Point(2, 2), new Point(3, 0), new Point(0, 3), new Point(3, 3) }; List\u0026lt;Point\u0026gt; hull = convexHull(points); System.out.println(\u0026#34;Points in the convex hull:\u0026#34;); for (Point p : hull) { System.out.println(\u0026#34;(\u0026#34; + p.x + \u0026#34;, \u0026#34; + p.y + \u0026#34;)\u0026#34;); } } } 叉积的定义和几何意义\n给定两个向量 $\\mathbf{u}$ 和 $\\mathbf{v}$，它们的叉积 $\\mathbf{u} \\times \\mathbf{v}$ 的几何意义为：\n$\\mathbf{u} \\times \\mathbf{v}$ 的符号决定了向量 $\\mathbf{v}$ 相对于 $\\mathbf{u}$ 的方向： 如果结果为正，则 $\\mathbf{v}$ 在 $\\mathbf{u}$ 的逆时针方向（左侧）。 如果结果为负，则 $\\mathbf{v}$ 在 $\\mathbf{u}$ 的顺时针方向（右侧）。 如果结果为零，则 $\\mathbf{u}$ 和 $\\mathbf{v}$ 共线（点 o、a、b 在同一条直线上）。 叉积公式\n对于二维平面上的两个向量 $\\mathbf{u} = (u_x, u_y)$ 和 $\\mathbf{v} = (v_x, v_y)$，它们的叉积公式为：\n$$ [ \\mathbf{u} \\times \\mathbf{v} = u_x \\cdot v_y - u_y \\cdot v_x ] $$\n在我们的问题中，向量 $\\mathbf{oa}$ 和 $\\mathbf{ob}$ 的坐标分别为：\n向量 $\\mathbf{oa}$： $(a.x - o.x, a.y - o.y)$ 向量 $\\mathbf{ob}$： $(b.x - o.x, b.y - o.y)$ 因此，向量 $\\mathbf{oa}$ 和 $\\mathbf{ob}$ 的叉积计算为：\n$$ [ (a.x - o.x) \\cdot (b.y - o.y) - (a.y - o.y) \\cdot (b.x - o.x) ] $$\n示例\n假设有点 o、a 和 b 的坐标分别为 (0, 0)、(1, 1) 和 (2, 0)：\n向量 $\\mathbf{oa}$：(1 - 0, 1 - 0) = (1, 1) 向量 $\\mathbf{ob}$：(2 - 0, 0 - 0) = (2, 0) 叉积的计算为：\n$$ [ (1 \\cdot 0) - (1 \\cdot 2) = 0 - 2 = -2 ] $$\n结果为负，说明点 b 在从点 o 指向点 a 的向量的右侧。\n2. 分治 # 分治法（Divide and Conquer）是一种算法设计范式，其核心思想是将一个复杂的问题分解成若干个相似但规模更小的子问题，递归地解决这些子问题，然后将它们的结果合并起来得到原问题的解。分治法广泛应用于许多经典算法中，例如合并排序（Merge Sort）、快速排序（Quick Sort）等。\n分治法的步骤\n分解（Divide）： 将问题划分为若干个规模更小的子问题。 解决（Conquer）： 递归地解决这些子问题。如果子问题规模足够小，可以直接求解。 合并（Combine）： 将子问题的结果合并成原问题的解。 示例 1：合并排序（Merge Sort）\n合并排序是一种基于分治法的排序算法。其主要步骤如下：\n分解： 将待排序数组分成两半。 解决： 递归地对每一半进行排序。 合并： 将排序好的两半合并成一个有序的数组。 public class MergeSort { public static void mergeSort(int[] array) { if (array.length \u0026lt; 2) { return; } int mid = array.length / 2; int[] left = new int[mid]; int[] right = new int[array.length - mid]; System.arraycopy(array, 0, left, 0, mid); System.arraycopy(array, mid, right, 0, array.length - mid); mergeSort(left); mergeSort(right); merge(array, left, right); } private static void merge(int[] array, int[] left, int[] right) { int i = 0, j = 0, k = 0; while (i \u0026lt; left.length \u0026amp;\u0026amp; j \u0026lt; right.length) { if (left[i] \u0026lt;= right[j]) { array[k++] = left[i++]; } else { array[k++] = right[j++]; } } while (i \u0026lt; left.length) { array[k++] = left[i++]; } while (j \u0026lt; right.length) { array[k++] = right[j++]; } } public static void main(String[] args) { int[] array = {38, 27, 43, 3, 9, 82, 10}; mergeSort(array); for (int i : array) { System.out.print(i + \u0026#34; \u0026#34;); } } } 示例 2：快速排序（Quick Sort）\n快速排序是一种高效的排序算法，其主要步骤如下：\n分解： 从数组中选择一个基准元素，将其他元素重新排列，使得比基准元素小的元素放在左边，比基准元素大的元素放在右边。 解决： 递归地对基准元素左边的子数组和右边的子数组进行排序。 合并： 由于每次分解已经部分排序，所以合并步骤是隐含的。 public class QuickSort { public static void quickSort(int[] array, int low, int high) { if (low \u0026lt; high) { int pi = partition(array, low, high); quickSort(array, low, pi - 1); quickSort(array, pi + 1, high); } } private static int partition(int[] array, int low, int high) { int pivot = array[high]; int i = low - 1; for (int j = low; j \u0026lt; high; j++) { if (array[j] \u0026lt; pivot) { i++; swap(array, i, j); } } swap(array, i + 1, high); return i + 1; } private static void swap(int[] array, int i, int j) { int temp = array[i]; array[i] = array[j]; array[j] = temp; } public static void main(String[] args) { int[] array = {10, 7, 8, 9, 1, 5}; int n = array.length; quickSort(array, 0, n - 1); for (int i : array) { System.out.print(i + \u0026#34; \u0026#34;); } } } 示例 3：最近对问题（Closest Pair Problem）\n最近对问题是指在给定的二维平面上找到距离最近的一对点。\n基本思想：\n分解： 将点集按 x 坐标排序，并划分为左右两部分。 解决： 递归地找到左右两部分中的最近对。 合并： 检查跨越分割线的点对，找到整体的最近对。 import java.util.Arrays; import java.util.Comparator; public class ClosestPair { static class Point { int x, y; Point(int x, int y) { this.x = x; this.y = y; } } public static double closestPair(Point[] points) { Point[] px = points.clone(); Arrays.sort(px, Comparator.comparingInt(p -\u0026gt; p.x)); Point[] py = points.clone(); Arrays.sort(py, Comparator.comparingInt(p -\u0026gt; p.y)); return closestPairRec(px, py); } private static double closestPairRec(Point[] px, Point[] py) { int n = px.length; if (n \u0026lt;= 3) { return bruteForce(px); } int mid = n / 2; Point midPoint = px[mid]; Point[] pxLeft = Arrays.copyOfRange(px, 0, mid); Point[] pxRight = Arrays.copyOfRange(px, mid, n); Point[] pyLeft = Arrays.stream(py).filter(p -\u0026gt; p.x \u0026lt;= midPoint.x).toArray(Point[]::new); Point[] pyRight = Arrays.stream(py).filter(p -\u0026gt; p.x \u0026gt; midPoint.x).toArray(Point[]::new); double dLeft = closestPairRec(pxLeft, pyLeft); double dRight = closestPairRec(pxRight, pyRight); double d = Math.min(dLeft, dRight); Point[] strip = Arrays.stream(py).filter(p -\u0026gt; Math.abs(p.x - midPoint.x) \u0026lt; d).toArray(Point[]::new); return Math.min(d, stripClosest(strip, d)); } private static double bruteForce(Point[] points) { double minDist = Double.MAX_VALUE; for (int i = 0; i \u0026lt; points.length; i++) { for (int j = i + 1; j \u0026lt; points.length; j++) { double dist = distance(points[i], points[j]); if (dist \u0026lt; minDist) { minDist = dist; } } } return minDist; } private static double stripClosest(Point[] strip, double d) { double minDist = d; for (int i = 0; i \u0026lt; strip.length; i++) { for (int j = i + 1; j \u0026lt; strip.length \u0026amp;\u0026amp; (strip[j].y - strip[i].y) \u0026lt; minDist; j++) { double dist = distance(strip[i], strip[j]); if (dist \u0026lt; minDist) { minDist = dist; } } } return minDist; } private static double distance(Point p1, Point p2) { return Math.sqrt(Math.pow(p1.x - p2.x, 2) + Math.pow(p1.y - p2.y, 2)); } public static void main(String[] args) { Point[] points = { new Point(2, 3), new Point(12, 30), new Point(40, 50), new Point(5, 1), new Point(12, 10), new Point(3, 4) }; System.out.println(\u0026#34;The smallest distance is \u0026#34; + closestPair(points)); } } 示例 4：凸包问题（Convex Hull Problem）\n老师给出的参考\n#include\u0026lt;bits/stdc++.h\u0026gt; using namespace std; #define Double long double const int MAXN = 1e6 + 10; struct Point{ Double x,y; Double rad; Point(Double x=0,Double y=0):x(x),y(y){} void read(){ scanf(\u0026#34;%Lf %Lf\u0026#34;,\u0026amp;x,\u0026amp;y); //rad = atan2(y,x); } }; typedef vector\u0026lt;Point\u0026gt; Polygon; typedef Point Vector; inline Vector operator - (Vector A,Vector B){ return Vector(A.x - B.x , A.y - B.y); } inline Double Dot(Vector A,Vector B){ return A.x * B.x + A.y * B.y; } inline Double Length(Vector A){ return sqrt(Dot(A , A)); } inline Double Cross(Vector A,Vector B){ return A.x * B.y - A.y * B.x; } int N; Point points[MAXN]; Polygon convex; bool cmpByX(Point A,Point B){ if(A.x != B.x) return A.x \u0026lt; B.x; else return A.y \u0026lt; B.y; } void findConvex(Point A,Point B,vector\u0026lt;Point\u0026gt; p){ if(!p.size()) return; Point C = p[0]; vector\u0026lt;Point\u0026gt; l; vector\u0026lt;Point\u0026gt; r; for(auto P:p){ if(Cross(B-A,P-A) \u0026gt; Cross(B-A,C-A)) C = P; } for(auto P:p){ if(Cross(C-A,P-A) \u0026gt; 0) l.push_back(P); if(Cross(B-C,P-C) \u0026gt; 0) r.push_back(P); } findConvex(A,C,l); convex.push_back(C); findConvex(C,B,r); } Double getPolygonPerimeter(Polygon polygon){ Double ret = 0; for(int i = 0;i \u0026lt; polygon.size(); i++){ ret += Length(polygon[(i + 1) % polygon.size()] - polygon[i]); } return ret; } void solve(){ scanf(\u0026#34;%d\u0026#34;,\u0026amp;N); for(int i = 1; i \u0026lt;= N; i++) points[i].read(); sort(points + 1,points + 1 + N,cmpByX); vector\u0026lt;Point\u0026gt; l; vector\u0026lt;Point\u0026gt; r; Point A = points[1]; Point B = points[N]; for(int i = 2;i \u0026lt;= N - 1; i++){ if(Cross(B - A,points[i] - A) \u0026gt; 0) l.push_back(points[i]); if(Cross(B - A,points[i] - A) \u0026lt; 0) r.push_back(points[i]); } convex.push_back(A); findConvex(A,B,l); convex.push_back(B); findConvex(B,A,r); printf(\u0026#34;%.2Lf\\n\u0026#34;,getPolygonPerimeter(convex)); } int main(){ solve(); } 凸包问题是指在平面上找到能够包含所有给定点的最小凸多边形。\n基本思想：\n分解： 将点集按 x 坐标排序，并划分为左右两部分。 解决： 递归地找到左右两部分的凸包。 合并： 合并左右两部分的凸包，得到整体的凸包。 import java.util.Arrays; import java.util.Stack; public class ConvexHull { static class Point implements Comparable\u0026lt;Point\u0026gt; { int x, y; Point(int x, int y) { this.x = x; this.y = y; } @Override public int compareTo(Point p) { return this.x != p.x ? this.x - p.x : this.y - p.y; } } public static Point[] convexHull(Point[] points) { Arrays.sort(points); Stack\u0026lt;Point\u0026gt; lower = new Stack\u0026lt;\u0026gt;(); for (Point p : points) { while (lower.size() \u0026gt;= 2 \u0026amp;\u0026amp; cross(lower.get(lower.size() - 2), lower.get(lower.size() - 1), p) \u0026lt;= 0) { lower.pop(); } lower.push(p); } Stack\u0026lt;Point\u0026gt; upper = new Stack\u0026lt;\u0026gt;(); for (int i = points.length - 1; i \u0026gt;= 0; i--) { Point p = points[i]; while (upper.size() \u0026gt;= 2 \u0026amp;\u0026amp; cross(upper.get(upper.size() - 2), upper.get(upper.size() - 1), p) \u0026lt;= 0) { upper.pop(); } upper.push(p); } lower.pop(); upper.pop(); Point[] hull = new Point[lower.size() + upper.size()]; int k = 0; for (Point p : lower) hull[k++] = p; for (Point p : upper) hull[k++] = p; return hull; } private static int cross(Point o, Point a, Point b) { return (a.x - o.x) * (b.y - o.y) - (a.y - o.y) * (b.x - o.x); } public static void main(String[] args) { Point[] points = { new Point(0, 3), new Point(2, 3), new Point(1, 1), new Point(2, 1), new Point(3, 0), new Point(0, 0), new Point(3, 3) }; Point[] hull = convexHull(points); System.out.println(\u0026#34;The points in the convex hull are:\u0026#34;); for (Point p : hull) { System.out.println(\u0026#34;(\u0026#34; + p.x + \u0026#34;, \u0026#34; + p.y + \u0026#34;)\u0026#34;); } } } 3. 时空权衡（Time-Space Tradeoff） # 时空权衡（Time-Space Tradeoff）是计算机科学中的一个重要概念，指的是在设计算法时，通过增加时间复杂度来减少空间复杂度，或者通过增加空间复杂度来减少时间复杂度。这个思想在许多算法设计中应用非常广泛，例如字符串匹配、B树和动态规划等。\n时空权衡思想\n时空权衡思想的基本原理是：在设计和优化算法时，可以在时间和空间之间做出选择和折中。例如，通过预处理数据并存储一些额外的信息，可以加快后续的查询速度；相反，减少存储空间则可能需要更复杂的计算过程来获得相同的结果。\n应用实例\n字符串匹配：\n朴素算法：时间复杂度为 $O(nm)$，空间复杂度为 $O(1)$。 KMP算法（Knuth-Morris-Pratt）：通过预处理构建部分匹配表，时间复杂度为 $O(n + m)$，空间复杂度为 $O(m)$。 B树：\nB树是一种平衡树数据结构，适用于在磁盘或其他存储设备上进行大数据量的存储和快速检索。 通过增加树的阶数，B树减少了树的高度，从而减少了访问磁盘的次数，以换取更大的节点大小。 动态规划：\n斐波那契数列：简单的递归算法时间复杂度为 $O(2^n)$，空间复杂度为 $O(n)$。 动态规划算法：通过存储子问题的解来避免重复计算，时间复杂度为 $O(n)$，空间复杂度可以是 $O(n)$ 或 $O(1)$（优化后的版本）。 示例 1：字符串匹配-Horpool算法\n示例 2：Boyer-Moore 算法\n示例 3：字符串匹配（KMP算法）\nKMP算法是一种改进的字符串匹配算法，通过预处理模式串构建部分匹配表（也称为“前缀函数”），在匹配过程中避免不必要的比较，从而提高匹配效率。\npublic class KMP { public static int[] computeLPSArray(String pattern) { int m = pattern.length(); int[] lps = new int[m]; int length = 0; int i = 1; lps[0] = 0; while (i \u0026lt; m) { if (pattern.charAt(i) == pattern.charAt(length)) { length++; lps[i] = length; i++; } else { if (length != 0) { length = lps[length - 1]; } else { lps[i] = 0; i++; } } } return lps; } public static void KMPSearch(String text, String pattern) { int n = text.length(); int m = pattern.length(); int[] lps = computeLPSArray(pattern); int i = 0; int j = 0; while (i \u0026lt; n) { if (pattern.charAt(j) == text.charAt(i)) { i++; j++; } if (j == m) { System.out.println(\u0026#34;Found pattern at index \u0026#34; + (i - j)); j = lps[j - 1]; } else if (i \u0026lt; n \u0026amp;\u0026amp; pattern.charAt(j) != text.charAt(i)) { if (j != 0) { j = lps[j - 1]; } else { i++; } } } } public static void main(String[] args) { String text = \u0026#34;ABABDABACDABABCABAB\u0026#34;; String pattern = \u0026#34;ABABCABAB\u0026#34;; KMPSearch(text, pattern); } } 示例 4：动态规划（斐波那契数列）\n动态规划通过存储子问题的解来避免重复计算，提高算法效率。下面展示两种计算斐波那契数列的动态规划方法。\npublic class Fibonacci { // 使用数组存储子问题的解 public static int fibDP(int n) { if (n \u0026lt;= 1) { return n; } int[] f = new int[n + 1]; f[0] = 0; f[1] = 1; for (int i = 2; i \u0026lt;= n; i++) { f[i] = f[i - 1] + f[i - 2]; } return f[n]; } // 使用两个变量优化空间复杂度 public static int fibOptimized(int n) { if (n \u0026lt;= 1) { return n; } int a = 0, b = 1, c; for (int i = 2; i \u0026lt;= n; i++) { c = a + b; a = b; b = c; } return b; } public static void main(String[] args) { int n = 10; System.out.println(\u0026#34;Fibonacci number using DP: \u0026#34; + fibDP(n)); System.out.println(\u0026#34;Fibonacci number using optimized DP: \u0026#34; + fibOptimized(n)); } } 示例 4：动态规划（币值最大化问题）\n示例 5：动态规划（找零问题）\n示例 6：动态规划（硬币收集问题）\n4. 贪心部分 # 考点：\n贪心法的思想和特点 特点：feasible、locally optimal、irrevocable 用贪心法解决经典问题： 活动相容问题和MST二选一； MST问题 不考察证明 掌握 Prim 或者 Kruskal 算法的算法过程 活动相容性问题 优化目标：安排的活动最多。先按结束时间排序，然后 不要求证明；需要掌握算法过程，选出对应活动。 可能要求写伪代码。 贪心法的思想和特点\n贪心法（Greedy Algorithm）是一种在求解最优化问题时采用的策略。它通过在每一步都选择当前最优解（局部最优解），以期最终获得全局最优解。贪心法的思想和特点可以从以下几个角度进行介绍：\nFeasible（可行性）：可行性指的是在每一步选择中，所选择的解必须满足问题的约束条件，即每一步选择都是合法的。例如，在找零钱问题中，每一步选择的硬币面值都必须是当前可以使用的硬币面值，这样最终能够凑出所需的总金额。 Locally Optimal（局部最优）：局部最优是贪心法的核心思想，即在每一个决策点上都选择当前看起来最好的选项，而不考虑未来的选择。这种策略依赖于以下假设：通过一系列局部最优的选择，最终能得到全局最优解。例如，在旅行推销员问题中，如果我们选择每一步都走向距离最近的未访问城市，这就是一种局部最优选择。 Irrevocable（不可撤销性）：不可撤销性意味着一旦在某一步做出了选择，就不能再回退或更改。贪心算法的这一特性要求在每一步选择时必须非常慎重，因为一旦选择了某个选项，就无法重新考虑之前的决策。例如，在最小生成树问题中的Prim算法或Kruskal算法中，一旦选择了一条边，就不会再撤销这个选择。 用贪心法解决经典问题\n活动选择问题（Activity Selection Problem） 题目描述：给定一组活动，每个活动有一个开始时间和一个结束时间。你需要选择尽可能多的活动，使得它们互不重叠。活动的选择需要先按结束时间排序。\n算法过程：\n按活动的结束时间升序排序。 从第一个活动开始，选择结束时间最早且不与已选择活动重叠的活动。 伪代码：\n活动选择(活动列表): 将活动列表按结束时间升序排序 选择第一个活动，并将其加入到已选择活动列表中 设定最后一个选择的活动的结束时间为第一个活动的结束时间 对于排序后活动列表中的每一个剩余活动: 如果 当前活动的开始时间 \u0026gt;= 最后一个选择的活动的结束时间: 选择当前活动 更新最后一个选择的活动的结束时间为当前活动的结束时间 返回已选择活动列表 import java.util.ArrayList; import java.util.Arrays; import java.util.List; public class ActivitySelection { static class Activity { int start; int end; Activity(int start, int end) { this.start = start; this.end = end; } } public static List\u0026lt;Activity\u0026gt; selectActivities(Activity[] activities) { Arrays.sort(activities, (a1, a2) -\u0026gt; a1.end - a2.end); List\u0026lt;Activity\u0026gt; result = new ArrayList\u0026lt;\u0026gt;(); int lastEndTime = -1; for (Activity activity : activities) { if (activity.start \u0026gt;= lastEndTime) { result.add(activity); lastEndTime = activity.end; } } return result; } public static void main(String[] args) { Activity[] activities = { new Activity(1, 4), new Activity(3, 5), new Activity(0, 6), new Activity(5, 7), new Activity(3, 9), new Activity(5, 9), new Activity(6, 10), new Activity(8, 11), new Activity(8, 12), new Activity(2, 14), new Activity(12, 16) }; List\u0026lt;Activity\u0026gt; selectedActivities = selectActivities(activities); System.out.println(\u0026#34;Selected activities:\u0026#34;); for (Activity activity : selectedActivities) { System.out.println(\u0026#34;Activity: (\u0026#34; + activity.start + \u0026#34;, \u0026#34; + activity.end + \u0026#34;)\u0026#34;); } } } 最小生成树问题（Minimum Spanning Tree, MST） 使用Kruskal算法\n题目描述：给定一个连通无向图，找到其最小生成树（MST）。即找到一个子图，使得所有节点连通且边的权重和最小。\nKruskal算法过程：\n将图中的所有边按权重从小到大排序。 初始化一个空森林（每个节点都是一个单独的树）。 按权重从小到大依次检查每条边，如果加入该边不会形成环，则将该边加入MST。 import java.util.*; public class KruskalMST { static class Edge implements Comparable\u0026lt;Edge\u0026gt; { int src, dest, weight; public int compareTo(Edge compareEdge) { return this.weight - compareEdge.weight; } } static class Subset { int parent, rank; } int V, E; Edge[] edges; KruskalMST(int v, int e) { V = v; E = e; edges = new Edge[E]; for (int i = 0; i \u0026lt; e; ++i) { edges[i] = new Edge(); } } int find(Subset[] subsets, int i) { if (subsets[i].parent != i) subsets[i].parent = find(subsets, subsets[i].parent); return subsets[i].parent; } void union(Subset[] subsets, int x, int y) { int xroot = find(subsets, x); int yroot = find(subsets, y); if (subsets[xroot].rank \u0026lt; subsets[yroot].rank) subsets[xroot].parent = yroot; else if (subsets[xroot].rank \u0026gt; subsets[yroot].rank) subsets[yroot].parent = xroot; else { subsets[yroot].parent = xroot; subsets[xroot].rank++; } } void KruskalMST() { Edge[] result = new Edge[V]; int e = 0; int i = 0; for (i = 0; i \u0026lt; V; ++i) result[i] = new Edge(); Arrays.sort(edges); Subset[] subsets = new Subset[V]; for (i = 0; i \u0026lt; V; ++i) subsets[i] = new Subset(); for (int v = 0; v \u0026lt; V; ++v) { subsets[v].parent = v; subsets[v].rank = 0; } i = 0; while (e \u0026lt; V - 1) { Edge next_edge = edges[i++]; int x = find(subsets, next_edge.src); int y = find(subsets, next_edge.dest); if (x != y) { result[e++] = next_edge; union(subsets, x, y); } } System.out.println(\u0026#34;Following are the edges in the constructed MST\u0026#34;); for (i = 0; i \u0026lt; e; ++i) System.out.println(result[i].src + \u0026#34; -- \u0026#34; + result[i].dest + \u0026#34; == \u0026#34; + result[i].weight); } public static void main(String[] args) { int V = 4; int E = 5; KruskalMST graph = new KruskalMST(V, E); graph.edges[0].src = 0; graph.edges[0].dest = 1; graph.edges[0].weight = 10; graph.edges[1].src = 0; graph.edges[1].dest = 2; graph.edges[1].weight = 6; graph.edges[2].src = 0; graph.edges[2].dest = 3; graph.edges[2].weight = 5; graph.edges[3].src = 1; graph.edges[3].dest = 3; graph.edges[3].weight = 15; graph.edges[4].src = 2; graph.edges[4].dest = 3; graph.edges[4].weight = 4; graph.KruskalMST(); } } 使用Prim算法\n题目描述：同样是最小生成树问题，使用Prim算法来解决。Prim算法从一个顶点开始，不断扩展已选顶点集合，每次选择连接到已选顶点集合的权重最小的边。\nPrim算法过程：\n从一个起点开始，将该点加入MST集合。 重复以下过程，直到所有顶点都被包括在MST中： 在不形成环的前提下，从已选择的顶点集合中选出一条连接到未选顶点的最小边。 import java.util.*; public class PrimMST { private static final int V = 5; int minKey(int[] key, Boolean[] mstSet) { int min = Integer.MAX_VALUE, minIndex = -1; for (int v = 0; v \u0026lt; V; v++) { if (!mstSet[v] \u0026amp;\u0026amp; key[v] \u0026lt; min) { min = key[v]; minIndex = v; } } return minIndex; } void printMST(int[] parent, int n, int[][] graph) { System.out.println(\u0026#34;Edge \\tWeight\u0026#34;); for (int i = 1; i \u0026lt; V; i++) System.out.println(parent[i] + \u0026#34; - \u0026#34; + i + \u0026#34;\\t\u0026#34; + graph[i][parent[i]]); } void primMST(int[][] graph) { int[] parent = new int[V]; int[] key = new int[V]; Boolean[] mstSet = new Boolean[V]; for (int i = 0; i \u0026lt; V; i++) { key[i] = Integer.MAX_VALUE; mstSet[i] = false; } key[0] = 0; parent[0] = -1; for (int count = 0; count \u0026lt; V - 1; count++) { int u = minKey(key, mstSet); mstSet[u] = true; for (int v = 0; v \u0026lt; V; v++) { if (graph[u][v] != 0 \u0026amp;\u0026amp; !mstSet[v] \u0026amp;\u0026amp; graph[u][v] \u0026lt; key[v]) { parent[v] = u; key[v] = graph[u][v]; } } } printMST(parent, V, graph); } public static void main(String[] args) { PrimMST t = new PrimMST(); int[][] graph = new int[][]{ {0, 2, 0, 6, 0}, {2, 0, 3, 8, 5}, {0, 3, 0, 0, 7}, {6, 8, 0, 0, 9}, {0, 5, 7, 9, 0} }; t.primMST(graph); } } 5. 动态规划 # 考点：\n动态规划的思想和特点 能写出问题分解为子问题的递归式（写出递归式就给大量分数） 能解决几个经典问题： LCS 最长公共子序列 序列：不连续、但是保持顺序的子集合。 序列 X、Y，找最长公共子序列 Z； 递归地从后往前面看； 回文子序列 矩阵连乘 LCS：\n回文子序列：DynamicProgrammingReview.pdf的P5 Problem 6\n动态规划的思想：\n动态规划（Dynamic Programming, DP）是一种通过将复杂问题分解为更小的子问题来解决复杂问题的算法技术。动态规划与分治法类似，但与分治法不同的是，动态规划会存储子问题的结果，从而避免重复计算。\n动态规划的特点：\n最优子结构：问题的最优解包含其子问题的最优解。 重叠子问题：子问题被多次重复计算。 子问题独立性：子问题间相互独立。 动态规划的经典问题\n最长公共子序列（LCS） 问题描述：给定两个序列X和Y，找出它们的最长公共子序列Z。最长公共子序列是指在不改变序列相对顺序的前提下，从两个序列中选取的最长子序列。\n递归式：\n设$X$的长度为$m$，$Y$的长度为$n$，$dp[i][j]$表示$X$的前$i$个字符和$Y$的前$j$个字符的最长公共子序列长度。\n如果$X[i-1] == Y[j-1]$，则$dp[i][j] = dp[i-1][j-1] + 1$ 如果$X[i-1] != Y[j-1]$，则$dp[i][j] = max(dp[i-1][j], dp[i][j-1])$ public class LCS { public static int longestCommonSubsequence(String X, String Y) { int m = X.length(); int n = Y.length(); int[][] dp = new int[m + 1][n + 1]; for (int i = 1; i \u0026lt;= m; i++) { for (int j = 1; j \u0026lt;= n; j++) { if (X.charAt(i - 1) == Y.charAt(j - 1)) { dp[i][j] = dp[i - 1][j - 1] + 1; } else { dp[i][j] = Math.max(dp[i - 1][j], dp[i][j - 1]); } } } return dp[m][n]; } public static void main(String[] args) { String X = \u0026#34;ABCBDAB\u0026#34;; String Y = \u0026#34;BDCAB\u0026#34;; System.out.println(\u0026#34;最长公共子序列长度: \u0026#34; + longestCommonSubsequence(X, Y)); } } 回文子序列 问题描述：给定一个字符串，找到它的最长回文子序列。回文子序列是指从左到右和从右到左都一样的子序列。\n递归式：设字符串长度为$n$，$dp[i][j]$表示字符串从第$i$个字符到第$j$个字符的最长回文子序列长度。\n如果$s[i] == s[j]$，则$dp[i][j] = dp[i+1][j-1] + 2$ 如果$s[i] != s[j]$，则$dp[i][j] = max(dp[i+1][j], dp[i][j-1])$ public class LongestPalindromicSubsequence { public static int longestPalindromeSubseq(String s) { int n = s.length(); int[][] dp = new int[n][n]; for (int i = n - 1; i \u0026gt;= 0; i--) { dp[i][i] = 1; for (int j = i + 1; j \u0026lt; n; j++) { if (s.charAt(i) == s.charAt(j)) { dp[i][j] = dp[i + 1][j - 1] + 2; } else { dp[i][j] = Math.max(dp[i + 1][j], dp[i][j - 1]); } } } return dp[n - 1][0]; } public static void main(String[] args) { String s = \u0026#34;bbbab\u0026#34;; System.out.println(\u0026#34;最长回文子序列长度: \u0026#34; + longestPalindromeSubseq(s)); } } 矩阵连乘问题 问题描述：给定一组矩阵，确定它们的相乘顺序，以使得乘积的计算代价最小。矩阵连乘问题中的计算代价由矩阵的维数决定。\n递归式：设$dp[i][j]$表示从第$i$个矩阵到第$j$个矩阵的最小乘积计算代价，$p$为矩阵链的维数数组。\n$dp[i][j] = min(dp[i][k] + dp[k+1][j] + p[i-1]*p[k]*p[j]) (i ≤ k \u0026lt; j)$ public class MatrixChainMultiplication { public static int matrixChainOrder(int[] p) { int n = p.length - 1; int[][] dp = new int[n][n]; for (int l = 2; l \u0026lt;= n; l++) { for (int i = 0; i \u0026lt; n - l + 1; i++) { int j = i + l - 1; dp[i][j] = Integer.MAX_VALUE; for (int k = i; k \u0026lt; j; k++) { int q = dp[i][k] + dp[k + 1][j] + p[i] * p[k + 1] * p[j + 1]; if (q \u0026lt; dp[i][j]) { dp[i][j] = q; } } } } return dp[0][n - 1]; } public static void main(String[] args) { int[] p = {1, 2, 3, 4}; System.out.println(\u0026#34;最小矩阵连乘计算代价: \u0026#34; + matrixChainOrder(p)); } } 6. 平滑函数性质和主定理 # 6.1 平滑函数\n平滑函数的背景：\nComplexity Analysis of BinCounting $$ f(x)=[\\frac{x}{2}] $$\n希望把取整符号打开；\n假设 $n=2^k$，那么$f(x)$就可以去掉取整符号，这时$f(x)$表达式看起来更舒服\n我们希望这种良好的表达式不仅在数轴上的特殊点，而是把特殊点的性质扩展到数轴上；就需要用Smooth Function\nSmooth 函数 考点1：什么是Smooth Function（定义）；说清楚 Smoothness Rule 考点2：Smooth函数有什么用？ 考点3：某个函数是不是另一个函数的Smooth函数 类似判断题、概念题；\n考点1：什么是 Smooth Function（定义）\n定义：Smooth Function（光滑函数）通常是指在其定义域内具有连续导数（包括高阶导数）的函数。这意味着在数学上，光滑函数不仅是连续的，而且其导数也是连续的。通常，光滑函数可以在其定义域内进行无穷次微分。\n在凸优化中，平滑函数是指具有一定光滑性（smoothness）的函数。具体来说，一个函数 $f(x)$ 被称为 $L$-平滑的（$L$-smooth），如果它的梯度满足以下条件：\n$$ [ |\\nabla f(x) - \\nabla f(y)| \\leq L |x - y| ] $$\n对所有 $x$ 和 $y$ 适用。这里，$L$ 是一个常数，称为 Lipschitz 常数，表示梯度变化的最大速率。\n这个定义表明函数的梯度变化不会太剧烈，即梯度是 Lipschitz 连续的。这在优化算法中非常重要，因为它可以帮助我们理解和控制梯度的变化。\n为什么需要定义平滑\n定义平滑性主要是为了描述函数梯度的变化程度。在优化算法中，函数的梯度趋近于零意味着我们正在接近极小值点。平滑性可以帮助我们理解梯度在不同点之间的变化，这对于分析和设计优化算法非常有用。\n平滑函数与凸性\n我们有以下命题：\n命题： 如果一个函数 $f(x)$ 是 $L$-平滑的，那么该函数是凸函数。\n证明：\n设 $f(x)$ 是 $L$-平滑的，定义函数 $g(x) = f(x) - \\frac{L}{2} |x|^2$。\n我们需要证明函数 $g(x)$ 是凸函数。为了证明这一点，我们需要证明 $g(x)$ 是单调的（即 $g(x)$ 的梯度是非递减的）。\n计算 $g(x)$ 的梯度：\n$$ [ \\nabla g(x) = \\nabla f(x) - Lx ] $$\n接下来，我们检查 $g(x)$ 的二阶导数是否非负。即 Hessian 矩阵是否半正定：\n$$ [ \\nabla^2 g(x) = \\nabla^2 f(x) - L I ] $$\n其中 $I$ 是单位矩阵。\n因为 $f(x)$ 是 $L$-平滑的，我们有：\n$$ [ |\\nabla f(x) - \\nabla f(y)| \\leq L |x - y| ] $$\n这意味着 Hessian 矩阵的特征值不大于 $L$，即：\n$$ [ \\nabla^2 f(x) \\preceq L I ] $$\n因此，$\\nabla^2 g(x) = \\nabla^2 f(x) - L I \\preceq 0$，即 Hessian 矩阵是负半定的。\n这表明函数 $g(x)$ 是凸的。\n根据函数 $g(x)$ 是凸函数，我们可以得到函数 $f(x)$ 的一个上界：\n$$ [ f(y) \\leq f(x) + \\nabla f(x)^T (y - x) + \\frac{L}{2} |y - x|^2 ] $$\n这个上界对于后续分析优化算法的收敛性非常重要。\n在公式\n$$ [ |\\nabla f(x) - \\nabla f(y)| \\leq L |x - y| ] $$\n中，符号\n$$ [ | \\cdot | ] $$\n表示向量的范数（norm），通常情况下是欧几里得范数（也称为二范数，L2范数）。以下是对范数的详细解释：\n范数的定义\n范数是一个函数，它将向量映射到非负实数，表示向量的“大小”或“长度”。常见的向量范数有几种类型：\n欧几里得范数（L2范数）：\n定义：对于一个向量 $[ \\mathbf{v} = (v_1, v_2, \\ldots, v_n) ]$，其欧几里得范数定义为：$[ |\\mathbf{v}|_2 = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2} ]$\n也可以表示为：$[ |\\mathbf{v}|2 = \\sqrt{\\sum{i=1}^n v_i^2} ]$\nL1范数：\n定义：对于一个向量 $[ \\mathbf{v} = (v_1, v_2, \\ldots, v_n) ]$，其L1范数定义为：$[ |\\mathbf{v}|_1 = |v_1| + |v_2| + \\cdots + |v_n| ]$\n也可以表示为：$[ |\\mathbf{v}|1 = \\sum{i=1}^n |v_i| ]$\n无穷范数（L∞范数）：\n定义：对于一个向量 $[ \\mathbf{v} = (v_1, v_2, \\ldots, v_n) ]$，其L∞范数定义为： $[ |\\mathbf{v}|_\\infty = \\max { |v_1|, |v_2|, \\ldots, |v_n| } ]$\n公式中的范数\n在公式 $[ |\\nabla f(x) - \\nabla f(y)| \\leq L |x - y| ]$ 中，范数通常是指欧几里得范数（L2范数），除非另有说明。这是因为欧几里得范数在优化和梯度计算中最为常用。具体来说：\n$[ |\\nabla f(x) - \\nabla f(y)| ]$ 表示梯度向量 $[ \\nabla f(x) ]$ 和 $[ \\nabla f(y) ]$ 之间的欧几里得距离（即梯度差的长度）。\n$[ |x - y| ]$ 表示点 $[ x ]$ 和 $[ y ]$ 之间的欧几里得距离（即坐标差的长度）。\nSmoothness Rule：光滑函数的一个基本规则是：一个函数 $f(x)$ 被称为是光滑的，当且仅当它在其定义域内的所有阶导数都存在且连续。\n形式化定义：如果 $f(x)$ 在区间 $[a, b]$ 内的任意点处都具有任意阶的连续导数，则称 $f(x)$ 是区间 $[a, b]$ 上的光滑函数。\n考点2：Smooth函数有什么用？\n光滑函数在数学和工程中有许多重要的应用，主要包括以下几个方面：\n简化计算和分析：在某些情况下，我们需要对离散或不连续的函数进行处理。例如，在复杂性分析中，处理取整符号（如 $\\left\\lfloor \\frac{x}{2} \\right\\rfloor$）的函数可能比较麻烦。通过用光滑函数进行近似，可以简化计算和分析。 信号处理：在信号处理中，光滑函数用于平滑数据，消除噪声。通过将不规则的数据转换为平滑的曲线，可以更容易地分析数据的趋势和模式。 数值分析和优化：在数值分析和优化问题中，光滑函数用于确保函数在迭代过程中具有良好的数值稳定性。光滑函数有助于避免不稳定的数值行为，并且在求解最优化问题时可以提高收敛速度。 计算机图形学：在计算机图形学中，光滑函数用于生成光滑的曲面和曲线，使图形渲染更加平滑和逼真。 考点3：某个函数是不是另一个函数的 Smooth 函数\n要判断一个函数是否是另一个函数的光滑函数，我们通常需要检查以下几个方面：\n连续性：检查函数是否在其定义域内是连续的。 导数的存在性和连续性：检查函数的导数是否存在，并且导数在定义域内是否也是连续的。如果是高阶光滑函数，则需要检查所有阶导数的连续性。 近似性：如果一个函数 $g(x)$ 是 $f(x)$ 的光滑版本，则 $g(x)$ 应该在定义域内良好地近似 $f(x)$。这意味着对于所有 $x$，$g(x)$ 应该尽可能接近 $f(x)$，并且这种近似在函数的导数上也是成立的。 示例\n例子：处理取整符号的光滑函数\n考虑函数 $f(x) = \\left\\lfloor \\frac{x}{2} \\right\\rfloor$。为了去掉取整符号，我们希望找到一个光滑函数来近似它。\n假设 $n = 2^k$，那么 $f(x) = \\left\\lfloor \\frac{x}{2} \\right\\rfloor$ 可以去掉取整符号，因为在这些点上 $x$ 是 2 的整数倍，因此取整符号对结果没有影响。\n为了将这种性质扩展到整个数轴上，我们可以使用一个光滑函数来近似它，例如：\n$$ [ g(x) = \\frac{x}{2} - \\frac{1}{2} \\sin\\left(2\\pi \\frac{x}{2}\\right) ] $$\n在这种情况下，$g(x)$ 是 $\\left\\lfloor \\frac{x}{2} \\right\\rfloor$ 的一个光滑近似，它在所有点上都是连续和可微的。\n验证：\n连续性：$g(x)$ 在整个定义域内是连续的。 导数的存在性和连续性：$g(x)$ 的导数存在并且是连续的。 近似性：对于所有 $x$，$g(x)$ 都接近于 $\\left\\lfloor \\frac{x}{2} \\right\\rfloor$。 因此，$g(x)$ 是 $\\left\\lfloor \\frac{x}{2} \\right\\rfloor$ 的一个光滑近似函数。\n6.2 分治法和主定理\n解方程 $T(n)=bT(n/c)+f(n)$\n不用背别的；就用Master Theorem就行 判断满足主定理的哪一条，算一下case 1、case2、case3，然后代入主定理的结论就行； T(n)=9T(n/3)+n 有3个辅助量；D、E、L 03-Recursion.ppt： 分治法和主定理\n分治法（Divide and Conquer）是一种常用的算法设计范式，其基本思想是将一个复杂的问题分解为若干个规模较小的子问题，递归地求解这些子问题，然后将子问题的解合并得到原问题的解。\n主定理（Master Theorem）\n主定理是用来分析分治算法时间复杂度的一种工具。它适用于以下形式的递归方程：\n$$ [ T(n) = aT\\left(\\frac{n}{b}\\right) + f(n) ] $$\n其中：\n$a \\geq 1$ 表示每次递归调用产生的子问题数。 $b \u0026gt; 1$ 表示子问题规模缩减的因子。 $f(n)$ 是分解和合并所需的时间。 主定理将递归方程的解分为三种情况：\n情况 1： 如果 $f(n) = O(n^c)$ 且 $c \u0026lt; \\log_b a$，则 $[ T(n) = \\Theta(n^{\\log_b a}) ]$\n情况 2： 如果 $f(n) = \\Theta(n^c)$ 且 $c = \\log_b a$，则 $[ T(n) = \\Theta(n^c \\log n) ]$\n情况 3： 如果 $f(n) = \\Omega(n^c)$ 且 $c \u0026gt; \\log_b a$，并且满足正则性条件 $af\\left(\\frac{n}{b}\\right) \\leq kf(n)$ 对于某个常数 $k \u0026lt; 1$ 和足够大的 $n$，则 $[ T(n) = \\Theta(f(n)) ]$\n示例：合并排序（Merge Sort）\n合并排序是一种典型的分治算法，其时间复杂度可以通过主定理来分析。\n合并排序的递归方程：\n$$ [ T(n) = 2T\\left(\\frac{n}{2}\\right) + O(n) ] $$\n$a = 2$：每次递归产生两个子问题。 $b = 2$：每个子问题的规模是原问题的一半。 $f(n) = O(n)$：合并两个子问题所需的时间。 应用主定理：\n计算 $\\log_b a$： $[ \\log_2 2 = 1 ]$\n对比 $f(n)$ 和 $n^{\\log_b a}$：\n$f(n) = O(n)$ $n^{\\log_b a} = n^1 = n$ 因为 $f(n) = \\Theta(n)$ 和 $n^{\\log_b a} = n$，这符合主定理的第二种情况。\n结论：\n根据主定理的第二种情况： $$ [ T(n) = \\Theta(n \\log n) ] $$\n另一个示例：二分查找（Binary Search）\n二分查找是一种经典的分治算法，用于在排序数组中查找元素，其时间复杂度也可以通过主定理来分析。\n二分查找的递归方程：\n$$ [ T(n) = T\\left(\\frac{n}{2}\\right) + O(1) ] $$\n$a = 1$：每次递归产生一个子问题。 $b = 2$：子问题的规模是原问题的一半。 $f(n) = O(1)$：分解和合并所需的时间是常数时间。 应用主定理：\n计算 $\\log_b a$： [ \\log_2 1 = 0 ]\n对比 $f(n)$ 和 $n^{\\log_b a}$：\n$f(n) = O(1)$ $n^{\\log_b a} = n^0 = 1$ 因为 $f(n) = \\Theta(1)$ 和 $n^{\\log_b a} = 1$，这符合主定理的第二种情况。\n结论：\n根据主定理的第二种情况： [ T(n) = \\Theta(\\log n) ]\n7. 复杂度表示法 # 考点：\n$O(f)$、$\\Omega (f)$、$\\Theta (f)$的性质\n考点1：三个性质是什么含义；（用Definition、极限或者自然语言描述） 考点2：性质transitive、对称性和properties这两页； 大O记号（$O(f)$）、大Ω记号（$\\Omega(f)$）、大Θ记号（$\\Theta(f)$）的性质\n考点1：三个性质的含义\n大O记号（$O(f)$） Definition: 对于两个函数 $f(n)$ 和 $g(n)$，如果存在正整数 $c$ 和 $n_0$，使得当 $n \\ge n_0$ 时，$0 \\le f(n) \\le c \\cdot g(n)$，则称 $f(n)$ 是 $g(n)$ 的大O记号，记作 $f(n) = O(g(n))$。\n极限描述:\n$$ f(n) = O(g(n)) \\iff \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} \\le C \\quad (C \\text{为常数}) $$\n自然语言描述: $f(n)$ 的增长速度最多和 $g(n)$ 的增长速度一样快（或更慢）。\n大Ω记号（$\\Omega(f)$） Definition: 对于两个函数 $f(n)$ 和 $g(n)$，如果存在正整数 $c$ 和 $n_0$，使得当 $n \\ge n_0$ 时，$0 \\le c \\cdot g(n) \\le f(n)$，则称 $f(n)$ 是 $g(n)$ 的大Ω记号，记作 $f(n) = \\Omega(g(n))$。\n极限描述:\n$$ f(n) = \\Omega(g(n)) \\iff \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} \\ge C \\quad (C \\text{为常数}) $$\n自然语言描述: $f(n)$ 的增长速度至少和 $g(n)$ 的增长速度一样快（或更快）。\n大Θ记号（$\\Theta(f)$） Definition: 对于两个函数 $f(n)$ 和 $g(n)$，如果存在正整数 $c_1, c_2$ 和 $n_0$，使得当 $n \\ge n_0$ 时，$0 \\le c_1 \\cdot g(n) \\le f(n) \\le c_2 \\cdot g(n)$，则称 $f(n)$ 是 $g(n)$ 的大Θ记号，记作 $f(n) = \\Theta(g(n))$。\n极限描述:\n$$ f(n) = \\Theta(g(n)) \\iff \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = C \\quad (C \\text{为常数}) $$\n自然语言描述: $f(n)$ 的增长速度和 $g(n)$ 的增长速度一样快（同阶）。\n考点2：性质transitive、对称性和properties\nTransitive（传递性） 大O记号： 如果 $f(n) = O(g(n))$ 且 $g(n) = O(h(n))$，则 $f(n) = O(h(n))$。 大Ω记号： 如果 $f(n) = \\Omega(g(n))$ 且 $g(n) = \\Omega(h(n))$，则 $f(n) = \\Omega(h(n))$。 大Θ记号： 如果 $f(n) = \\Theta(g(n))$ 且 $g(n) = \\Theta(h(n))$，则 $f(n) = \\Theta(h(n))$。 对称性 大O记号： 大O记号没有对称性，即 $f(n) = O(g(n))$ 并不意味着 $g(n) = O(f(n))$。\n大Ω记号： 大Ω记号没有对称性，即 $f(n) = \\Omega(g(n))$ 并不意味着 $g(n) = \\Omega(f(n))$。\n大Θ记号： 大Θ记号具有对称性，即 $f(n) = \\Theta(g(n))$ 意味着 $g(n) = \\Theta(f(n))$。\nProperties（其他性质） 大O记号的性质：\n如果 $f_1(n) = O(g_1(n))$ 且 $f_2(n) = O(g_2(n))$，则 $f_1(n) + f_2(n) = O(\\max(g_1(n), g_2(n)))$。 如果 $f(n) = O(g(n))$ 且 $g(n) = O(h(n))$，则 $f(n) = O(h(n))$（传递性）。 如果 $c$ 是常数，且 $f(n) = O(g(n))$，则 $c \\cdot f(n) = O(g(n))$。 大Ω记号的性质：\n如果 $f_1(n) = \\Omega(g_1(n))$ 且 $f_2(n) = \\Omega(g_2(n))$，则 $f_1(n) + f_2(n) = \\Omega(\\min(g_1(n), g_2(n)))$。 如果 $f(n) = \\Omega(g(n))$ 且 $g(n) = \\Omega(h(n))$，则 $f(n) = \\Omega(h(n))$（传递性）。 如果 $c$ 是常数，且 $f(n) = \\Omega(g(n))$，则 $c \\cdot f(n) = \\Omega(g(n))$。 大Θ记号的性质：\n如果 $f_1(n) = \\Theta(g_1(n))$ 且 $f_2(n) = \\Theta(g_2(n))$，则 $f_1(n) + f_2(n) = \\Theta(\\max(g_1(n), g_2(n)))$。 如果 $f(n) = \\Theta(g(n))$ 且 $g(n) = \\Theta(h(n))$，则 $f(n) = \\Theta(h(n))$（传递性）。 如果 $c$ 是常数，且 $f(n) = \\Theta(g(n))$，则 $c \\cdot f(n) = \\Theta(g(n))$。 8. NPC # 考点1：NPC的定义是什么？介绍NPC理论；P、NP、NPC 含义和实际应用场景；发现一个NPC问题； 考点2：列举一些NPC问题 子集和问题 哈米尔顿图、哈米尔顿回路 01背包 装箱问题 考点3：怎么证装箱问题和01背包是NPC问题 首先证明NP，写NP算法； step1：猜测；genCertificate()这一步；猜k次；包含k个点的组合；并行多道；O(1)时间； step2：验证；任意两个点之间都有边，就是完全子图； 如果满足条件，输出yes 然后证明它们是NP Hard 问题 （没听懂老师说的）能不能用另一个NP定义？多项式时间内验证、证明，找完全子图；blablabla\u0026hellip; 也是可以的 如何证明装箱问题和01背包是NP Hard 问题？ 会给这样给题目：已知装箱问题是NP Hard 问题，请你给出多项式规约，证明 01背包和装箱问题其实是等价的，因此01背包是NP Hard 问题 证明过程后续会在moodle上给出详细的pdf资料，认真看就行。 多项式规约的含义？规约=证明问题等价。多项式规约意思是我们能用一个多项式时间复杂度的算法，将问题A的输入和约束转换为已解决问题B的输入和约束。这个转换过程就是多项式规约。 P、NP、NPC 含义及实际应用场景\nhttps://blog.csdn.net/huang1024rui/article/details/49154507\nP 类问题（Polynomial time problems）\n含义：可以在多项式时间内（即时间复杂度为多项式函数）由确定性图灵机解决的问题。 应用场景：许多常见的计算问题，如排序（例如快速排序、归并排序）、搜索（例如二分搜索）等。 NP 类问题（Non-deterministic Polynomial time problems）\n含义：如果一个问题的解可以在多项式时间内由确定性图灵机验证，非确定性图灵机求解，则该问题属于 NP 类。 应用场景：许多复杂的决策和优化问题，如旅行商问题（TSP）、着色问题等。 NPC 类问题（NP-Complete problems）\n含义：如果一个问题既属于 NP 类，又是 NP 难的（NP-Hard），则称该问题为 NP 完全问题（NPC）。NP 完全问题是最难的 NP 类问题。 应用场景：包括许多实际中的复杂问题，如工作调度、资源分配、网络设计等。 NPC 理论\nNPC 理论研究的是问题的计算复杂性，特别是关于问题的可计算性和计算难度。对于一个给定的 NP 完全问题，如果能找到一个多项式时间算法来解决它，那么所有的 NP 问题都可以在多项式时间内解决。这就是著名的 P=NP 问题。\n发现一个 NPC 问题\n发现一个新的 NPC 问题通常涉及两个步骤：\n证明问题属于 NP 类：需要展示一个多项式时间内验证解的算法。 证明问题是 NP 难的（NP-Hard）：需要找到一个已知的 NPC 问题，并通过多项式时间规约（Polynomial-time reduction）将其转化为该问题。 考点2：列举一些 NPC 问题\n子集和问题（Subset Sum Problem）\n描述：给定一个整数集合和一个目标值，确定是否存在一个子集，其元素和等于目标值。 哈米尔顿图、哈米尔顿回路（Hamiltonian Path and Cycle）\n描述：给定一个图，确定是否存在一条经过每个顶点一次的路径（哈米尔顿路径）或回路（哈米尔顿回路）。 01 背包问题（0/1 Knapsack Problem）\n描述：给定一组物品，每个物品有重量和价值，在总重量不超过容量的情况下，选择一些物品使得总价值最大。 装箱问题（Bin Packing Problem）\n描述：给定一组物品和若干个固定容量的箱子，确定如何将物品装入箱子，使得使用的箱子数最少。 考点3：证明装箱问题和 01 背包问题是 NPC 问题\n装箱问题\n首先证明 NP 猜测：生成一个可能的物品分配方案，即猜测每个物品放在哪个箱子里。 验证：检查每个箱子的总重量是否不超过容量。如果所有箱子的重量都符合要求，输出“是”；否则，输出“否”。 证明 NP-Hard 假设有一个已知的 NP 完全问题，如子集和问题。通过多项式时间规约，将子集和问题转换为装箱问题。 为了证明装箱问题和 01 背包问题是 NP-Hard，需要进行多项式规约。 证明装箱问题是 NP-Hard：假设装箱问题是 NP-Hard。将装箱问题转化为 01 背包问题，展示如何使用装箱问题的解来解决 01 背包问题。 01 背包问题\nNP 证明： 猜测：生成一个可能的物品选择方案，即猜测每个物品是否被选择。 验证：检查选择的物品总重量是否不超过容量，并计算总价值。如果总重量符合要求且总价值达到最大，输出“是”；否则，输出“否”。 证明 NP-Hard 为了证明装箱问题和 01 背包问题是 NP-Hard，需要进行多项式规约。 9. 近似算法 # 推荐下面的博文\nhttps://www.jiqizhixin.com/articles/2020-12-30-7\nhttps://www.cnblogs.com/cy0628/p/14015173.html\n考点1：近似算法的定义和如何衡量近似算法的好坏？\n考点2：掌握几种装箱问题的贪心算法；（FF、NF等）\n具体：\n近似定理： 求最小还是最大，始终让近似比大于等于1 让算法的比值越小越接近1，近似精度越好；否则相反； 不管证明；搞懂3种过程（都是贪心） NF算法 Next Fit 放不放得下？ 放得下。放进箱子。 放不下，来个新箱子。 O(n) 只看当前箱子 FF算法 First Fit 从第一个箱子开始判断，会遍历前面的箱子； 放不放得下？ 放得下。放进箱子。 放不下，来个新箱子。 O(nm)约等于O(n^2) FFD算法：先把所有元素排序；然后尽可能装最大的 伪代码：先写排序； 箱子大小是单位1； for-i for-j；i是物品；j是箱子； O(ilogi);O(ij) 不是在线算法；因为输入已经结束了，知道了所有输入； 3个算法哪个是/不是在线算法？复杂度如何？ FFD不是在线算法，其他都是；因为FFD要做排序，做了排序就不是在线算法了（相当于你已经看过所有数据了，即数据输入已经结束了，在线算法意思是要实时接收算法输入） 考点1：近似算法的定义和如何衡量近似算法的好坏？\n近似算法定义：对于某些NP完全问题，近似算法能在多项式时间内给出一个可行解，但该解可能不是最优解。近似算法的输出是输入实例的一个近似解，通常使用多项式时间完成。 衡量标准：近似算法的好坏通过“近似比”（Approximation Ratio）来衡量。对于最小化问题，近似比定义为算法得到的解与最优解的比值的最大可能值。理想情况下，这个比值接近1，表示近似解非常接近最优解。 考点2：掌握几种装箱问题的贪心算法（FF、NF等） NF算法（Next Fit）：\n策略：始终尝试将新物品放入当前打开的箱子中。如果放不下，则开一个新的箱子。 时间复杂度：O(n)，因为它只检查当前的箱子。 特点：是一个在线算法，因为它不需要知道所有的输入。 FF算法（First Fit）：\n策略：从第一个箱子开始尝试，依次检查每个已打开的箱子，看是否能放入当前物品。如果所有箱子都放不下，开一个新的箱子。 时间复杂度：O(nm)，约等于O(n^2)（m为箱子数量）。 特点：也是一个在线算法，因为处理每个物品只依赖之前的箱子状态。 FFD算法（First Fit Decreasing）：\n策略：先将所有物品按大小降序排序，然后使用FF算法尝试装箱。 时间复杂度：O(n log n)（排序）和O(nm)（装箱），总体约O(n^2)。 特点：不是一个在线算法，因为需要在处理前先看到所有的物品来进行排序。 在这三种算法中，FFD算法虽然在处理前需要所有数据（因此不是在线算法），但通常能提供更好的装箱效率。FF和NF都是在线算法，能够逐个处理输入数据，但在最坏情况下它们的表现可能不如FFD算法。\n","date":"12 June 2024","permalink":"/posts/reviews/%E7%A0%94%E7%A9%B6%E7%94%9F%E5%A4%8D%E4%B9%A0/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95/","section":"博客","summary":"南京大学软件学院高级算法复习资料","title":"高级算法考试复习笔记"},{"content":"往年试卷 # 15年试卷 # Where do software architecture come from? List five possible sources of software architecture. What distinguishes an architecture for a software product line from an architecture for a simple product? How to model quality attribute scenarios? Graphically model two quality attributes in \u0026ldquo;stimulus-response\u0026rdquo; format: availability and performance. Describe relationships between architecture patterns and tactics. List four tactics names and describe their usage. Briefly describe the general activities involved in a software architecture process. Mapping, and list 4 views for each style. (sa07, p.9) Explain the context, benefits and limitations of Broker Architecture Pattern. Why should a software architecture be documented using different views? Give the name and purposes of 4 example views. Briefly describe the fundamental principles of SOA and discuss the impact of SOA on quality attributes like interoperability, scalability and security. Describe outputs generated from each phase of ATAM process. Why SPL and MDA have high reusability? Compare and discuss their commonality and differences. 17年考题 # Briefly describe the general activities in a software architecture process, and the major inputs and outputs at each activity. What distinguishes an architecture for a software product line from an architecture for a single product? What are generic design strategies applied in designing software? Give a concise working example with software architecture for each strategy. How to model quality attribute scenarios? Graphically model two quality attributes in “stimulus-response” format: availability and modifiability. Describe outputs generated from each phase of ATAM process. Map, and list four views of each category of style. What are ASR? List four sources and methods for extracting and identifying ASRs. Please name at least three Object-Oriented principles, and explain how they are applied in Strategy pattern? What should be included in a typical software architecture documentation package? Briefly describe each component and its purpose. Describe 4+1 view 软件设计的的三个变化维度，每个维度的变化点。differing binding time如何影响可修改性和可测试性。 18 年考题（梁神回忆） # 软件架构的关注点有哪些？利益相关方有哪些？ Software requirements, quality attributes, ASRs 的区别和联系 What is the nature of component-connector style? 以 MVC pattern 举例 如何对质量属性场景建模？画出 availability 和 modifiability 的刺激-响应图 risks, sensitivity points, trade-off points 是什么？各举一个例子 连线，并对每种 style 列出四种视图 Layered pattern 和 Multi-tier pattern 的区别 描述 ADD 过程 为什么软件架构需要用不同的视图描述？举出四种视图的例子（列出名称和目的） 软件产品线架构如何实现可变性？描述可变性机制的工作方式 设计一个飞行模拟软件，要求能模拟多种飞机的特性。为了在将来支持更多飞机种类，要求使用策略模式。画出架构图和类图 太复杂，忘了 19 年最终考题 （law回忆） # 如何对质量属性场景建模？画出 Interoperability 和 modifiability 的刺激-响应图 What are ASR? List four sources and methods for extracting and identifying ASRs. 4+1 视图介绍（还要画图，我的图画的的有点问题，去wiki百科上可以看） What are generic design strategies applied in designing software? Give a concise working example with software architecture for each strategy. (和17年一样的) Map, and list four views of each category of style.（每年必考题） What should be included in a typical software architecture documentation package? Briefly describe each component and its purpose. （和17年基本一样） 描述 在 ATAM 的每一个过程中 有哪些 stake holder 和他们的职责 软件产品线架构如何实现可变性？描述可变性机制的工作方式，和变化点。 （和去年一样） Explain the context, benefits and limitations of Broker Architecture Pattern. 微服务 和 SOA 的区别，相同点 一个买票系统的设计题，不同的角色有不同的打折方案，用策略模式设计， 最后画图，还要说明策略模式的使用场景。 设计题，和 15 年的设计题一模一样 ","date":"11 June 2024","permalink":"/posts/reviews/%E7%A0%94%E7%A9%B6%E7%94%9F%E5%A4%8D%E4%B9%A0/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/","section":"博客","summary":"南京大学软件学院体系结构复习资料","title":"体系结构考试复习笔记"},{"content":"一、基本概念 # 选品、选客：从庞大的商品库（视频/节目/用户数据）中按照特定的规则筛选出需要的商品（视频/节目/用户数据等）的过程 实体：圈选的主题，比如商品、节目、视频、用户等，不同的实体对应不同类型的选品，比如商品选品、节目选品等 标签：选品等最小单位，比如：性别、年龄、地域、观看时长等 规则：标签的各种组合方式，比如：性别=男 \u0026amp;\u0026amp; 年龄在20到35之间 \u0026amp;\u0026amp; (地域=北京||地域=上海) SCG(standard content group)：标准内容组 二、效果图 # 三、具体设计 # 1、中间件介绍 # mysql：略 es： Elasticsearch 底层数据结构 Greenplum AnalyticDB redis HBase 2、库表设计 # 主要涉及五张表：实体表、类目表、标签表、标签值表、SCG表\n实体表（entity） CREATE TEBALE `entity` ( `id` int NOT NULL AUTO_INCREMENT, `name` varchar(45) NOT NULL COMMENT \u0026#39;实体名称\u0026#39;, `ename` varchar(50) NOT NULL COMMENT \u0026#39;实体唯一标识\u0026#39;, `status` int NOT NULL DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;实体状态 0:正常 1:异常 默认0\u0026#39;, `create_time` varchar(45) DEFAULT NOT NULL COMMENT \u0026#39;创建时间\u0026#39;, `create_user` varchar(45) DEFAULT NOT NULL COMMENT \u0026#39;创建人\u0026#39;, `update_user` varchar(45) DEFAULT NOT NULL COMMENT \u0026#39;更新人\u0026#39;, PRIMARY KEY(`id`), UNIQUE KEY `ename_UNIQUE` (`ename`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE-utf8mb4_0900_ai_ci INSERT INTO `entity` (`name`, `ename`, `status`, `create_time`, `create_user`, `update_user`) VALUES (\u0026#39;实体1\u0026#39;, \u0026#39;entity1\u0026#39;, 0, \u0026#39;2024-06-10 12:00:00\u0026#39;, \u0026#39;creator1\u0026#39;, \u0026#39;updater1\u0026#39;), (\u0026#39;实体2\u0026#39;, \u0026#39;entity2\u0026#39;, 0, \u0026#39;2024-06-10 13:00:00\u0026#39;, \u0026#39;creator2\u0026#39;, \u0026#39;updater2\u0026#39;), (\u0026#39;实体3\u0026#39;, \u0026#39;entity3\u0026#39;, 1, \u0026#39;2024-06-10 14:00:00\u0026#39;, \u0026#39;creator3\u0026#39;, \u0026#39;updater3\u0026#39;); 类目表（category） CREATE TABLE `category` ( `id` int NOT NULL AUTO_INCREMENT, `name` varchar(45) NOT NULL COMMENT \u0026#39;类目名称\u0026#39;, `status` int DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;类目状态 0:正常 1:异常\u0026#39;, `level` int DEFAULT \u0026#39;1\u0026#39; COMMENT \u0026#39;类目级别\u0026#39;, `parent_id` int DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;父类目ID 0代表顶级类目\u0026#39; PRIMATY KEY(`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE-utf8mb4_0900_ai_ci INSERT INTO `category` (`name`, `level`, `parent_id`) VALUES (\u0026#39;基础属性\u0026#39;, 1, 0), (\u0026#39;视频属性\u0026#39;, 1, 0), (\u0026#39;基本特征\u0026#39;, 2, 1), (\u0026#39;地域属性\u0026#39;, 2, 1), (\u0026#39;近30天\u0026#39;, 2, 2), (\u0026#39;近90天\u0026#39;, 2, 2); 标签表（tags） CREATE TABLE `tags` ( `id` int NOT NULL AUTO_INCREMENT, `name` varchar(45) NOT NULL COMMENT \u0026#39;标签名称\u0026#39;, `tag_key` varchar(50) NOT NULL COMMENT \u0026#39;标签唯一标识\u0026#39;, `status` int NOT NULL DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;标签状态 0:正常 1:失效 默认0\u0026#39;, `create_time` varchar(45) NOT NULL COMMENT \u0026#39;创建时间\u0026#39; `update_time` varchar(45) DEFAULT NULL COMMENT \u0026#39;更新时间\u0026#39; `tag_type` varchar(45) DEFAULT \u0026#39;STRING\u0026#39;COMMENT \u0026#39;标签类型 STRING NUMBER BOOLEAN DATE\u0026#39;, `is_multiple` int DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;是否支持多值 0:单值 1:单值 默认0\u0026#39; `category_id` int DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;类目ID\u0026#39; `control_type` varchar(45) DEFAULT NULL COMMENT \u0026#39;控件类型 INPUT SELECTIVE DATE NUMBE_RANGE DATE_RANGE\u0026#39;, PRIMARY KEY(`id`), UNIQUE KEY `ename_UNIQUE` (`tag_key`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE-utf8mb4_0900_ai_ci INSERT INTO `tags` (`name`, `tag_key`, `status`, `create_time`, `update_time`, `tag_type`, `is_multiple`, `category_id`, `control_type`) VALUES (\u0026#39;性别\u0026#39;, \u0026#39;sex\u0026#39;, 0, \u0026#39;2024-06-11 10:00:00\u0026#39;, NULL, \u0026#39;STRING\u0026#39;, 0, 6, \u0026#39;SELECTIVE\u0026#39;), (\u0026#39;姓名\u0026#39;, \u0026#39;name\u0026#39;, 0, \u0026#39;2024-06-11 10:05:00\u0026#39;, NULL, \u0026#39;STRING\u0026#39;, 1, 0, \u0026#39;INPUT\u0026#39;), (\u0026#39;所在城市\u0026#39;, \u0026#39;area\u0026#39;, 0, \u0026#39;2024-06-11 10:10:00\u0026#39;, NULL, \u0026#39;STRING\u0026#39;, 0, 1, \u0026#39;SELECTIVE\u0026#39;), (\u0026#39;看过的剧\u0026#39;, \u0026#39;view_tv\u0026#39;, 0, \u0026#39;2024-06-11 10:15:00\u0026#39;, NULL, \u0026#39;STRING\u0026#39;, 1, 5, \u0026#39;SELECTIVE\u0026#39;); 标签值表（tag_values） 真实业务不会采用mysql存放标签值，可以采用ES/hbase等\nCREATE TABLE `tag_values` ( `id` int NOT NULL AUTO_INCREMENT, `tag_key` varchar(50) NOT NULL COMMENT \u0026#39;标签唯一标识\u0026#39;, `tag_value` varchar(45) NOT NULL COMMENT \u0026#39;标签值\u0026#39; `status` int NOT NULL DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;标签状态 0:正常 1:失效 默认0\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE-utf8mb4_0900_ai_ci INSERT INTO `tag_values` (`tag_key`, `tag_value`) VALUES (\u0026#39;sex\u0026#39;, \u0026#39;男\u0026#39;), (\u0026#39;sex\u0026#39;, \u0026#39;女\u0026#39;), (\u0026#39;area\u0026#39;, \u0026#39;北京市\u0026#39;), (\u0026#39;area\u0026#39;, \u0026#39;上海市\u0026#39;), (\u0026#39;view_tv\u0026#39;, \u0026#39;天龙八部\u0026#39;); (\u0026#39;view_tv\u0026#39;, \u0026#39;理想之城\u0026#39;); SCG表（scg） CREATE TABLE `scg` ( `id` int NOT NULL AUTO_INCREMENT, `name` varchar(45) NOT NULL COMMENT `scg名称`, `status` int NOT NULL DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;状态 0:正常 1:异常 默认0\u0026#39;, `create_time` varchar(45) NOT NULL COMMENT \u0026#39;创建时间\u0026#39;, `update_time` varchar(45) DEFAULT NULL COMMENT \u0026#39;更新时间\u0026#39;, `create_user` varchar(45) DEFAULT NULL COMMENT \u0026#39;创建人\u0026#39;, `update_user` varchar(45) DEFAULT NULL COMMENT \u0026#39;更新人\u0026#39;, `rule` json DEFAULT NULL COMMENT \u0026#39;具体规则 JSON格式\u0026#39;, `entity_ename` varchar(45) NOT NULL COMMENT \u0026#39;SCG所属实体\u0026#39;, `encircle_type` int DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;圈选类型 0:定投 1:选品\u0026#39;, `update_frency` int DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;更新频率 0:秒级别 1:分钟级 2:天级\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE-utf8mb4_0900_ai_ci INSERT INTO `scg` (`name`, `status`, `create_time`, `update_time`, `create_user`, `update_user`, `rule`, `entity_ename`, `encircle_type`, `update_frency`) VALUES (\u0026#39;20到35岁的男性用户\u0026#39;, 0, \u0026#39;2024-06-11 10:00:00\u0026#39;, NULL, \u0026#39;创建用户A\u0026#39;, NULL, \u0026#39;{\u0026#34;condition\u0026#34;: \u0026#34;age \u0026lt;= 35 \u0026amp;\u0026amp; age \u0026gt;= 20\u0026#34;}\u0026#39;, \u0026#39;crowd\u0026#39;, 1, 1), (\u0026#39;状态正常并且隐私等级为公开的视频\u0026#39;, 0, \u0026#39;2024-06-11 11:00:00\u0026#39;, NULL, \u0026#39;创建用户B\u0026#39;, NULL, \u0026#39;{}\u0026#39;, \u0026#39;video\u0026#39;, 1, 1), 3、通用规则设计 # 规则实体类定义：\nCommonSearchQueryDTO（最终规则形式） public class CommonSearchQueryDTO { /** * 查询规则 */ private CommonSearchQueryGroupDTO groupDTO; /** * 排序规则 */ private List\u0026lt;CommonSearchSortDTO\u0026gt; searchSortDTOList; } CommonSearchQueryGroupDTO（查询规则） public class CommonSearchQueryGroupDTO { /** * 策略类型 {@link CommonSearchPolicyTypeEnum} */ private String policyType; /** * 规则单项 */ private List\u0026lt;CommonSearchQueryItemDTO\u0026gt; itemList = new LinkedList\u0026lt;CommonSearchQueryItemDTO\u0026gt;(); /** * 规则分组 */ private List\u0026lt;ComonSearchQueryGroupDTO\u0026gt; groupList = new LinkedList\u0026lt;CommonSearchGroupDTO\u0026gt;(); } CommonSearchSortDTO（排序规则） public class CommonSearchSortDTO { /** * 标签唯一标识 */ private String tagKey; /** * 是否升序 */ private boolean asc; } CommonSearchQueryItemDTO（标签具体信息） public class CommonSearchQueryItemDTO { /** * 标签唯一标识 */ private String tagKey; /** * 操作 {@link CommonSearchOperateEnum} */ private String operate; /** * 包含条件 */ private List\u0026lt;String\u0026gt; terms; /** * 范围条件 */ private CommonSearchQueryRangeDTO range; /** * 精确或全文检索条件 */ private String value; /** * 扩展存储 */ private Map\u0026lt;String, String\u0026gt; extraMap; } CommonSearchOperateEnum（操作类型枚举） public enum CommonSearchOperateEnum { CONTAIN(\u0026#34;包含\u0026#34;), NOT_CONTAIN(\u0026#34;不包含\u0026#34;), RANGE(\u0026#34;在范围内\u0026#34;), EQUAL(\u0026#34;等于\u0026#34;), NOT_EQUAL(\u0026#34;不等于\u0026#34;), LIKE(\u0026#34;模糊匹配\u0026#34;), EXIST(\u0026#34;是否存在\u0026#34;); } CommonSearchQueryRangeDTO（选品策略-范围查询枚举值） public class CommonSearchQueryRangeDTO { /** * 大于，和gte只能有一个出现，同时出现gt字短会被忽视 */ private String gt; /** * 大于等于 */ private String gte; /** * 小于，和lte只能有一个出现，同时出现lt字段会被忽视 */ private String lt; /** * 小于等于 */ private String lte; } CommonSearchPolicyTypeEnum（选品策略-标签之间的关系 与/或） public enum commonSearchPolicyTypeEnum { AND (\u0026#34;以下Group内满足所有条件\u0026#34;), OR (\u0026#34;以下Group内满足任一条件\u0026#34;); } 另外提供一个规则转换类Adb3GenerateManager，用于将规则转化为SQL（也可以转化为ES语法）\nimport java.util.ArrayList; import java.util.List; import com.google.common.collect.Lists; // 假设导入了Google Guava库中的Lists类 import org.apache.commons.lang3.StringUtils; // 假设导入了Apache Commons Lang库中的StringUtils类 public class Adb3GenerateManager { /** * 控制哪些标签支持like */ public static List\u0026lt;String\u0026gt; likeTagKeyList = Lists.newArrayList(\u0026#34;name\u0026#34;); public String generateResultSql(CommonSearchQueryDTO queryDTO, Map\u0026lt;String, TagsDO\u0026gt; tableFieldPropertyMap) { if (queryDTO == null || tableFieldPropertyMap == null || tableFieldPropertyMap.size() == 0) { return \u0026#34;\u0026#34;; } CommonSearchQueryGroupDTO groupDTO = queryDTO.getGroupDTO(); List\u0026lt;CommonSearchSortDTO\u0026gt; searchSortDTOList = queryDTO.getSearchSortDTOList(); String searchSql = generateSearchConditions(groupDTO, tableFieldPropertyMap); String orderSql = generateOrderBys(searchSortDTOList); if (orderSql != null \u0026amp;\u0026amp; orderSql.length() \u0026gt; 0) { searchSql += \u0026#34; order by \u0026#34; + orderSql; } return searchSql; } public String generateOrderBys(List\u0026lt;CommonSearchSortDTO\u0026gt; sorts) { StringBuilder sortSql = new StringBuilder(); // 在SQL中添加排序信息 if (sorts != null \u0026amp;\u0026amp; sorts.size() \u0026gt; 0) { for (CommonSearchSortDTO sort : sorts) { sortSql.append(sort.getTagKey()) .append(\u0026#34; \u0026#34;) .append(sort.isAsc() ? \u0026#34;ASC\u0026#34; : \u0026#34;DESC\u0026#34;).append(\u0026#34;,\u0026#34;); } if (sortSql.length() \u0026gt; 0) { sortSql.deleteCharAt(sortSql.length() - 1); } } return sortSql.toString(); } public String generateSearchConditions(CommonSearchQueryGroupDTO schema,Map\u0026lt;String, TagsDO\u0026gt; tableFieldPropertyMap) { StringBuilder conditions = new StringBuilder(); if (tableFieldPropertyMap == null) { tableFieldPropertyMap = new HashMap(); } if (schema != null) { boolean hasConditions = false; String joinFlag = \u0026#34;AND\u0026#34;; if (CommonSearchPolicyTypeEnum.OR.name().equals(schema.getPolicyType())) { joinFlag = \u0026#34;OR\u0026#34;; } conditions.append(\u0026#34;( \u0026#34;); // 将单个的条件组装多个条件，用AND或OR操作符连接起来 boolean notFirst = false; if (schema.getItemList() != null \u0026amp;\u0026amp; schema.getItemList().size() \u0026gt; 0) { for (CommonSearchQueryItemDTO item : schema.getItemList()) { TagsDO tagsDO = tableFieldPropertyMap.get(item.getTagKey()); String singleCondition = generateSingleSearchCondition(item, tagsDO); if (StringUtils.isNotEmpty(singleCondition)) { if (notFirst) { conditions.append(\u0026#34; \u0026#34;).append(joinFlag).append(\u0026#34; \u0026#34;); } conditions.append(singleCondition); notFirst = true; hasConditions = true; } } } // 如果还有多个Group，递归调用groupList生成其它的条件，不过递归调用时不传递排序和白名单、黑名单数据 if (schema.getGroupList() != null \u0026amp;\u0026amp; schema.getGroupList().size() \u0026gt; 0) { for (CommonSearchQueryGroupDTO group : schema.getGroupList()) { String groupCondition = generateSearchConditions(group, tableFieldPropertyMap); if (StringUtils.isNotEmpty(groupCondition)) { if (notFirst) { conditions.append(\u0026#34; \u0026#34;).append(joinFlag).append(\u0026#34; \u0026#34;); } conditions.append(groupCondition); notFirst = true; hasConditions = true; } } } conditions.append(\u0026#34; )\u0026#34;); // 如果最终发现没有条件，那么要删除conditions中已经初始化的内容 if (false == hasConditions) { conditions.delete(0, conditions.length()); } } return conditions.toString(); } private static boolean valueIsString(TagsDO tagsDO) { if (tagsDO == null) { return false; } String type = tagsDO.getTagType(); if (\u0026#34;STRING\u0026#34;.equals(type)) { return true; } return false; } private static boolean valueIsBoolean(TagsDO tagsDO) { if (tagsDO == null) { return false; } String type = tagsDO.getTagType(); if (\u0026#34;BOOLEAN\u0026#34;.equals(type)) { return true; } return false; } private String generateSingleSearchCondition(CommonSearchQueryItemDTO item, TagsDO tagsDO) { if (item == null || tagsDO == null) { return \u0026#34;\u0026#34;; } CommonSearchOperateEnum operateEnum = CommonSearchOperateEnum.index(item.getOperate()); if (operateEnum == null) { return \u0026#34;\u0026#34;; } switch (operateEnum) { case CONTAIN: case NOT_CONTAIN: return generateContain(item, tagsDO); case EQUAL: case NOT_EQUAL: return generateEqual(item, tagsDO); case RANGE: return generateRange(item, tagsDO); case LIKE: return generateLike(item, tagsDO); case EXIST: return generateExist(item, tagsDO); default: break; } return null; } private String generateExist(CommonSearchQueryItemDTO item, TagsDO tagsDO) { StringBuilder sb = new StringBuilder(); String value = item.getValue(); if (BooleanUtils.toBoolean(value)) { sb.append(item.getTagKey()).append(\u0026#34; IS NOT NULL\u0026#34;); } else { sb.append(item.getTagKey()).append(\u0026#34; IS NULL\u0026#34;); } return sb.toString(); } private String generateLike(CommonSearchQueryItemDTO item, TagsDO tagsDO) { StringBuilder sb = new StringBuilder(); String value = item.getValue(); sb.append(item.getTagKey()).append(\u0026#34; LIKE \u0026#39;%\u0026#34;).append(value).append(\u0026#34;%\u0026#39;\u0026#34;); return sb.toString(); } private String generateRange(CommonSearchQueryItemDTO item, TagsDO tagsDO) { StringBuilder multiConditions = new StringBuilder(); String joinFlag = \u0026#34;\u0026#34;; if (StringUtils.isNotEmpty(item.getRange().getLt())) { String value = item.getRange().getLt(); multiConditions.append(joinFlag).append(item.getTagKey()).append(\u0026#34; \u0026lt; \u0026#34;).append(value); joinFlag = \u0026#34; AND \u0026#34;; } if (StringUtils.isNotEmpty(item.getRange().getLte())) { String value = item.getRange().getLte(); multiConditions.append(joinFlag).append(item.getTagKey()).append(\u0026#34; \u0026lt;= \u0026#34;).append(value); joinFlag = \u0026#34; AND \u0026#34;; } if (StringUtils.isNotEmpty(item.getRange().getGt())) { String value = item.getRange().getGt(); multiConditions.append(joinFlag).append(item.getTagKey()).append(\u0026#34; \u0026gt; \u0026#34;).append(value); joinFlag = \u0026#34; AND \u0026#34;; } if (StringUtils.isNotEmpty(item.getRange().getGte())) { String value = item.getRange().getGte(); multiConditions.append(joinFlag).append(item.getTagKey()).append(\u0026#34; \u0026gt;= \u0026#34;).append(value); } return multiConditions.toString(); } private String generateEqual(CommonSearchQueryItemDTO item, TagsDO tagsDO) { CommonSearchOperateEnum operateEnum = CommonSearchOperateEnum.index(item.getOperate()); String operator = \u0026#34;=\u0026#34;; if (CommonSearchOperateEnum.NOT_EQUAL.equals(operateEnum)) { operator = \u0026#34;!=\u0026#34;; } String value = item.getValue(); if (valueIsString(tagsDO)) { value = \u0026#34;\u0026#39;\u0026#34; + value + \u0026#34;\u0026#39;\u0026#34;; } else if (valueIsBoolean(tagsDO)) { value = String.valueOf(BooleanUtils.toBoolean(value)); } return item.getTagKey() + \u0026#34; \u0026#34; + operator + \u0026#34; \u0026#34; + value; } private static String generateContain(CommonSearchQueryItemDTO item, TagsDO tagsDO) { CommonSearchOperateEnum operateEnum = CommonSearchOperateEnum.index(item.getOperate()); if (likeTagKeyList.contains(item.getTagKey())) { List\u0026lt;String\u0026gt; values = new ArrayList\u0026lt;\u0026gt;(); if (item.getTerms() != null \u0026amp;\u0026amp; item.getTerms().size() \u0026gt; 0) { values.addAll(item.getTerms()); } else if (StringUtils.isNotBlank(item.getValue())) { String trimValue = item.getValue(); String[] valueStrings = trimValue.split(\u0026#34;,\u0026#34;); for (String value : valueStrings) { if (StringUtils.isNotBlank(value)) { values.add(value.trim()); } } } if (values != null \u0026amp;\u0026amp; values.size() \u0026gt; 0) { StringBuilder sb = new StringBuilder(); sb.append(\u0026#34;(\u0026#34;); int index = 0; for (String value : values) { if (StringUtils.isBlank(value.trim())) { continue; } if (index \u0026gt; 0) { sb.append(\u0026#34; OR \u0026#34;); } index++; String tempValue = value; sb.append(item.getTagKey()).append(\u0026#34; like \u0026#39;%\u0026#34;).append(tempValue).append(\u0026#34;%\u0026#39;\u0026#34;); } sb.append(\u0026#34;)\u0026#34;); if (CommonSearchOperateEnum.NOT_CONTAIN.equals(operateEnum)) { return \u0026#34; not \u0026#34; + sb.toString(); } return sb.toString(); } return \u0026#34;\u0026#34;; } // 包含还是不包含应该取terms属性的值，但是如果没有，那就取value，把value当做英文逗号分隔的多个值. StringBuilder values = new StringBuilder(); if (item.getTerms() != null \u0026amp;\u0026amp; item.getTerms().size() \u0026gt; 0) { if (valueIsString(tagsDO)) { item.getTerms().forEach(value -\u0026gt; { String tempValue = value; if (StringUtils.isNotBlank(tempValue)) { values.append(\u0026#34;\u0026#39;\u0026#34;).append(tempValue).append(\u0026#34;\u0026#39;,\u0026#34;); } }); } else { item.getTerms().forEach(value -\u0026gt; { String tempValue = value; if (StringUtils.isNotBlank(tempValue)) { values.append(tempValue).append(\u0026#34;,\u0026#34;); } }); } } else if (StringUtils.isNotBlank(item.getValue())) { String trimValue = item.getValue(); String[] valueStrings = trimValue.split(\u0026#34;,\u0026#34;); if (valueStrings != null \u0026amp;\u0026amp; valueStrings.length \u0026gt; 0) { if (valueIsString(tagsDO)) { for (String value : valueStrings) { String tempValue = value; if (StringUtils.isNotBlank(tempValue)) { values.append(\u0026#34;\u0026#39;\u0026#34;).append(tempValue).append(\u0026#34;\u0026#39;,\u0026#34;); } } } else { for (String value : valueStrings) { String tempValue = value; if (StringUtils.isNotBlank(tempValue)) { values.append(tempValue).append(\u0026#34;,\u0026#34;); } } } } } if (values.length() \u0026gt; 0) { values.deleteCharAt(values.length() - 1); } if (tagsDO.getIsMultiple() != null \u0026amp;\u0026amp; tagsDO.getIsMultiple() == 1) { // ADB存的是多值使用多值语法 if (CommonSearchOperateEnum.NOT_CONTAIN.equals(operateEnum)) { return \u0026#34;( ref(\u0026#34; + item.getTagKey() + \u0026#34;, 0) not in (\u0026#34; + values + \u0026#34;) or \u0026#34; + item.getTagKey() + \u0026#34; is null )\u0026#34;; } // 使用ADB3的多值查询语法查询 return \u0026#34;ref(\u0026#34; + item.getTagKey() + \u0026#34;, 0) in (\u0026#34; + values + \u0026#34;) \u0026#34;; } else { // ADB存的是单值使用in语法 if (CommonSearchOperateEnum.NOT_CONTAIN.equals(operateEnum)) { return \u0026#34;(\u0026#34; + item.getTagKey() + \u0026#34; not in (\u0026#34; + values + \u0026#34;) or \u0026#34; + item.getTagKey() + \u0026#34; is null )\u0026#34;; } return item.getTagKey() + \u0026#34; in (\u0026#34; + values + \u0026#34;) \u0026#34;; } } } 下面是两个例子：\ndemo1: 性别=男 \u0026amp;\u0026amp; 年龄在20-35之间 \u0026amp;\u0026amp; (地域=北京||地域=上海) 的用户，返回结果按照id升序排列，年龄降序\n{ \u0026#34;groupDTO\u0026#34;: { \u0026#34;policyType\u0026#34;: \u0026#34;AND\u0026#34;, \u0026#34;itemList\u0026#34;: [ { \u0026#34;tagKey\u0026#34;: \u0026#34;sex\u0026#34;, \u0026#34;operate\u0026#34;: \u0026#34;EQUAL\u0026#34;, \u0026#34;terms\u0026#34;: null, \u0026#34;range\u0026#34;: null, \u0026#34;value\u0026#34;: \u0026#34;男性\u0026#34;, \u0026#34;extraMap\u0026#34;: null }, { \u0026#34;tagKey\u0026#34;: \u0026#34;age\u0026#34;, \u0026#34;operate\u0026#34;: null, \u0026#34;terms\u0026#34;: null, \u0026#34;range\u0026#34;: { \u0026#34;gt\u0026#34;: null, \u0026#34;gte\u0026#34;: \u0026#34;20\u0026#34;, \u0026#34;lt\u0026#34;: null, \u0026#34;lte\u0026#34;: \u0026#34;35\u0026#34; }, \u0026#34;value\u0026#34;: null, \u0026#34;extraMap\u0026#34;: null }, { \u0026#34;tagKey\u0026#34;: \u0026#34;area\u0026#34;, \u0026#34;operate\u0026#34;: \u0026#34;CONTAIN\u0026#34;, \u0026#34;terms\u0026#34;: [\u0026#34;北京\u0026#34;, \u0026#34;上海\u0026#34;], \u0026#34;range\u0026#34;: null, \u0026#34;value\u0026#34;: null, \u0026#34;extraMap\u0026#34;: null } ], \u0026#34;groupList\u0026#34;: null }, \u0026#34;searchSortDTOList\u0026#34;: [ { \u0026#34;tagkey\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;asc\u0026#34;: true }, { \u0026#34;tagkey\u0026#34;: \u0026#34;age\u0026#34;, \u0026#34;asc\u0026#34;: false } ] } 经过Adb3GenerateManager转换成SQL如下：\n(sex = \u0026#39;男性\u0026#39; AND ref(area, 0) in (\u0026#39;北京\u0026#39;, \u0026#39;上海\u0026#39;)) order by id ASC, age DESC 4、系统架构图 # ","date":"10 June 2024","permalink":"/posts/reviews/%E9%80%89%E5%93%81%E7%B3%BB%E7%BB%9F/","section":"博客","summary":"手把手教你从0到1搭建选品/选客系统","title":"从零开始搭建选品/选客系统"},{"content":"","date":"6 June 2024","permalink":"/posts/architecture/","section":"博客","summary":"软件架构是关于如何组织和设计软件系统的原则和模式的领域，它关注应用程序的结构、组件互操作和数据流。它在高层次上决定软件的整体布局，以满足功能需求和性能要求；计算机体系结构则是计算机硬件组件的布局和互连方式的设计，包括中央处理器、内存、输入输出设备等。计算机体系结构决定了计算机如何执行程序和处理数据，对性能、能效性和可扩展性产生影响。","title":"Architecture"},{"content":"","date":"6 June 2024","permalink":"/tags/ddd/","section":"Tags","summary":"","title":"Ddd"},{"content":"01 | 领域驱动设计：微服务设计为什么要选择DDD？ # 我们知道，微服务设计过程中往往会面临边界如何划定的问题，我经常看到项目团队为微服务到底应该拆多小而争得面红耳赤。不同的人会根据自己对微服务的理解而拆分出不同的微服务，于是大家各执一词，谁也说服不了谁，都觉得自己很有道理。\n那在实际落地过程中，我也确实见过不少项目在面临这种微服务设计困惑时，是靠拍脑袋硬完成的，上线后运维的压力就可想而知了。那是否有合适的理论或设计方法来指导微服务设计呢？当你看到这一讲的题目时，我想你已经知道答案了。\n没错，就是 DDD。那么今天我就给你详细讲解下：“微服务设计为什么要选择领域驱动设计？”\n软件架构模式的演进\n在进入今天的主题之前，我们先来了解下背景。\n我们知道，这些年来随着设备和新技术的发展，软件的架构模式发生了很大的变化。软件架构模式大体来说经历了从单机、集中式到分布式微服务架构三个阶段的演进。随着分布式技术的快速兴起，我们已经进入到了微服务架构时代。\n我们先来分析一下软件架构模式演进的三个阶段。\n第一阶段是单机架构：采用面向过程的设计方法，系统包括客户端 UI 层和数据库两层，采用 C/S 架构模式，整个系统围绕数据库驱动设计和开发，并且总是从设计数据库和字段开始。\n单机架构、集中式架构对应的都是单体架构，这类架构中，所有服务在一个结点部署，如果只部署了一个结点，就叫单机，如果部署了多个结点，就叫集群\n优点：部署简单，所有功能一个war包 缺点：技术栈要统一，代码量太大，启动慢，代码耦合性强，改了一处bug，产生n处bug 文章里写的可扩展性和弹性伸缩开始没搞懂，说的可能是单个功能的扩展和伸缩性，假设一个单体服务包含2个功能，用户登录和修改密码，登录的流量大需要10台机器，修改密码的流量小需要1台机器就够，就没法对登录这个功能进行单个功能进行水平扩展机器，两个功能一起扩展\n第二阶段是集中式架构：采用面向对象的设计方法，系统包括业务接入层、业务逻辑层和数据库层，采用经典的三层架构，也有部分应用采用传统的 SOA 架构。这种架构容易使系统变得臃肿，可扩展性和弹性伸缩性差。\n第三阶段是分布式微服务架构：随着微服务架构理念的提出，集中式架构正向分布式微服务架构演进。微服务架构可以很好地实现应用之间的解耦，解决单体应用扩展性和弹性伸缩能力不足的问题。 我们知道，在单机和集中式架构时代，系统分析、设计和开发往往是独立、分阶段割裂进行的。\n对于老师的这句话，我倒是不完全认同。先不阐明自己的观点，想先谈下老师对于架构的划分。 架构划分不同角度，会有不同的划分，只要合理就行。现在市面上流行的方式，就是将老师说的“单机”和“集中式”架构，都差不多说成了“单机应用架构”，也就是所有的功能都集成在一个应用上。尽管为了系统可用性，会做应用层的负载均衡，做集群，但是，每一台应用的功能是完全相同的，也就是无论请求从哪台机器进，输出的结果都是一样的。所以，市面流行这样的说法 当然，老师提到了“单机”架构，这种采用面向过程的设计方式的架构，即原来的那种使用 C/S 模式的系统。讽刺的是，这类架构，较多使用的计算机语言差不多都是面向对象的 C plus，开发的系统竟然是面向过程的。无论如何，也算是回顾了一下过去的系统架构演变历史。\n现在，回到正题，谈谈老师说的“在单机和集中式架构时代，系统分析、设计和开发往往是独立、分阶段割裂进行的” 我想说，即使到了分布式微服务，一样也会有这类问题的出现。技术Leader们将系统架构设计好，做好了微服务系统，然后，分拆出去，各自团队各自负责。这个时候，其实小团队又进入了老师所说的“单机和集中式架构”，这个时候，也都是各个岗位负责各自的事情。 就拿很多规模较大的互联网公司来说。他们差不多采用敏捷开发方式，业务和系统复杂，进行中台建设，搭建微服务系统。对于单一业务线而言，也是业务线上的 OP 提出需求，然后 PM 进行需求分析，再然后，就是 RD 负责方案设计和代码开发，其实也都是独立分开的\n所以，这句话只能说不够严谨 其实，单体最大的问题不在这里，跟开发流程关系不大，真正的问题在于扩展性，也就是如何快速响应市场变化，这个才是最大的限制\n而且在单机和集中式架构这两种模式下，软件无法快速响应需求和业务的迅速变化，最终错失发展良机。此时，分布式微服务的出现就有点恰逢其时的意思了。\n微服务设计和拆分的困境\n那进入微服务架构时代以后，微服务确实也解决了原来采用集中式架构的单体应用的很多问题，比如扩展性、弹性伸缩能力、小规模团队的敏捷开发等等。\n但在看到这些好处的同时，微服务实践过程中也产生了不少的争论和疑惑：微服务的粒度应该多大呀？微服务到底应该如何拆分和设计呢？微服务的边界应该在哪里？\n可以说，很久以来都没有一套系统的理论和方法可以指导微服务的拆分，包括微服务架构模式的提出者 Martin Fowler 在提出微服务架构的时候，也没有告诉我们究竟应该如何拆分微服务。\n于是，在这段较长的时间里，就有不少人对微服务的理解产生了一些曲解。有人认为：“微服务很简单，不过就是把原来一个单体包拆分为多个部署包，或者将原来的单体应用架构替换为一套支持微服务架构的技术框架，就算是微服务了。” 还有人说：“微服务嘛，就是要微要小，拆得越小效果越好。”\n但我想，这两年，你在技术圈中一定听说过一些项目因为前期微服务拆分过度，导致项目复杂度过高，无法上线和运维。\n综合来看，我认为微服务拆分困境产生的根本原因就是不知道业务或者微服务的边界到底在什么地方。换句话说，确定了业务边界和应用边界，这个困境也就迎刃而解了。\n那如何确定，是否有相关理论或知识体系支持呢？在回答这些问题之前，我们先来了解一下领域驱动设计与微服务的前世今生。\n2004 年埃里克·埃文斯（Eric Evans）发表了《领域驱动设计》（Domain-Driven Design –Tackling Complexity in the Heart of Software）这本书，从此领域驱动设计（Domain Driven Design，简称 DDD）诞生。DDD 核心思想是通过领域驱动设计方法定义领域模型，从而确定业务和应用边界，保证业务模型与代码模型的一致性。\n但 DDD 提出后在软件开发领域一直都是“雷声大，雨点小”！直到 Martin Fowler 提出微服务架构，DDD 才真正迎来了自己的时代。\n有些熟悉 DDD 设计方法的软件工程师在进行微服务设计时，发现可以利用 DDD 设计方法来建立领域模型，划分领域边界，再根据这些领域边界从业务视角来划分微服务边界。而按照 DDD 方法设计出的微服务的业务和应用边界都非常合理，可以很好地实现微服务内部和外部的“高内聚、低耦合”。于是越来越多的人开始把 DDD 作为微服务设计的指导思想。\n现在，很多大型互联网企业已经将 DDD 设计方法作为微服务的主流设计方法了。DDD 也从过去“雷声大，雨点小”，开始真正火爆起来。\n为什么 DDD 适合微服务？\n“众里寻他千百度。蓦然回首，那人却在灯火阑珊处。”在经历了多年的迷茫和争论后，微服务终于寻到了他的心上人。\n那 DDD 到底是何方神圣，拥有什么神器呢？\nDDD 是一种处理高度复杂领域的设计思想，它试图分离技术实现的复杂性，并围绕业务概念构建领域模型来控制业务的复杂性，以解决软件难以理解，难以演进的问题。DDD 不是架构，而是一种架构设计方法论，它通过边界划分将复杂业务领域简单化，帮我们设计出清晰的领域和应用边界，可以很容易地实现架构演进。\nDDD 包括战略设计和战术设计两部分。\n战略设计主要从业务视角出发，建立业务领域模型，划分领域边界，建立通用语言的限界上下文，限界上下文可以作为微服务设计的参考边界。\n战术设计则从技术视角出发，侧重于领域模型的技术实现，完成软件开发和落地，包括：聚合根、实体、值对象、领域服务、应用服务和资源库等代码逻辑的设计和实现。\n我们不妨来看看 DDD 是如何进行战略设计的。\nDDD 战略设计会建立领域模型，领域模型可以用于指导微服务的设计和拆分。事件风暴是建立领域模型的主要方法，它是一个从发散到收敛的过程。它通常采用用例分析、场景分析和用户旅程分析，尽可能全面不遗漏地分解业务领域，并梳理领域对象之间的关系，这是一个发散的过程。事件风暴过程会产生很多的实体、命令、事件等领域对象，我们将这些领域对象从不同的维度进行聚类，形成如聚合、限界上下文等边界，建立领域模型，这就是一个收敛的过程。\n我们可以用三步来划定领域模型和微服务的边界。\n作者的划分微服务的步骤是从下到上，从小到大的方式，是一个聚合过程。 而不是从上到下，从大到小的方式，这样的方式是一个拆分过程。\n第一步：在事件风暴中梳理业务过程中的用户操作、事件以及外部依赖关系等，根据这些要素梳理出领域实体等领域对象。\n第二步：根据领域实体之间的业务关联性，将业务紧密相关的实体进行组合形成聚合，同时确定聚合中的聚合根、值对象和实体。在这个图里，聚合之间的边界是第一层边界，它们在同一个微服务实例中运行，这个边界是逻辑边界，所以用虚线表示。\n第三步：根据业务及语义边界等因素，将一个或者多个聚合划定在一个限界上下文内，形成领域模型。在这个图里，限界上下文之间的边界是第二层边界，这一层边界可能就是未来微服务的边界，不同限界上下文内的领域逻辑被隔离在不同的微服务实例中运行，物理上相互隔离，所以是物理边界，边界之间用实线来表示。\n有了这两层边界，微服务的设计就不是什么难事了。\n根据DDD设计微服务：\n网关（与DDD无关） bff层，面向客户端的服务，通过服务聚合/编排形成对外开放的接口，根据客户端场景创建 领域层，后端的微服务应用，其接口不对外开放 聚合层，领域内部的逻辑化分 在战略设计中我们建立了领域模型，划定了业务领域的边界，建立了通用语言和限界上下文，确定了领域模型中各个领域对象的关系。到这儿，业务端领域模型的设计工作基本就完成了，这个过程同时也基本确定了应用端的微服务边界。\n在从业务模型向微服务落地的过程中，也就是从战略设计向战术设计的实施过程中，我们会将领域模型中的领域对象与代码模型中的代码对象建立映射关系，将业务架构和系统架构进行绑定。当我们去响应业务变化调整业务架构和领域模型时，系统架构也会同时发生调整，并同步建立新的映射关系。\n使用了DDD方法论就会有这种效果：适应变化！因为DDD这种单纯的业务和技术的映射就产生了业务变化技术就跟着变化的效果。业务就像是一个生态园林，而有了DDD的方法论就让技术有了生存之道能够适应这个环境。达尔文的适者生存法则无论在哪个领域都有效！DDD就是适者生存的法则\nDDD 与微服务的关系\n有了上面的讲解，现在我们不妨再次总结下 DDD 与微服务的关系。\nDDD 是一种架构设计方法，微服务是一种架构风格，两者从本质上都是为了追求高响应力，而从业务视角去分离应用系统建设复杂度的手段。两者都强调从业务出发，其核心要义是强调根据业务发展，合理划分领域边界，持续调整现有架构，优化现有代码，以保持架构和代码的生命力，也就是我们常说的演进式架构。\nDDD 主要关注：从业务领域视角划分领域边界，构建通用语言进行高效沟通，通过业务抽象，建立领域模型，维持业务和代码的逻辑一致性。 微服务主要关注：运行时的进程间通信、容错和故障隔离，实现去中心化数据管理和去中心化服务治理，关注微服务的独立开发、测试、构建和部署。 02 | 领域、子域、核心域、通用域和支撑域：傻傻分不清？ # DDD 的知识体系提出了很多的名词，像：领域、子域、核心域、通用域、支撑域、限界上下文、聚合、聚合根、实体、值对象等等，非常多。这些名词，都是关键概念，但它们实在有些晦涩难懂，可能导致你还没开始实践 DDD 就打起了退堂鼓。\n除此之外，我想说的是，这些名词在你的微服务设计和开发过程中不一定都用得上，但它可以帮你理解 DDD 的核心设计思想和理念。而这些思想和理念，在 IT 战略设计、业务建模和微服务设计中都是可以借鉴的。\n如何理解领域和子域？\n我们先看一下汉语词典中对领域的解释：“领域是从事一种专门活动或事业的范围、部类或部门。”百度百科对领域的解释：“领域具体指一种特定的范围或区域。”\n两个解释有一个共同点——范围。对了！领域就是用来确定范围的，范围即边界，这也是 DDD 在设计中不断强调边界的原因。\n在研究和解决业务问题时，DDD 会按照一定的规则将业务领域进行细分，当领域细分到一定的程度后，DDD 会将问题范围限定在特定的边界内，在这个边界内建立领域模型，进而用代码实现该领域模型，解决相应的业务问题。简言之，DDD 的领域就是这个边界内要解决的业务问题域。\n领域就是公司所从事的所有业务范围\n既然领域是用来限定业务边界和范围的，那么就会有大小之分，领域越大，业务范围就越大，反之则相反。\n领域可以进一步划分为子领域。我们把划分出来的多个子领域称为子域，每个子域对应一个更小的问题域或更小的业务范围。\n我们知道，DDD 是一种处理高度复杂领域的设计思想，它试图分离技术实现的复杂度。那么面对错综复杂的业务领域，DDD 是如何使业务从复杂变得简单，更容易让人理解，技术实现更容易呢？\n核心是处理复杂业务领域，重点在业务，复杂。它非技术方案。\n其实很好理解，DDD 的研究方法与自然科学的研究方法类似。当人们在自然科学研究中遇到复杂问题时，通常的做法就是将问题一步一步地细分，再针对细分出来的问题域，逐个深入研究，探索和建立所有子域的知识体系。当所有问题子域完成研究时，我们就建立了全部领域的完整知识体系了。\n我们来看一下上面这张图。这个例子是在讲如何给桃树建立一个完整的生物学知识体系。初中生物课其实早就告诉我们研究方法了。它的研究过程是这样的。\n第一步：确定研究对象，即研究领域，这里是一棵桃树。\n第二步：对研究对象进行细分，将桃树细分为器官，器官又分为营养器官和生殖器官两种。其中营养器官包括根、茎和叶，生殖器官包括花、果实和种子。桃树的知识体系是我们已经确定要研究的问题域，对应 DDD 的领域。根、茎、叶、花、果实和种子等器官则是细分后的问题子域。这个过程就是 DDD 将领域细分为多个子域的过程。\n第三步：对器官进行细分，将器官细分为组织。比如，叶子器官可细分为保护组织、营养组织和输导组织等。这个过程就是 DDD 将子域进一步细分为多个子域的过程。\n第四步：对组织进行细分，将组织细分为细胞，细胞成为我们研究的最小单元。细胞之间的细胞壁确定了单元的边界，也确定了研究的最小边界。\n这里先剧透一点聚合、聚合根、实体以及值对象的内容，我还会在 [第 04 讲] 和 [第 05 讲] 中详细讲解。\n我们知道细胞核、线粒体、细胞膜等物质共同构成细胞，这些物质一起协作让细胞具有这类细胞特定的生物功能。在这里你可以把细胞理解为 DDD 的聚合，细胞内的这些物质就可以理解为聚合里面的聚合根、实体以及值对象等，在聚合内这些实体一起协作完成特定的业务功能。这个过程类似 DDD 设计时，确定微服务内功能要素和边界的过程。\n这里总结一下，就是说每一个细分的领域都会有一个知识体系，也就是 DDD 的领域模型。在所有子域的研究完成后，我们就建立了全域的知识体系了，也就建立了全域的领域模型。\n上面我们用自然科学研究的方法，说明了领域可以通过细分为子域的方法，来降低研究的复杂度。现在我们把这个话题再切换到业务领域，对比验证下，二者的细分过程是否是一致的。这里以我从事的保险行业为例。\n保险是个比较大的领域，很早以前的保险核心系统把所有的功能都放在一个系统里来实现，这个系统就是我们常说的单体系统。后来单体系统开始无法适应保险业务的发展，因此保险公司开始了中台转型，引入分布式微服务架构来替换原来的单体系统。而分布式微服务架构就需要划分业务领域边界，建立领域模型，并实现微服务落地了。\n为实现保险领域建模和微服务建设，我们可以根据业务关联度以及流程边界将保险领域细分为：承保、收付、再保以及理赔等子域，而承保子域还可以继续细分为投保、保全（寿险）、批改（财险）等子子域。\n这可以理解为划分领域的方法论。通过业务关联度和业务流程来划分领域\n在投保这个限界上下文内可以建立投保的领域模型，投保的领域模型最后映射到系统就是投保微服务。这就是一个保险领域的细分和微服务的建设过程。\n那么你可能会说，我不是保险行业的人，我怎么理解这个过程呢？我认为，不同行业的业务模型可能会不一样，但领域建模和微服务建设的过程和方法基本类似，其核心思想就是将问题域逐步分解，降低业务理解和系统实现的复杂度。\n如何理解核心域、通用域和支撑域？\n核心域：业务服务，是成功的关键 通用域：公共服务，如认证、鉴权、限流等，是可以买的 支撑域：具有企业特性，数据字典等，是可以外包的。比如Vip分 金vip：1;银vip：2。 在领域不断划分的过程中，领域会细分为不同的子域，子域可以根据自身重要性和功能属性划分为三类子域，它们分别是：核心域、通用域和支撑域。\n为什么需要这样的划分呢？说白了，就是有侧重点呗，给不同的模块分等级，对应到微服务，也就是微服务的重要程度，给的关注、资源也会不一样。\n决定产品和公司核心竞争力的子域是核心域，它是业务成功的主要因素和公司的核心竞争力。没有太多个性化的诉求，同时被多个子域使用的通用功能子域是通用域。还有一种功能子域是必需的，但既不包含决定产品和公司核心竞争力的功能，也不包含通用功能的子域，它就是支撑域。\n支撑域，就是并非通用，但是某个业务域必须依赖的\n这三类子域相较之下，核心域是最重要的，我们下面讲目的的时候还会以核心域为例详细介绍。通用域和支撑域如果对应到企业系统，举例来说的话，通用域则是你需要用到的通用系统，比如认证、权限等等，这类应用很容易买到，没有企业特点限制，不需要做太多的定制化。而支撑域则具有企业特性，但不具有通用性，例如数据代码类的数据字典等系统。\n那为什么要划分核心域、通用域和支撑域，主要目的是什么呢？\n还是拿上图的桃树来说吧。我们将桃树细分为了根、茎、叶、花、果实和种子等六个子域，那桃树是否有核心域？有的话，到底哪个是核心域呢？\n不同的人对桃树的理解是不同的。如果这棵桃树生长在公园里，在园丁的眼里，他喜欢的是“人面桃花相映红”的阳春三月，这时花就是桃树的核心域。但如果这棵桃树生长在果园里，对果农来说，他则是希望在丰收的季节收获硕果累累的桃子，这时果实就是桃树的核心域。\n在不同的场景下，不同的人对桃树核心域的理解是不同的，因此对桃树的处理方式也会不一样。园丁更关注桃树花期的营养，而果农则更关注桃树落果期的营养，有时为了保证果实的营养供给，还会裁剪掉疯长的茎和叶（通用域或支撑域）。\n同样的道理，公司在 IT 系统建设过程中，由于预算和资源有限，对不同类型的子域应有不同的关注度和资源投入策略，记住好钢要用在刀刃上。\n很多公司的业务，表面看上去相似，但商业模式和战略方向是存在很大差异的，因此公司的关注点会不一样，在划分核心域、通用域和支撑域时，其结果也会出现非常大的差异。\n比如同样都是电商平台的淘宝、天猫、京东和苏宁易购，他们的商业模式是不同的。淘宝是 C2C 网站，个人卖家对个人买家，而天猫、京东和苏宁易购则是 B2C 网站，是公司卖家对个人买家。即便是苏宁易购与京东都是 B2C 的模式，他们的商业模式也是不一样的，苏宁易购是典型的传统线下卖场转型成为电商，京东则是直营加部分平台模式。\n商业模式的不同会导致核心域划分结果的不同。有的公司核心域可能在客户服务，有的可能在产品质量，有的可能在物流。在公司领域细分、建立领域模型和系统建设时，我们就要结合公司战略重点和商业模式，找到核心域了，且重点关注核心域。\n如果你的公司刚好有意向转型微服务架构的话，我建议你和你的技术团队要将核心域的建设排在首位，最好是有绝对的掌控能力和自主研发能力，如果资源实在有限的话，可以在支撑域或者通用域上想想办法，暂时采用外购的方式也未尝不可。\n商城就是领域，一个商城可以分为用户，订单，商品等服务这些都是子域。\n子域可以根据自身的重要性跟功能属性划分为核心子域，支撑子域跟通用子域。\n核心子域要具体分析。就拿订单服务而言，相对于商品来说就是核心子域，商品是支撑子域。我们要下单首先得登陆，登陆需要鉴权认证，这类服务就是通用的属于通用子域。\n03 | 限界上下文：定义领域边界的利器 # 在 DDD 领域建模和系统建设过程中，有很多的参与者，包括领域专家、产品经理、项目经理、架构师、开发经理和测试经理等。对同样的领域知识，不同的参与角色可能会有不同的理解，那大家交流起来就会有障碍，怎么办呢？因此，在 DDD 中就出现了“通用语言”和“限界上下文”这两个重要的概念。\n这两者相辅相成，通用语言定义上下文含义，限界上下文则定义领域边界，以确保每个上下文含义在它特定的边界内都具有唯一的含义，领域模型则存在于这个边界之内。你是不是感觉这么描述很抽象？没关系，接下来我会给你一一详细讲解。\n通用语言对事物进行定义，限界上下文描述了定义的有效范围，出了这个范围定义就不明确了。比如使用通用语言对计算机内的易失存储定义为“内存”，在计算机领域内我们能理解它的涵义。但如果进入了手机领域，“内存”就有了另外的含义。\n在这之前，我想请你先看这样两个问题，这也是这部分内容的核心。\n为什么要提出限界上下文的概念（也就是说除了解决交流障碍这个广义的原因，还有更具体的吗）？ 上下文相当于语言表达中的语境，聚合根和实体对象就相当于句子。所以上下文的生命周期紧限于某个或多个聚合形成的环境内。\n限界上下文在微服务设计中的作用和意义是什么？ 划分边界，拆分微服务，解藕。\n什么是通用语言？\n为了更好地理解限界上下文，回答这两个问题，我们先从通用语言讲起。\n怎么理解通用语言这个概念呢？在事件风暴过程中，通过团队交流达成共识的，能够简单、清晰、准确描述业务涵义和规则的语言就是通用语言。也就是说，通用语言是团队统一的语言，不管你在团队中承担什么角色，在同一个领域的软件生命周期里都使用统一的语言进行交流。\n那么，通用语言的价值也就很明了了，它可以解决交流障碍这个问题，使领域专家和开发人员能够协同合作，从而确保业务需求的正确表达。\n但是，对这个概念的理解，到这里还不够。\n通用语言包含术语和用例场景，并且能够直接反映在代码中。通用语言中的名词可以给领域对象命名，如商品、订单等，对应实体对象；而动词则表示一个动作或事件，如商品已下单、订单已付款等，对应领域事件或者命令。\n通用语言贯穿 DDD 的整个设计过程。作为项目团队沟通和协商形成的统一语言，基于它，你就能够开发出可读性更好的代码，将业务需求准确转化为代码设计。\n下面我带你看一张图，这张图描述了从事件风暴建立通用语言到领域对象设计和代码落地的完整过程。\n1.事件风暴-\u0026gt;2.领域故事分析-\u0026gt;3.提取领域对象-\u0026gt;4.领域对象与代码模型映射-\u0026gt;5.代码落地\n先事件风暴，建立通用语言。在事件风暴的过程中，领域专家会和设计、开发人员一起建立领域模型，在领域建模的过程中会形成通用的业务术语和用户故事。事件风暴也是一个项目团队统一语言的过程。 构建领域对象。通过用户故事分析会形成一个个的领域对象，这些领域对象对应领域模型的业务对象，每一个业务对象和领域对象都有通用的名词术语，并且一一映射。 代码实战。微服务代码模型来源于领域模型，每个代码模型的代码对象跟领域对象一一对应。 这里我再给你分享一条经验，我自己经常用，特别有效。设计过程中我们可以用一些表格，来记录事件风暴和微服务设计过程中产生的领域对象及其属性。比如，领域对象在 DDD 分层架构中的位置、属性、依赖关系以及与代码模型对象的映射关系等。\n下面是一个微服务设计实例的部分数据，表格中的这些名词术语就是项目团队在事件风暴过程中达成一致、可用于团队内部交流的通用语言。在这个表格里面我们可以看到，DDD 分析过程中所有的领域对象以及它们的属性都被记录下来了，除了 DDD 的领域对象，我们还记录了在微服务设计过程中领域对象所对应的代码对象，并将它们一一映射。\n到这里，我要再强调一次。DDD 分析和设计过程中的每一个环节都需要保证限界上下文内术语的统一，在代码模型设计的时侯就要建立领域对象和代码对象的一一映射，从而保证业务模型和代码模型的一致，实现业务语言与代码语言的统一。\n打通产品和技术的沟通障碍\n如果你做到了这一点，也就是建立了领域对象和代码对象的映射关系，那就可以指导软件开发人员准确无误地按照设计文档完成微服务开发了。即使是不熟悉代码的业务人员，也可以很快找到代码的位置。\n什么是限界上下文？\n那刚刚提到的限界上下文又是用来做什么的呢？\n我们知道语言都有它的语义环境，同样，通用语言也有它的上下文环境。为了避免同样的概念或语义在不同的上下文环境中产生歧义，DDD 在战略设计上提出了“限界上下文”这个概念，用来确定语义所在的领域边界。\n我们可以将限界上下文拆解为两个词：限界和上下文。限界就是领域的边界，而上下文则是语义环境。通过领域的限界上下文，我们就可以在统一的领域边界内用统一的语言进行交流。\n综合一下，我认为限界上下文的定义就是：用来封装通用语言和领域对象，提供上下文环境，保证在领域之内的一些术语、业务相关对象等（通用语言）有一个确切的含义，没有二义性。这个边界定义了模型的适用范围，使团队所有成员能够明确地知道什么应该在模型中实现，什么不应该在模型中实现。\n进一步理解限界上下文\n我们可以通过一些例子进一步理解一下这个概念，不要小看它，彻底弄懂会给你后面实践 DDD 打下一个坚实的基础。\n都说中文这门语言非常丰富，在不同的时空和背景下，同样的一句话会有不同的涵义。有一个例子你应该听说过。\n在一个明媚的早晨，孩子起床问妈妈：“今天应该穿几件衣服呀？”妈妈回答：“能穿多少就穿多少！”\n那到底是穿多还是穿少呢？\n如果没有具体的语义环境，还真不太好理解。但是，如果你已经知道了这句话的语义环境，比如是寒冬腊月或者是炎炎夏日，那理解这句话的涵义就会很容易了。\n所以语言离不开它的语义环境。\n而业务的通用语言就有它的业务边界，我们不大可能用一个简单的术语没有歧义地去描述一个复杂的业务领域。限界上下文就是用来细分领域，从而定义通用语言所在的边界。\n限界上下文就是我们理解通用语言的语义环境，脱离了语义环境，通用语言的含义就不明确了，太大或太小都不利于通用语言的理解，就难以进行清晰的业务划分和技术落地。\n现在我们用一个保险领域的例子来说明下术语的边界。保险业务领域有投保单、保单、批单、赔案等保险术语，它们分别应用于保险的不同业务流程。\n客户投保时，业务人员记录投保信息，系统对应有投保单实体对象。 缴费完成后，业务人员将投保单转为保单，系统对应有保单实体对象，保单实体与投保单实体关联。 如客户需要修改保单信息，保单变为批单，系统对应有批单实体对象，批单实体与保单实体关联。 如果客户发生理赔，生成赔案，系统对应有报案实体对象，报案实体对象与保单或者批单实体关联。 如果你对我从事的保险业不大了解也没关系，电商肯定再熟悉不过了吧？\n正如电商领域的商品一样，商品在不同的阶段有不同的术语，在销售阶段是商品，而在运输阶段则变成了货物。同样的一个东西，由于业务领域的不同，赋予了这些术语不同的涵义和职责边界，这个边界就可能会成为未来微服务设计的边界。看到这，我想你应该非常清楚了，领域边界就是通过限界上下文来定义的。\n限界上下文和微服务的关系\n接下来，我们对这个概念做进一步的延伸。看看限界上下文和微服务具体存在怎样的关系。\n我想你买过车险吧，或者听过吧。车险承保的流程包含了投保、缴费、出单等几个主要流程。如果出险了还会有报案、查勘、定损、理算等理赔流程。\n保险领域还是很复杂的，在这里我用一个简化的保险模型来说明下限界上下文和微服务的关系。这里还会用到之前学到一些基础知识，比如领域和子域。\n首先，领域可以拆分为多个子领域。一个领域相当于一个问题域，领域拆分为子域的过程就是大问题拆分为小问题的过程。在这个图里面保险领域被拆分为：投保、支付、保单管理和理赔四个子域。\n1个领域/问题域：N+个子域 1个子域：N+个限界上下文 1个限界上下文：1个微服务 子域还可根据需要进一步拆分为子子域，比如，支付子域可继续拆分为收款和付款子子域。拆到一定程度后，有些子子域的领域边界就可能变成限界上下文的边界了。\n原子子域就是限界上下文边界\n子域可能会包含多个限界上下文，如理赔子域就包括报案、查勘和定损等多个限界上下文（限界上下文与理赔的子子域领域边界重合）。也有可能子域本身的边界就是限界上下文边界，如投保子域。\n每个领域模型都有它对应的限界上下文，团队在限界上下文内用通用语言交流。领域内所有限界上下文的领域模型构成整个领域的领域模型。\n理论上限界上下文就是微服务的边界。我们将限界上下文内的领域模型映射到微服务，就完成了从问题域到软件的解决方案。\n可以说，限界上下文是微服务设计和拆分的主要依据。在领域模型中，如果不考虑技术异构、团队沟通等其它外部因素，一个限界上下文理论上就可以设计为一个微服务。\n04 | 实体和值对象：从领域模型的基础单元看系统设计 # 这讲我们来学习 DDD 战术设计中的两个重要概念：实体和值对象。\n这两个概念都是领域模型中的领域对象。它们在领域模型中起什么作用，战术设计时如何将它们映射到代码和数据模型中去？就是我们这一讲重点要关注的问题。\n另外，在战略设计向战术设计过渡的这个过程中，理解和区分实体和值对象在不同阶段的形态是很重要的，毕竟阶段不同，它们的形态也会发生变化，这与我们的设计和代码实现密切相关。\n接下来，我们就分别看看实体和值对象的这些问题，从中找找答案。\n实体\n我们先来看一下实体是什么东西？\n在 DDD 中有这样一类对象，它们拥有唯一标识符，且标识符在历经各种状态变更后仍能保持一致。对这些对象而言，重要的不是其属性，而是其延续性和标识，对象的延续性和标识会跨越甚至超出软件的生命周期。我们把这样的对象称为实体。\n应该就是贯穿业务流程的业务数据模型，在每个业务节点它们的属性会变化（比如订单的状态会变化），但对象本身没有变化。\n1. 实体的业务形态\n在 DDD 不同的设计过程中，实体的形态是不同的。在战略设计时，实体是领域模型的一个重要对象。领域模型中的实体是多个属性、操作或行为的载体。在事件风暴中，我们可以根据命令、操作或者事件，找出产生这些行为的业务实体对象，进而按照一定的业务规则将依存度高和业务关联紧密的多个实体对象和值对象进行聚类，形成聚合。你可以这么理解，实体和值对象是组成领域模型的基础单元。\n2. 实体的代码形态\n在代码模型中，实体的表现形式是实体类，这个类包含了实体的属性和方法，通过这些方法实现实体自身的业务逻辑。在 DDD 里，这些实体类通常采用充血模型，与这个实体相关的所有业务逻辑都在实体类的方法中实现，跨多个实体的领域逻辑则在领域服务中实现。\n3. 实体的运行形态\n实体以 DO（领域对象）的形式存在，每个实体对象都有唯一的 ID。我们可以对一个实体对象进行多次修改，修改后的数据和原来的数据可能会大不相同。但是，由于它们拥有相同的 ID，它们依然是同一个实体。比如商品是商品上下文的一个实体，通过唯一的商品 ID 来标识，不管这个商品的数据如何变化，商品的 ID 一直保持不变，它始终是同一个商品。\n4. 实体的数据库形态\n与传统数据模型设计优先不同，DDD 是先构建领域模型，针对实际业务场景构建实体对象和行为，再将实体对象映射到数据持久化对象。\n在领域模型映射到数据模型时，一个实体可能对应 0 个、1 个或者多个数据库持久化对象。大多数情况下实体与持久化对象是一对一。在某些场景中，有些实体只是暂驻静态内存的一个运行态实体，它不需要持久化。比如，基于多个价格配置数据计算后生成的折扣实体。\n而在有些复杂场景下，实体与持久化对象则可能是一对多或者多对一的关系。比如，用户 user 与角色 role 两个持久化对象可生成权限实体，一个实体对应两个持久化对象，这是一对多的场景。再比如，有些场景为了避免数据库的联表查询，提升系统性能，会将客户信息 customer 和账户信息 account 两类数据保存到同一张数据库表中，客户和账户两个实体可根据需要从一个持久化对象中生成，这就是多对一的场景。\n值对象\n值对象相对实体来说，会更加抽象一些，概念上我们会结合例子来讲。\n我们先看一下《实现领域驱动设计》一书中对值对象的定义：通过对象属性值来识别的对象，它将多个相关属性组合为一个概念整体。在 DDD 中用来描述领域的特定方面，并且是一个没有标识符的对象，叫作值对象。\n也就说，值对象描述了领域中的一件东西，这个东西是不可变的，它将不同的相关属性组合成了一个概念整体。当度量和描述改变时，可以用另外一个值对象予以替换。它可以和其它值对象进行相等性比较，且不会对协作对象造成副作用。这部分在后面讲“值对象的运行形态”时还会有例子。\n上面这两段对于定义的阐述，如果你还是觉得有些晦涩，我们不妨“翻译”一下，用更通俗的语言把定义讲清楚。\n简单来说，值对象本质上就是一个集合。那这个集合里面有什么呢？若干个用于描述目的、具有整体概念和不可修改的属性。那这个集合存在的意义又是什么？在领域建模的过程中，值对象可以保证属性归类的清晰和概念的完整性，避免属性零碎。\n这里我举个简单的例子，请看下面这张图：\n人员实体原本包括：姓名、年龄、性别以及人员所在的省、市、县和街道等属性。这样显示地址相关的属性就很零碎了对不对？现在，我们可以将“省、市、县和街道等属性”拿出来构成一个“地址属性集合”，这个集合就是值对象了。\n1. 值对象的业务形态\n值对象是 DDD 领域模型中的一个基础对象，它跟实体一样都来源于事件风暴所构建的领域模型，都包含了若干个属性，它与实体一起构成聚合。\n我们不妨对照实体，来看值对象的业务形态，这样更好理解。本质上，实体是看得到、摸得着的实实在在的业务对象，实体具有业务属性、业务行为和业务逻辑。而值对象只是若干个属性的集合，只有数据初始化操作和有限的不涉及修改数据的行为，基本不包含业务逻辑。值对象的属性集虽然在物理上独立出来了，但在逻辑上它仍然是实体属性的一部分，用于描述实体的特征。\n在值对象中也有部分共享的标准类型的值对象，它们有自己的限界上下文，有自己的持久化对象，可以建立共享的数据类微服务，比如数据字典。\n2. 值对象的代码形态\n值对象在代码中有这样两种形态。如果值对象是单一属性，则直接定义为实体类的属性；如果值对象是属性集合，则把它设计为 Class 类，Class 将具有整体概念的多个属性归集到属性集合，这样的值对象没有 ID，会被实体整体引用。\n我们看一下下面这段代码，person 这个实体有若干个单一属性的值对象，比如 Id、name 等属性；同时它也包含多个属性的值对象，比如地址 address。\n3. 值对象的运行形态\n实体实例化后的 DO 对象的业务属性和业务行为非常丰富，但值对象实例化的对象则相对简单和乏味。除了值对象数据初始化和整体替换的行为外，其它业务行为就很少了。\n值对象嵌入到实体的话，有这样两种不同的数据格式，也可以说是两种方式，分别是属性嵌入的方式和序列化大对象的方式。\n引用单一属性的值对象或只有一条记录的多属性值对象的实体，可以采用属性嵌入的方式嵌入。引用一条或多条记录的多属性值对象的实体，可以采用序列化大对象的方式嵌入。比如，人员实体可以有多个通讯地址，多个地址序列化后可以嵌入人员的地址属性。值对象创建后就不允许修改了，只能用另外一个值对象来整体替换。\n如果你对这两种方式不够了解，可以看看下面的例子。\n案例 1：以属性嵌入的方式形成的人员实体对象，地址值对象直接以属性值嵌入人员实体中。\n案例 2：以序列化大对象的方式形成的人员实体对象，地址值对象被序列化成大对象 Json 串后，嵌入人员实体中。\n这种存储方式增加按照地址进行分页查询的复杂度\n4. 值对象的数据库形态\nDDD 引入值对象是希望实现从“数据建模为中心”向“领域建模为中心”转变，减少数据库表的数量和表与表之间复杂的依赖关系，尽可能地简化数据库设计，提升数据库性能。\n如何理解用值对象来简化数据库设计呢？\n传统的数据建模大多是根据数据库范式设计的，每一个数据库表对应一个实体，每一个实体的属性值用单独的一列来存储，一个实体主表会对应 N 个实体从表。而值对象在数据库持久化方面简化了设计，它的数据库设计大多采用非数据库范式，值对象的属性值和实体对象的属性值保存在同一个数据库实体表中。\n学校学的数据库三范式，也并非用于实际的工作准则中。有时候遵循三范式的设计，反而会让代码或系统变得复杂。\n举个例子，还是基于上述人员和地址那个场景，实体和数据模型设计通常有两种解决方案：\n第一是把地址值对象的所有属性都放到人员实体表中，创建人员实体，创建人员数据表； 第二是创建人员和地址两个实体，同时创建人员和地址两张表。 第一个方案会破坏地址的业务涵义和概念完整性，第二个方案增加了不必要的实体和表，需要处理多个实体和表的关系，从而增加了数据库设计的复杂性。\n那到底应该怎样设计，才能让业务含义清楚，同时又不让数据库变得复杂呢？\n我们可以综合这两个方案的优势，扬长避短。在领域建模时，我们可以把地址作为值对象，人员作为实体，这样就可以保留地址的业务涵义和概念完整性。而在数据建模时，我们可以将地址的属性值嵌入人员实体数据库表中，只创建人员数据库表。这样既可以兼顾业务含义和表达，又不增加数据库的复杂度。\n值对象就是通过这种方式，简化了数据库设计，总结一下就是：在领域建模时，我们可以将部分对象设计为值对象，保留对象的业务涵义，同时又减少了实体的数量；在数据建模时，我们可以将值对象嵌入实体，减少实体表的数量，简化数据库设计。\n另外，也有 DDD 专家认为，要想发挥对象的威力，就需要优先做领域建模，弱化数据库的作用，只把数据库作为一个保存数据的仓库即可。即使违反数据库设计原则，也不用大惊小怪，只要业务能够顺利运行，就没什么关系。\n5. 值对象的优势和局限\n值对象是一把双刃剑，它的优势是可以简化数据库设计，提升数据库性能。但如果值对象使用不当，它的优势就会很快变成劣势。“知彼知己，方能百战不殆”，你需要理解值对象真正适合的场景。\n值对象采用序列化大对象的方法简化了数据库设计，减少了实体表的数量，可以简单、清晰地表达业务概念。这种设计方式虽然降低了数据库设计的复杂度，但却无法满足基于值对象的快速查询，会导致搜索值对象属性值变得异常困难。\n值对象采用属性嵌入的方法提升了数据库的性能，但如果实体引用的值对象过多，则会导致实体堆积一堆缺乏概念完整性的属性，这样值对象就会失去业务涵义，操作起来也不方便。\n值对象通常独立成表，通常会嵌入到关联的实体对象所在表中，嵌入方式有两种：\n大对象序列化（单字段JSON存储）方式，无法针对JSON体内字段快速查询。 多字段嵌入方式，实体对象数据和值对象属性容易混淆。 所以，你可以对照着以上这些优劣势，结合你的业务场景，好好想一想了。那如果在你的业务场景中，值对象的这些劣势都可以避免掉，那就请放心大胆地使用值对象吧。\n实体和值对象的关系\n实体和值对象是微服务底层的最基础的对象，一起实现实体最基本的核心领域逻辑。\n值对象和实体在某些场景下可以互换，很多 DDD 专家在这些场景下，其实也很难判断到底将领域对象设计成实体还是值对象？可以说，值对象在某些场景下有很好的价值，但是并不是所有的场景都适合值对象。你需要根据团队的设计和开发习惯，以及上面的优势和局限分析，选择最适合的方法。\n关于值对象，我还要多说几句。其实，DDD 引入值对象还有一个重要的原因，就是到底领域建模优先还是数据建模优先？\nDDD 提倡从领域模型设计出发，而不是先设计数据模型。前面讲过了，传统的数据模型设计通常是一个表对应一个实体，一个主表关联多个从表，当实体表太多的时候就很容易陷入无穷无尽的复杂的数据库设计，领域模型就很容易被数据模型绑架。可以说，值对象的诞生，在一定程度上，和实体是互补的。\n我们还是以前面的图示为例：\n在领域模型中人员是实体，地址是值对象，地址值对象被人员实体引用。在数据模型设计时，地址值对象可以作为一个属性集整体嵌入人员实体中，组合形成上图这样的数据模型；也可以以序列化大对象的形式加入到人员的地址属性中，前面表格有展示。\n从这个例子中，我们可以看出，同样的对象在不同的场景下，可能会设计出不同的结果。有些场景中，地址会被某一实体引用，它只承担描述实体的作用，并且它的值只能整体替换，这时候你就可以将地址设计为值对象，比如收货地址。而在某些业务场景中，地址会被经常修改，地址是作为一个独立对象存在的，这时候它应该设计为实体，比如行政区划中的地址信息维护。\n经常修改，无需索引的部分可以设计为值对象，其他场景下作为实体，类似于feature字段\n陈述一下我的学习心得：实体和值对象的目的都是抽象聚合若干属性以简化设计和沟通，有了这一层抽象，我们在使用人员实体时，不会产生歧义，在引用地址值对象时，不用列举其全部属性，在同一个限界上下文中，大幅降低误解、缩小偏差，两者的区别如下：\n两者都经过属性聚类形成，实体有唯一性，值对象没有。在本文案例的限界上下文中，人员有唯一性，一旦某个人员被系统纳入管理，它就被赋予了在事件、流程和操作中被唯一识别的能力，而值对象没有也不必具备唯一性。 实体着重唯一性和延续性，不在意属性的变化，属性全变了，它还是原来那个它；值对象着重描述性，对属性的变化很敏感，属性变了，它就不是那个它了。 战略上的思考框架稳定不变，战术上的模型设计却灵活多变，实体和值对象也有可能随着系统业务关注点的不同而更换位置。比如，如果换一个特殊的限界上下文，这个上下文更关注地址，而不那么关注与这个地址产生联系的人员，那么就应该把地址设计成实体，而把人员设计成值对象。 05 | 聚合和聚合根：怎样设计聚合？ # 在事件风暴中，我们会根据一些业务操作和行为找出实体（Entity）或值对象（ValueObject），进而将业务关联紧密的实体和值对象进行组合，构成聚合，再根据业务语义将多个聚合划定到同一个限界上下文（Bounded Context）中，并在限界上下文内完成领域建模。\n实体/值对象 -\u0026gt; 聚合 -\u0026gt; 划定限界上下文（上下文边界） -\u0026gt; 领域建模\n那你知道为什么要在限界上下文和实体之间增加聚合和聚合根这两个概念吗？它们的作用是什么？怎么设计聚合？这就是我们这一讲重点要关注的问题。\n聚合\n在 DDD 中，实体和值对象是很基础的领域对象。实体一般对应业务对象，它具有业务属性和业务行为；而值对象主要是属性集合，对实体的状态和特征进行描述。但实体和值对象都只是个体化的对象，它们的行为表现出来的是个体的能力。\n属性只是对实体的状态特征描述，实体具有业务属性和业务行为\n那聚合在其中起什么作用呢？\n举个例子。社会是由一个个的个体组成的，象征着我们每一个人。随着社会的发展，慢慢出现了社团、机构、部门等组织，我们开始从个人变成了组织的一员，大家可以协同一致的工作，朝着一个最大的目标前进，发挥出更大的力量。\n领域模型内的实体和值对象就好比个体，而能让实体和值对象协同工作的组织就是聚合，它用来确保这些领域对象在实现共同的业务逻辑时，能保证数据的一致性。\n聚合保证其中包含的实体的数据持久化的一致性\n你可以这么理解，聚合就是由业务和逻辑紧密关联的实体和值对象组合而成的，聚合是数据修改和持久化的基本单元，每一个聚合对应一个仓储，实现数据的持久化。\n聚合保证其中包含的实体的数据持久化的一致性\n聚合有一个聚合根和上下文边界，这个边界根据业务单一职责和高内聚原则，定义了聚合内部应该包含哪些实体和值对象，而聚合之间的边界是松耦合的。按照这种方式设计出来的微服务很自然就是“高内聚、低耦合”的。\n一个聚合包含了多个实体对象和值对象，其中有一个实体对象做为聚合根。这些对象聚集在一起形成了一个比较完整独立的业务边界，称为上下文边界。\n聚合在 DDD 分层架构里属于领域层，领域层包含了多个聚合，共同实现核心业务逻辑。聚合内实体以充血模型实现个体业务能力，以及业务逻辑的高内聚。跨多个实体的业务逻辑通过领域服务来实现，跨多个聚合的业务逻辑通过应用服务来实现。比如有的业务场景需要同一个聚合的 A 和 B 两个实体来共同完成，我们就可以将这段业务逻辑用领域服务来实现；而有的业务逻辑需要聚合 C 和聚合 D 中的两个服务共同完成，这时你就可以用应用服务来组合这两个服务。\n根据下文所提”比如有的业务场景需要同一个聚合的 A 和 B 两个实体来共同完成，我们就可以将这段业务逻辑用领域服务来实现“ 这么说聚合其实仅仅是把多个实体、值对象组合在一起，并提供其对应的安全访问入口，实际并不组织业务逻辑。这个事情是交给领域服务完成的。比如：\n聚合：Item 实体1：Item.sku 实体2：Item.category 领域服务：ItemModifyDomainService 商品信息修改方法：ItemModifyDomainService.modify(){ item.sku.update(); item.category.update(); } 聚合根\n聚合根的主要目的是为了避免由于复杂数据模型缺少统一的业务规则控制，而导致聚合、实体之间数据不一致性的问题。\n聚合了多个实体和值对象后，需要寻找一个主负责人(聚合)来对外提供一致性的访问。传统开发上，每个实体都可以被其他业务逻辑引入和修改，增加了维护一致性的难度。\n例如，A实体表也对外暴露了一个对应的interface并公开，很多外部逻辑可以直接引用进行update，没统一归口就会增加代码的维护难度。\n传统数据模型中的每一个实体都是对等的，如果任由实体进行无控制地调用和数据修改，很可能会导致实体之间数据逻辑的不一致。而如果采用锁的方式则会增加软件的复杂度，也会降低系统的性能。\n平时我们的代码方法，到处都有其他的业务调用，导致没有规律，如果需要改动其中一个方法的话其他的地方都需要相应的修改，这时候把方法比喻图，每个方法就是其中的node，错乱交接，毫无规律。 聚合根类似树一样，有条理的上下级，下级的领域都只能通过根节点访问\n如果把聚合比作组织，那聚合根就是这个组织的负责人。聚合根也称为根实体，它不仅是实体，还是聚合的管理者。\n首先它作为实体本身，拥有实体的属性和业务行为，实现自身的业务逻辑。\n其次它作为聚合的管理者，在聚合内部负责协调实体和值对象按照固定的业务规则协同完成共同的业务逻辑。\n最后在聚合之间，它还是聚合对外的接口人，以聚合根 ID 关联的方式接受外部任务和请求，在上下文内实现聚合之间的业务协同。也就是说，聚合之间通过聚合根 ID 关联引用，如果需要访问其它聚合的实体，就要先访问聚合根，再导航到聚合内部实体，外部对象不能直接访问聚合内实体。\n怎样设计聚合？\nDDD 领域建模通常采用事件风暴，它通常采用用例分析、场景分析和用户旅程分析等方法，通过头脑风暴列出所有可能的业务行为和事件，然后找出产生这些行为的领域对象，并梳理领域对象之间的关系，找出聚合根，找出与聚合根业务紧密关联的实体和值对象，再将聚合根、实体和值对象组合，构建聚合。\n下面我们以保险的投保业务场景为例，看一下聚合的构建过程主要都包括哪些步骤。\n第 1 步：采用事件风暴，根据业务行为，梳理出在投保过程中发生这些行为的所有的实体和值对象，比如投保单、标的、客户、被保人等等。 第 2 步：从众多实体中选出适合作为对象管理者的根实体，也就是聚合根。判断一个实体是否是聚合根，你可以结合以下场景分析：是否有独立的生命周期？是否有全局唯一 ID？是否可以创建或修改其它对象？是否有专门的模块来管这个实体。图中的聚合根分别是投保单和客户实体。 第 3 步：根据业务单一职责和高内聚原则，找出与聚合根关联的所有紧密依赖的实体和值对象。构建出 1 个包含聚合根（唯一）、多个实体和值对象的对象集合，这个集合就是聚合。在图中我们构建了客户和投保这两个聚合。 第 4 步：在聚合内根据聚合根、实体和值对象的依赖关系，画出对象的引用和依赖模型。这里我需要说明一下：投保人和被保人的数据，是通过关联客户 ID 从客户聚合中获取的，在投保聚合里它们是投保单的值对象，这些值对象的数据是客户的冗余数据，即使未来客户聚合的数据发生了变更，也不会影响投保单的值对象数据。从图中我们还可以看出实体之间的引用关系，比如在投保聚合里投保单聚合根引用了报价单实体，报价单实体则引用了报价规则子实体。 第 5 步：多个聚合根据业务语义和上下文一起划分到同一个限界上下文内。 这就是一个聚合诞生的完整过程了。\n聚合的一些设计原则\n在一致性边界内建模真正的不变条件。聚合用来封装真正的不变性，而不是简单地将对象组合在一起。聚合内有一套不变的业务规则，各实体和值对象按照统一的业务规则运行，实现对象数据的一致性，边界之外的任何东西都与该聚合无关，这就是聚合能实现业务高内聚的原因。 设计小聚合。如果聚合设计得过大，聚合会因为包含过多的实体，导致实体之间的管理过于复杂，高频操作时会出现并发冲突或者数据库锁，最终导致系统可用性变差。而小聚合设计则可以降低由于业务过大导致聚合重构的可能性，让领域模型更能适应业务的变化。 通过唯一标识引用其它聚合。聚合之间是通过关联外部聚合根 ID 的方式引用，而不是直接对象引用的方式。外部聚合的对象放在聚合边界内管理，容易导致聚合的边界不清晰，也会增加聚合之间的耦合度。 在边界之外使用最终一致性。聚合内数据强一致性，而聚合之间数据最终一致性。在一次事务中，最多只能更改一个聚合的状态。如果一次业务操作涉及多个聚合状态的更改，应采用领域事件的方式异步修改相关的聚合，实现聚合之间的解耦（相关内容我会在领域事件部分详解）。 通过应用层实现跨聚合的服务调用。为实现微服务内聚合之间的解耦，以及未来以聚合为单位的微服务组合和拆分，应避免跨聚合的领域服务调用和跨聚合的数据库表关联。 上面的这些原则是 DDD 的一些通用的设计原则，还是那句话：“适合自己的才是最好的。”在系统设计过程时，你一定要考虑项目的具体情况，如果面临使用的便利性、高性能要求、技术能力缺失和全局事务管理等影响因素，这些原则也并不是不能突破的，总之一切以解决实际问题为出发点。\n","date":"6 June 2024","permalink":"/posts/architecture/ddd/%E5%9F%BA%E7%A1%80%E7%AF%87/","section":"博客","summary":"基于 DDD 的微服务拆分与设计","title":"DDD实战课-基础篇"},{"content":"","date":"6 June 2024","permalink":"/posts/architecture/ddd/","section":"博客","summary":"DDD（领域驱动设计）是一种软件开发方法，强调以业务领域为核心，通过与领域专家合作，构建反映业务逻辑的模型。它注重领域语言、一致性边界和聚合，提升软件灵活性和可维护性。","title":"DDD领域驱动"},{"content":"","date":"6 June 2024","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"kubernetes 是一个开源的容器编排平台，用于自动化、部署、扩展和运维容器化应用，提供高度可扩展性和强大的管理工具。\n","date":"1 June 2024","permalink":"/posts/architecture/virtualization/k8s/","section":"博客","summary":"kubernetes 是一个开源的容器编排平台，用于自动化、部署、扩展和运维容器化应用，提供高度可扩展性和强大的管理工具。","title":"kubernetes"},{"content":"从进程开始 # 在前面的 4 篇预习文章中，我梳理了“容器”这项技术的来龙去脉，通过这些内容，我希望你能理解如下几个事实：\n容器技术的兴起源于 PaaS 技术的普及； Docker 公司发布的 Docker 项目具有里程碑式的意义； Docker 项目通过“容器镜像”，解决了应用打包这个根本性难题。 紧接着，我详细介绍了容器技术圈在过去五年里的“风云变幻”，而通过这部分内容，我希望你能理解这样一个道理：容器本身没有价值，有价值的是\u0026quot;容器编排\u0026quot;。\n也正因为如此，容器技术生态才爆发了一场关于“容器编排”的“战争”。而这次战争，最终以 Kubernetes 项目和 CNCF 社区的胜利而告终。所以，这个专栏后面的内容，我会以 Docker 和 Kubernetes 项目为核心，为你详细介绍容器技术的各项实践与其中的原理。\n不过在此之前，你还需要搞清楚一个更为基础的问题：容器，到底是怎么一回事儿？\n容器其实是一种沙盒技术。顾名思义，沙盒就是能够像一个集装箱一样，把你的应用“装”起来的技术。这样，应用与应用之间，就因为有了边界而不至于相互干扰；而被装进集装箱的应用，也可以被方便地搬来搬去，这不就是 PaaS 最理想的状态嘛。\n不过，这两个能力说起来简单，但要用技术手段去实现它们，可能大多数人就无从下手了。\n所以，我就先来跟你说说这个“边界”的实现手段。\n假如，现在你要写一个计算加法的小程序，这个程序需要的输入来自于一个文件，计算完成后的结果则输出到另一个文件中。\n由于计算机只认识 0 和 1，所以无论用哪种语言编写这段代码，最后都需要通过某种方式翻译成二进制文件，才能在计算机操作系统中运行起来。\n而为了能够让这些代码正常运行，我们往往还要给它提供数据，比如我们这个加法程序所需要的输入文件。这些数据加上代码本身的二进制文件，放在磁盘上，就是我们平常所说的一个“程序”，也叫代码的可执行镜像（executable image）。\n镜像的概念就是一个持续运行环境的数据化、静态化\n然后，我们就可以在计算机上运行这个“程序”了。\n首先，操作系统从“程序”中发现输入数据保存在一个文件中，所以这些数据就会被加载到内存中待命。同时，操作系统又读取到了计算加法的指令，这时，它就需要指示 CPU 完成加法操作。而 CPU 与内存协作进行加法计算，又会使用寄存器存放数值、内存堆栈保存执行的命令和变量。同时，计算机里还有被打开的文件，以及各种各样的 I/O 设备在不断地调用中修改自己的状态。\n就这样，一旦“程序”被执行起来，它就从磁盘上的二进制文件，变成了计算机内存中的数据、寄存器里的值、堆栈中的指令、被打开的文件，以及各种设备的状态信息的一个集合。像这样一个程序运行起来后的计算机执行环境的总和，就是我们今天的主角：进程。\n所以，对于进程来说，它的静态表现就是程序，平常都安安静静地待在磁盘上；而一旦运行起来，它就变成了计算机里的数据和状态的总和，这就是它的动态表现。\n而容器技术的核心功能，就是通过约束和修改进程的动态表现，从而为其创造出一个“边界”。\n1、约束和修改指的是什么？\n2、什么是约束、什么是修改\n3、怎么约束、怎么修改\n修改：通过linux提供namespace实现 linux中创建进程的方式是通过clone函数，里边可以设置参数（CLONE_NEWPID），它为创建的进程提供了一个新的进程空间，将和原来的进程隔离开。比如：linux中的/bin/bash 的PID 100，用了namespace后 在这个进程中查看PID的情况时，就会发现/bin/bash的PID为 1。\n对于 Docker 等大多数 Linux 容器来说，Cgroups 技术是用来制造约束的主要手段，而 Namespace 技术则是用来修改进程视图的主要方法。\nCgroups资源限制 Namespace隔离资源 你可能会觉得 Cgroups 和 Namespace 这两个概念很抽象，别担心，接下来我们一起动手实践一下，你就很容易理解这两项技术了。\n假设你已经有了一个 Linux 操作系统上的 Docker 项目在运行，比如我的环境是 Ubuntu 16.04 和 Docker CE 18.05。\n接下来，让我们首先创建一个容器来试试。\n容器，其实就是操作系统在启动进程时通过设置一些参数实现了隔离不相关资源后的一个特殊进程\n$ docker run -it busybox /bin/sh / # 这个命令是 Docker 项目最重要的一个操作，即大名鼎鼎的 docker run。\n而 -it 参数告诉了 Docker 项目在启动容器后，需要给我们分配一个文本输入 / 输出环境，也就是 TTY，跟容器的标准输入相关联，这样我们就可以和这个 Docker 容器进行交互了。而 /bin/sh 就是我们要在 Docker 容器里运行的程序。\n所以，上面这条指令翻译成人类的语言就是：请帮我启动一个容器，在容器里执行 /bin/sh，并且给我分配一个命令行终端跟这个容器交互。\n这样，我的 Ubuntu 16.04 机器就变成了一个宿主机，而一个运行着 /bin/sh 的容器，就跑在了这个宿主机里面。\n上面的例子和原理，如果你已经玩过 Docker，一定不会感到陌生。此时，如果我们在容器里执行一下 ps 指令，就会发现一些更有趣的事情：\n/ # ps PID USER TIME COMMAND 1 root 0:00 /bin/sh 10 root 0:00 ps 可以看到，我们在 Docker 里最开始执行的 /bin/sh，就是这个容器内部的第 1 号进程（PID=1），而这个容器里一共只有两个进程在运行。这就意味着，前面执行的 /bin/sh，以及我们刚刚执行的 ps，已经被 Docker 隔离在了一个跟宿主机完全不同的世界当中。\n这究竟是怎么做到的呢？\n本来，每当我们在宿主机上运行了一个 /bin/sh 程序，操作系统都会给它分配一个进程编号，比如 PID=100。这个编号是进程的唯一标识，就像员工的工牌一样。所以 PID=100，可以粗略地理解为这个 /bin/sh 是我们公司里的第 100 号员工，而第 1 号员工就自然是比尔 · 盖茨这样统领全局的人物。\n而现在，我们要通过 Docker 把这个 /bin/sh 程序运行在一个容器当中。这时候，Docker 就会在这个第 100 号员工入职时给他施一个“障眼法”，让他永远看不到前面的其他 99 个员工，更看不到比尔 · 盖茨。这样，他就会错误地以为自己就是公司里的第 1 号员工。\n这种机制，其实就是对被隔离应用的进程空间做了手脚，使得这些进程只能看到重新计算过的进程编号，比如 PID=1。可实际上，他们在宿主机的操作系统里，还是原来的第 100 号进程。\n这种技术，就是 Linux 里面的 Namespace 机制。而 Namespace 的使用方式也非常有意思：它其实只是 Linux 创建新进程的一个可选参数。我们知道，在 Linux 系统中创建进程的系统调用是 clone()，比如：\nint pid = clone(main_function, stack_size, SIGCHLD, NULL); 这个系统调用就会为我们创建一个新的进程，并且返回它的进程号 pid。\n而当我们用 clone() 系统调用创建一个新进程时，就可以在参数中指定 CLONE_NEWPID 参数，比如：\nint pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL); 这时，新创建的这个进程将会“看到”一个全新的进程空间，在这个进程空间里，它的 PID 是 1。之所以说“看到”，是因为这只是一个“障眼法”，在宿主机真实的进程空间里，这个进程的 PID 还是真实的数值，比如 100。\n当然，我们还可以多次执行上面的 clone() 调用，这样就会创建多个 PID Namespace，而每个 Namespace 里的应用进程，都会认为自己是当前容器里的第 1 号进程，它们既看不到宿主机里真正的进程空间，也看不到其他 PID Namespace 里的具体情况。\n而除了我们刚刚用到的 PID Namespace，Linux 操作系统还提供了 Mount、UTS、IPC、Network 和 User 这些 Namespace，用来对各种不同的进程上下文进行“障眼法”操作。\n比如，Mount Namespace，用于让被隔离进程只看到当前 Namespace 里的挂载点信息；Network Namespace，用于让被隔离进程看到当前 Namespace 里的网络设备和配置。\n这，就是 Linux 容器最基本的实现原理了。\n所以，Docker 容器这个听起来玄而又玄的概念，实际上是在创建容器进程时，指定了这个进程所需要启用的一组 Namespace 参数。这样，容器就只能“看”到当前 Namespace 所限定的资源、文件、设备、状态，或者配置。而对于宿主机以及其他不相关的程序，它就完全看不到了。\n所以说，容器，其实是一种特殊的进程而已。\n隔离与限制 # Namespace 技术实际上修改了应用进程看待整个计算机“视图”，即它的“视线”被操作系统做了限制，只能“看到”某些指定的内容。但对于宿主机来说，这些被“隔离”了的进程跟其他进程并没有太大区别。\nNamespace是一种资源逻辑隔离机制，在架构设计时经常用到。\n说到这一点，相信你也能够知道我在上一篇文章最后给你留下的第一个思考题的答案了：在之前虚拟机与容器技术的对比图里，不应该把 Docker Engine 或者任何容器管理工具放在跟 Hypervisor 相同的位置，因为它们并不像 Hypervisor 那样对应用进程的隔离环境负责，也不会创建任何实体的“容器”，真正对隔离环境负责的是宿主机操作系统本身：\nDocker本身并没有对应用进程进行隔离，也没有创建所谓的“容器”，而是通过宿主机的操作系统通过namespace对应用进程进行隔离，通过cgroups对被隔离的应用进程限制相关资源属性\n所以，在这个对比图里，我们应该把 Docker 画在跟应用同级别并且靠边的位置。这意味着，用户运行在容器里的应用进程，跟宿主机上的其他进程一样，都由宿主机操作系统统一管理，只不过这些被隔离的进程拥有额外设置过的 Namespace 参数。而 Docker 项目在这里扮演的角色，更多的是旁路式的辅助和管理工作。\n这里还有一件事情没有说清楚，不同的Docker软件在不同操作系统上的实现，Linux估计是最直接的，但是其他如Windows、Mac这些是如何实现的，另外如果说是Docker软件在负责屏蔽运行时的差异，那哪些情况下会使用虚拟机来运行操作系统，哪些情况下时直接使用原生OS来运行的。\n这里考虑linux\n后续分享 CRI 和容器运行时的时候还会专门介绍，其实像 Docker 这样的角色甚至可以去掉。\n2020年12月，k8s已经决定不使用docker，降低docker相对于容器运行时之外部分的维护成本，直接通过CRI与容器运行时进行交互。\n这样的架构也解释了为什么 Docker 项目比虚拟机更受欢迎的原因。\n这是因为，使用虚拟化技术作为应用沙盒，就必须要由 Hypervisor 来负责创建虚拟机，这个虚拟机是真实存在的，并且它里面必须运行一个完整的 Guest OS 才能执行用户的应用进程。这就不可避免地带来了额外的资源消耗和占用。\n而相比之下，容器化后的用户应用，却依然还是一个宿主机上的普通进程，这就意味着这些因为虚拟化而带来的性能损耗都是不存在的；而另一方面，使用 Namespace 作为隔离手段的容器并不需要单独的 Guest OS，这就使得容器额外的资源占用几乎可以忽略不计。\n所以说，“敏捷”和“高性能”是容器相较于虚拟机最大的优势，也是它能够在 PaaS 这种更细粒度的资源管理平台上大行其道的重要原因。\n敏捷是指快速打包， 快速发布 高性能是指容器进程本来就是系统里的普通进程，没有虚拟化层的性能损失。 但缺点是没有虚拟化层的隔离性 不过，有利就有弊，基于 Linux Namespace 的隔离机制相比于虚拟化技术也有很多不足之处，其中最主要的问题就是：隔离得不彻底。\n容器化对资源隔离的不足，最终它是通过共享宿主机上的操作系统内核来运行的。\nnamespace是对进程的隔离，解决可见性问题 cgroup是对系统资源的隔离，解决资源隔离问题 首先，既然容器只是运行在宿主机上的一种特殊的进程，那么多个容器之间使用的就还是同一个宿主机的操作系统内核。\n尽管你可以在容器里通过 Mount Namespace 单独挂载其他不同版本的操作系统文件，比如 CentOS 或者 Ubuntu，但这并不能改变共享宿主机内核的事实。这意味着，如果你要在 Windows 宿主机上运行 Linux 容器，或者在低版本的 Linux 宿主机上运行高版本的 Linux 容器，都是行不通的。\n其次，在 Linux 内核中，有很多资源和对象是不能被 Namespace 化的，最典型的例子就是：时间。\n使用容器时要考虑2个不能被隔离的资源，内核和时间。\n这就意味着，如果你的容器中的程序使用 settimeofday(2) 系统调用修改了时间，整个宿主机的时间都会被随之修改，这显然不符合用户的预期。相比于在虚拟机里面可以随便折腾的自由度，在容器里部署应用的时候，“什么能做，什么不能做”，就是用户必须考虑的一个问题。\n此外，由于上述问题，尤其是共享宿主机内核的事实，容器给应用暴露出来的攻击面是相当大的，应用“越狱”的难度自然也比虚拟机低得多。\n更为棘手的是，尽管在实践中我们确实可以使用 Seccomp 等技术，对容器内部发起的所有系统调用进行过滤和甄别来进行安全加固，但这种方法因为多了一层对系统调用的过滤，必然会拖累容器的性能。何况，默认情况下，谁也不知道到底该开启哪些系统调用，禁止哪些系统调用。\n所以，在生产环境中，没有人敢把运行在物理机上的 Linux 容器直接暴露到公网上。当然，我后续会讲到的基于虚拟化或者独立内核技术的容器实现，则可以比较好地在隔离与性能之间做出平衡。\n在介绍完容器的“隔离”技术之后，我们再来研究一下容器的“限制”问题。\n也许你会好奇，我们不是已经通过 Linux Namespace 创建了一个“容器”吗，为什么还需要对容器做“限制”呢？\n我还是以 PID Namespace 为例，来给你解释这个问题。\n虽然容器内的第 1 号进程在“障眼法”的干扰下只能看到容器里的情况，但是宿主机上，它作为第 100 号进程与其他所有进程之间依然是平等的竞争关系。这就意味着，虽然第 100 号进程表面上被隔离了起来，但是它所能够使用到的资源（比如 CPU、内存），却是可以随时被宿主机上的其他进程（或者其他容器）占用的。当然，这个 100 号进程自己也可能把所有资源吃光。这些情况，显然都不是一个“沙盒”应该表现出来的合理行为。\n而 Linux Cgroups 就是 Linux 内核中用来为进程设置资源限制的一个重要功能。\n有意思的是，Google 的工程师在 2006 年发起这项特性的时候，曾将它命名为“进程容器”（process container）。实际上，在 Google 内部，“容器”这个术语长期以来都被用于形容被 Cgroups 限制过的进程组。后来 Google 的工程师们说，他们的 KVM 虚拟机也运行在 Borg 所管理的“容器”里，其实也是运行在 Cgroups“容器”当中。这和我们今天说的 Docker 容器差别很大。\nLinux Cgroups 的全称是 Linux Control Group。它最主要的作用，就是限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等。\n此外，Cgroups 还能够对进程进行优先级设置、审计，以及将进程挂起和恢复等操作。在今天的分享中，我只和你重点探讨它与容器关系最紧密的“限制”能力，并通过一组实践来带你认识一下 Cgroups。\n在 Linux 中，Cgroups 给用户暴露出来的操作接口是文件系统，即它以文件和目录的方式组织在操作系统的 /sys/fs/cgroup 路径下。在 Ubuntu 16.04 机器里，我可以用 mount 指令把它们展示出来，这条命令是：\n$ mount -t cgroup cpuset on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset) cpu on /sys/fs/cgroup/cpu type cgroup (rw,nosuid,nodev,noexec,relatime,cpu) cpuacct on /sys/fs/cgroup/cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct) blkio on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio) memory on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory) ... 它的输出结果，是一系列文件系统目录。如果你在自己的机器上没有看到这些目录，那你就需要自己去挂载 Cgroups，具体做法可以自行 Google。\nsudo apt install cgroupfs-mount 可以看到，在 /sys/fs/cgroup 下面有很多诸如 cpuset、cpu、 memory 这样的子目录，也叫子系统。这些都是我这台机器当前可以被 Cgroups 进行限制的资源种类。而在子系统对应的资源种类下，你就可以看到该类资源具体可以被限制的方法。比如，对 CPU 子系统来说，我们就可以看到如下几个配置文件，这个指令是：\n$ ls /sys/fs/cgroup/cpu cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us cpu.shares notify_on_release cgroup.procs cpu.cfs_quota_us cpu.rt_runtime_us cpu.stat tasks 如果熟悉 Linux CPU 管理的话，你就会在它的输出里注意到 cfs_period 和 cfs_quota 这样的关键词。这两个参数需要组合使用，可以用来限制进程在长度为 cfs_period 的一段时间内，只能被分配到总量为 cfs_quota 的 CPU 时间。\n而这样的配置文件又如何使用呢？\n你需要在对应的子系统下面创建一个目录，比如，我们现在进入 /sys/fs/cgroup/cpu 目录下：\nroot@ubuntu:/sys/fs/cgroup/cpu$ mkdir container root@ubuntu:/sys/fs/cgroup/cpu$ ls container/ cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us cpu.shares notify_on_release cgroup.procs cpu.cfs_quota_us cpu.rt_runtime_us cpu.stat tasks 这个目录就称为一个“控制组”。你会发现，操作系统会在你新创建的 container 目录下，自动生成该子系统对应的资源限制文件。\n现在，我们在后台执行这样一条脚本：\n$ while : ; do : ; done \u0026amp; [1] 226 在输出里可以看到，CPU 的使用率已经 100% 了（%Cpu0 :100.0 us）。\n而此时，我们可以通过查看 container 目录下的文件，看到 container 控制组里的 CPU quota 还没有任何限制（即：-1），CPU period 则是默认的 100 ms（100000 us）：\n$ cat /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us -1 $ cat /sys/fs/cgroup/cpu/container/cpu.cfs_period_us 100000 接下来，我们可以通过修改这些文件的内容来设置限制。\n比如，向 container 组里的 cfs_quota 文件写入 20 ms（20000 us）：\n$ echo 20000 \u0026gt; /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us 结合前面的介绍，你应该能明白这个操作的含义，它意味着在每 100 ms 的时间里，被该控制组限制的进程只能使用 20 ms 的 CPU 时间，也就是说这个进程只能使用到 20% 的 CPU 带宽。\n接下来，我们把被限制的进程的 PID 写入 container 组里的 tasks 文件，上面的设置就会对该进程生效了：\n$ echo 226 \u0026gt; /sys/fs/cgroup/cpu/container/tasks 我们可以用 top 指令查看一下：\n$ top %Cpu0 : 20.3 us, 0.0 sy, 0.0 ni, 79.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st 可以看到，计算机的 CPU 使用率立刻降到了 20%（%Cpu0 : 20.3 us）。\n除 CPU 子系统外，Cgroups 的每一个子系统都有其独有的资源限制能力，比如：\nblkio，为​​​块​​​设​​​备​​​设​​​定​​​I/O 限​​​制，一般用于磁盘等设备； cpuset，为进程分配单独的 CPU 核和对应的内存节点； memory，为进程设定内存使用的限制。 Linux Cgroups 的设计还是比较易用的，简单粗暴地理解呢，它就是一个子系统目录加上一组资源限制文件的组合。而对于 Docker 等 Linux 容器项目来说，它们只需要在每个子系统下面，为每个容器创建一个控制组（即创建一个新目录），然后在启动容器进程之后，把这个进程的 PID 填写到对应控制组的 tasks 文件中就可以了。\n而至于在这些控制组下面的资源文件里填上什么值，就靠用户执行 docker run 时的参数指定了，比如这样一条命令：\n$ docker run -it --cpu-period=100000 --cpu-quota=20000 ubuntu /bin/bash 在启动这个容器后，我们可以通过查看 Cgroups 文件系统下，CPU 子系统中，“docker”这个控制组里的资源限制文件的内容来确认：\n$ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_period_us 100000 $ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_quota_us 20000 这就意味着这个 Docker 容器，只能使用到 20% 的 CPU 带宽。\n深入理解容器镜像 # Linux 容器最基础的两种技术：Namespace 和 Cgroups。希望此时，你已经彻底理解了“容器的本质是一种特殊的进程”这个最重要的概念。\n而正如前面所说的，Namespace 的作用是“隔离”，它让应用进程只能看到该 Namespace 内的“世界”；而 Cgroups 的作用是“限制”，它给这个“世界”围上了一圈看不见的墙。这么一折腾，进程就真的被“装”在了一个与世隔绝的房间里，而这些房间就是 PaaS 项目赖以生存的应用“沙盒”。\n可是，还有一个问题不知道你有没有仔细思考过：这个房间四周虽然有了墙，但是如果容器进程低头一看地面，又是怎样一副景象呢？\n换句话说，容器里的进程看到的文件系统又是什么样子的呢？\n可能你立刻就能想到，这一定是一个关于 Mount Namespace 的问题：容器里的应用进程，理应看到一份完全独立的文件系统。这样，它就可以在自己的容器目录（比如 /tmp）下进行操作，而完全不会受宿主机以及其他容器的影响。\n那么，真实情况是这样吗？\n“左耳朵耗子”叔在多年前写的一篇关于 Docker 基础知识 的博客里，曾经介绍过一段小程序。这段小程序的作用是，在创建子进程时开启指定的 Namespace。\n下面，我们不妨使用它来验证一下刚刚提到的问题。\n#define _GNU_SOURCE #include \u0026lt;sys/mount.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/wait.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;sched.h\u0026gt; #include \u0026lt;signal.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #define STACK_SIZE (1024 * 1024) static char container_stack[STACK_SIZE]; char* const container_args[] = { \u0026#34;/bin/bash\u0026#34;, NULL }; int container_main(void* arg) { printf(\u0026#34;Container - inside the container!\\n\u0026#34;); execv(container_args[0], container_args); printf(\u0026#34;Something\u0026#39;s wrong!\\n\u0026#34;); return 1; } int main() { printf(\u0026#34;Parent - start a container!\\n\u0026#34;); int container_pid = clone(container_main, container_stack+STACK_SIZE, CLONE_NEWNS | SIGCHLD , NULL); waitpid(container_pid, NULL, 0); printf(\u0026#34;Parent - container stopped!\\n\u0026#34;); return 0; } 这段代码的功能非常简单：在 main 函数里，我们通过 clone() 系统调用创建了一个新的子进程 container_main，并且声明要为它启用 Mount Namespace（即：CLONE_NEWNS 标志）。\n而这个子进程执行的，是一个“/bin/bash”程序，也就是一个 shell。所以这个 shell 就运行在了 Mount Namespace 的隔离环境中。\n我们来一起编译一下这个程序：\n$ gcc -o ns ns.c $ ./ns Parent - start a container! Container - inside the container! 这样，我们就进入了这个“容器”当中。可是，如果在“容器”里执行一下 ls 指令的话，我们就会发现一个有趣的现象： /tmp 目录下的内容跟宿主机的内容是一样的。\n$ ls /tmp # 你会看到好多宿主机的文件 也就是说：即使开启了 Mount Namespace，容器进程看到的文件系统也跟宿主机完全一样\n这是怎么回事呢？\n仔细思考一下，你会发现这其实并不难理解：Mount Namespace 修改的，是容器进程对文件系统“挂载点”的认知。但是，这也就意味着，只有在“挂载”这个操作发生之后，进程的视图才会被改变。而在此之前，新创建的容器会直接继承宿主机的各个挂载点。\nmout namespace只隔离增量，不隔离存量\n这时，你可能已经想到了一个解决办法：创建新进程时，除了声明要启用 Mount Namespace 之外，我们还可以告诉容器进程，有哪些目录需要重新挂载，就比如这个 /tmp 目录。于是，我们在容器进程执行前可以添加一步重新挂载 /tmp 目录的操作：\nint container_main(void* arg) { printf(\u0026#34;Container - inside the container!\\n\u0026#34;); // 如果你的机器的根目录的挂载类型是shared，那必须先重新挂载根目录 // mount(\u0026#34;\u0026#34;, \u0026#34;/\u0026#34;, NULL, MS_PRIVATE, \u0026#34;\u0026#34;); mount(\u0026#34;none\u0026#34;, \u0026#34;/tmp\u0026#34;, \u0026#34;tmpfs\u0026#34;, 0, \u0026#34;\u0026#34;); execv(container_args[0], container_args); printf(\u0026#34;Something\u0026#39;s wrong!\\n\u0026#34;); return 1; } 可以看到，在修改后的代码里，我在容器进程启动之前，加上了一句 mount(“none”, “/tmp”, “tmpfs”, 0, “”) 语句。就这样，我告诉了容器以 tmpfs（内存盘）格式，重新挂载了 /tmp 目录。\nmount(source, target, type, options)\nsource: 要挂载的文件系统的来源，这里是 \u0026ldquo;none\u0026rdquo;，表示一个空的文件系统，通常用于创建临时文件系统。 target: 挂载点，也就是文件系统将要被挂载到的目录，这里是 \u0026ldquo;/tmp\u0026rdquo;。 type: 文件系统的类型，这里是 \u0026ldquo;tmpfs\u0026rdquo;，表示挂载一个临时文件系统到指定目录。 options: 挂载选项，这里是一个空字符串，表示使用默认选项。 上述命令的含义是将一个空的临时文件系统挂载到 \u0026ldquo;/tmp\u0026rdquo; 目录。在 Linux 中，tmpfs 是一种基于内存的文件系统，通常用于存储临时数据。这样的文件系统在系统重新启动时会被清空，因此适用于存储临时文件。\n这段修改后的代码，编译执行后的结果又如何呢？我们可以试验一下：\n$ gcc -o ns ns.c $ ./ns Parent - start a container! Container - inside the container! $ ls /tmp 可以看到，这次 /tmp 变成了一个空目录，这意味着重新挂载生效了。我们可以用 mount -l 检查一下：\n$ mount -l | grep tmpfs none on /tmp type tmpfs (rw,relatime) 可以看到，容器里的 /tmp 目录是以 tmpfs 方式单独挂载的。\n更重要的是，因为我们创建的新进程启用了 Mount Namespace，所以这次重新挂载的操作，只在容器进程的 Mount Namespace 中有效。如果在宿主机上用 mount -l 来检查一下这个挂载，你会发现它是不存在的：\n# 在宿主机上 $ mount -l | grep tmpfs 这就是 Mount Namespace 跟其他 Namespace 的使用略有不同的地方：它对容器进程视图的改变，一定是伴随着挂载操作（mount）才能生效。\n可是，作为一个普通用户，我们希望的是一个更友好的情况：每当创建一个新容器时，我希望容器进程看到的文件系统就是一个独立的隔离环境，而不是继承自宿主机的文件系统。怎么才能做到这一点呢？\n不难想到，我们可以在容器进程启动之前重新挂载它的整个根目录“/”。而由于 Mount Namespace 的存在，这个挂载对宿主机不可见，所以容器进程就可以在里面随便折腾了。\n在 Linux 操作系统里，有一个名为 chroot 的命令可以帮助你在 shell 中方便地完成这个工作。顾名思义，它的作用就是帮你“change root file system”，即改变进程的根目录到你指定的位置。它的用法也非常简单。\n假设，我们现在有一个 $HOME/test 目录，想要把它作为一个 /bin/bash 进程的根目录。\n首先，创建一个 test 目录和几个 lib 文件夹：\n$ mkdir -p $HOME/test $ mkdir -p $HOME/test/{bin,lib64,lib} $ cd $T 然后，把 bash 命令拷贝到 test 目录对应的 bin 路径下：\n$ cp -v /bin/{bash,ls} $HOME/test/bin 接下来，把 bash 命令需要的所有 so 文件，也拷贝到 test 目录对应的 lib 路径下。找到 so 文件可以用 ldd 命令：\n$ T=$HOME/test $ list=\u0026#34;$(ldd /bin/ls | egrep -o \u0026#39;/lib.*\\.[0-9]\u0026#39;)\u0026#34; $ for i in $list; do cp -v \u0026#34;$i\u0026#34; \u0026#34;${T}${i}\u0026#34;; done 最后，执行 chroot 命令，告诉操作系统，我们将使用 $HOME/test 目录作为 /bin/bash 进程的根目录：\n$ chroot $HOME/test /bin/bash 这时，你如果执行 \u0026ldquo;ls /\u0026quot;，就会看到，它返回的都是 $HOME/test 目录下面的内容，而不是宿主机的内容。\n更重要的是，对于被 chroot 的进程来说，它并不会感受到自己的根目录已经被“修改”成 $HOME/test 了。\n这种视图被修改的原理，是不是跟我之前介绍的 Linux Namespace 很类似呢？\n没错！\n实际上，Mount Namespace 正是基于对 chroot 的不断改良才被发明出来的，它也是 Linux 操作系统里的第一个 Namespace。\n当然，为了能够让容器的这个根目录看起来更“真实”，我们一般会在这个容器的根目录下挂载一个完整操作系统的文件系统，比如 Ubuntu16.04 的 ISO。这样，在容器启动之后，我们在容器里通过执行 \u0026ldquo;ls /\u0026rdquo; 查看根目录下的内容，就是 Ubuntu 16.04 的所有目录和文件。\n而这个挂载在容器根目录上、用来为容器进程提供隔离后执行环境的文件系统，就是所谓的“容器镜像”。它还有一个更为专业的名字，叫作：rootfs（根文件系统）。\n所以，一个最常见的 rootfs，或者说容器镜像，会包括如下所示的一些目录和文件，比如 /bin，/etc，/proc 等等：\n$ ls / bin dev etc home lib lib64 mnt opt proc root run sbin sys tmp usr var 而你进入容器之后执行的 /bin/bash，就是 /bin 目录下的可执行文件，与宿主机的 /bin/bash 完全不同。\n附录 # 容器 = cgroup + namespace + rootfs + 容器引擎\nCgroup：资源控制 namespace：访问隔离 rootfs：文件系统隔离。镜像的本质就是一个rootfs文件 容器引擎：生命周期控制 Cgroup # Cgroup 是 Control group 的简称，是 Linux 内核提供的一个特性，用于限制和隔离一组进程对系统资源的使用。对不同资源的具体管理是由各个子系统分工完成的。\n子系统 作用 devices 设备权限控制 cpuset 分配指定的CPU和内存节点 CPU 控制CPU使用率 cpuacct 统计CPU使用情况 memory 限制内存的使用上限 freezer 暂停Cgroup 中的进程 net_cls 配合流控限制网络带宽 net_prio 设置进程的网络流量优先级 perf_event 允许 Perf 工具基于 Cgroup 分组做性能检测 huge_tlb 限制 HugeTLB 的使用 在 Cgroup 出现之前，只能对一个进程做资源限制，如 ulimit 限制一个进程的打开文件上限、栈大小。而 Cgroup 可以对进程进行任意分组，如何分组由用户自定义。\n子系统介绍\ncpuset 子系统 cpuset 可以为一组进程分配指定的CPU和内存节点。 cpuset 一开始用在高性能计算上，在 NUMA(non-uniform memory access) 架构的服务器上，通过将进程绑定到固定的 CPU 和内存节点上，来避免进程在运行时因跨节点内存访问而导致的性能下降。\ncpuset 的主要接口如下：\ncpuset.cpus: 允许进程使用的CPU列表 cpuset.mems： 允许进程使用的内存节点列表 cpu 子系统 cpu 子系统用于限制进程的 CPU 利用率。具体支持三个功能：\nCPU 比重分配。使用 cpu.shares 接口。 CPU 带宽限制。使用 cpu.cfs_period_us 和 cpu.cfs_quota_us 接口。 实时进程的 CPU 带宽限制。使用 cpu_rt_period_us 和 cpu_rt_quota_us 接口。 cpuacct 子系统 统计各个 Cgroup 的 CPU 使用情况，有如下接口：\ncpuacct.stat: 报告这个 Cgroup 在用户态和内核态消耗的 CPU 时间，单位是 赫兹。 cpuacct.usage： 报告该 Cgroup 消耗的总 CPU 时间。 cpuacct.usage_percpu：报告该 Cgroup 在每个 CPU 上的消耗时间。 memory 子系统 限制 Cgroup 所能使用的内存上限。\nmemory.limit_in_bytes：设定内存上限，单位字节。默认情况下，如果使用的内存超过上限，Linux 内核会试图回收内存，如果这样仍无法将内存降到限制的范围内，就会触发 OOM，选择杀死该Cgroup 中的某个进程。 memory.memsw,limit_in_bytes: 设定内存加上交换内存区的总量。 memory.oom_control： 如果设置为0，那么内存超过上限时，不会杀死进程，而是阻塞等待进程释放内存；同时系统会向用户态发送事件通知。 memory.stat: 报告内存使用信息。 blkio 限制 Cgroup 对 阻塞 IO 的使用。\nblkio.weight: 设置权值，范围在[100, 1000]，属于比重分配，不是绝对带宽。因此只有当不同 Cgroup 争用同一个 阻塞设备时才起作用 blkio.weight_device： 对具体设备设置权值。它会覆盖上面的选项值。 blkio.throttle.read_bps_device: 对具体的设备，设置每秒读磁盘的带宽上限。 blkio.throttle.write_bps_device: 对具体的设备，设置每秒写磁盘的带宽上限。 blkio.throttle.read_iops_device: 对具体的设备，设置每秒读磁盘的IOPS带宽上限。 blkio.throttle.write_iops_device: 对具体的设备，设置每秒写磁盘的IOPS带宽上限。 devices 子系统 控制 Cgroup 的进程对哪些设备有访问权限\ndevices.list: 只读文件，显示目前允许被访问的设备列表，文件格式为 类型[a|b|c] 设备号[major:minor] 权限[r/w/m 的组合] a/b/c 表示 所有设备、块设备和字符设备。 devices.allow： 只写文件，以上述格式描述允许相应设备的访问列表。 devices.deny： 只写文件，以上述格式描述禁止相应设备的访问列表。 Namespace # Namespace 是将内核的全局资源做封装，使得每个namespace 都有一份独立的资源，因此不同的进程在各自的namespace内对同一种资源的使用互不干扰。 举个例子，执行 sethostname 这个系统调用会改变主机名，这个主机名就是全局资源，内核通过 UTS Namespace可以将不同的进程分隔在不同的 UTS Namespace 中，在某个 Namespace 修改主机名时，另一个 Namespace 的主机名保持不变。\n目前，Linux 内核实现了6种 Namespace。\nNamespace 作用 IPC 隔离 System V IPC 和 POSIX 消息队列 Network 隔离网络资源 Mount 隔离文件系统挂载点 PID 隔离进程ID UTS 隔离主机名和域名 User 隔离用户和用户组 与命名空间相关的三个系统调用：\nclone创建全新的Namespace，由clone创建的新进程就位于这个新的namespace里。创建时传入 flags参数，可选值有 CLONE_NEWIPC, CLONE_NEWNET, CLONE_NEWNS, CLONE_NEWPID, CLONE_NEWUTS, CLONE_NEWUSER， 分别对应上面六种namespace。 unshare为已有进程创建新的namespace。 setns把某个进程放在已有的某个namespace里。 6种命名空间\nUTS namespace UTS namespace 对主机名和域名进行隔离。为什么要隔离主机名？因为主机名可以代替IP来访问。如果不隔离，同名访问会出冲突。\nIPC namespace Linux 提供很多种进程通信机制，IPC namespace 针对 System V 和 POSIX 消息队列，这些 IPC 机制会使用标识符来区别不同的消息队列，然后两个进程通过标识符找到对应的消息队列。\nIPC namespace 使得 相同的标识符在两个 namespace 代表不同的消息队列，因此两个namespace 中的进程不能通过 IPC 来通信。\nPID namespace 隔离进程号，不同namespace 的进程可以使用相同的进程号。\n当创建一个 PID namespace 时，第一个进程的PID 是1，即 init 进程。它负责回收所有孤儿进程的资源，所有发给 init 进程的信号都会被屏蔽。\nMount namespace 隔离文件挂载点，每个进程能看到的文件系统都记录在 /proc/$$/mounts 里。在一个 namespace 里挂载、卸载的动作不会影响到其他 namespace。\nNetwork namespace 隔离网络资源。每个 namespace 都有自己的网络设备、IP、路由表、/proc/net 目录、端口号等。网络隔离可以保证独立使用网络资源，比如开发两个web 应用可以使用80端口。 新创建的 Network namespace 只有 loopback 一个网络设备，需要手动添加网络设备。\nUser namespace 隔离用户和用户组。它的厉害之处在于，可以让宿主机上的一个普通用户在 namespace 里成为 0 号用户，也就是 root 用户。这样普通用户可以在容器内“随心所欲”，但是影响也仅限在容器内。\n最后，回到 Docker 上，经过上述讨论，namespace 和 cgroup 的使用很灵活，需要注意的地方也很多。 Docker 通过 Libcontainer 来做这些脏活累活。用户只需要使用 Docker API 就可以优雅地创建一个容器。docker exec 的底层实现就是上面提过的 setns 。\nrootfs # rootfs 代表一个 Docker 容器在启动时(而非运行后)其内部进程可见的文件系统视角，或者叫 Docker 容器的根目录。\n先来看一下，Linux 操作系统内核启动时，内核会先挂载一个只读的 rootfs，当系统检测其完整性之后，决定是否将其切换到读写模式。\nDocker 沿用这种思想，不同的是，挂载rootfs 完毕之后，没有像 Linux 那样将容器的文件系统切换到读写模式，而是利用联合挂载技术，在这个只读的 rootfs 上挂载一个读写的文件系统，挂载后该读写文件系统空空如也。Docker 文件系统简单理解为：只读的 rootfs + 可读写的文件系统。\n","date":"1 June 2024","permalink":"/posts/architecture/virtualization/k8s/%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90-kubernetes/%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%E6%A6%82%E5%BF%B5/","section":"博客","summary":"容器技术概念-k8s入门篇","title":"容器技术概念"},{"content":"","date":"1 June 2024","permalink":"/posts/architecture/virtualization/","section":"博客","summary":"虚拟化技术是一种创建虚拟版本的物理资源的方法，例如服务器、存储设备、网络资源和操作系统。这项技术允许多个虚拟系统在同一硬件上独立运行，提高了资源的利用效率，同时也支持更灵活的系统管理。虚拟化广泛应用于云计算、数据中心管理和应用测试等领域。","title":"虚拟化技术"},{"content":"","date":"1 June 2024","permalink":"/posts/architecture/virtualization/k8s/%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90-kubernetes/docker%E5%8F%91%E5%B1%95%E5%8F%B2-%E5%85%AB%E5%8D%A6%E7%AF%87/","section":"博客","summary":"docker小鲸鱼大事记-八卦篇","title":"docker小鲸鱼大事记-八卦篇"},{"content":"","date":"31 May 2024","permalink":"/tags/java/","section":"Tags","summary":"","title":"Java"},{"content":"","date":"31 May 2024","permalink":"/posts/architecture/high-concurrency/redis/","section":"博客","summary":"Redis（Remote Dictionary Server）是一个开源的内存数据库，遵守BSD 协议，它提供了一个高性能的键值（key-value）存储系统，常用于缓存、消息队列、会话存储等应用场景。","title":"redis"},{"content":"","date":"31 May 2024","permalink":"/tags/redis/","section":"Tags","summary":"","title":"Redis"},{"content":"","date":"31 May 2024","permalink":"/posts/architecture/high-concurrency/redis/redis-%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98/","section":"博客","summary":"Redis 核心技术与实战","title":"Redis 核心技术与实战"},{"content":"","date":"31 May 2024","permalink":"/posts/architecture/high-concurrency/redis/redis-%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98/%E5%9F%BA%E7%A1%80%E7%AF%87/","section":"博客","summary":"Redis 核心技术与实战 - 基础篇","title":"Redis 核心技术与实战 - 基础篇"},{"content":"","date":"31 May 2024","permalink":"/posts/architecture/high-concurrency/","section":"博客","summary":"慎入，高并发的水太深，什么高并发，大流量的东西都是虚拟的，笔者还太年轻，没有那个经历，把握不住。","title":"高并发"},{"content":"","date":"30 May 2024","permalink":"/posts/architecture/iac/","section":"博客","summary":"基础架构即代码（IaC）是通过机器可读的定义文件而不是物理硬件配置或交互式配置工具来管理和配置计算机数据中心资源的过程。定义可以在版本控制系统中。定义文件中的代码可以使用脚本或声明式定义，而不是通过人工流程来维护代码，但 IaC 更经常使用声明式方法。","title":"Infrastructure as Code"},{"content":"","date":"30 May 2024","permalink":"/posts/architecture/iac/terraform/","section":"博客","summary":"Terraform 是由HashiCorp 创建的“基础架构即代码”开源工具，让程序员可以安全高效地构建、更改基础架构并标示其版本。 Terraform 是一款声明性编码工具，使开发人员能够使用名为HashiCorp 配置语言(HCL) 的高级配置语言来描述用于运行应用程序的云端或本地部署基础架构所需的“最终状态”","title":"terraform"},{"content":"","date":"30 May 2024","permalink":"/tags/terraform/","section":"Tags","summary":"","title":"Terraform"},{"content":"Terraform Provider的全网标识符 # Terraform的Provider在全网的的标识符由三部分组成，分别为hostname，namespace和type组成，即\u0026lt;hostname\u0026gt;/\u0026lt;namespace\u0026gt;/\u0026lt;type\u0026gt;。hostname是指分发、下载Provider的域名，默认为registry.terraform.io。namespace是指提供、开发Provider的组织的命名空间，默认为hashicorp。type是指Provider的具体类型。\n例如有以下Terraform模板：\nterraform { required_providers { alicloud = { source = \u0026#34;aliyun/alicloud\u0026#34; version = \u0026#34;1.126.0\u0026#34; } } } 上述模板使用terraform init命令会默认去registry.terraform.io下载aliyun开发的alicloudProvider的1.126.0版本。\n本地安装Terraform Provider # 如果使用本地安装插件有两种方法。首先两种方法都需要将下载的Provider或者本地编译完成的Provider放置在以下文件目录层级：\nXX(e.g. /usr/share/terraform/providers/) └── \u0026lt;hostname\u0026gt;(e.g. registry.terraform.io) └── \u0026lt;namespace\u0026gt;(e.g. aliyun) └── \u0026lt;type\u0026gt;(e.g. alicloud) └── \u0026lt;version\u0026gt;(e.g. 1.127.0) └── \u0026lt;your OS\u0026gt;(e.g. linux_amd64) └── \u0026lt;binary file\u0026gt;(e.g. terraform-provider-alicloud) 方法一：使用terraform init的自带参数 # 第一种方法，使用terraform init的plugin-dir参数：\nterraform init -plugin-dir=/usr/share/terraform/providers 方法二：编写配置文件 # 第二种方法，编写./terraformrc配置文件，该文件需要放在$HOME/目录下：\nprovider_installation { filesystem_mirror { path = \u0026#34;/usr/share/terraform/providers\u0026#34; include = [\u0026#34;registry.terraform.io/*/*\u0026#34;] } } 其中include字段是指符合该通配符全网标识符的Provider，需要去/usr/share/terraform/providers查找本地Provider。./terraformrc的编写更详细的参数可以参考 官网\nResources # https://developer.aliyun.com/article/786134 ","date":"30 May 2024","permalink":"/posts/architecture/iac/terraform/terraform%E4%BD%BF%E7%94%A8%E6%9C%AC%E5%9C%B0%E7%BC%96%E8%AF%91in-house%E7%9A%84providers/","section":"博客","summary":"Terraform Provider的全网标识符 # Terraform的Provider在全网的的标识符由三部分组成，分别为hostname，namespace和type组成，即\u0026lt;hostname\u0026gt;/\u0026lt;namespace\u0026gt;/\u0026lt;type\u0026gt;。hostname是指分发、下载Provider的域名，默认为registry.","title":"Terraform使用本地编译（In-house）的Providers"},{"content":"","date":"29 May 2024","permalink":"/posts/architecture/distributed/","section":"博客","summary":"主要研究分散式系统（Distributed system）如何进行计算。分散式系统是一组电脑，透过网路相互连接传递讯息与通讯后并协调它们的行为而形成的系统。","title":"Distributed Technology"},{"content":"","date":"29 May 2024","permalink":"/posts/architecture/distributed/kafka/","section":"博客","summary":"Kafka 是什么呢？用一句话概括一下：Apache Kafka 是一款开源的消息引擎系统。而且不是单纯的消息中间件，主要是大数据应用，分布式日志提交。","title":"kafka"},{"content":"","date":"29 May 2024","permalink":"/tags/kafka/","section":"Tags","summary":"","title":"Kafka"},{"content":"01 | 消息引擎系统ABC # kafka 是什么呢？\n用一句话概括一下：Apache Kafka 是一款开源的消息引擎系统。\n倘若“消息引擎系统”这个词对你来说有点陌生的话，那么“消息队列”“消息中间件”的提法想必你一定是有所耳闻的。不过说实话我更愿意使用消息引擎系统这个称谓，因为消息队列给出了一个很不明确的暗示，仿佛 Kafka 是利用队列的方式构建的；而消息中间件的提法有过度夸张“中间件”之嫌，让人搞不清楚这个中间件到底是做什么的。\n像 Kafka 这一类的系统国外有专属的名字叫 Messaging System，国内很多文献将其简单翻译成消息系统。我个人认为并不是很恰当，因为它片面强调了消息主体的作用，而忽视了这类系统引以为豪的消息传递属性，就像引擎一样，具备某种能量转换传输的能力，所以我觉得翻译成消息引擎反倒更加贴切。\n根据维基百科的定义，消息引擎系统是一组规范。企业利用这组规范在不同系统之间传递语义准确的消息，实现松耦合的异步式数据传递。换句话说就是：系统 A 发送消息给消息引擎系统，系统 B 从消息引擎系统中读取 A 发送的消息。\n最基础的消息引擎就是做这点事的！但是要达成的共识是：\n消息引擎传输的对象是消息； 如何传输消息属于消息引擎设计机制的一部分。 既然消息引擎是用于在不同系统之间传输消息的，那么如何设计待传输消息的格式从来都是一等一的大事。试问一条消息如何做到信息表达业务语义而无歧义，同时它还要能最大限度地提供可重用性以及通用性？稍微停顿几秒去思考一下，如果是你，你要如何设计你的消息编码格式。\n一个比较容易想到的是使用已有的一些成熟解决方案，比如使用 CSV、XML 亦或是 JSON；又或者你可能熟知国外大厂开源的一些序列化框架，比如 Google 的 Protocol Buffer 或 Facebook 的 Thrift。这些都是很酷的办法。那么现在我告诉你 Kafka 的选择：它使用的是纯二进制的字节序列。当然消息还是结构化的，只是在使用之前都要将其转换成二进制的字节序列。\n消息设计出来之后还不够，消息引擎系统还要设定具体的传输协议，即我用什么方法把消息传输出去。常见的有两种方法：\n点对点模型：也叫消息队列模型。如果拿上面那个“民间版”的定义来说，那么系统 A 发送的消息只能被系统 B 接收，其他任何系统都不能读取 A 发送的消息。日常生活的例子比如电话客服就属于这种模型：同一个客户呼入电话只能被一位客服人员处理，第二个客服人员不能为该客户服务。 发布 / 订阅模型：与上面不同的是，它有一个主题（Topic）的概念，你可以理解成逻辑语义相近的消息容器。该模型也有发送方和接收方，只不过提法不同。发送方也称为发布者（Publisher），接收方称为订阅者（Subscriber）。和点对点模型不同的是，这个模型可能存在多个发布者向相同的主题发送消息，而订阅者也可能存在多个，它们都能接收到相同主题的消息。生活中的报纸订阅就是一种典型的发布 / 订阅模型。 kafka同时支持点对点＆发布/订阅这两种消息引擎模型。\n提到消息引擎系统，你可能会问 JMS 和它是什么关系。JMS 是 Java Message Service，它也是支持上面这两种消息引擎模型的。严格来说它并非传输协议而仅仅是一组 API 罢了。不过可能是 JMS 太有名气以至于很多主流消息引擎系统都支持 JMS 规范，比如 ActiveMQ、RabbitMQ、IBM 的 WebSphere MQ 和 Apache Kafka。当然 Kafka 并未完全遵照 JMS 规范，相反，它另辟蹊径，探索出了一条特有的道路。\n为什么要使用kafka呢？\n为什么系统 A 不能直接发送消息给系统 B，中间还要隔一个消息引擎呢？\n答案就是“削峰填谷”。这四个字简直比消息引擎本身还要有名气。\n所谓的“削峰填谷”就是指缓冲上下游瞬时突发流量，使其更平滑。消息队列的使用场景是上下游系统间数据传输速度不平衡，如果上游系统发送数据的速度与下游系统消费数据的速度相仿的话，使用消息队列是没有太大意义的。具体解释一下：\n对于那种发送能力很强的上游系统，如果没有消息引擎的保护，“脆弱”的下游系统可能会直接被压垮导致全链路服务“雪崩”。但是，一旦有了消息引擎，它能够有效地对抗上游的流量冲击，真正做到将上游的“峰”填满到“谷”中，避免了流量的震荡。 发送方和接收方的松耦合，这也在一定程度上简化了应用的开发，减少了系统间不必要的交互。 ","date":"29 May 2024","permalink":"/posts/architecture/distributed/kafka/kafka-%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98/","section":"博客","summary":"01 | 消息引擎系统ABC # kafka 是什么呢？","title":"Kafka 核心技术与实战"},{"content":"","date":"27 May 2024","permalink":"/posts/language/java/","section":"博客","summary":"Java是一多用途、面向对象、跨平台编程语言，具备强大的生态系统和丰富的库支持。它的特点包括自动内存管理、垃圾回收、跨平台性、多线程支持、安全性和可移植性。Java广泛用于开发Web应用、移动应用、嵌入式系统和大型企业级应用程序。Java的特殊之处在于其独立的虚拟机（JVM），允许在不同平台上运行相同的Java程序，使其成为一种受欢迎的编程语言。","title":"Java"},{"content":"","date":"27 May 2024","permalink":"/posts/language/java/spring/","section":"博客","summary":"Spring是一个开源容器框架，它集成各类型的工具，通过核心的Bean factory实现了底层的类的实例化和生命周期的管理。 在整个框架中，各类型的功能被抽象成一个个的Bean，这样就可以实现各种功能的管理，包括动态加载和切面编程。","title":"Spring"},{"content":"大家在开发的过程中应该经常会看到各种各样的Starter\n当我们需要集成某个功能的时候，Spring或是第三方都会提供一个Starter来帮助我们更简单的集成对应的功能到我们的Spring Boot项目中\n准备 # 现在我们假定，我们实现了一个A类用于提供我们封装好的功能\npublic class A { ... } 一般情况下我们会使用@Component往Spring容器中注入实例，如下：\n@Component public class A { ... } 现在当我们要把A单独抽出来做成一个 Starter 时 @Component 就不太合适了，那么我们应该怎么实现呢，让我们先给我们的 Starter 取个名字吧哈哈哈\n取名 # 首先我们要先确定我们的Starter的名字\nSpring本身就有很多自带的Starter，比如：\nspring-boot-starter-web spring-boot-starter-data-redis spring-boot-starter-websocket spring-cloud-starter-netflix-eureka-client spring-cloud-starter-openfeign spring-cloud-starter-gateway 可以发现这些自带的Starter的名称格式都是spring-boot-starter-xxx或是spring-cloud-starter-xxx\n另外我们也可以看到很多第三方库的Starter，比如：\nredisson-spring-boot-starter mybatis-plus-boot-starter 一般来说，第三方的Starter会把starter放后面，xxx-spring-boot-starter或是xxx-boot-starter或是xxx-starter\n不过我个人习惯还是xxx-spring-boot-starter感觉更标准一点\n所以现在就把我们要实现的Starter取名为a-spring-boot-starter\n配置类 # 之前说@Component已经不太合适了，那么要怎么把A注入到Spring的容器中呢\n答案是：@Configuration+@Bean，如下\n@Configuration public class AConfiguration { @Bean public A a() { return new A(); } } 这个用法大家应该也是比较熟悉，一般在一个项目中也会有一些标记了@Configuration的配置类\n只要Spring能够扫描到这个类，A实例就能被注入\n如果这个配置类是写在我们自己的包下，那么Spring默认的扫描路径就能扫到\n但是现在我们如果做成一个Starter，对应的包名可能就扫不到了\n所以我们需要用另外的方式来导入这个配置类\n导入方式 # 接下来就可以决定我们的Starter的导入方式了\n常用的导入方式有两种：使用@EnableXXX或是spring.factories\n我们经常能看到有些组件的会需要你添加@EnableXXX的注解来启用某个功能，比如：\n@EnableDiscoveryClient @EnableFeignClients 这种方式光引入包还不够，需要手动添加注解来启用\n而使用spring.factories就只要引入包就可以直接生效了\n这两种方式其实用哪种都一样，主要是看有没有必要额外配置一个注解\n比如@EnableFeignClients这个注解是可以配置扫描路径的，所以额外添加一个注解更加合适（这里使用配置文件是不合适的，因为我们的包结构是确定的，如果配置在配置文件里面反而多余又容易写错）\n注解导入 # 我们先使用注解的方式来导入，定义一个@EnableA\n@Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Import(AConfiguration.class) public @interface EnableA { } 使用@Import注解导入AConfiguration.class就可以了\n当我们需要集成这个功能的时候只要添加这个注解就行了\n@EnableA @SpringBootApplication public class SampleApplication { public static void main(String[] args) { SpringApplication.run(SampleApplication.class, args); } } 注解参数 # 这个时候可能就有同学要问了，如果我的注解上有参数呢，上面的写法好像没办法拿到参数啊\n接下来我们来解决这个问题\n现在我们给@EnableA注解添加一个参数enabled，当enabled为true时导入AConfiguration.class，当enabled为false时不导入AConfiguration.class\n@Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Import(AConfiguration.class) public @interface EnableA { boolean enabled() default true; } 接着我们实现一个ImportSelector\npublic class AImportSelector implements ImportSelector { @Override public String[] selectImports(AnnotationMetadata metadata) { Map\u0026lt;String, Object\u0026gt; attributes = metadata .getAnnotationAttributes(EnableA.class.getName()); boolean enabled = (boolean) attributes.get(\u0026#34;enabled\u0026#34;); if (enabled) { return new String[]{AConfiguration.class.getName()}; } else { return new String[]{}; } } } 我们可以通过ImportSelector中提供给我们的AnnotationMetadata来获得EnableA中的属性enabled\n当enabled为true时，我们返回AConfiguration.class的全限定名；当enabled为false时，返回空数组即可\n最后我们将@Import(AConfiguration.class)改为@Import(AImportSelector.class)就行了\n@Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Import(AImportSelector.class) public @interface EnableA { boolean enabled() default true; } 当我们将enabled设置为false时，就不会配置AConfiguration.class了\n@EnableA(enabled = false) @SpringBootApplication public class SampleApplication { public static void main(String[] args) { SpringApplication.run(SampleApplication.class, args); } } 其实还有另一种方式也可以拿到注解的属性，那就是ImportBeanDefinitionRegistrar\npublic interface ImportBeanDefinitionRegistrar { default void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) { } } 和ImportSelector不同的是，ImportBeanDefinitionRegistrar可以直接注册BeanDefinition\n如果我们用ImportBeanDefinitionRegistrar来实现上面的功能大概就是这个样子\npublic class AImportBeanDefinitionRegistrar implements ImportBeanDefinitionRegistrar { @Override public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry) { Map\u0026lt;String, Object\u0026gt; attributes = metadata .getAnnotationAttributes(EnableA.class.getName()); boolean enabled = (boolean) attributes.get(\u0026#34;enabled\u0026#34;); if (enabled) { registry.registerBeanDefinition(\u0026#34;a\u0026#34;, new RootBeanDefinition(A.class)); } } } 然后同样的把@Import(AConfiguration.class)改为@Import(AImportBeanDefinitionRegistrar.class)就行了\n@Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Import(AImportBeanDefinitionRegistrar.class) public @interface EnableA { boolean enabled() default true; } spring.factories导入 # 接下来我们使用spring.factories来导入配置（注解和spring.factories选择一种就可以啦）\n我们需要在resources目录下新建一个META-INF目录，然后在META-INF目录下创建spring.factories文件\n接着我们需要在spring.factories中将AConfiguration.class配置上去\norg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\ com.xxx.xxx.AConfiguration 一般情况下，如果是配置在spring.factories中的配置类都会取名xxxAutoConfiguration，所以我们在这里修改名称为AAutoConfiguration\n最后在spring.factories中的配置\norg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\ com.xxx.xxx.AAutoConfiguration 这样当你的项目启动后，Spring就会自动读取spring.factories将AAutoConfiguration(AConfiguration)扫描进去了\n配置文件 # 正常情况下，我们很有可能需要在application.yml或application.properties中配置一些参数\n所以我们现在需要一个属性a.enabled来控制是否注入A\n还需要一个属性a.b.type来配置A的某个字段\n那么怎么在我们的AAutoConfiguration中获得这两个属性呢\n大家可能会想，简单啊，用@Value不就好了？\n虽然@Value确实能拿到配置文件中的值，但是有更好的方式\n那就是用@ConfigurationProperties+@EnableConfigurationProperties\n我们需要先定义一个AProperties\n@Data @ConfigurationProperties(prefix = \u0026#34;a\u0026#34;) public class AProperties { //映射 a.enabled; private boolean enabled = true; private B b = new B(); @Data public static class B { //映射 a.b.type; private String type; } } 同时给AProperties添加ConfigurationProperties注解并标记前缀为a\n接着我们在AAutoConfiguration上添加@EnableConfigurationProperties就行了\n@Configuration @EnableConfigurationProperties(AProperties.class) public class AConfiguration { @Bean @ConditionalOnProperty(name = \u0026#34;a.enabled\u0026#34;, havingValue = \u0026#34;true\u0026#34;, matchIfMissing = true) public A a(AProperties properties) { String type = properties.getB().getType(); return new A(); } } 我们可以通过@ConditionalOnProperty来根据a.enabled控制是否注入A\n在方法参数中也可以直接注入AProperties对象，并且里面的属性已经根据配置文件绑定好了\n自动提示 # 不知道大家有没有发现，Spring自带的配置是会有提示的，但是我们自定义的配置就没有\n有没有什么办法让我们的AProperties也能自动提示呢\n只要引入下面这个包就行啦\nannotationProcessor \u0026#39;org.springframework.boot:spring-boot-configuration-processor\u0026#39; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-configuration-processor\u0026lt;/artifactId\u0026gt; \u0026lt;optional\u0026gt;true\u0026lt;/optional\u0026gt; \u0026lt;/dependency\u0026gt; 如果AProperties有改动需要重新编译才会生效哦\n配置代理 # @Configuration的proxyBeanMethods可以指定该配置中的方法是否进行代理，具体有什么作用呢\n假设现在我们的A需要依赖B实例，那我们的配置可以这样写\n@Configuration public class AConfiguration { @Bean public B b() { return new B(); } @Bean public A a() { return new A(b()); } } @Configuration的proxyBeanMethods默认是true，所以在a()中调用b()是会从Spring的容器中获得B实例\n如果我们不启用方法代理可以这样写\n@Configuration(proxyBeanMethods = false) public class AConfiguration { @Bean public B b() { return new B(); } @Bean public A a(B b) { return new A(b); } } 直接在方法参数中注入即可\n不启用方法代理的情况下，如果直接调用方法，就是普通的方法调用，每调用一次就会新建一个B实例\n配置依赖 # 接着之前的假设，A需要依赖B实例，但是现在B允许为null\n那么之前的配置方式就不行了\n@Configuration public class AConfiguration { @Bean public A a(B b) { return new A(b); } } 如果直接在方法上注入B实例，就会报错找不到对应的Bean\n这种情况下，我们可以使用ObjectProvider，如下：\n@Configuration public class AConfiguration { @Bean public A a(ObjectProvider\u0026lt;B\u0026gt; bProvider) { return new A(bProvider.getIfUnique()); } } 条件装配 # 在我们写Starter的过程中，条件装配也是经常用到的功能\n最常用的其实就是@ConditionalOnMissingBean了\n我们可以这样用\n@Configuration public class AConfiguration { @Bean @ConditionalOnMissingBean public A a() { return new A(); } } 当Spring发现当前已经存在A对应的实例时，就不会再注入这个配置中的A实例了\n一般当我们重写了某个库中的某个组件后，该库中该组件的默认实现就不会生效了，便于我们扩展一些自定义的功能来替换默认实现\n但是这个注解如果用不好也可能出现问题\n假设现在我们的A有一个扩展类A1\n我们来看下面的配置1\n@Configuration public class AConfiguration { @Bean @ConditionalOnMissingBean public A1 a() { return new A1(); } } @ConditionalOnMissingBean的判断逻辑是：当容器中存在A1类型的对象就不会再注入这个配置中的A1实例\n接着我们再看下面的配置2\n@Configuration public class AConfiguration { @Bean @ConditionalOnMissingBean public A a() { return new A1(); } } @ConditionalOnMissingBean的判断逻辑是：当容器中存在A类型的对象就不会再注入这个配置中的A1实例\n如果在这个时候，容器中存在A2(A的另一个扩展类)实例，配置1中的A1还是会被注入，配置2中A1不会被注入\n因为@ConditionalOnMissingBean的缺省值是方法的返回类型，所以大家在使用时需要多加注意，保险起见可以指定@ConditionalOnMissingBean中的值，例如：\n@Configuration public class AConfiguration { @Bean @ConditionalOnMissingBean(A.class) public A1 a() { return new A1(); } } 其他常用的条件注解 # @ConditionalOnBean 当对应的Bean存在时生效 @ConditionalOnClass 当对应的Class存在时生效 @ConditionalOnMissingClass 当对应的Class不存在时生效 @ConditionalOnProperty 当对应的配置匹配时生效 @ConditionalOnWebApplication 可以指定在Servlet或Reactive环境中生效 配置顺序 # 在某些情况下，我们可能会发现一些条件注解不生效\n这个时候我们可以尝试指定配置顺序（并不保证能够解决所有的失效问题）\n@AutoConfigureBefore 在某个配置之前进行配置 @AutoConfigureAfter 在某个配置之后进行配置 @AutoConfigureOrder 指定配置顺序 不过这里需要注意这几个注解只能对自动配置生效，也就是需要定义在spring.factories中的配置\n添加注解的类的可以是任意的配置类，但是注解中指定的类需要是spring.factories中的配置的类\n打包发布 # 最后就是打包发布就行啦，之后就可以通过Gradle或Maven从中央仓库或私库中拉下来使用了\n赶快去写一个自己的Starter吧\n","date":"27 May 2024","permalink":"/posts/language/java/spring/spring-boot-%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E5%86%99%E4%B8%80%E4%B8%AA-starter/","section":"博客","summary":"大家在开发的过程中应该经常会看到各种各样的Starter","title":"Spring Boot 手把手教你写一个 Starter"},{"content":"","date":"27 May 2024","permalink":"/posts/language/","section":"博客","summary":"编程语言指的是一套规则和符号，用于编写和交流计算机程序。编程语言允许开发者创建、控制和优化软件，以便计算机能够理解和执行特定任务。不同的编程语言适用于不同的应用领域和用途，如C++、Python、Java等。","title":"编程语言"},{"content":"","date":"24 May 2024","permalink":"/posts/ai/","section":"博客","summary":"人工智能（缩写为AI）亦称机器智能，指由人制造出来的机器所表现出来的智能。通常人工智能是指通过普通计算机程序来呈现人类智能的技术。","title":"Artificial Intelligence"},{"content":"","date":"24 May 2024","permalink":"/posts/ai/llm/langchain/","section":"博客","summary":"LangChain 实战课","title":"LangChain 实战课"},{"content":"基本概念 # Elasticsearch is a highly scalable open-source full-text search and analytics engine. It allows you to store, search, and analyze big volumes of data quickly and in near real time. It is generally used as the underlying engine/technology that powers applications that have complex search features and requirements.\nElasticsearch 是一个高可扩展、开源、全文本的搜索与数据分析引擎。它使您可以近实时地快速存储、搜索和分析大规模数据。它通常用作支持具有复杂搜索功能和要求的应用程序的底层引擎/技术。\nElasticsearch 是 面向文档 的，意味着它存储整个对象或 文档。Elasticsearch 不仅存储文档，而且 索引 每个文档的内容，使之可以被检索。在 Elasticsearch 中，我们对文档进行索引、检索、排序和过滤—而不是对行列数据。这是一种完全不同的思考数据的方式，也是 Elasticsearch 能支持复杂全文检索的原因。Elasticsearch 使用 JavaScript Object Notation（或者 JSON）作为文档的序列化格式。\n下面这个 JSON 文档代表了一个 user 对象：\n{ \u0026#34;name\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;age\u0026#34;: 25, \u0026#34;sex\u0026#34;: \u0026#34;Male\u0026#34;, \u0026#34;info\u0026#34;: { \u0026#34;bio\u0026#34;: \u0026#34;Eco-warrior and defender of the weak\u0026#34;, \u0026#34;tel\u0026#34;: 18612344321, \u0026#34;interests\u0026#34;: [ \u0026#34;dolphins\u0026#34;, \u0026#34;whales\u0026#34; ] }, \u0026#34;join_date\u0026#34;: \u0026#34;2014/05/01\u0026#34; } 用Mysql这样的数据库存储就会容易想到建立一张User表，有balabala的字段等，在Elasticsearch里这就是一个文档，当然这个文档会属于一个User的类型，各种各样的类型存在于一个索引当中。这里有一份简易的将Elasticsearch和关系型数据术语对照表:\n一个 Elasticsearch 集群可以包含多个索引(数据库)，也就是说其中包含了很多类型(表)。这些类型中包含了很多的文档(行)，然后每个文档中又包含了很多的字段(列)。\n倒排索引 # Elasticsearch最关键的就是提供强大的索引能力，为了提高搜索的性能，Elasticsearch 使用一种称为 倒排索引 的结构，它适用于快速的全文搜索。一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。\n继续上面的例子，假设有这么几条数据(为了简单，去掉info, join_date这两个field):\nID Name Age Sex 1 Kate 24 Female 2 John 24 Male 3 Bill 29 Male ID是Elasticsearch自建的文档id，那么Elasticsearch建立的索引如下:\nName：\nTerm Posting List Kate 1 John 2 Bill 3 Age：\nTerm Posting List 24 [1, 2] 29 3 Sex：\nTerm Posting List Female 1 Male [2, 3] Posting list # Elasticsearch分别为每个field都建立了一个倒排索引，一个字段有一个自己的倒排索引。Kate, John, 24, Female这些叫term，而[1,2]就是Posting List。Posting list就是一个int的数组，存储了所有符合某个term的文档id。\n那么什么是 term dictionary 和 term index？\nTerm Dictionary # 假设我们有很多个 term，比如：\nCarla,Sara,Elin,Ada,Patty,Kate,Selena 如果按照这样的顺序排列，找出某个特定的 term 一定很慢，因为 term 没有排序，需要全部过滤一遍才能找出特定的 term。排序之后就变成了：\nAda,Carla,Elin,Kate,Patty,Sara,Selena 这样我们可以用二分查找的方式，比全遍历更快地找出目标的 term。这个就是 term dictionary。\n有了 term dictionary 之后，可以用 logN 次磁盘查找得到目标。但是磁盘的随机读操作仍然是非常昂贵的（一次 random access 大概需要 10ms 的时间）。所以为了尽量少的读磁盘，有必要把一些数据缓存到内存里。但是整个 term dictionary 本身又太大了，无法完整地放到内存里。于是就有了 term index。\nTerm index # term index 有点像一本字典的大的章节表。比如：\nA 开头的 term ……………. Xxx 页 C 开头的 term ……………. Xxx 页 E 开头的 term ……………. Xxx 页 如果所有的 term 都是英文字符的话，可能这个 term index 就真的是 26 个英文字符表构成的了。但是实际的情况是，term 未必都是英文字符，term 可以是任意的 byte 数组。而且 26 个英文字符也未必是每一个字符都有均等的 term，比如 x 字符开头的 term 可能一个都没有，而 s 开头的 term 又特别多。实际的 term index 是一棵 trie 树：\n例子是一个包含 “A”, “to”, “tea”, “ted”, “ten”, “i”, “in”, 和 “inn” 的 trie 树。这棵树不会包含所有的 term，它包含的是 term 的一些前缀。通过 term index 可以快速地定位到 term dictionary 的某个 offset，然后从这个位置再往后顺序查找。再加上一些压缩技术（搜索 Lucene Finite State Transducers） term index 的尺寸可以只有所有 term 的尺寸的几十分之一，使得用内存缓存整个 term index 变成可能。\n整体上来说就是这样的效果：\n现在我们可以回答“为什么 Elasticsearch/Lucene 检索可以比 Mysql 快了。Mysql 只有 term dictionary 这一层，是以 b-tree 排序的方式存储在磁盘上的。检索一个 term 需要若干次的 random access 的磁盘操作。而 Lucene 在 term dictionary 的基础上添加了 term index 来加速检索，term index 以树的形式缓存在内存中。从 term index 查到对应的 term dictionary 的 block 位置之后，再去磁盘上找 term，大大减少了磁盘的 random access 次数。\nterm index 在内存中是以 FST（finite state transducers）的形式保存的，其特点是非常节省内存。Term dictionary 在磁盘上是以分 block 的方式保存的，一个 block 内部利用公共前缀压缩，比如都是 Ab 开头的单词就可以把 Ab 省去。这样 term dictionary 可以比 b-tree 更节约磁盘空间。\nWeighted Finite-State Transducer # 前面我们说到，term index 在内存中是以 FST（finite state transducers）的形式保存的，那到底FST是什么呢？\nFSM(Finite State Machines)有限状态机: 表示有限个状态（State）集合以及这些状态之间转移和动作的数学模型。其中一个状态被标记为开始状态，0个或更多的状态被标记为final状态。一个FSM同一时间只处于1个状态。\nFST有两个优点：\n空间占用小。通过对词典中单词前缀和后缀的重复利用，压缩了存储空间； 查询速度快。O(len(str))的查询时间复杂度。 下面简单描述下FST的构造过程( 工具演示)\n我们假设创建以下一组映射：$Key \\rightarrow Value$\n“cat” -\u0026gt; 5, “deep” -\u0026gt; 10， “do” -\u0026gt; 15 “dog” -\u0026gt; 2, “dogs” -\u0026gt; 8, 对于经典FST算法来说，要求Key必须按字典序从小到大加入到FST中，原因主要是因为在处理大数据的情况下，我们不太可能把整个FST数据结构都同时放在内存中，而是要边建图边将建好的图存储在外部文件中，以便节省内存。所以我们第一步要对所有的Key排序，对于我给这个例子来说，已经保证了字典序的顺序。\n根据此例子的输入我们可以建立下图所示的FST：\n从上图可以看出，每条边有两个属性，一个表示label（key的元素），另一个表示Value(out)。注意Value不一定是数字，还可一是另一个字符串，但要求Value必须满足叠加性，如这里的正整数2 + 8 = 10。字符串的叠加行为： aa + b = aab。\n建完这个图之后，我们就可以很容易的查找出任意一个key的Value了。例如：查找dog，我们查找的路径为：0 → 4 → 8 → 9。 其权值和为： 2 + 0 + 0 + 0 = 2。其中最后一个零表示 node[9].finalOut = 0。所以“dog”的Value为2。\n到这里，我们已经对FST有了一个感性的认识，下面我们详细讨论FST的建图过程：\n建一个空节点，表示FST的入口，所有的Key都从这个入口开始。 如果还有未处理的Key，则枚举Key的每一个label。处理流程如下： 如果当前节点存在含此label的边，则 如果Value包含该边的out值，则 Value = Value – out 否则，令 temp=out–Value； out = Value 并使下一个节点的所有边out都加上temp。如果下一节点是Final节点 则 FinalOut += temp 进入下一个节点 否则： 新建一个节点另其 out = Value， Value = 0。 如果你看不懂，没关系，我们将用例子演示一遍概算法：\n之前我们介绍的都是对term进行压缩的方法，但是对于 posting list 也需要压缩。\n比如说对于最开始的例子，如果Elasticsearch需要对同学的性别进行索引(这时传统关系型数据库已经哭晕在厕所……)，会怎样？如果有上千万个同学，而世界上只有男/女这样两个性别，每个posting list都会有至少百万个文档id。\nFrame Of Reference # 因此为了能够有效地计算交集和并集，我们需要对这些 posting list 进行排序。这个决定的一个很好的副作用是可以使用增量编码（delta-encoding）压缩 posting list.\n例如，假设 posting list 是 [73, 300, 302, 332, 343, 372]，则增量列表将是 [73, 227, 2, 30, 11, 29]。这里有趣的是，所有的增量都在 0 到 255 之间，所以每个值只需要一个字节。这是 Lucene 用于在磁盘上编码倒排索引的技术：posting list 被分成包含 256 个文档 ID 的块，然后使用增量编码和位打包（bit packing）分别压缩每个块：Lucene 计算最大位数需要在块中存储增量，将此信息添加到块头，然后使用此位数对块的所有增量进行编码。这种编码技术在文献中被称为Frame Of Reference (FOR)，从 Lucene 4.1 开始使用。\n对posting list进行压缩时进行了正序排序。 切分成blocks。具体是怎么做的呢？Lucene是规定每个block是256个delta，这里为了简化一下，搞成3个delta。 看下每个block最大的delta是多少。上图的第一个block，最大的delta是227，最接近的2次幂是256(8bits)，于是规定这个block里都用8bits来编码（看绿色的header就是8），第二个block，最大的delta是30，最接近的2次幂是32（5bits），于是规定这个block里都用5bit来编码（看绿色的header就是5） 很多文章没有说清楚 bitmap 和 roaring bitmaps 跟 frame of reference 有什么区别，应用场景是什么，这里说明一下。\n首先需要明白过滤情况（filtering context）和查询情况（query context）的区别。Elasticsearch 使用的查询语言（DSL）拥有一套查询组件，这些组件可以以无限组合的方式进行搭配。这套组件可以在以下两种情况下使用：过滤情况（filtering context）和查询情况（query context）。\n当使用于 过滤情况 时，查询被设置成一个“不评分”或者“过滤”查询。即，这个查询只是简单的问一个问题：“这篇文档是否匹配？”。回答也是非常的简单，yes 或者 no ，二者必居其一。 过滤查询（Filtering queries）只是简单的检查包含或者排除，这就使得计算起来非常快。考虑到至少有一个过滤查询（filtering query）的结果是 “稀少的”（很少匹配的文档），并且经常使用不评分查询（non-scoring queries），结果会被缓存到内存中以便快速读取，所以有各种各样的手段来优化查询结果。 当使用于 查询情况 时，查询就变成了一个“评分”的查询。和不评分的查询类似，也要去判断这个文档是否匹配，同时它还需要判断这个文档匹配的有 多好（匹配程度如何）。 相反，评分查询（scoring queries）不仅仅要找出匹配的文档，还要计算每个匹配文档的相关性，计算相关性使得它们比不评分查询费力的多。同时，查询结果并不缓存。 过滤（filtering）的目标是减少那些需要通过评分查询（scoring queries）进行检查的文档。\n由于我们只缓存常用的过滤器，压缩率并不像倒排索引那么重要，倒排索引需要为每个可能的词条编码匹配文档。但是，我们需要缓存过滤器比重新执行过滤器更快，因此使用良好的数据结构很重要。\n很长一段时间以来，Lucene 一直使用 bitmap 来将过滤器缓存到内存中。然而，在 Lucene 5 中，我们切换到 Daniel Lemire 的 roaring bitmaps。\n综上，cached filters是保存在内存的，倒排索引是典型的保存在磁盘的。\nBitmap\nBitmap是一种数据结构，假设有某个posting list：[1,3,4,7,10]\n对应的bitmap就是：[1,0,1,1,0,0,1,0,0,1]\n非常直观，用0/1表示某个值是否存在，比如10这个值就对应第10位，对应的bit值是1，这样用一个字节就可以代表8个文档id，旧版本(5.0之前)的Lucene就是用这样的方式来压缩的，但这样的压缩方式仍然不够高效，如果有1亿个文档，那么需要12.5MB的存储空间，这仅仅是对应一个索引字段(我们往往会有很多个索引字段)。于是有人想出了Roaring bitmaps这样更高效的数据结构。\nBitmap的缺点是存储空间随着文档个数线性增长。\nRoaring bitmaps\n它首先根据 16 个最高位将发布列表分成块。 这意味着，例如，第一个块将编码介于 0 和 65535 之间的值，第二个块将编码介于 65536 和 131071 之间的值，等等。然后在每个块中我们独立编码最低的 16 位：如果每块的个数少于 4096，将使用数组表示每个数字，否则使用bitmap。 在此阶段需要注意的重要一点是，虽然我们过去使用上述数组编码每个值需要 4 个字节，但这里的数组只需要为每个值存储 2 个字节，因为块 ID 隐含地为我们提供了 16 个最高位。\n注意：如果一块超过了4096 个值，直接用bitset存，2个字节就用个简单的数组存放好了，比如short[]。\n为什么它使用 4096 作为阈值？ 仅仅因为这个块中的文档数量超过这个数，位图变得比数组更节省内存：\n这就是 roaring bitmaps 有趣的原因：它们基于两种具有非常不同的压缩特性的快速编码技术，并根据内存效率动态决定使用哪种。\nRoaring bit maps 有很多特性，但在 Lucene 的上下文中，真正让我们感兴趣的只有两个：\n迭代所有匹配的文档。如果您在缓存过滤器上运行 constant_score 查询，则通常会使用它。 前进到集合中包含的第一个大于或等于给定整数的文档 ID。如果您将过滤器与查询相交，这通常会被使用。 联合索引 # 所以给定查询过滤条件 age=18 的过程就是先从 term index 找到 18 在 term dictionary 的大概位置，然后再从 term dictionary 里精确地找到 18 这个 term，然后得到一个 posting list 或者一个指向 posting list 位置的指针。然后再查询 gender = 女 的过程也是类似的。最后得出 age=18 AND gender= 女 就是把两个 posting list 做一个“与”的合并。\n这个理论上的“与”合并的操作可不容易。对于 mysql 来说，如果你给 age 和 gender 两个字段都建立了索引，查询的时候只会选择其中最 selective 的来用，然后另外一个条件是在遍历行的过程中在内存中计算之后过滤掉。那么要如何才能联合使用两个索引呢？有两种办法：\n使用 skip list 数据结构。同时遍历 gender 和 age 的 posting list，互相 skip； 使用 bitset 数据结构，对 gender 和 age 两个 filter 分别求出 bitset，对两个 bitset 做 AN 操作。 PostgreSQL 从 8.4 版本开始支持通过 bitmap 联合使用两个索引，就是利用了 bitset 数据结构来做到的。当然一些商业的关系型数据库也支持类似的联合索引的功能。Elasticsearch 支持以上两种的联合索引方式，如果查询的 filter 缓存到了内存中（以 bitset 的形式），那么合并就是两个 bitset 的 AND。如果查询的 filter 没有缓存，那么就用 skip list 的方式去遍历两个 on disk 的 posting list。\nSkip list\n跳跃表具有以下性质：\n由多层有序链表组成。 最底层Level 1的链表包含所有的其他链表的元素。 如果一个元素在链表Level n中存在，那么他在Level n以下的所有链表中都存在。 每个节点都包含连个指针，分别指同Level链表的下一个元素和下一层的元素。 从概念上来说，对于一个很长的 posting list，比如：\n[1,3,13,101,105,108,255,256,257] 我们可以把这个 list 分成三个 block：\n[1,3,13] [101,105,108] [255,256,257] 然后可以构建出 skip list 的第二层：\n[1,101,255] 1,101,255 分别指向自己对应的 block。这样就可以很快地跨 block 的移动指向位置了。\nLucene 自然会对这个 block 再次进行压缩。其压缩方式就是之前介绍的 Frame Of Reference 编码。\n利用 Bitset 合并\nBitset 是一种很直观的数据结构，对应 posting list 如：\n[1,3,4,7,10]\n对应的 bitset 就是：\n[1,0,1,1,0,0,1,0,0,1]\n每个文档按照文档 id 排序对应其中的一个 bit。Bitset 自身就有压缩的特点，其用一个 byte 就可以代表 8 个文档。所以 100 万个文档只需要 12.5 万个 byte。但是考虑到文档可能有数十亿之多，在内存里保存 bitset 仍然是很奢侈的事情。而且对于个每一个 filter 都要消耗一个 bitset，比如 age=18 缓存起来的话是一个 bitset，18\u0026lt;=age\u0026lt;25 是另外一个 filter 缓存起来也要一个 bitset。\n所以秘诀就在于需要有一个数据结构：\n可以很压缩地保存上亿个 bit 代表对应的文档是否匹配 filter； 这个压缩的 bitset 仍然可以很快地进行 AND 和 OR 的逻辑操作。 Lucene 使用的这个数据结构就是之前介绍的 Roaring Bitmap。\n如何减少文档数？\n一种常见的压缩存储时间序列的方式是把多个数据点合并成一行。Opentsdb支持海量数据的一个绝招就是定期把很多行数据合并成一行，这个过程叫compaction。类似的vivdcortext使用mysql存储的时候，也把一分钟的很多数据点合并存储到mysql的一行里以减少行数。\n这个过程可以示例如下：\nTime Value 12:05:00 10 12:05:01 15 12:05:02 14 12:05:03 16 合并之后就变成了：\nTime Value1 Value2 Value3 Value4 12:05 10 15 14 16 可以看到，行变成了列了。每一列可以代表这一分钟内一秒的数据。\nElasticsearch有一个功能可以实现类似的优化效果，那就是Nested Document。我们可以把一段时间的很多个数据点打包存储到一个父文档里，变成其嵌套的子文档。示例如下：\n{timestamp:12:05:01, idc:sz, value1:10,value2:11} {timestamp:12:05:02, idc:sz, value1:9,value2:9} {timestamp:12:05:02, idc:sz, value1:18,value:17} 可以打包成：\n{ max_timestamp:12:05:02, min_timestamp: 1205:01, idc:sz, records: [ {timestamp:12:05:01, value1:10,value2:11} {timestamp:12:05:02, value1:9,value2:9} {timestamp:12:05:02, value1:18,value:17} ] } 这样可以把数据点公共的维度字段上移到父文档里，而不用在每个子文档里重复存储，从而减少索引的尺寸。\n在存储的时候，无论父文档还是子文档，对于 Lucene 来说都是文档，都会有文档 Id。但是对于嵌套文档来说，可以保存起子文档和父文档的文档 id 是连续的，而且父文档总是最后一个。有这样一个排序性作为保障，那么有一个所有父文档的 posting list 就可以跟踪所有的父子关系。也可以很容易地在父子文档 id 之间做转换。把父子关系也理解为一个 filter，那么查询时检索的时候不过是又 AND 了另外一个 filter 而已。前面我们已经看到了 Elasticsearch 可以非常高效地处理多 filter 的情况，充分利用底层的索引。\n使用了嵌套文档之后，对于 term 的 posting list 只需要保存父文档的 doc id 就可以了，可以比保存所有的数据点的 doc id 要少很多。如果我们可以在一个父文档里塞入 50 个嵌套文档，那么 posting list 可以变成之前的 1/50。\nES底层读写工作原理 # es 写数据过程 # 客户端选择一个 node 发送请求过去，这个 node 就是 coordinating node （协调节点）。 coordinating node 对 document 进行路由，将请求转发给对应的 node（有 primary shard）。 实际的 node 上的 primary shard 处理请求，然后将数据同步到 replica node 。 coordinating node 如果发现 primary node 和所有 replica node 都搞定之后，就返回响应结果给客户端。 写数据原理\n先写入内存 buffer，在 buffer 里的时候数据是搜索不到的；同时将数据写入 translog 日志文件。\n如果 buffer 快满了，或者到一定时间，就会将内存 buffer 数据 refresh 到一个新的 segment file 中，但是此时数据不是直接进入 segment file 磁盘文件，而是先进入 os cache 。这个过程就是 refresh 。\n每隔 1 秒钟，es 将 buffer 中的数据写入一个新的 segment file ，每秒钟会产生一个新的磁盘文件 segment file ，这个 segment file 中就存储最近 1 秒内 buffer 中写入的数据。\n但是如果 buffer 里面此时没有数据，那当然不会执行 refresh 操作，如果 buffer 里面有数据，默认 1 秒钟执行一次 refresh 操作，刷入一个新的 segment file 中。\n操作系统里面，磁盘文件其实都有一个东西，叫做 os cache ，即操作系统缓存，就是说数据写入磁盘文件之前，会先进入 os cache ，先进入操作系统级别的一个内存缓存中去。只要 buffer 中的数据被 refresh 操作刷入 os cache 中，这个数据就可以被搜索到了。\n为什么叫 es 是准实时的？ NRT ，全称 near real-time 。默认是每隔 1 秒 refresh 一次的，所以 es 是准实时的，因为写入的数据 1 秒之后才能被看到。可以通过 es 的 restful api 或者 java api ，手动执行一次 refresh 操作，就是手动将 buffer 中的数据刷入 os cache 中，让数据立马就可以被搜索到。只要数据被输入 os cache 中，buffer 就会被清空了，因为不需要保留 buffer 了，数据在 translog 里面已经持久化到磁盘去一份了。\n重复上面的步骤，新的数据不断进入 buffer 和 translog，不断将 buffer 数据写入一个又一个新的 segment file 中去，每次 refresh 完 buffer 清空，translog 保留。随着这个过程推进，translog 会变得越来越大。当 translog 达到一定长度的时候，就会触发 commit 操作。\ncommit 操作发生第一步，就是将 buffer 中现有数据 refresh 到 os cache 中去，清空 buffer。然后，将一个 commit point 写入磁盘文件，里面标识着这个 commit point 对应的所有 segment file ，同时强行将 os cache 中目前所有的数据都 fsync 到磁盘文件中去。最后清空 现有 translog 日志文件，重启一个 translog，此时 commit 操作完成。\n这个 commit 操作叫做 flush 。默认 30 分钟自动执行一次 flush ，但如果 translog 过大，也会触发 flush 。flush 操作就对应着 commit 的全过程，我们可以通过 es api，手动执行 flush 操作，手动将 os cache 中的数据 fsync 强刷到磁盘上去。\ntranslog 日志文件的作用是什么？你执行 commit 操作之前，数据要么是停留在 buffer 中，要么是停留在 os cache 中，无论是 buffer 还是 os cache 都是内存，一旦这台机器死了，内存中的数据就全丢了。所以需要将数据对应的操作写入一个专门的日志文件 translog 中，一旦此时机器宕机，再次重启的时候，es 会自动读取 translog 日志文件中的数据，恢复到内存 buffer 和 os cache 中去。\ntranslog 其实也是先写入 os cache 的，默认每隔 5 秒刷一次到磁盘中去，所以默认情况下，可能有 5 秒的数据会仅仅停留在 buffer 或者 translog 文件的 os cache 中，如果此时机器挂了，会丢失 5 秒钟的数据。但是这样性能比较好，最多丢 5 秒的数据。也可以将 translog 设置成每次写操作必须是直接 fsync 到磁盘，但是性能会差很多。\nindex.translog.sync_interval 控制 translog 多久 fsync 到磁盘,最小为 100ms； index.translog.durability translog 是每 5 秒钟刷新一次还是每次请求都 fsync，这个参数有 2 个取值：request(每次请求都执行 fsync,es 要等 translog fsync 到磁盘后才会返回成功)和 async(默认值，translog 每隔 5 秒钟 fsync 一次)。 所以关于数据丢失问题，数据写入 1 秒后可以搜索到；可能会丢失数据的，有 5 秒的数据，停留在 buffer、translog os cache、segment file os cache 当中，而不在磁盘上，此时如果宕机，会导致 5 秒的数据丢失。\n总结一下，数据先写入内存 buffer，然后每隔 1s，将数据 refresh 到 os cache，到了 os cache 数据就能被搜索到（所以我们才说 es 从写入到能被搜索到，中间有 1s 的延迟）。每隔 5s，将数据写入 translog 文件（这样如果机器宕机，内存数据全没，最多会有 5s 的数据丢失），translog 大到一定程度，或者默认每隔 30mins，会触发 commit 操作，将缓冲区的数据都 flush 到 segment file 磁盘文件中。\n数据写入 segment file 之后，同时就建立好了倒排索引。\nes 读数据过程 # 可以通过 doc id 来查询，会根据 doc id 进行 hash，判断出来当时把 doc id 分配到了哪个 shard 上面去，从那个 shard 去查询。\n客户端发送请求到任意一个 node，成为 coordinate node 。 coordinate node 对 doc id 进行哈希路由，将请求转发到对应的 node，此时会使用 round-robin 随机轮询算法，在 primary shard 以及其所有 replica 中随机选择一个，让读请求负载均衡。 接收请求的 node 返回 document 给 coordinate node 。 coordinate node 返回 document 给客户端。 es 搜索数据过程 # es 最强大的是做全文检索，就是比如你有三条数据：\njava真好玩儿啊 java好难学啊 j2ee特别牛 你根据 java 关键词来搜索，将包含 java 的 document 给搜索出来。es 就会给你返回：java 真好玩儿啊，java 好难学啊。\n客户端发送请求到一个 coordinate node 。 协调节点将搜索请求转发到所有的 shard 对应的 primary shard 或 replica shard ，都可以。 query phase：每个 shard 将自己的搜索结果（其实就是一些 doc id ）返回给协调节点，由协调节点进行数据的合并、排序、分页等操作，产出最终结果。 fetch phase：接着由协调节点根据 doc id 去各个节点上拉取实际的 document 数据，最终返回给客户端。 写请求是写入 primary shard，然后同步给所有的 replica shard；\n读请求可以从 primary shard 或 replica shard 读取，采用的是随机轮询算法。\n删除/更新数据底层原理 # 如果是删除操作，commit 的时候会生成一个 .del 文件，里面将某个 doc 标识为 deleted 状态，那么搜索的时候根据 .del 文件就知道这个 doc 是否被删除了。\n如果是更新操作，就是将原来的 doc 标识为 deleted 状态，然后新写入一条数据。\nbuffer 每 refresh 一次，就会产生一个 segment file ，所以默认情况下是 1 秒钟一个 segment file ，这样下来 segment file 会越来越多，此时会定期执行 merge。每次 merge 的时候，会将多个 segment file 合并成一个，同时这里会将标识为 deleted 的 doc 给物理删除掉，然后将新的 segment file 写入磁盘，这里会写一个 commit point ，标识所有新的 segment file ，然后打开 segment file 供搜索使用，同时删除旧的 segment file 。\n底层 lucene # 简单来说，lucene 就是一个 jar 包，里面包含了封装好的各种建立倒排索引的算法代码。我们用 Java 开发的时候，引入 lucene jar，然后基于 lucene 的 api 去开发就可以了。\n通过 lucene，我们可以将已有的数据建立索引，lucene 会在本地磁盘上面，给我们组织索引的数据结构。\n","date":"22 May 2024","permalink":"/posts/architecture/distributed/es/","section":"博客","summary":"Elasticsearch 是一个高可扩展、开源、全文本的搜索与数据分析引擎。它使您可以近实时地快速存储、搜索和分析大规模数据。它通常用作支持具有复杂搜索功能和要求的应用程序的底层引擎/技术。","title":"Elasticsearch 底层数据结构"},{"content":"Spring基础知识 # Spring 最基础的知识就是那些 Spring 最本质的实现和思想。\none\n在进行“传统的”Java 编程时，对象与对象之间的关系都是紧密耦合的，例如服务类 Service 使用组件 ComponentA，则可能写出这样的代码：\npublic class Service { private ComponentA component = new ComponentA(\u0026#34;first component\u0026#34;); } 在没有 Spring 之前，你应该会觉得这段代码并没有多大问题，毕竟大家都这么写，而且也没有什么更好的方式。就像只有一条大路可走时，大家都朝一个方向走，你大概率不会反思是不是有捷径。\n而随着项目的开发推进，你会发现检验一个方式好不好的硬性标准之一，就是看它有没有拥抱变化的能力。假设有一天，我们的 ComponentA 类的构造器需要更多的参数了，你会发现，上述代码到处充斥着这行需要改进的代码：\nprivate ComponentA component = new ComponentA(\u0026#34;first component\u0026#34;); 此时你可能会想了，那我用下面这种方式来构造 Service 就可以了吧？\npublic class Service { private ComponentA component； public Service(ComponentA component){ this.component = component; } } 当然不行，你忽略了一点，你在构建 Service 对象的时候，不还得使用 new 关键字来构建 Component？需要修改的调用处并不少！\ntwo\n很明显，这是一个噩梦。那么，除了这点，还有没有别的不好的地方呢？上面说的是非单例的情况，如果 ComponentA 本身是一个单例，会不会好些？毕竟我们可能找一个地方 new 一次 ComponentA 实例就足够了，但是你可能会发现另外一些问题。\n下面是一段用“双重检验锁”实现的 CompoentA 类：\npublic class ComponentA{ private volatile static ComponentA INSTANCE; private ComponentA() {} public static ComponentA getInstance(){ if (INSTANCE== null) { synchronized (ComponentA.class) { if (INSTANCE== null) { INSTANCE= new ComponentA(); } } } return INSTANCE; } } 其实写了这么多代码，最终我们只是要一个单例而已。而且假设我们有 ComponentB、ComponentC、ComponentD 等，那上面的重复性代码不都得写一遍？也是烦的不行，不是么？\nothers\n除了上述两个典型问题，还有不易于测试、不易扩展功能（例如支持 AOP）等缺点。说白了，所有问题的根源（之一）就是对象与对象之间耦合性太强了。\nso\n所以 Spring 的引入，解决了上面这些零零种种的问题。那么它是怎么解决的呢？\n这里套用一个租房的场景。我们为什么喜欢通过中介来租房子呢？因为省事呀，只要花点小钱就不用与房东产生直接的“纠缠”了。\nSpring 就是这个思路，它就像一个“中介”公司。当你需要一个依赖的对象（房子）时，你直接把你的需求告诉 Spring（中介）就好了，它会帮你搞定这些依赖对象，按需创建它们，而无需你的任何额外操作。\n不过，在 Spring 中，房东和租房者都是对象实例，只不过换了一个名字叫 Bean 而已。\n可以说，通过一套稳定的生产流程，作为“中介”的 Spring 完成了生产和预装（牵线搭桥）这些 Bean 的任务。此时，你可能想了解更多。例如，如果一个 Bean（租房者）需要用到另外一个 Bean（房子）时，具体是怎么操作呢？\n本质上只能从 Spring“中介”里去找，有时候我们直接根据名称（小区名）去找，有时候则根据类型（户型），各种方式不尽相同。你就把 Spring 理解成一个 Map 型的公司即可，实现如下：\npublic class BeanFactory { private Map\u0026lt;String, Bean\u0026gt; beanMap = new HashMap\u0026lt;\u0026gt;(); public Bean getBean(String key){ return beanMap.get(key) ; } } 如上述代码所示，Bean 所属公司提供了对于 Map 的操作来完成查找，找到 Bean 后装配给其它对象，这就是依赖查找、自动注入的过程。具体的bean的生命周期的可见 spring中bean的生命周期\n那么回过头看，这些 Bean 又是怎么被创建的呢？\n对于一个项目而言，不可避免会出现两种情况：一些对象是需要 Spring 来管理的，另外一些（例如项目中其它的类和依赖的 Jar 中的类）又不需要。所以我们得有一个办法去标识哪些是需要成为 Spring Bean，因此各式各样的注解才应运而生，例如 Component 注解等。\n那有了这些注解后，谁又来做“发现”它们的工作呢？直接配置指定自然不成问题，但是很明显“自动发现”更让人省心。此时，我们往往需要一个扫描器，可以模拟写下这样一个扫描器：\npublic class AnnotationScan { //通过扫描包名来找到Bean void scan(String packages) { // } } 有了扫描器，我们就知道哪些类是需要成为 Bean。\n那怎么实例化为 Bean（也就是一个对象实例而已）呢？很明显，只能通过反射来做了。不过这里面的方式可能有多种：\njava.lang.Class.newInsance() java.lang.reflect.Constructor.newInstance() ReflectionFactory.newConstructorForSerialization() 有了创建，有了装配，一个 Bean 才能成为自己想要的样子。\n而需求总是源源不断的，我们有时候想记录一个方法调用的性能，有时候我们又想在方法调用时输出统一的调用日志。诸如此类，我们肯定不想频繁再来个散弹式的修改。所以我们有了 AOP，帮忙拦截方法调用，进行功能扩展。拦截谁呢？在 Spring 中自然就是 Bean 了。\n其实 AOP 并不神奇，结合刚才的 Bean（中介）公司来讲，假设我们判断出一个 Bean 需要“增强”了，我们直接让它从公司返回的时候，就使用一个代理对象作为返回不就可以了么？示例如下：\npublic class BeanFactory { private Map\u0026lt;String, Bean\u0026gt; beanMap = new HashMap\u0026lt;\u0026gt;(); public Bean getBean(String key){ //查找是否创建过 Bean bean = beanMap.get(key); if(bean != null){ return bean; } //创建一个Bean Bean bean = createBean(); //判断要不要AOP boolean needAop = judgeIfNeedAop(bean); try{ if(needAop) //创建代理对象 bean = createProxyObject(bean); return bean; else: return bean }finally{ beanMap.put(key, bean); } } } 那么怎么知道一个对象要不要 AOP？既然一个对象要 AOP，它肯定被标记了一些“规则”，例如拦截某个类的某某方法，示例如下：\n@Aspect @Service public class AopConfig { @Around(\u0026#34;execution(* com.spring.puzzle.ComponentA.execute()) \u0026#34;) public void recordPayPerformance(ProceedingJoinPoint joinPoint) throws Throwable { // } } 这个时候，很明显了，假设你的 Bean 名字是 ComponentA，那么就应该返回 ComponentA 类型的代理对象了。至于这些规则是怎么建立起来的呢？你看到它上面使用的各种注解大概就能明白其中的规则了，无非就是扫描注解，根据注解创建规则。\nSpring Bean 定义常见错误 # Spring 的核心是围绕 Bean 进行的，不管是 Spring Boot 还是 Spring Cloud，只要名称中带有 Spring 关键字的技术都脱离不了 Bean，而要使用一个 Bean 少不了要先定义出来，所以定义一个 Bean 就变得格外重要了。\n当然，对于这么重要的工作，Spring 自然给我们提供了很多简单易用的方式。然而，这种简单易用得益于 Spring 的“约定大于配置”，但我们往往不见得会对所有的约定都了然于胸，所以仍然会在 Bean 的定义上犯一些经典的错误。\n接下来我们就来了解下那些经典错误以及它们背后的原理，你也可以对照着去看看自己是否也曾犯过，后来又是如何解决的。\n案例 1：隐式扫描不到 Bean 的定义\n在构建 Web 服务时，我们常使用 Spring Boot 来快速构建。例如，使用下面的包结构和相关代码来完成一个简易的 Web 版 HelloWorld：\n其中，负责启动程序的 Application 类定义如下：\npackage com.spring.puzzle.class1.example1.application //省略 import @SpringBootApplication public class Application { public static void main(String[] args) { SpringApplication.run(Application.class, args); } } 提供接口的 HelloWorldController 代码如下：\npackage com.spring.puzzle.class1.example1.application //省略 import @RestController public class HelloWorldController { @RequestMapping(path = \u0026#34;hi\u0026#34;, method = RequestMethod.GET) public String hi(){ return \u0026#34;helloworld\u0026#34;; }; } 上述代码即可实现一个简单的功能：访问 http://localhost:8080/hi 返回 helloworld。两个关键类位于同一个包（即 application）中。其中 HelloWorldController 因为添加了 @RestController，最终被识别成一个 Controller 的 Bean。\n但是，假设有一天，当我们需要添加多个类似的 Controller，同时又希望用更清晰的包层次和结构来管理时，我们可能会去单独建立一个独立于 application 包之外的 Controller 包，并调整类的位置。调整后结构示意如下：\n实际上，我们没有改变任何代码，只是改变了包的结构，但是我们会发现这个 Web 应用失效了，即不能识别出 HelloWorldController 了。也就是说，我们找不到 HelloWorldController 这个 Bean 了。这是为何？\n案例解析\n要了解 HelloWorldController 为什么会失效，就需要先了解之前是如何生效的。对于 Spring Boot 而言，关键点在于 Application.java 中使用了 SpringBootApplication 注解。而这个注解继承了另外一些注解，具体定义如下：\n@Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) }) public @interface SpringBootApplication { //省略非关键代码 } 从定义可以看出，SpringBootApplication 开启了很多功能，其中一个关键功能就是 ComponentScan，参考其配置如下：\n@ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class) 当 Spring Boot 启动时，ComponentScan 的启用意味着会去扫描出所有定义的 Bean，那么扫描什么位置呢？这是由 ComponentScan 注解的 basePackages 属性指定的，具体可参考如下定义：\npublic @interface ComponentScan { /** * Base packages to scan for annotated components. * \u0026lt;p\u0026gt;{@link #value} is an alias for (and mutually exclusive with) this * attribute. * \u0026lt;p\u0026gt;Use {@link #basePackageClasses} for a type-safe alternative to * String-based package names. */ @AliasFor(\u0026#34;value\u0026#34;) String[] basePackages() default {}; //省略其他非关键代码 } 而在我们的案例中，我们直接使用的是 SpringBootApplication 注解定义的 ComponentScan，它的 basePackages 没有指定，所以默认为空（即{}）。此时扫描的是什么包？这里不妨带着这个问题去调试下（调试位置参考 ComponentScanAnnotationParser#parse 方法），调试视图如下：\n从上图可以看出，当 basePackages 为空时，扫描的包会是 declaringClass 所在的包，在本案例中，declaringClass 就是 Application.class，所以扫描的包其实就是它所在的包，即 com.spring.puzzle.class1.example1.application。\n对比我们重组包结构前后，我们自然就找到了这个问题的根源：在调整前，HelloWorldController 在扫描范围内，而调整后，它已经远离了扫描范围（不和 Application.java 一个包了），虽然代码没有一丝丝改变，但是这个功能已经失效了。\n所以，综合来看，这个问题是因为我们不够了解 Spring Boot 的默认扫描规则引起的。我们仅仅享受了它的便捷，但是并未了解它背后的故事，所以稍作变化，就可能玩不转了。\n问题修正\n针对这个案例，有了源码的剖析，我们可以快速找到解决方案了。当然了，我们所谓的解决方案肯定不是说把 HelloWorldController 移动回原来的位置，而是真正去满足需求。在这里，真正解决问题的方式是显式配置 @ComponentScan。具体修改方式如下：\n@SpringBootApplication @ComponentScan(\u0026#34;com.spring.puzzle.class1.example1.controller\u0026#34;) public class Application { public static void main(String[] args) { SpringApplication.run(Application.class, args); } } 通过上述修改，我们显式指定了扫描的范围为 com.spring.puzzle.class1.example1.controller。不过需要注意的是，显式指定后，默认的扫描范围（即 com.spring.puzzle.class1.example1.application）就不会被添加进去了。另外，我们也可以使用 @ComponentScans 来修复问题，使用方式如下：\n@ComponentScans(value = { @ComponentScan(value = \u0026#34;com.spring.puzzle.class1.example1.controller\u0026#34;), @ComponentScan(value = \u0026#34;com.spring.puzzle.class1.example1.application\u0026#34;) }) 顾名思义，可以看出 ComponentScans 相比较 ComponentScan 多了一个 s，支持多个包的扫描范围指定。\n此时，细心的你可能会发现：如果对源码缺乏了解，很容易会顾此失彼。以 ComponentScan 为例，原有的代码扫描了默认包而忽略了其它包；而一旦显式指定其它包，原来的默认扫描包就被忽略了。\n案例 2：定义的 Bean 缺少隐式依赖\n初学 Spring 时，我们往往不能快速转化思维。例如，在程序开发过程中，有时候，一方面我们把一个类定义成 Bean，同时又觉得这个 Bean 的定义除了加了一些 Spring 注解外，并没有什么不同。所以在后续使用时，有时候我们会不假思索地去随意定义它，例如我们会写出下面这样的代码：\n@Service public class ServiceImpl { private String serviceName; public ServiceImpl(String serviceName){ this.serviceName = serviceName; } } ServiceImpl 因为标记为 @Service 而成为一个 Bean。另外我们 ServiceImpl 显式定义了一个构造器。但是，上面的代码不是永远都能正确运行的，有时候会报下面这种错误：\nParameter 0 of constructor in com.spring.puzzle.class1.example2.ServiceImpl required a bean of type \u0026#39;java.lang.String\u0026#39; that could not be found. 那这种错误是怎么发生的呢？下面我们来分析一下。\n案例解析\n当创建一个 Bean 时，调用的方法是 AbstractAutowireCapableBeanFactory#createBeanInstance。它主要包含两大基本步骤：寻找构造器和通过反射调用构造器创建实例。对于这个案例，最核心的代码执行，你可以参考下面的代码片段：\n// Candidate constructors for autowiring? Constructor\u0026lt;?\u0026gt;[] ctors = determineConstructorsFromBeanPostProcessors(beanClass, beanName); if (ctors != null || mbd.getResolvedAutowireMode() == AUTOWIRE_CONSTRUCTOR || mbd.hasConstructorArgumentValues() || !ObjectUtils.isEmpty(args)) { return autowireConstructor(beanName, mbd, ctors, args); } Spring 会先执行 determineConstructorsFromBeanPostProcessors 方法来获取构造器，然后通过 autowireConstructor 方法带着构造器去创建实例。很明显，在本案例中只有一个构造器，所以非常容易跟踪这个问题。\nautowireConstructor 方法要创建实例，不仅需要知道是哪个构造器，还需要知道构造器对应的参数，这点从最后创建实例的方法名也可以看出，参考如下（即 ConstructorResolver#instantiate）：\nprivate Object instantiate( String beanName, RootBeanDefinition mbd, Constructor\u0026lt;?\u0026gt; constructorToUse, Object[] argsToUse) 那么上述方法中存储构造参数的 argsToUse 如何获取呢？换言之，当我们已经知道构造器 ServiceImpl(String serviceName)，要创建出 ServiceImpl 实例，如何确定 serviceName 的值是多少？\n很明显，这里是在使用 Spring，我们不能直接显式使用 new 关键字来创建实例。Spring 只能是去寻找依赖来作为构造器调用参数。\n那么这个参数如何获取呢？可以参考下面的代码片段（即 ConstructorResolver#autowireConstructor）：\nargsHolder = createArgumentArray(beanName, mbd, resolvedValues, bw, paramTypes, paramNames, getUserDeclaredConstructor(candidate), autowiring, candidates.length == 1); 我们可以调用 createArgumentArray 方法来构建调用构造器的参数数组，而这个方法的最终实现是从 BeanFactory 中获取 Bean，可以参考下述调用：\nreturn this.beanFactory.resolveDependency( new DependencyDescriptor(param, true), beanName, autowiredBeanNames, typeConverter); 如果用调试视图，我们则可以看到更多的信息：\n如图所示，上述的调用即是根据参数来寻找对应的 Bean，在本案例中，如果找不到对应的 Bean 就会抛出异常，提示装配失败。\n问题修正\n从源码级别了解了错误的原因后，现在反思为什么会出现这个错误。追根溯源，正如开头所述，因为不了解很多隐式的规则：我们定义一个类为 Bean，如果再显式定义了构造器，那么这个 Bean 在构建时，会自动根据构造器参数定义寻找对应的 Bean，然后反射创建出这个 Bean。\n了解了这个隐式规则后，解决这个问题就简单多了。我们可以直接定义一个能让 Spring 装配给 ServiceImpl 构造器参数的 Bean，例如定义如下：\n//这个bean装配给ServiceImpl的构造器参数“serviceName” @Bean public String serviceName(){ return \u0026#34;MyServiceName\u0026#34;; } 再次运行程序，发现一切正常了。\n所以，我们在使用 Spring 时，不要总想着定义的 Bean 也可以在非 Spring 场合直接用 new 关键字显式使用，这种思路是不可取的。\n另外，类似的，假设我们不了解 Spring 的隐式规则，在修正问题后，我们可能写出更多看似可以运行的程序，代码如下：\n@Service public class ServiceImpl { private String serviceName; public ServiceImpl(String serviceName){ this.serviceName = serviceName; } public ServiceImpl(String serviceName, String otherStringParameter){ this.serviceName = serviceName; } } 如果我们仍用非 Spring 的思维去审阅这段代码，可能不会觉得有什么问题，毕竟 String 类型可以自动装配了，无非就是增加了一个 String 类型的参数而已。\n但是如果你了解 Spring 内部是用反射来构建 Bean 的话，就不难发现问题所在：存在两个构造器，都可以调用时，到底应该调用哪个呢？最终 Spring 无从选择，只能尝试去调用默认构造器，而这个默认构造器又不存在，所以测试这个程序它会出错。\nTips:\nSpring @Required 注解 Spring @Autowired 注解 Spring @Qualifier 注解 案例 3：原型 Bean 被固定\n接下来，我们再来看另外一个关于 Bean 定义不生效的案例。在定义 Bean 时，有时候我们会使用原型 Bean，例如定义如下：\n@Service @Scope(ConfigurableBeanFactory.SCOPE_PROTOTYPE) public class ServiceImpl { } 然后我们按照下面的方式去使用它：\n@RestController public class HelloWorldController { @Autowired private ServiceImpl serviceImpl; @RequestMapping(path = \u0026#34;hi\u0026#34;, method = RequestMethod.GET) public String hi(){ return \u0026#34;helloworld, service is : \u0026#34; + serviceImpl; }; } 结果，我们会发现，不管我们访问多少次http://localhost:8080/hi，访问的结果都是不变的，如下：\nhelloworld, service is : com.spring.puzzle.class1.example3.error.ServiceImpl@4908af 很明显，这很可能和我们定义 ServiceImpl 为原型 Bean 的初衷背道而驰，如何理解这个现象呢？\n案例解析\n当一个属性成员 serviceImpl 声明为 @Autowired 后，那么在创建 HelloWorldController 这个 Bean 时，会先使用构造器反射出实例，然后来装配各个标记为 @Autowired 的属性成员（装配方法参考 AbstractAutowireCapableBeanFactory#populateBean）。\n具体到执行过程，它会使用很多 BeanPostProcessor 来做完成工作，其中一种是 AutowiredAnnotationBeanPostProcessor，它会通过 DefaultListableBeanFactory#findAutowireCandidates 寻找到 ServiceImpl 类型的 Bean，然后设置给对应的属性（即 serviceImpl 成员）。\n关键执行步骤可参考 AutowiredAnnotationBeanPostProcessor.AutowiredFieldElement#inject：\nprotected void inject(Object bean, @Nullable String beanName, @Nullable PropertyValues pvs) throws Throwable { Field field = (Field) this.member; Object value; //寻找“bean” if (this.cached) { value = resolvedCachedArgument(beanName, this.cachedFieldValue); } else { //省略其他非关键代码 value = beanFactory.resolveDependency(desc, beanName, autowiredBeanNames, typeConverter); } if (value != null) { //将bean设置给成员字段 ReflectionUtils.makeAccessible(field); field.set(bean, value); } } 待我们寻找到要自动注入的 Bean 后，即可通过反射设置给对应的 field。这个 field 的执行只发生了一次，所以后续就固定起来了，它并不会因为 ServiceImpl 标记了 SCOPE_PROTOTYPE 而改变。\n所以，当一个单例的 Bean，使用 autowired 注解标记其属性时，你一定要注意这个属性值会被固定下来，即通过autowire引入一定是个单例的。\n问题修正\n通过上述源码分析，我们可以知道要修正这个问题，肯定是不能将 ServiceImpl 的 Bean 固定到属性上的，而应该是每次使用时都会重新获取一次。所以这里我提供了两种修正方式：\n1. 自动注入 Context\n即自动注入 ApplicationContext，然后定义 getServiceImpl() 方法，在方法中获取一个新的 ServiceImpl 类型实例。修正代码如下：\n@RestController public class HelloWorldController { @Autowired private ApplicationContext applicationContext; @RequestMapping(path = \u0026#34;hi\u0026#34;, method = RequestMethod.GET) public String hi(){ return \u0026#34;helloworld, service is : \u0026#34; + getServiceImpl(); }; public ServiceImpl getServiceImpl(){ return applicationContext.getBean(ServiceImpl.class); } } 2. 使用 Lookup 注解\n类似修正方法 1，也添加一个 getServiceImpl 方法，不过这个方法是被 Lookup 标记的。修正代码如下：\n@RestController public class HelloWorldController { @RequestMapping(path = \u0026#34;hi\u0026#34;, method = RequestMethod.GET) public String hi(){ return \u0026#34;helloworld, service is : \u0026#34; + getServiceImpl(); }; @Lookup public ServiceImpl getServiceImpl(){ return null; } } 通过这两种修正方式，再次测试程序，我们会发现结果已经符合预期（每次访问这个接口，都会创建新的 Bean）。\n这里我们不妨再拓展下，讨论下 Lookup 是如何生效的。毕竟在修正代码中，我们看到 getServiceImpl 方法的实现返回值是 null，这或许很难说服自己。\n首先，我们可以通过调试方式看下方法的执行，参考下图：\n从上图我们可以看出，我们最终的执行因为标记了 Lookup 而走入了 CglibSubclassingInstantiationStrategy.LookupOverrideMethodInterceptor，这个方法的关键实现参考 LookupOverrideMethodInterceptor#intercept：\nprivate final BeanFactory owner; public Object intercept(Object obj, Method method, Object[] args, MethodProxy mp) throws Throwable { LookupOverride lo = (LookupOverride) getBeanDefinition().getMethodOverrides().getOverride(method); Assert.state(lo != null, \u0026#34;LookupOverride not found\u0026#34;); Object[] argsToUse = (args.length \u0026gt; 0 ? args : null); // if no-arg, don\u0026#39;t insist on args at all if (StringUtils.hasText(lo.getBeanName())) { return (argsToUse != null ? this.owner.getBean(lo.getBeanName(), argsToUse) : this.owner.getBean(lo.getBeanName())); } else { return (argsToUse != null ? this.owner.getBean(method.getReturnType(), argsToUse) : this.owner.getBean(method.getReturnType())); } } 我们的方法调用最终并没有走入案例代码实现的 return null 语句，而是通过 BeanFactory 来获取 Bean。所以从这点也可以看出，其实在我们的 getServiceImpl 方法实现中，随便怎么写都行，这不太重要。\n例如，我们可以使用下面的实现来测试下这个结论：\n@Lookup public ServiceImpl getServiceImpl(){ //下面的日志会输出么？ log.info(\u0026#34;executing this method\u0026#34;); return null; } 以上代码，添加了一行代码输出日志。测试后，我们会发现并没有日志输出。这也验证了，当使用 Lookup 注解一个方法时，这个方法的具体实现已并不重要。\n再回溯下前面的分析，为什么我们走入了 CGLIB 搞出的类，这是因为我们有方法标记了 Lookup。我们可以从下面的这段代码得到验证，参考 SimpleInstantiationStrategy#instantiate：\n@Override public Object instantiate(RootBeanDefinition bd, @Nullable String beanName, BeanFactory owner) { // Don\u0026#39;t override the class with CGLIB if no overrides. if (!bd.hasMethodOverrides()) { // return BeanUtils.instantiateClass(constructorToUse); } else { // Must generate CGLIB subclass. return instantiateWithMethodInjection(bd, beanName, owner); } } 在上述代码中，当 hasMethodOverrides 为 true 时，则使用 CGLIB。而在本案例中，这个条件的成立在于解析 HelloWorldController 这个 Bean 时，我们会发现有方法标记了 Lookup，此时就会添加相应方法到属性 methodOverrides 里面去（此过程由 AutowiredAnnotationBeanPostProcessor#determineCandidateConstructors 完成）。\n添加后效果图如下：\nSpring Bean 依赖注入常见错误 # 提及 Spring 的优势或特性，我们都会立马想起“控制反转、依赖注入”这八字真言。而 @Autowired 正是用来支持依赖注入的核心利器之一。表面上看，它仅仅是一个注解，在使用上不应该出错。但是，在实际使用中，我们仍然会出现各式各样的错误，而且都堪称经典。所以这节课我就带着你学习下这些经典错误及其背后的原因，以防患于未然。\n案例 1：过多赠予，无所适从\n在使用 @Autowired 时，不管你是菜鸟级还是专家级的 Spring 使用者，都应该制造或者遭遇过类似的错误：\nrequired a single bean, but 2 were found 顾名思义，我们仅需要一个 Bean，但实际却提供了 2 个（这里的“2”在实际错误中可能是其它大于 1 的任何数字）。\n为了重现这个错误，我们可以先写一个案例来模拟下。假设我们在开发一个学籍管理系统案例，需要提供一个 API 根据学生的学号（ID）来移除学生，学生的信息维护肯定需要一个数据库来支撑，所以大体上可以实现如下：\n@RestController @Slf4j @Validated public class StudentController { @Autowired DataService dataService; @RequestMapping(path = \u0026#34;students/{id}\u0026#34;, method = RequestMethod.DELETE) public void deleteStudent(@PathVariable(\u0026#34;id\u0026#34;) @Range(min = 1,max = 100) int id){ dataService.deleteStudent(id); }; } 其中 DataService 是一个接口，其实现依托于 Oracle，代码示意如下：\npublic interface DataService { void deleteStudent(int id); } @Repository @Slf4j public class OracleDataService implements DataService{ @Override public void deleteStudent(int id) { log.info(\u0026#34;delete student info maintained by oracle\u0026#34;); } } 截止目前，运行并测试程序是毫无问题的。但是需求往往是源源不断的，某天我们可能接到节约成本的需求，希望把一些部分非核心的业务从 Oracle 迁移到社区版 Cassandra，所以我们自然会先添加上一个新的 DataService 实现，代码如下：\n@Repository @Slf4j public class CassandraDataService implements DataService{ @Override public void deleteStudent(int id) { log.info(\u0026#34;delete student info maintained by cassandra\u0026#34;); } } 实际上，当我们完成支持多个数据库的准备工作时，程序就已经无法启动了，报错如下：\n很显然，上述报错信息正是我们这一小节讨论的错误，那么这个错误到底是怎么产生的呢？接下来我们具体分析下。\n案例解析\n要找到这个问题的根源，我们就需要对 @Autowired 实现的依赖注入的原理有一定的了解。首先，我们先来了解下 @Autowired 发生的位置和核心过程。\n当一个 Bean 被构建时，核心包括两个基本步骤：\n执行 AbstractAutowireCapableBeanFactory#createBeanInstance 方法：通过构造器反射构造出这个 Bean，在此案例中相当于构建出 StudentController 的实例； 执行 AbstractAutowireCapableBeanFactory#populate 方法：填充（即设置）这个 Bean，在本案例中，相当于设置 StudentController 实例中被 @Autowired 标记的 dataService 属性成员。 在步骤 2 中，“填充”过程的关键就是执行各种 BeanPostProcessor 处理器，关键代码如下：\nprotected void populateBean(String beanName, RootBeanDefinition mbd, @Nullable BeanWrapper bw) { //省略非关键代码 for (BeanPostProcessor bp : getBeanPostProcessors()) { if (bp instanceof InstantiationAwareBeanPostProcessor) { InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; PropertyValues pvsToUse = ibp.postProcessProperties(pvs, bw.getWrappedInstance(), beanName); //省略非关键代码 } } } } 在上述代码执行过程中，因为 StudentController 含有标记为 Autowired 的成员属性 dataService，所以会使用到 AutowiredAnnotationBeanPostProcessor（BeanPostProcessor 中的一种）来完成“装配”过程：找出合适的 DataService 的 bean 并设置给 StudentController#dataService。如果深究这个装配过程，又可以细分为两个步骤：\n寻找出所有需要依赖注入的字段和方法，参考 AutowiredAnnotationBeanPostProcessor#postProcessProperties 中的代码行： InjectionMetadata metadata = findAutowiringMetadata(beanName, bean.getClass(), pvs); 根据依赖信息寻找出依赖并完成注入，以字段注入为例，参考 AutowiredFieldElement#inject 方法： @Override protected void inject(Object bean, @Nullable String beanName, @Nullable PropertyValues pvs) throws Throwable { Field field = (Field) this.member; Object value; //省略非关键代码 try { DependencyDescriptor desc = new DependencyDescriptor(field, this.required); //寻找“依赖”，desc为\u0026#34;dataService\u0026#34;的DependencyDescriptor value = beanFactory.resolveDependency(desc, beanName, autowiredBeanNames, typeConverter); } //省略非关键代码 if (value != null) { ReflectionUtils.makeAccessible(field); //装配“依赖” field.set(bean, value); } } 说到这里，我们基本了解了 @Autowired 过程发生的位置和过程。而且很明显，我们案例中的错误就发生在上述“寻找依赖”的过程中（上述代码的第 9 行），那么到底是怎么发生的呢？我们可以继续刨根问底。\n为了更清晰地展示错误发生的位置，我们可以采用调试的视角展示其位置（即 DefaultListableBeanFactory#doResolveDependency 中代码片段），参考下图：\n如上图所示，当我们根据 DataService 这个类型来找出依赖时，我们会找出 2 个依赖，分别为 CassandraDataService 和 OracleDataService。在这样的情况下，如果同时满足以下两个条件则会抛出本案例的错误：\n调用 determineAutowireCandidate 方法来选出优先级最高的依赖，但是发现并没有优先级可依据。具体选择过程可参考 DefaultListableBeanFactory#determineAutowireCandidate： protected String determineAutowireCandidate(Map\u0026lt;String, Object\u0026gt; candidates, DependencyDescriptor descriptor) { Class\u0026lt;?\u0026gt; requiredType = descriptor.getDependencyType(); String primaryCandidate = determinePrimaryCandidate(candidates, requiredType); if (primaryCandidate != null) { return primaryCandidate; } String priorityCandidate = determineHighestPriorityCandidate(candidates, requiredType); if (priorityCandidate != null) { return priorityCandidate; } // Fallback for (Map.Entry\u0026lt;String, Object\u0026gt; entry : candidates.entrySet()) { String candidateName = entry.getKey(); Object beanInstance = entry.getValue(); if ((beanInstance != null \u0026amp;\u0026amp; this.resolvableDependencies.containsValue(beanInstance)) || matchesBeanName(candidateName, descriptor.getDependencyName())) { return candidateName; } } return null; } 如代码所示，优先级的决策是先根据 @Primary 来决策，其次是 @Priority 决策，最后是根据 Bean 名字的严格匹配来决策。如果这些帮助决策优先级的注解都没有被使用，名字也不精确匹配，则返回 null，告知无法决策出哪种最合适。\n@Autowired 要求是必须注入的（即 required 保持默认值为 true），或者注解的属性类型并不是可以接受多个 Bean 的类型，例如数组、Map、集合。这点可以参考 DefaultListableBeanFactory#indicatesMultipleBeans 的实现： private boolean indicatesMultipleBeans(Class\u0026lt;?\u0026gt; type) { return (type.isArray() || (type.isInterface() \u0026amp;\u0026amp; (Collection.class.isAssignableFrom(type) || Map.class.isAssignableFrom(type)))); } 对比上述两个条件和我们的案例，很明显，案例程序能满足这些条件，所以报错并不奇怪。而如果我们把这些条件想得简单点，或许更容易帮助我们去理解这个设计。就像我们遭遇多个无法比较优劣的选择，却必须选择其一时，与其偷偷地随便选择一种，还不如直接报错，起码可以避免更严重的问题发生。\n问题修正\n针对这个案例，有了源码的剖析，我们可以很快找到解决问题的方法：打破上述两个条件中的任何一个即可，即让候选项具有优先级或压根可以不去选择。不过需要你注意的是，不是每一种条件的打破都满足实际需求，例如我们可以通过使用标记 @Primary 的方式来让被标记的候选者有更高优先级，从而避免报错，但是它并不一定符合业务需求，这就好比我们本身需要两种数据库都能使用，而不是顾此失彼。\n@Repository @Primary @Slf4j public class OracleDataService implements DataService{ //省略非关键代码 } 现在，请你仔细研读上述的两个条件，要同时支持多种 DataService，且能在不同业务情景下精确匹配到要选择到的 DataService，我们可以使用下面的方式去修改：\n@Autowired DataService oracleDataService; 如代码所示，修改方式的精髓在于将属性名和 Bean 名字精确匹配，这样就可以让注入选择不犯难：需要 Oracle 时指定属性名为 oracleDataService，需要 Cassandra 时则指定属性名为 cassandraDataService。\n案例 2：显式引用 Bean 时首字母忽略大小写\n针对案例 1 的问题修正，实际上还存在另外一种常用的解决办法，即采用 @Qualifier 来显式指定引用的是那种服务，例如采用下面的方式：\n@Autowired() @Qualifier(\u0026#34;cassandraDataService\u0026#34;) DataService dataService; 这种方式之所以能解决问题，在于它能让寻找出的 Bean 只有一个（即精确匹配），所以压根不会出现后面的决策过程，可以参考 DefaultListableBeanFactory#doResolveDependency：\n@Nullable public Object doResolveDependency(DependencyDescriptor descriptor, @Nullable String beanName, @Nullable Set\u0026lt;String\u0026gt; autowiredBeanNames, @Nullable TypeConverter typeConverter) throws BeansException { //省略其他非关键代码 //寻找bean过程 Map\u0026lt;String, Object\u0026gt; matchingBeans = findAutowireCandidates(beanName, type, descriptor); if (matchingBeans.isEmpty()) { if (isRequired(descriptor)) { raiseNoMatchingBeanFound(type, descriptor.getResolvableType(), descriptor); } return null; } //省略其他非关键代码 if (matchingBeans.size() \u0026gt; 1) { //省略多个bean的决策过程，即案例1重点介绍内容 } //省略其他非关键代码 } 我们会使用 @Qualifier 指定的名称去匹配，最终只找到了唯一一个。不过在使用 @Qualifier 时，我们有时候会犯另一个经典的小错误，就是我们可能会忽略 Bean 的名称首字母大小写。这里我们把校正后的案例稍稍变形如下：\n@Autowired @Qualifier(\u0026#34;CassandraDataService\u0026#34;) DataService dataService; 运行程序，我们会报错如下：\nException encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name \u0026lsquo;studentController\u0026rsquo;: Unsatisfied dependency expressed through field \u0026lsquo;dataService\u0026rsquo;; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type \u0026lsquo;com.spring.puzzle.class2.example2.DataService\u0026rsquo; available: expected at least 1 bean which qualifies as autowire candidate. Dependency annotations: {@org.springframework.beans.factory.annotation.Autowired(required=true), @org.springframework.beans.factory.annotation.Qualifier(value=CassandraDataService)}\n这里我们很容易得出一个结论：对于 Bean 的名字，如果没有显式指明，就应该是类名，不过首字母应该小写。但是这个轻松得出的结论成立么？\n不妨再测试下，假设我们需要支持 SQLite 这种数据库，我们定义了一个命名为 SQLiteDataService 的实现，然后借鉴之前的经验，我们很容易使用下面的代码来引用这个实现：\n@Autowired @Qualifier(\u0026#34;sQLiteDataService\u0026#34;) DataService dataService; 满怀信心运行完上面的程序，依然会出现之前的错误，而如果改成 SQLiteDataService，则运行通过了。这和之前的结论又矛盾了。所以，显式引用 Bean 时，首字母到底是大写还是小写呢？\n案例解析\n对于这种错误的报错位置，其实我们正好在本案例的开头就贴出了（即第二段代码清单的第 9 行）：\nraiseNoMatchingBeanFound(type, descriptor.getResolvableType(), descriptor); 即当因为名称问题（例如引用 Bean 首字母搞错了）找不到 Bean 时，会直接抛出 NoSuchBeanDefinitionException。\n在这里，我们真正需要关心的问题是：不显式设置名字的 Bean，其默认名称首字母到底是大写还是小写呢？\n看案例的话，当我们启动基于 Spring Boot 的应用程序时，会自动扫描我们的 Package，以找出直接或间接标记了 @Component 的 Bean 的定义（即 BeanDefinition）。例如 CassandraDataService、SQLiteDataService 都被标记了 @Repository，而 Repository 本身被 @Component 标记，所以它们都是间接标记了 @Component。\n一旦找出这些 Bean 的信息，就可以生成这些 Bean 的名字，然后组合成一个个 BeanDefinitionHolder 返回给上层。这个过程关键步骤可以查看下图的代码片段（ClassPathBeanDefinitionScanner#doScan）：\n基本匹配我们前面描述的过程，其中方法调用 BeanNameGenerator#generateBeanName 即用来产生 Bean 的名字，它有两种实现方式。因为 DataService 的实现都是使用注解标记的，所以 Bean 名称的生成逻辑最终调用的其实是 AnnotationBeanNameGenerator#generateBeanName 这种实现方式，我们可以看下它的具体实现，代码如下：\n@Override public String generateBeanName(BeanDefinition definition, BeanDefinitionRegistry registry) { if (definition instanceof AnnotatedBeanDefinition) { String beanName = determineBeanNameFromAnnotation((AnnotatedBeanDefinition) definition); if (StringUtils.hasText(beanName)) { // Explicit bean name found. return beanName; } } // Fallback: generate a unique default bean name. return buildDefaultBeanName(definition, registry); } 大体流程只有两步：看 Bean 有没有显式指明名称，如果有则用显式名称，如果没有则产生一个默认名称。很明显，在我们的案例中，是没有给 Bean 指定名字的，所以产生的 Bean 的名称就是生成的默认名称，查看默认名的产生方法 buildDefaultBeanName，其实现如下：\nprotected String buildDefaultBeanName(BeanDefinition definition) { String beanClassName = definition.getBeanClassName(); Assert.state(beanClassName != null, \u0026#34;No bean class name set\u0026#34;); String shortClassName = ClassUtils.getShortName(beanClassName); return Introspector.decapitalize(shortClassName); } 首先，获取一个简短的 ClassName，然后调用 Introspector#decapitalize 方法，设置首字母大写或小写，具体参考下面的代码实现：\npublic static String decapitalize(String name) { if (name == null || name.length() == 0) { return name; } if (name.length() \u0026gt; 1 \u0026amp;\u0026amp; Character.isUpperCase(name.charAt(1)) \u0026amp;\u0026amp; Character.isUpperCase(name.charAt(0))){ return name; } char chars[] = name.toCharArray(); chars[0] = Character.toLowerCase(chars[0]); return new String(chars); } 到这，我们很轻松地明白了前面两个问题出现的原因：如果一个类名是以两个大写字母开头的，则首字母不变，其它情况下默认首字母变成小写。结合我们之前的案例，SQLiteDataService 的 Bean，其名称应该就是类名本身，而 CassandraDataService 的 Bean 名称则变成了首字母小写（cassandraDataService）。\n问题修正\n现在我们已经从源码级别了解了 Bean 名字产生的规则，就可以很轻松地修正案例中的两个错误了。以引用 CassandraDataService 类型的 Bean 的错误修正为例，可以采用下面这两种修改方式：\n引用处纠正首字母大小写问题： @Autowired @Qualifier(\u0026#34;cassandraDataService\u0026#34;) DataService dataService; 定义处显式指定 Bean 名字，我们可以保持引用代码不变，而通过显式指明 CassandraDataService 的 Bean 名称为 CassandraDataService 来纠正这个问题。 @Repository(\u0026#34;CassandraDataService\u0026#34;) @Slf4j public class CassandraDataService implements DataService { //省略实现 } 现在，我们的程序就可以精确匹配到要找的 Bean 了。比较一下这两种修改方法的话，如果你不太了解源码，不想纠结于首字母到底是大写还是小写，建议你用第二种方法去避免困扰。\n案例 3：引用内部类的 Bean 遗忘类名\n解决完案例 2，是不是就意味着我们能搞定所有 Bean 的显式引用，不再犯错了呢？天真了。我们可以沿用上面的案例，稍微再添加点别的需求，例如我们需要定义一个内部类来实现一种新的 DataService，代码如下：\npublic class StudentController { @Repository public static class InnerClassDataService implements DataService{ @Override public void deleteStudent(int id) { //空实现 } } //省略其他非关键代码 } 遇到这种情况，我们一般都会很自然地用下面的方式直接去显式引用这个 Bean：\n@Autowired @Qualifier(\u0026#34;innerClassDataService\u0026#34;) DataService innerClassDataService; 很明显，有了案例 2 的经验，我们上来就直接采用了首字母小写以避免案例 2 中的错误，但这样的代码是不是就没问题了呢？实际上，仍然会报错“找不到 Bean”，这是为什么？\n案例解析\n实际上，我们遭遇的情况是“如何引用内部类的 Bean”。解析案例 2 的时候，我曾经贴出了如何产生默认 Bean 名的方法（即 AnnotationBeanNameGenerator#buildDefaultBeanName），当时我们只关注了首字母是否小写的代码片段，而在最后变换首字母之前，有一行语句是对 class 名字的处理，代码如下：\nString shortClassName = ClassUtils.getShortName(beanClassName); 我们可以看下它的实现，参考 ClassUtils#getShortName 方法：\npublic static String getShortName(String className) { Assert.hasLength(className, \u0026#34;Class name must not be empty\u0026#34;); int lastDotIndex = className.lastIndexOf(PACKAGE_SEPARATOR); int nameEndIndex = className.indexOf(CGLIB_CLASS_SEPARATOR); if (nameEndIndex == -1) { nameEndIndex = className.length(); } String shortName = className.substring(lastDotIndex + 1, nameEndIndex); shortName = shortName.replace(INNER_CLASS_SEPARATOR, PACKAGE_SEPARATOR); return shortName; } 很明显，假设我们是一个内部类，例如下面的类名：\ncom.spring.puzzle.class2.example3.StudentController.InnerClassDataService 在经过这个方法的处理后，我们得到的其实是下面这个名称：\nStudentController.InnerClassDataService 最后经过 Introspector.decapitalize 的首字母变换，最终获取的 Bean 名称如下：\nstudentController.InnerClassDataService 所以我们在案例程序中，直接使用 innerClassDataService 自然找不到想要的 Bean。\n问题修正\n通过案例解析，我们很快就找到了这个内部类，Bean 的引用问题顺手就修正了，如下：\n@Autowired @Qualifier(\u0026#34;studentController.InnerClassDataService\u0026#34;) DataService innerClassDataService; 这个引用看起来有些许奇怪，但实际上是可以工作的，反而直接使用 innerClassDataService 来引用倒是真的不可行。\n通过这个案例我们可以看出，对源码的学习是否全面决定了我们以后犯错的可能性大小。如果我们在学习案例 2 时，就对 class 名称的变化部分的源码进行了学习，那么这种错误是不容易犯的。不过有时候我们确实很难一上来就把学习开展的全面而深入，总是需要时间和错误去锤炼的。\n上面介绍了 3 个 Spring 编程中关于依赖注入的错误案例，这些错误都是比较常见的。如果你仔细分析的话，你会发现它们大多都是围绕着 @Autowired、@Qualifier 的使用而发生，而且自动注入的类型也都是普通对象类型。\n那在实际应用中，我们也会使用 @Value 等不太常见的注解来完成自动注入，同时也存在注入到集合、数组等复杂类型的场景。这些情况下，我们也会遇到一些问题。所以这一讲我们不妨来梳理下。\n案例 1：@Value 没有注入预期的值\n在装配对象成员属性时，我们常常会使用 @Autowired 来装配。但是，有时候我们也使用 @Value 进行装配。不过这两种注解使用风格不同，使用 @Autowired 一般都不会设置属性值，而 @Value 必须指定一个字符串值，因为其定义做了要求，定义代码如下：\npublic @interface Value { /** * The actual value expression \u0026amp;mdash; for example, \u0026lt;code\u0026gt;#{systemProperties.myProp}\u0026lt;/code\u0026gt;. */ String value(); } 另外在比较这两者的区别时，我们一般都会因为 @Value 常用于 String 类型的装配而误以为 @Value 不能用于非内置对象的装配，实际上这是一个常见的误区。例如，我们可以使用下面这种方式来 Autowired 一个属性成员：\n@Value(\u0026#34;#{student}\u0026#34;) private Student student; 其中 student 这个 Bean 定义如下：\n@Bean public Student student(){ Student student = createStudent(1, \u0026#34;xie\u0026#34;); return student; } 当然，正如前面提及，我们使用 @Value 更多是用来装配 String，而且它支持多种强大的装配方式，典型的方式参考下面的示例：\n//注册正常字符串 @Value(\u0026#34;我是字符串\u0026#34;) private String text; //注入系统参数、环境变量或者配置文件中的值 @Value(\u0026#34;${ip}\u0026#34;) private String ip //注入其他Bean属性，其中student为bean的ID，name为其属性 @Value(\u0026#34;#{student.name}\u0026#34;) private String name; 上面我给你简单介绍了 @Value 的强大功能，以及它和 @Autowired 的区别。那么在使用 @Value 时可能会遇到那些错误呢？这里分享一个最为典型的错误，即使用 @Value 可能会注入一个不是预期的值。\n我们可以模拟一个场景，我们在配置文件 application.properties 配置了这样一个属性：\nusername=admin password=pass 然后我们在一个 Bean 中，分别定义两个属性来引用它们：\n@RestController @Slf4j public class ValueTestController { @Value(\u0026#34;${username}\u0026#34;) private String username; @Value(\u0026#34;${password}\u0026#34;) private String password; @RequestMapping(path = \u0026#34;user\u0026#34;, method = RequestMethod.GET) public String getUser(){ return username + \u0026#34;:\u0026#34; + password; }; } 当我们去打印上述代码中的 username 和 password 时，我们会发现 password 正确返回了，但是 username 返回的并不是配置文件中指明的 admin，而是运行这段程序的计算机用户名。很明显，使用 @Value 装配的值没有完全符合我们的预期。\n案例解析\n通过分析运行结果，我们可以知道 @Value 的使用方式应该是没有错的，毕竟 password 这个字段装配上了，但是为什么 username 没有生效成正确的值？接下来我们就来具体解析下。\n我们首先了解下对于 @Value，Spring 是如何根据 @Value 来查询“值”的。我们可以先通过方法 DefaultListableBeanFactory#doResolveDependency 来了解 @Value 的核心工作流程，代码如下：\n@Nullable public Object doResolveDependency(DependencyDescriptor descriptor, @Nullable String beanName, @Nullable Set\u0026lt;String\u0026gt; autowiredBeanNames, @Nullable TypeConverter typeConverter) throws BeansException { //省略其他非关键代码 Class\u0026lt;?\u0026gt; type = descriptor.getDependencyType(); //寻找@Value Object value = getAutowireCandidateResolver().getSuggestedValue(descriptor); if (value != null) { if (value instanceof String) { //解析Value值 String strVal = resolveEmbeddedValue((String) value); BeanDefinition bd = (beanName != null \u0026amp;\u0026amp; containsBean(beanName) ? getMergedBeanDefinition(beanName) : null); value = evaluateBeanDefinitionString(strVal, bd); } //转化Value解析的结果到装配的类型 TypeConverter converter = (typeConverter != null ? typeConverter : getTypeConverter()); try { return converter.convertIfNecessary(value, type, descriptor.getTypeDescriptor()); } catch (UnsupportedOperationException ex) { //异常处理 } } //省略其他非关键代码 } ​可以看到，@Value 的工作大体分为以下三个核心步骤。\n寻找 @Value 在这步中，主要是判断这个属性字段是否标记为 @Value，依据的方法参考 QualifierAnnotationAutowireCandidateResolver#findValue：\n@Nullable protected Object findValue(Annotation[] annotationsToSearch) { if (annotationsToSearch.length \u0026gt; 0) { AnnotationAttributes attr = AnnotatedElementUtils.getMergedAnnotationAttributes( AnnotatedElementUtils.forAnnotations(annotationsToSearch), this.valueAnnotationType); //valueAnnotationType即为@Value if (attr != null) { return extractValue(attr); } } return null; } 解析 @Value 的字符串值 如果一个字段标记了 @Value，则可以拿到对应的字符串值，然后就可以根据字符串值去做解析，最终解析的结果可能是一个字符串，也可能是一个对象，这取决于字符串怎么写。\n将解析结果转化为要装配的对象的类型 当拿到第二步生成的结果后，我们会发现可能和我们要装配的类型不匹配。假设我们定义的是 UUID，而我们获取的结果是一个字符串，那么这个时候就会根据目标类型来寻找转化器执行转化，字符串到 UUID 的转化实际上发生在 UUIDEditor 中：\npublic class UUIDEditor extends PropertyEditorSupport { @Override public void setAsText(String text) throws IllegalArgumentException { if (StringUtils.hasText(text)) { //转化操作 setValue(UUID.fromString(text.trim())); } else { setValue(null); } } //省略其他非关代码 } 通过对上面几个关键步骤的解析，我们大体了解了 @Value 的工作流程。结合我们的案例，很明显问题应该发生在第二步，即解析 Value 指定字符串过程，执行过程参考下面的关键代码行：\nString strVal = resolveEmbeddedValue((String) value); 这里其实是在解析嵌入的值，实际上就是“替换占位符”工作。具体而言，它采用的是 PropertySourcesPlaceholderConfigurer 根据 PropertySources 来替换。不过当使用 ${username} 来获取替换值时，其最终执行的查找并不是局限在 application.property 文件中的。通过调试，我们可以看到下面的这些“源”都是替换依据：\n[ConfigurationPropertySourcesPropertySource {name=\u0026#39;configurationProperties\u0026#39;}, StubPropertySource {name=\u0026#39;servletConfigInitParams\u0026#39;}, ServletContextPropertySource {name=\u0026#39;servletContextInitParams\u0026#39;}, PropertiesPropertySource {name=\u0026#39;systemProperties\u0026#39;}, OriginAwareSystemEnvironmentPropertySource {name=\u0026#39;systemEnvironment\u0026#39;}, RandomValuePropertySource {name=\u0026#39;random\u0026#39;},OriginTrackedMapPropertySource {name=\u0026#39;applicationConfig: classpath:/application.properties]\u0026#39;},MapPropertySource {name=\u0026#39;devtools\u0026#39;}] 而具体的查找执行，我们可以通过下面的代码（PropertySourcesPropertyResolver#getProperty）来获取它的执行方式：\n@Nullable protected \u0026lt;T\u0026gt; T getProperty(String key, Class\u0026lt;T\u0026gt; targetValueType, boolean resolveNestedPlaceholders) { if (this.propertySources != null) { for (PropertySource\u0026lt;?\u0026gt; propertySource : this.propertySources) { Object value = propertySource.getProperty(key); if (value != null) { //查到value即退出 return convertValueIfNecessary(value, targetValueType); } } } return null; } 从这可以看出，在解析 Value 字符串时，其实是有顺序的（查找的源是存在 CopyOnWriteArrayList 中，在启动时就被有序固定下来），一个一个“源”执行查找，在其中一个源找到后，就可以直接返回了。\n如果我们查看 systemEnvironment 这个源，会发现刚好有一个 username 和我们是重合的，且值不是 pass。\n所以，讲到这里，你应该知道问题所在了吧？这是一个误打误撞的例子，刚好系统环境变量（systemEnvironment）中含有同名的配置。实际上，对于系统参数（systemProperties）也是一样的，这些参数或者变量都有很多，如果我们没有意识到它的存在，起了一个同名的字符串作为 @Value 的值，则很容易引发这类问题。\n问题修正\n针对这个案例，有了源码的剖析，我们就可以很快地找到解决方案了。例如我们可以避免使用同一个名称，具体修改如下：\nuser.name=admin user.password=pass 但是如果我们这么改的话，其实还是不行的。实际上，通过之前的调试方法，我们可以找到类似的原因，在 systemProperties 这个 PropertiesPropertySource 源中刚好存在 user.name，真是无巧不成书。所以命名时，我们一定要注意不仅要避免和环境变量冲突，也要注意避免和系统变量等其他变量冲突，这样才能从根本上解决这个问题。\n通过这个案例，我们可以知道：Spring 给我们提供了很多好用的功能，但是这些功能交织到一起后，就有可能让我们误入一些坑，只有了解它的运行方式，我们才能迅速定位问题、解决问题。\n案例 2：错乱的注入集合\n前面我们介绍了很多自动注入的错误案例，但是这些案例都局限在单个类型的注入，对于集合类型的注入并无提及。实际上，集合类型的自动注入是 Spring 提供的另外一个强大功能。\n假设我们存在这样一个需求：存在多个学生 Bean，我们需要找出来，并存储到一个 List 里面去。多个学生 Bean 的定义如下：\n@Bean public Student student1(){ return createStudent(1, \u0026#34;xie\u0026#34;); } @Bean public Student student2(){ return createStudent(2, \u0026#34;fang\u0026#34;); } private Student createStudent(int id, String name) { Student student = new Student(); student.setId(id); student.setName(name); return student; } 有了集合类型的自动注入后，我们就可以把零散的学生 Bean 收集起来了，代码示例如下：\n@RestController @Slf4j public class StudentController { private List\u0026lt;Student\u0026gt; students; public StudentController(List\u0026lt;Student\u0026gt; students){ this.students = students; } @RequestMapping(path = \u0026#34;students\u0026#34;, method = RequestMethod.GET) public String listStudents(){ return students.toString(); }; } 通过上述代码，我们就可以完成集合类型的注入工作，输出结果如下：\n[Student(id=1, name=xie), Student(id=2, name=fang)] 然而，业务总是复杂的，需求也是一直变动的。当我们持续增加一些 student 时，可能就不喜欢用这种方式来注入集合类型了，而是倾向于用下面的方式去完成注入工作：\n@Bean public List\u0026lt;Student\u0026gt; students(){ Student student3 = createStudent(3, \u0026#34;liu\u0026#34;); Student student4 = createStudent(4, \u0026#34;fu\u0026#34;); return Arrays.asList(student3, student4); } 为了好记，这里我们不妨将上面这种方式命名为“直接装配方式”，而将之前的那种命名为“收集方式”。\n实际上，如果这两种方式是非此即彼的存在，自然没有任何问题，都能玩转。但是如果我们不小心让这 2 种方式同时存在了，结果会怎样？\n这时候很多人都会觉得 Spring 很强大，肯定会合并上面的结果，或者认为肯定是以直接装配结果为准。然而，当我们运行起程序，就会发现后面的注入方式根本没有生效。即依然返回的是前面定义的 2 个学生。为什么会出现这样的错误呢？\n案例解析\n要了解这个错误的根本原因，你就得先清楚这两种注入风格在 Spring 中是如何实现的。对于收集装配风格，Spring 使用的是 DefaultListableBeanFactory#resolveMultipleBeans 来完成装配工作，针对本案例关键的核心代码如下：\nprivate Object resolveMultipleBeans(DependencyDescriptor descriptor, @Nullable String beanName, @Nullable Set\u0026lt;String\u0026gt; autowiredBeanNames, @Nullable TypeConverter typeConverter) { final Class\u0026lt;?\u0026gt; type = descriptor.getDependencyType(); if (descriptor instanceof StreamDependencyDescriptor) { //装配stream return stream; } else if (type.isArray()) { //装配数组 return result; } else if (Collection.class.isAssignableFrom(type) \u0026amp;\u0026amp; type.isInterface()) { //装配集合 //获取集合的元素类型 Class\u0026lt;?\u0026gt; elementType = descriptor.getResolvableType().asCollection().resolveGeneric(); if (elementType == null) { return null; } //根据元素类型查找所有的bean Map\u0026lt;String, Object\u0026gt; matchingBeans = findAutowireCandidates(beanName, elementType, new MultiElementDescriptor(descriptor)); if (matchingBeans.isEmpty()) { return null; } if (autowiredBeanNames != null) { autowiredBeanNames.addAll(matchingBeans.keySet()); } //转化查到的所有bean放置到集合并返回 TypeConverter converter = (typeConverter != null ? typeConverter : getTypeConverter()); Object result = converter.convertIfNecessary(matchingBeans.values(), type); //省略非关键代码 return result; } else if (Map.class == type) { //解析map return matchingBeans; } else { return null; } } 到这，我们就不难概括出这种收集式集合装配方式的大体过程了。\n获取集合类型的元素类型 针对本案例，目标类型定义为 List students，所以元素类型为 Student，获取的具体方法参考代码行：\nClass elementType = descriptor.getResolvableType().asCollection().resolveGeneric(); 根据元素类型，找出所有的 Bean 有了上面的元素类型，即可根据元素类型来找出所有的 Bean，关键代码行如下：\nMap matchingBeans = findAutowireCandidates(beanName, elementType, new MultiElementDescriptor(descriptor)); 将匹配的所有的 Bean 按目标类型进行转化 经过步骤 2，我们获取的所有的 Bean 都是以 java.util.LinkedHashMap.LinkedValues 形式存储的，和我们的目标类型大概率不同，所以最后一步需要做的是按需转化。在本案例中，我们就需要把它转化为 List，转化的关键代码如下：\nObject result = converter.convertIfNecessary(matchingBeans.values(), type); 如果我们继续深究执行细节，就可以知道最终是转化器 CollectionToCollectionConverter 来完成这个转化过程。\n学习完收集方式的装配原理，我们再来看下直接装配方式的执行过程，实际上这步在前面的课程中我们就提到过（即 DefaultListableBeanFactory#findAutowireCandidates 方法执行），具体的执行过程这里就不多说了。\n知道了执行过程，接下来无非就是根据目标类型直接寻找匹配的 Bean。在本案例中，就是将 Bean 名称为 students 的 List 装配给 StudentController#students 属性。\n了解了这两种方式，我们再来思考这两种方式的关系：当同时满足这两种装配方式时，Spring 是如何处理的？这里我们可以参考方法 DefaultListableBeanFactory#doResolveDependency 的几行关键代码，代码如下：\nObject multipleBeans = resolveMultipleBeans(descriptor, beanName, autowiredBeanNames, typeConverter); if (multipleBeans != null) { return multipleBeans; } Map\u0026lt;String, Object\u0026gt; matchingBeans = findAutowireCandidates(beanName, type, descriptor); 很明显，这两种装配集合的方式是不能同存的，结合本案例，当使用收集装配方式来装配时，能找到任何一个对应的 Bean，则返回，如果一个都没有找到，才会采用直接装配的方式。说到这里，你大概能理解为什么后期以 List 方式直接添加的 Student Bean 都不生效了吧。\n问题修正\n现在如何纠正这个问题就变得简单多了，就是你一定要下意识地避免这 2 种方式共存去装配集合，只用一个这个问题就迎刃而解了。例如，在这里，我们可以使用直接装配的方式去修正问题，代码如下：\n@Bean public List\u0026lt;Student\u0026gt; students(){ Student student1 = createStudent(1, \u0026#34;xie\u0026#34;); Student student2 = createStudent(2, \u0026#34;fang\u0026#34;); Student student3 = createStudent(3, \u0026#34;liu\u0026#34;); Student student4 = createStudent(4, \u0026#34;fu\u0026#34;); return Arrays.asList(student1，student2，student3, student4); } 也可以使用收集方式来修正问题时，代码如下：\n@Bean public Student student1(){ return createStudent(1, \u0026#34;xie\u0026#34;); } @Bean public Student student2(){ return createStudent(2, \u0026#34;fang\u0026#34;); } @Bean public Student student3(){ return createStudent(3, \u0026#34;liu\u0026#34;); } @Bean public Student student4(){ return createStudent(4, \u0026#34;fu\u0026#34;); } 总之，都是可以的。还有一点要注意：在对于同一个集合对象的注入上，混合多种注入方式是不可取的，这样除了错乱，别无所得。\nSpring Bean 生命周期常见错误 # 虽然说 Spring 容器上手简单，可以仅仅通过学习一些有限的注解，即可达到快速使用的目的。但在工程实践中，我们依然会从中发现一些常见的错误。尤其当你对 Spring 的生命周期还没有深入了解时，类初始化及销毁过程中潜在的约定就不会很清楚。\n这会导致这样一些状况发生：有些错误，我们可以在 Spring 的异常提示下快速解决，但却不理解背后的原理；而另一些错误，并不容易在开发环境下被发现，从而在产线上造成较为严重的后果。\n接下来我们就具体解析下这些常见案例及其背后的原理。\n案例 1：构造器内抛空指针异常\n先看个例子。在构建宿舍管理系统时，有 LightMgrService 来管理 LightService，从而控制宿舍灯的开启和关闭。我们希望在 LightMgrService 初始化时能够自动调用 LightService 的 check 方法来检查所有宿舍灯的电路是否正常，代码如下：\nimport org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Component; @Component public class LightMgrService { @Autowired private LightService lightService; public LightMgrService() { lightService.check(); } } 我们在 LightMgrService 的默认构造器中调用了通过 @Autoware 注入的成员变量 LightService 的 check 方法：\n@Service public class LightService { public void start() { System.out.println(\u0026#34;turn on all lights\u0026#34;); } public void shutdown() { System.out.println(\u0026#34;turn off all lights\u0026#34;); } public void check() { System.out.println(\u0026#34;check all lights\u0026#34;); } } 以上代码定义了 LightService 对象的原始类。\n从整个案例代码实现来看，我们的期待是在 LightMgrService 初始化过程中，LightService 因为标记为 @Autowired，所以能被自动装配好；然后在 LightMgrService 的构造器执行中，LightService 的 check 方法能被自动调用；最终打印出 check all lights。\n然而事与愿违，我们得到的只会是 NullPointerException，错误示例如下：\n这是为什么呢？\n案例解析\n显然这是新手最常犯的错误，但是问题的根源，是我们对 Spring 类初始化过程没有足够的了解。下面这张时序图描述了 Spring 启动时的一些关键结点：\n这个图初看起来复杂，我们不妨将其分为三部分：\n第一部分，将一些必要的系统类，比如 Bean 的后置处理器类，注册到 Spring 容器，其中就包括接下来要讲到的 CommonAnnotationBeanPostProcessor 类； 第二部分，将这些后置处理器实例化，并注册到 Spring 的容器中； 第三部分，实例化所有用户定制类，调用后置处理器进行辅助装配、类初始化等等。 第一部分和第二部分并非是我们今天要讨论的重点，这里仅仅是为了让你知道 CommonAnnotationBeanPostProcessor 这个后置处理类是何时被 Spring 加载和实例化的。\n请先学习 Spring中Bean的生命周期\n这里我顺便给你拓展两个知识点：\n很多必要的系统类，尤其是 Bean 后置处理器（比如 CommonAnnotationBeanPostProcessor、AutowiredAnnotationBeanPostProcessor 等），都是被 Spring 统一加载和管理的，并在 Spring 中扮演了非常重要的角色； 通过 Bean 后置处理器，Spring 能够非常灵活地在不同的场景调用不同的后置处理器，比如接下来我会讲到示例问题如何修正，修正方案中提到的 PostConstruct 注解，它的处理逻辑就需要用到 CommonAnnotationBeanPostProcessor（继承自 InitDestroyAnnotationBeanPostProcessor）这个后置处理器。 现在我们重点看下第三部分，即 Spring 初始化单例类的一般过程，基本都是 getBean()-\u0026gt;doGetBean()-\u0026gt;getSingleton()，如果发现 Bean 不存在，则调用 createBean()-\u0026gt;doCreateBean() 进行实例化。\n查看 doCreateBean() 的源代码如下：\nprotected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final @Nullable Object[] args) throws BeanCreationException { //省略非关键代码 if (instanceWrapper == null) { instanceWrapper = createBeanInstance(beanName, mbd, args); } final Object bean = instanceWrapper.getWrappedInstance(); //省略非关键代码 Object exposedObject = bean; try { populateBean(beanName, mbd, instanceWrapper); exposedObject = initializeBean(beanName, exposedObject, mbd); } catch (Throwable ex) { //省略非关键代码 } 上述代码完整地展示了 Bean 初始化的三个关键步骤，按执行顺序分别是第 5 行的 createBeanInstance，第 12 行的 populateBean，以及第 13 行的 initializeBean，分别对应实例化 Bean，注入 Bean 依赖，以及初始化 Bean （例如执行 @PostConstruct 标记的方法 ）这三个功能，这也和上述时序图的流程相符。\n而用来实例化 Bean 的 createBeanInstance 方法通过依次调用 DefaultListableBeanFactory.instantiateBean() \u0026gt;SimpleInstantiationStrategy.instantiate()，最终执行到 BeanUtils.instantiateClass()，其代码如下：\npublic static \u0026lt;T\u0026gt; T instantiateClass(Constructor\u0026lt;T\u0026gt; ctor, Object... args) throws BeanInstantiationException { Assert.notNull(ctor, \u0026#34;Constructor must not be null\u0026#34;); try { ReflectionUtils.makeAccessible(ctor); return (KotlinDetector.isKotlinReflectPresent() \u0026amp;\u0026amp; KotlinDetector.isKotlinType(ctor.getDeclaringClass()) ? KotlinDelegate.instantiateClass(ctor, args) : ctor.newInstance(args)); } catch (InstantiationException ex) { throw new BeanInstantiationException(ctor, \u0026#34;Is it an abstract class?\u0026#34;, ex); } //省略非关键代码 } 这里因为当前的语言并非 Kotlin，所以最终将调用 ctor.newInstance() 方法实例化用户定制类 LightMgrService，而默认构造器显然是在类实例化的时候被自动调用的，Spring 也无法控制。而此时负责自动装配的 populateBean 方法还没有被执行，LightMgrService 的属性 LightService 还是 null，因而得到空指针异常也在情理之中。\n问题修正\n通过源码分析，现在我们知道了问题的根源，就是在于使用 @Autowired 直接标记在成员属性上而引发的装配行为是发生在构造器执行之后的。所以这里我们可以通过下面这种修订方法来纠正这个问题：\n@Component public class LightMgrService { private LightService lightService; public LightMgrService(LightService lightService) { this.lightService = lightService; lightService.check(); } } 当使用上面的代码时，构造器参数 LightService 会被自动注入 LightService 的 Bean，从而在构造器执行时，不会出现空指针。可以说，使用构造器参数来隐式注入是一种 Spring 最佳实践，因为它成功地规避了案例 1 中的问题。\n另外，除了这种纠正方式，有没有别的方式？\n实际上，Spring 在类属性完成注入之后，会回调用户定制的初始化方法。即在 populateBean 方法之后，会调用 initializeBean 方法，我们来看一下它的关键代码：\nprotected Object initializeBean(final String beanName, final Object bean, @Nullable RootBeanDefinition mbd) { //省略非关键代码 if (mbd == null || !mbd.isSynthetic()) { wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); } try { invokeInitMethods(beanName, wrappedBean, mbd); } //省略非关键代码 } 这里你可以看到 applyBeanPostProcessorsBeforeInitialization 和 invokeInitMethods 这两个关键方法的执行，它们分别处理了 @PostConstruct 注解和 InitializingBean 接口这两种不同的初始化方案的逻辑。这里我再详细地给你讲讲。\napplyBeanPostProcessorsBeforeInitialization 与 @PostConstruct applyBeanPostProcessorsBeforeInitialization 方法最终执行到后置处理器 InitDestroyAnnotationBeanPostProcessor 的 buildLifecycleMetadata 方法（CommonAnnotationBeanPostProcessor 的父类）：\nprivate LifecycleMetadata buildLifecycleMetadata(final Class\u0026lt;?\u0026gt; clazz) { //省略非关键代码 do { //省略非关键代码 final List\u0026lt;LifecycleElement\u0026gt; currDestroyMethods = new ArrayList\u0026lt;\u0026gt;(); ReflectionUtils.doWithLocalMethods(targetClass, method -\u0026gt; { //此处的 this.initAnnotationType 值，即为 PostConstruct.class if (this.initAnnotationType != null \u0026amp;\u0026amp; method.isAnnotationPresent(this.initAnnotationType)) { LifecycleElement element = new LifecycleElement(method); currInitMethods.add(element); //非关键代码 } 在这个方法里，Spring 将遍历查找被 PostConstruct.class 注解过的方法，返回到上层，并最终调用此方法。\ninvokeInitMethods 与 InitializingBean 接口 invokeInitMethods 方法会判断当前 Bean 是否实现了 InitializingBean 接口，只有在实现了该接口的情况下，Spring 才会调用该 Bean 的接口实现方法 afterPropertiesSet()。\nprotected void invokeInitMethods(String beanName, final Object bean, @Nullable RootBeanDefinition mbd) throws Throwable { boolean isInitializingBean = (bean instanceof InitializingBean); if (isInitializingBean \u0026amp;\u0026amp; (mbd == null || !mbd.isExternallyManagedInitMethod(\u0026#34;afterPropertiesSet\u0026#34;))) { // 省略非关键代码 else { ((InitializingBean) bean).afterPropertiesSet(); } } // 省略非关键代码 } 学到此处，答案也就呼之欲出了。我们还有两种方式可以解决此问题。\n添加 init 方法，并且使用 PostConstruct 注解进行修饰： import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Component; @Component public class LightMgrService { @Autowired private LightService lightService; @PostConstruct public void init() { lightService.check(); } } 实现 InitializingBean 接口，在其 afterPropertiesSet() 方法中执行初始化代码： import org.springframework.beans.factory.InitializingBean; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Component; @Component public class LightMgrService implements InitializingBean { @Autowired private LightService lightService; @Override public void afterPropertiesSet() throws Exception { lightService.check(); } } 案例 2：意外触发 shutdown 方法\n上述实例我给你讲解了类初始化时最容易遇到的问题，同样，在类销毁时，也会有一些相对隐蔽的约定，导致一些难以察觉的错误。\n接下来，我们再来看一个案例，还是沿用之前的场景。这里我们可以简单复习一下 LightService 的实现，它包含了 shutdown 方法，负责关闭所有的灯，关键代码如下：\nimport org.springframework.stereotype.Service; @Service public class LightService { //省略其他非关键代码 public void shutdown(){ System.out.println(\u0026#34;shutting down all lights\u0026#34;); } //省略其他非关键代码 } 在之前的案例中，如果我们的宿舍管理系统在重启时，灯是不会被关闭的。但是随着业务的需求变化，我们可能会去掉 @Service 注解，而是使用另外一种产生 Bean 的方式：创建一个配置类 BeanConfiguration（标记 @Configuration）来创建一堆 Bean，其中就包含了创建 LightService 类型的 Bean，并将其注册到 Spring 容器：\nimport org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; @Configuration public class BeanConfiguration { @Bean public LightService getTransmission(){ return new LightService(); } } 复用案例 1 的启动程序，稍作修改，让 Spring 启动完成后立马关闭当前 Spring 上下文。这样等同于模拟宿舍管理系统的启停：\n@SpringBootApplication public class Application { public static void main(String[] args) { ConfigurableApplicationContext context = SpringApplication.run(Application.class, args); context.close(); } } 以上代码没有其他任何方法的调用，仅仅是将所有符合约定的类初始化并加载到 Spring 容器，完成后再关闭当前的 Spring 容器。按照预期，这段代码运行后不会有任何的 log 输出，毕竟我们只是改变了 Bean 的产生方式。\n但实际运行这段代码后，我们可以看到控制台上打印了 shutting down all lights。显然 shutdown 方法未按照预期被执行了，这导致一个很有意思的 bug：在使用新的 Bean 生成方式之前，每一次宿舍管理服务被重启时，宿舍里所有的灯都不会被关闭。但是修改后，只有服务重启，灯都被意外关闭了。如何理解这个 bug?\n案例解析\n通过调试，我们发现只有通过使用 Bean 注解注册到 Spring 容器的对象，才会在 Spring 容器被关闭的时候自动调用 shutdown 方法，而使用 @Component（Service 也是一种 Component）将当前类自动注入到 Spring 容器时，shutdown 方法则不会被自动执行。\n我们可以尝试到 Bean 注解类的代码中去寻找一些线索，可以看到属性 destroyMethod 有非常大段的注释，基本上解答了我们对于这个问题的大部分疑惑。\n使用 Bean 注解的方法所注册的 Bean 对象，如果用户不设置 destroyMethod 属性，则其属性值为 AbstractBeanDefinition.INFER_METHOD。此时 Spring 会检查当前 Bean 对象的原始类中是否有名为 shutdown 或者 close 的方法，如果有，此方法会被 Spring 记录下来，并在容器被销毁时自动执行；当然如若没有，那么自然什么都不会发生。\n下面我们继续查看 Spring 的源代码来进一步分析此问题。\n首先我们可以查找 INFER_METHOD 枚举值的引用，很容易就找到了使用该枚举值的方法 DisposableBeanAdapter#inferDestroyMethodIfNecessary：\nprivate String inferDestroyMethodIfNecessary(Object bean, RootBeanDefinition beanDefinition) { String destroyMethodName = beanDefinition.getDestroyMethodName(); if (AbstractBeanDefinition.INFER_METHOD.equals(destroyMethodName) ||(destroyMethodName == null \u0026amp;\u0026amp; bean instanceof AutoCloseable)) { if (!(bean instanceof DisposableBean)) { try { //尝试查找 close 方法 return bean.getClass().getMethod(CLOSE_METHOD_NAME).getName(); } catch (NoSuchMethodException ex) { try { //尝试查找 shutdown 方法 return bean.getClass().getMethod(SHUTDOWN_METHOD_NAME).getName(); } catch (NoSuchMethodException ex2) { // no candidate destroy method found } } } return null; } return (StringUtils.hasLength(destroyMethodName) ? destroyMethodName : null); } 我们可以看到，代码逻辑和 Bean 注解类中对于 destroyMethod 属性的注释完全一致 destroyMethodName 如果等于 INFER_METHOD，且当前类没有实现 DisposableBean 接口，那么首先查找类的 close 方法，如果找不到，就在抛出异常后继续查找 shutdown 方法；如果找到了，则返回其方法名（close 或者 shutdown）。\n接着，继续逐级查找引用，最终得到的调用链从上到下为 doCreateBean-\u0026gt;registerDisposableBeanIfNecessary-\u0026gt;registerDisposableBean(new DisposableBeanAdapter)-\u0026gt;inferDestroyMethodIfNecessary。\n然后，我们追溯到了顶层的 doCreateBean 方法，代码如下：\nprotected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final @Nullable Object[] args) throws BeanCreationException { //省略非关键代码 if (instanceWrapper == null) { instanceWrapper = createBeanInstance(beanName, mbd, args); } //省略非关键代码 // Initialize the bean instance. Object exposedObject = bean; try { populateBean(beanName, mbd, instanceWrapper); exposedObject = initializeBean(beanName, exposedObject, mbd); } //省略非关键代码 // Register bean as disposable. try { registerDisposableBeanIfNecessary(beanName, bean, mbd); } catch (BeanDefinitionValidationException ex) { throw new BeanCreationException( mbd.getResourceDescription(), beanName, \u0026#34;Invalid destruction signature\u0026#34;, ex); } return exposedObject; } 到这，我们就可以对 doCreateBean 方法做一个小小的总结了。可以说 doCreateBean 管理了 Bean 的整个生命周期中几乎所有的关键节点，直接负责了 Bean 对象的生老病死，其主要功能包括：\nBean 实例的创建； Bean 对象依赖的注入； 定制类初始化方法的回调； Disposable 方法的注册。 接着，继续查看 registerDisposableBean 方法：\npublic void registerDisposableBean(String beanName, DisposableBean bean) { //省略其他非关键代码 synchronized (this.disposableBeans) { this.disposableBeans.put(beanName, bean); } //省略其他非关键代码 } 在 registerDisposableBean 方法内，DisposableBeanAdapter 类（其属性 destroyMethodName 记录了使用哪种 destory 方法）被实例化并添加到 DefaultSingletonBeanRegistry#disposableBeans 属性内，disposableBeans 将暂存这些 DisposableBeanAdapter 实例，直到 AnnotationConfigApplicationContext 的 close 方法被调用。\n而当 AnnotationConfigApplicationContext 的 close 方法被调用时，即当 Spring 容器被销毁时，最终会调用到 DefaultSingletonBeanRegistry#destroySingleton。此方法将遍历 disposableBeans 属性逐一获取 DisposableBean，依次调用其中的 close 或者 shutdown 方法：\npublic void destroySingleton(String beanName) { // Remove a registered singleton of the given name, if any. removeSingleton(beanName); // Destroy the corresponding DisposableBean instance. DisposableBean disposableBean; synchronized (this.disposableBeans) { disposableBean = (DisposableBean) this.disposableBeans.remove(beanName); } destroyBean(beanName, disposableBean); } 很明显，最终我们的案例调用了 LightService#shutdown 方法，将所有的灯关闭了。\n问题修正\n现在，我们已经知道了问题的根源，解决起来就非常简单了。\n我们可以通过避免在 Java 类中定义一些带有特殊意义动词的方法来解决，当然如果一定要定义名为 close 或者 shutdown 方法，也可以通过将 Bean 注解内 destroyMethod 属性设置为空的方式来解决这个问题。\n第一种修改方式比较简单，所以这里只展示第二种修改方式，代码如下：\nimport org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; @Configuration public class BeanConfiguration { @Bean(destroyMethod=\u0026#34;\u0026#34;) public LightService getTransmission(){ return new LightService(); } } 另外，针对这个问题我想再多提示一点。如果我们能养成良好的编码习惯，在使用某个不熟悉的注解之前，认真研读一下该注解的注释，也可以大概率规避这个问题。\n不过说到这里，你也可能还是会疑惑，为什么 @Service 注入的 LightService，其 shutdown 方法不能被执行？这里我想补充说明下。\n想要执行，则必须要添加 DisposableBeanAdapter，而它的添加是有条件的：\nprotected void registerDisposableBeanIfNecessary(String beanName, Object bean, RootBeanDefinition mbd) { AccessControlContext acc = (System.getSecurityManager() != null ? getAccessControlContext() : null); if (!mbd.isPrototype() \u0026amp;\u0026amp; requiresDestruction(bean, mbd)) { if (mbd.isSingleton()) { // Register a DisposableBean implementation that performs all destruction // work for the given bean: DestructionAwareBeanPostProcessors, // DisposableBean interface, custom destroy method. registerDisposableBean(beanName, new DisposableBeanAdapter(bean, beanName, mbd, getBeanPostProcessors(), acc)); } else { //省略非关键代码 } } } 参考上述代码，关键的语句在于：\n!mbd.isPrototype() \u0026amp;\u0026amp; requiresDestruction(bean, mbd) 很明显，在案例代码修改前后，我们都是单例，所以区别仅在于是否满足 requiresDestruction 条件。翻阅它的代码，最终的关键调用参考 DisposableBeanAdapter#hasDestroyMethod：\npublic static boolean hasDestroyMethod(Object bean, RootBeanDefinition beanDefinition) { if (bean instanceof DisposableBean || bean instanceof AutoCloseable) { return true; } String destroyMethodName = beanDefinition.getDestroyMethodName(); if (AbstractBeanDefinition.INFER_METHOD.equals(destroyMethodName)) { return (ClassUtils.hasMethod(bean.getClass(), CLOSE_METHOD_NAME) || ClassUtils.hasMethod(bean.getClass(), SHUTDOWN_METHOD_NAME)); } return StringUtils.hasLength(destroyMethodName); } 如果我们是使用 @Service 来产生 Bean 的，那么在上述代码中我们获取的 destroyMethodName 其实是 null；而使用 @Bean 的方式，默认值为 AbstractBeanDefinition.INFER_METHOD，参考 Bean 的定义：\npublic @interface Bean { //省略其他非关键代码 String destroyMethod() default AbstractBeanDefinition.INFER_METHOD; } 继续对照代码，你就会发现 @Service 标记的 LightService 也没有实现 AutoCloseable、DisposableBean，最终没有添加一个 DisposableBeanAdapter。所以最终我们定义的 shutdown 方法没有被调用。\nSpring AOP 常见错误 # Spring AOP 是 Spring 中除了依赖注入外（DI）最为核心的功能，顾名思义，AOP 即 Aspect Oriented Programming，翻译为面向切面编程。\n而 Spring AOP 则利用 CGlib 和 JDK 动态代理等方式来实现运行期动态方法增强，其目的是将与业务无关的代码单独抽离出来，使其逻辑不再与业务代码耦合，从而降低系统的耦合性，提高程序的可重用性和开发效率。因而 AOP 便成为了日志记录、监控管理、性能统计、异常处理、权限管理、统一认证等各个方面被广泛使用的技术。\n追根溯源，我们之所以能无感知地在容器对象方法前后任意添加代码片段，那是由于 Spring 在运行期帮我们把切面中的代码逻辑动态“织入”到了容器对象方法内，所以说 AOP 本质上就是一个代理模式。然而在使用这种代理模式时，我们常常会用不好，那么这节课我们就来解析下有哪些常见的问题，以及背后的原理是什么。\n案例 1：this 调用的当前类方法无法被拦截\n假设我们正在开发一个宿舍管理系统，这个模块包含一个负责电费充值的类 ElectricService，它含有一个充电方法 charge()：\n@Service public class ElectricService { public void charge() throws Exception { System.out.println(\u0026#34;Electric charging ...\u0026#34;); this.pay(); } public void pay() throws Exception { System.out.println(\u0026#34;Pay with alipay ...\u0026#34;); Thread.sleep(1000); } } 在这个电费充值方法 charge() 中，我们会使用支付宝进行充值。因此在这个方法中，我加入了 pay() 方法。为了模拟 pay() 方法调用耗时，代码执行了休眠 1 秒，并在 charge() 方法里使用 this.pay() 的方式调用这种支付方法。\n但是因为支付宝支付是第三方接口，我们需要记录下接口调用时间。这时候我们就引入了一个 @Around 的增强 ，分别记录在 pay() 方法执行前后的时间，并计算出执行 pay() 方法的耗时。\n@Aspect @Service @Slf4j public class AopConfig { @Around(\u0026#34;execution(* com.spring.puzzle.class5.example1.ElectricService.pay()) \u0026#34;) public void recordPayPerformance(ProceedingJoinPoint joinPoint) throws Throwable { long start = System.currentTimeMillis(); joinPoint.proceed(); long end = System.currentTimeMillis(); System.out.println(\u0026#34;Pay method time cost（ms）: \u0026#34; + (end - start)); } } 最后我们再通过定义一个 Controller 来提供电费充值接口，定义如下：\n@RestController public class HelloWorldController { @Autowired ElectricService electricService; @RequestMapping(path = \u0026#34;charge\u0026#34;, method = RequestMethod.GET) public void charge() throws Exception{ electricService.charge(); }; } 完成代码后，我们访问上述接口，会发现这段计算时间的切面并没有执行到，输出日志如下：\nElectric charging ... Pay with alipay ... 回溯之前的代码可知，在 @Around 的切面类中，我们很清晰地定义了切面对应的方法，但是却没有被执行到。这说明了在类的内部，通过 this 方式调用的方法，是没有被 Spring AOP 增强的。这是为什么呢？我们来分析一下。\n案例解析\n我们可以从源码中找到真相。首先来设置个断点，调试看看 this 对应的对象是什么样的：\n可以看到，this 对应的就是一个普通的 ElectricService 对象，并没有什么特别的地方。再看看在 Controller 层中自动装配的 ElectricService 对象是什么样：\n可以看到，这是一个被 Spring 增强过的 Bean，所以执行 charge() 方法时，会执行记录接口调用时间的增强操作。而 this 对应的对象只是一个普通的对象，并没有做任何额外的增强。\n为什么 this 引用的对象只是一个普通对象呢？这还要从 Spring AOP 增强对象的过程来看。但在此之前，有些基础我需要在这里强调下。\nSpring AOP 的实现 Spring AOP 的底层是动态代理。而创建代理的方式有两种，JDK 的方式和 CGLIB 的方式。JDK 动态代理只能对实现了接口的类生成代理，而不能针对普通类。而 CGLIB 是可以针对类实现代理，主要是对指定的类生成一个子类，覆盖其中的方法，来实现代理对象。具体区别可参考下图：\n如何使用 Spring AOP 在 Spring Boot 中，我们一般只要添加以下依赖就可以直接使用 AOP 功能：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-aop\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 而对于非 Spring Boot 程序，除了添加相关 AOP 依赖项外，我们还常常会使用 @EnableAspectJAutoProxy 来开启 AOP 功能。这个注解类引入（Import）AspectJAutoProxyRegistrar，它通过实现 ImportBeanDefinitionRegistrar 的接口方法来完成 AOP 相关 Bean 的准备工作。\n补充完最基本的 Spring 底层知识和使用知识后，我们具体看下创建代理对象的过程。先来看下调用栈：\n创建代理对象的时机就是创建一个 Bean 的时候，而创建的的关键工作其实是由 AnnotationAwareAspectJAutoProxyCreator 完成的。它本质上是一种 BeanPostProcessor。所以它的执行是在完成原始 Bean 构建后的初始化 Bean（initializeBean）过程中。而它到底完成了什么工作呢？我们可以看下它的 postProcessAfterInitialization 方法：\npublic Object postProcessAfterInitialization(@Nullable Object bean, String beanName) { if (bean != null) { Object cacheKey = getCacheKey(bean.getClass(), beanName); if (this.earlyProxyReferences.remove(cacheKey) != bean) { return wrapIfNecessary(bean, beanName, cacheKey); } } return bean; } 上述代码中的关键方法是 wrapIfNecessary，顾名思义，在需要使用 AOP 时，它会把创建的原始的 Bean 对象 wrap 成代理对象作为 Bean 返回。具体到这个 wrap 过程，可参考下面的关键代码行：\nprotected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) { // 省略非关键代码 Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, null); if (specificInterceptors != DO_NOT_PROXY) { this.advisedBeans.put(cacheKey, Boolean.TRUE); Object proxy = createProxy( bean.getClass(), beanName, specificInterceptors, new SingletonTargetSource(bean)); this.proxyTypes.put(cacheKey, proxy.getClass()); return proxy; } // 省略非关键代码 } 上述代码中，第 6 行的 createProxy 调用是创建代理对象的关键。具体到执行过程，它首先会创建一个代理工厂，然后将通知器（advisors）、被代理对象等信息加入到代理工厂，最后通过这个代理工厂来获取代理对象。一些关键过程参考下面的方法：\nprotected Object createProxy(Class\u0026lt;?\u0026gt; beanClass, @Nullable String beanName, @Nullable Object[] specificInterceptors, TargetSource targetSource) { // 省略非关键代码 ProxyFactory proxyFactory = new ProxyFactory(); if (!proxyFactory.isProxyTargetClass()) { if (shouldProxyTargetClass(beanClass, beanName)) { proxyFactory.setProxyTargetClass(true); } else { evaluateProxyInterfaces(beanClass, proxyFactory); } } Advisor[] advisors = buildAdvisors(beanName, specificInterceptors); proxyFactory.addAdvisors(advisors); proxyFactory.setTargetSource(targetSource); customizeProxyFactory(proxyFactory); // 省略非关键代码 return proxyFactory.getProxy(getProxyClassLoader()); } 经过这样一个过程，一个代理对象就被创建出来了。我们从 Spring 中获取到的对象都是这个代理对象，所以具有 AOP 功能。而之前直接使用 this 引用到的只是一个普通对象，自然也就没办法实现 AOP 的功能了。\n问题修正\n从上述案例解析中，我们知道，只有引用的是被动态代理创建出来的对象，才会被 Spring 增强，具备 AOP 该有的功能。那什么样的对象具备这样的条件呢？\n有两种。一种是被 @Autowired 注解的，于是我们的代码可以改成这样，即通过 @Autowired 的方式，在类的内部，自己引用自己：\n@Service public class ElectricService { @Autowired ElectricService electricService; public void charge() throws Exception { System.out.println(\u0026#34;Electric charging ...\u0026#34;); //this.pay(); electricService.pay(); } public void pay() throws Exception { System.out.println(\u0026#34;Pay with alipay ...\u0026#34;); Thread.sleep(1000); } } 另一种方法就是直接从 AopContext 获取当前的 Proxy。那你可能会问了，AopContext 是什么？简单说，它的核心就是通过一个 ThreadLocal 来将 Proxy 和线程绑定起来，这样就可以随时拿出当前线程绑定的 Proxy。\n不过使用这种方法有个小前提，就是需要在 @EnableAspectJAutoProxy 里加一个配置项 exposeProxy = true，表示将代理对象放入到 ThreadLocal，这样才可以直接通过 AopContext.currentProxy() 的方式获取到，否则会报错如下：\n按这个思路，我们修改下相关代码：\nimport org.springframework.aop.framework.AopContext; import org.springframework.stereotype.Service; @Service public class ElectricService { public void charge() throws Exception { System.out.println(\u0026#34;Electric charging ...\u0026#34;); ElectricService electric = ((ElectricService) AopContext.currentProxy()); electric.pay(); } public void pay() throws Exception { System.out.println(\u0026#34;Pay with alipay ...\u0026#34;); Thread.sleep(1000); } } 同时，不要忘记修改 EnableAspectJAutoProxy 注解的 exposeProxy 属性，示例如下：\n@SpringBootApplication @EnableAspectJAutoProxy(exposeProxy = true) public class Application { // 省略非关键代码 } 这两种方法的效果其实是一样的，最终我们打印出了期待的日志，到这，问题顺利解决了。\nElectric charging ... Pay with alipay ... Pay method time cost(ms): 1005 案例 2：直接访问被拦截类的属性抛空指针异常\n接上一个案例，在宿舍管理系统中，我们使用了 charge() 方法进行支付。在统一结算的时候我们会用到一个管理员用户付款编号，这时候就用到了几个新的类。\nUser 类，包含用户的付款编号信息：\npublic class User { private String payNum; public User(String payNum) { this.payNum = payNum; } public String getPayNum() { return payNum; } public void setPayNum(String payNum) { this.payNum = payNum; } } AdminUserService 类，包含一个管理员用户（User），其付款编号为 202101166；另外，这个服务类有一个 login() 方法，用来登录系统。\n@Service public class AdminUserService { public final User adminUser = new User(\u0026#34;202101166\u0026#34;); public void login() { System.out.println(\u0026#34;admin user login...\u0026#34;); } } 我们需要修改 ElectricService 类实现这个需求：在电费充值时，需要管理员登录并使用其编号进行结算。完整代码如下：\nimport org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Service; @Service public class ElectricService { @Autowired private AdminUserService adminUserService; public void charge() throws Exception { System.out.println(\u0026#34;Electric charging ...\u0026#34;); this.pay(); } public void pay() throws Exception { adminUserService.login(); String payNum = adminUserService.adminUser.getPayNum(); System.out.println(\u0026#34;User pay num : \u0026#34; + payNum); System.out.println(\u0026#34;Pay with alipay ...\u0026#34;); Thread.sleep(1000); } } 代码完成后，执行 charge() 操作，一切正常：\nElectric charging ... admin user login... User pay num : 202101166 Pay with alipay ... 这时候，由于安全需要，就需要管理员在登录时，记录一行日志以便于以后审计管理员操作。所以我们添加一个 AOP 相关配置类，具体如下：\n@Aspect @Service @Slf4j public class AopConfig { @Before(\u0026#34;execution(* com.spring.puzzle.class5.example2.AdminUserService.login(..)) \u0026#34;) public void logAdminLogin(JoinPoint pjp) throws Throwable { System.out.println(\u0026#34;! admin login ...\u0026#34;); } } 添加这段代码后，我们执行 charge() 操作，发现不仅没有相关日志，而且在执行下面这一行代码的时候直接抛出了 NullPointerException：\nString payNum = dminUserService.user.getPayNum(); 本来一切正常的代码，因为引入了一个 AOP 切面，抛出了 NullPointerException。这会是什么原因呢？我们先 debug 一下，来看看加入 AOP 后调用的对象是什么样子。\n可以看出，加入 AOP 后，我们的对象已经是一个代理对象了，如果你眼尖的话，就会发现在上图中，属性 adminUser 确实为 null。为什么会这样？为了解答这个诡异的问题，我们需要进一步理解 Spring 使用 CGLIB 生成 Proxy 的原理。\n案例解析\n我们在上一个案例中解析了创建 Spring Proxy 的大体过程，在这里，我们需要进一步研究一下通过 Proxy 创建出来的是一个什么样的对象。正常情况下，AdminUserService 只是一个普通的对象，而 AOP 增强过的则是一个 AdminUserService $$EnhancerBySpringCGLIB$$xxxx。\n这个类实际上是 AdminUserService 的一个子类。它会 overwrite 所有 public 和 protected 方法，并在内部将调用委托给原始的 AdminUserService 实例。\n从具体实现角度看，CGLIB 中 AOP 的实现是基于 org.springframework.cglib.proxy 包中 Enhancer 和 MethodInterceptor 两个接口来实现的。\n整个过程，我们可以概括为三个步骤：\n定义自定义的 MethodInterceptor 负责委托方法执行； 创建 Enhance 并设置 Callback 为上述 MethodInterceptor； enhancer.create() 创建代理。 接下来，我们来具体分析一下 Spring 的相关实现源码。\n在上个案例分析里，我们简要提及了 Spring 的动态代理对象的初始化机制。在得到 Advisors 之后，会通过 ProxyFactory.getProxy 获取代理对象：\npublic Object getProxy(ClassLoader classLoader) { return createAopProxy().getProxy(classLoader); } 在这里，我们以 CGLIB 的 Proxy 的实现类 CglibAopProxy 为例，来看看具体的流程：\npublic Object getProxy(@Nullable ClassLoader classLoader) { // 省略非关键代码 // 创建及配置 Enhancer Enhancer enhancer = createEnhancer(); // 省略非关键代码 // 获取Callback：包含DynamicAdvisedInterceptor，亦是MethodInterceptor Callback[] callbacks = getCallbacks(rootClass); // 省略非关键代码 // 生成代理对象并创建代理（设置 enhancer 的 callback 值） return createProxyClassAndInstance(enhancer, callbacks); // 省略非关键代码 } 上述代码中的几个关键步骤大体符合之前提及的三个步骤，其中最后一步一般都会执行到 CglibAopProxy 子类 ObjenesisCglibAopProxy 的 createProxyClassAndInstance() 方法：\nprotected Object createProxyClassAndInstance(Enhancer enhancer, Callback[] callbacks) { //创建代理类Class Class\u0026lt;?\u0026gt; proxyClass = enhancer.createClass(); Object proxyInstance = null; //spring.objenesis.ignore默认为false //所以objenesis.isWorthTrying()一般为true if (objenesis.isWorthTrying()) { try { // 创建实例 proxyInstance = objenesis.newInstance(proxyClass, enhancer.getUseCache()); } catch (Throwable ex) { // 省略非关键代码 } } if (proxyInstance == null) { // 尝试普通反射方式创建实例 try { Constructor\u0026lt;?\u0026gt; ctor = (this.constructorArgs != null ? proxyClass.getDeclaredConstructor(this.constructorArgTypes) : proxyClass.getDeclaredConstructor()); ReflectionUtils.makeAccessible(ctor); proxyInstance = (this.constructorArgs != null ? ctor.newInstance(this.constructorArgs) : ctor.newInstance()); //省略非关键代码 } } // 省略非关键代码 ((Factory) proxyInstance).setCallbacks(callbacks); return proxyInstance; } 这里我们可以了解到，Spring 会默认尝试使用 objenesis 方式实例化对象，如果失败则再次尝试使用常规方式实例化对象。现在，我们可以进一步查看 objenesis 方式实例化对象的流程。\n参照上述截图所示调用栈，objenesis 方式最后使用了 JDK 的 ReflectionFactory.newConstructorForSerialization() 完成了代理对象的实例化。而如果你稍微研究下这个方法，你会惊讶地发现，这种方式创建出来的对象是不会初始化类成员变量的。\n所以说到这里，聪明的你可能已经觉察到真相已经暴露了，我们这个案例的核心是代理类实例的默认构建方式很特别。在这里，我们可以总结和对比下通过反射来实例化对象的方式，包括：\njava.lang.Class.newInsance() java.lang.reflect.Constructor.newInstance() sun.reflect.ReflectionFactory.newConstructorForSerialization().newInstance() 前两种初始化方式都会同时初始化类成员变量，但是最后一种通过 ReflectionFactory.newConstructorForSerialization().newInstance() 实例化类则不会初始化类成员变量，这就是当前问题的最终答案了。\n问题修正\n了解了问题的根本原因后，修正起来也就不困难了。既然是无法直接访问被拦截类的成员变量，那我们就换个方式，在 UserService 里写个 getUser() 方法，从内部访问获取变量。\n我们在 AdminUserService 里加了个 getUser() 方法：\npublic User getUser() { return user; } 在 ElectricService 里通过 getUser() 获取 User 对象：\n// 原来出错的方式： //String payNum = = adminUserService.adminUser.getPayNum(); // 修改后的方式： String payNum = adminUserService.getAdminUser().getPayNum(); 运行下来，一切正常，可以看到管理员登录日志了：\nElectric charging ... ! admin login ... admin user login... User pay num : 202101166 Pay with alipay ... 但你有没有产生另一个困惑呢？既然代理类的类属性不会被初始化，那为什么可以通过在 AdminUserService 里写个 getUser() 方法来获取代理类实例的属性呢？\n我们再次回顾 createProxyClassAndInstance 的代码逻辑，创建代理类后，我们会调用 setCallbacks 来设置拦截后需要注入的代码：\nprotected Object createProxyClassAndInstance(Enhancer enhancer, Callback[] callbacks) { Class\u0026lt;?\u0026gt; proxyClass = enhancer.createClass(); Object proxyInstance = null; if (objenesis.isWorthTrying()) { try { proxyInstance = objenesis.newInstance(proxyClass, enhancer.getUseCache()); } // 省略非关键代码 ((Factory) proxyInstance).setCallbacks(callbacks); return proxyInstance; } 通过代码调试和分析，我们可以得知上述的 callbacks 中会存在一种服务于 AOP 的 DynamicAdvisedInterceptor，它的接口是 MethodInterceptor（callback 的子接口），实现了拦截方法 intercept()。我们可以看下它是如何实现这个方法的：\npublic Object intercept(Object proxy, Method method, Object[] args, MethodProxy methodProxy) throws Throwable { // 省略非关键代码 TargetSource targetSource = this.advised.getTargetSource(); // 省略非关键代码 if (chain.isEmpty() \u0026amp;\u0026amp; Modifier.isPublic(method.getModifiers())) { Object[] argsToUse = AopProxyUtils.adaptArgumentsIfNecessary(method, args); retVal = methodProxy.invoke(target, argsToUse); } else { // We need to create a method invocation... retVal = new CglibMethodInvocation(proxy, target, method, args, targetClass, chain, methodProxy).proceed(); } retVal = processReturnType(proxy, target, method, retVal); return retVal; } //省略非关键代码 } 当代理类方法被调用，会被 Spring 拦截，从而进入此 intercept()，并在此方法中获取被代理的原始对象。而在原始对象中，类属性是被实例化过且存在的。因此代理类是可以通过方法拦截获取被代理对象实例的属性。\n说到这里，我们已经解决了问题。但如果你看得仔细，就会发现，其实你改变一个属性，也可以让产生的代理对象的属性值不为 null。例如修改启动参数 spring.objenesis.ignore 如下：\n此时再调试程序，你会发现 adminUser 已经不为 null 了：\n所以这也是解决这个问题的一种方法。\n当一个系统采用的切面越来越多时，因为执行顺序而导致的问题便会逐步暴露出来，下面我们就重点看一下。\n案例 1：错乱混合不同类型的增强\n还是沿用上节课的宿舍管理系统开发场景。\n这里我们先回顾下，你就不用去翻代码了。这个宿舍管理系统保护了一个电费充值模块，它包含了一个负责电费充值的类 ElectricService，还有一个充电方法 charge()：\n@Service public class ElectricService { public void charge() throws Exception { System.out.println(\u0026#34;Electric charging ...\u0026#34;); } } 为了在执行 charge() 之前，鉴定下调用者的权限，我们增加了针对于 Electric 的切面类 AopConfig，其中包含一个 @Before 增强。这里的增强没有做任何事情，仅仅是打印了一行日志，然后模拟执行权限校验功能（占用 1 秒钟）。\n//省略 imports @Aspect @Service @Slf4j public class AspectService { @Before(\u0026#34;execution(* com.spring.puzzle.class6.example1.ElectricService.charge()) \u0026#34;) public void checkAuthority(JoinPoint pjp) throws Throwable { System.out.println(\u0026#34;validating user authority\u0026#34;); Thread.sleep(1000); } } 执行后，我们得到以下 log，接着一切按照预期继续执行：\nvalidating user authority Electric charging ... 一段时间后，由于业务发展，ElectricService 中的 charge() 逻辑变得更加复杂了，我们需要仅仅针对 ElectricService 的 charge() 做性能统计。为了不影响原有的业务逻辑，我们在 AopConfig 中添加了另一个增强，代码更改后如下：\n//省略 imports @Aspect @Service public class AopConfig { @Before(\u0026#34;execution(* com.spring.puzzle.class6.example1.ElectricService.charge()) \u0026#34;) public void checkAuthority(JoinPoint pjp) throws Throwable { System.out.println(\u0026#34;validating user authority\u0026#34;); Thread.sleep(1000); } @Around(\u0026#34;execution(* com.spring.puzzle.class6.example1.ElectricService.charge()) \u0026#34;) public void recordPerformance(ProceedingJoinPoint pjp) throws Throwable { long start = System.currentTimeMillis(); pjp.proceed(); long end = System.currentTimeMillis(); System.out.println(\u0026#34;charge method time cost: \u0026#34; + (end - start)); } } 执行后得到日志如下：\nvalidating user authority Electric charging … charge method time cost 1022 (ms) 通过性能统计打印出的日志，我们可以得知 charge() 执行时间超过了 1 秒钟。然而，该方法仅打印了一行日志，它的执行不可能需要这么长时间。\n因此我们很容易看出问题所在：当前 ElectricService 中 charge() 的执行时间，包含了权限验证的时间，即包含了通过 @Around 增强的 checkAuthority() 执行的所有时间。这并不符合我们的初衷，我们需要统计的仅仅是 ElectricService.charge() 的性能统计，它并不包含鉴权过程。\n当然，这些都是从日志直接观察出的现象。实际上，这个问题出现的根本原因和 AOP 的执行顺序有关。针对这个案例而言，当同一个切面（Aspect）中同时包含多个不同类型的增强时（Around、Before、After、AfterReturning、AfterThrowing 等），它们的执行是有顺序的。那么顺序如何？我们不妨来解析下。\n案例解析\n其实一切都可以从源码中得到真相！Spring 初始化单例类的一般过程，基本都是 getBean()-\u0026gt;doGetBean()-\u0026gt;getSingleton()，如果发现 Bean 不存在，则调用 createBean()-\u0026gt;doCreateBean() 进行实例化。\n而如果我们的代码里使用了 Spring AOP，doCreateBean() 最终会返回一个代理对象。至于代理对象如何创建，大体流程我们在上一讲已经概述过了。如果你记忆力比较好的话，应该记得在代理对象的创建过程中，我们贴出过这样一段代码（参考 AbstractAutoProxyCreator#createProxy）：\nprotected Object createProxy(Class\u0026lt;?\u0026gt; beanClass, @Nullable String beanName, @Nullable Object[] specificInterceptors, TargetSource targetSource) { //省略非关键代码 Advisor[] advisors = buildAdvisors(beanName, specificInterceptors); proxyFactory.addAdvisors(advisors); proxyFactory.setTargetSource(targetSource); //省略非关键代码 return proxyFactory.getProxy(getProxyClassLoader()); } 其中 advisors 就是增强方法对象，它的顺序决定了面临多个增强时，到底先执行谁。而这个集合对象本身是由 specificInterceptors 构建出来的，而 specificInterceptors 又是由 AbstractAdvisorAutoProxyCreator#getAdvicesAndAdvisorsForBean 方法构建：\n@Override @Nullable protected Object[] getAdvicesAndAdvisorsForBean( Class\u0026lt;?\u0026gt; beanClass, String beanName, @Nullable TargetSource targetSource) { List\u0026lt;Advisor\u0026gt; advisors = findEligibleAdvisors(beanClass, beanName); if (advisors.isEmpty()) { return DO_NOT_PROXY; } return advisors.toArray(); } 简单说，其实就是根据当前的 beanClass、beanName 等信息，结合所有候选的 advisors，最终找出匹配（Eligible）的 Advisor，为什么如此？毕竟 AOP 拦截点可能会配置多个，而我们执行的方法不见得会被所有的拦截配置拦截。寻找匹配 Advisor 的逻辑参考 AbstractAdvisorAutoProxyCreator#findEligibleAdvisors：\nprotected List\u0026lt;Advisor\u0026gt; findEligibleAdvisors(Class\u0026lt;?\u0026gt; beanClass, String beanName) { //寻找候选的 Advisor List\u0026lt;Advisor\u0026gt; candidateAdvisors = findCandidateAdvisors(); //根据候选的 Advisor 和当前 bean 算出匹配的 Advisor List\u0026lt;Advisor\u0026gt; eligibleAdvisors = findAdvisorsThatCanApply(candidateAdvisors, beanClass, beanName); extendAdvisors(eligibleAdvisors); if (!eligibleAdvisors.isEmpty()) { //排序 eligibleAdvisors = sortAdvisors(eligibleAdvisors); } return eligibleAdvisors; } 通过研读代码，最终 Advisors 的顺序是由两点决定：\ncandidateAdvisors 的顺序； sortAdvisors 进行的排序。 这里我们可以重点看下对本案例起关键作用的 candidateAdvisors 排序。实际上，它的顺序是在 @Aspect 标记的 AopConfig Bean 构建时就决定了。具体而言，就是在初始化过程中会排序自己配置的 Advisors，并把排序结果存入了缓存（BeanFactoryAspectJAdvisorsBuilder#advisorsCache）。\n后续 Bean 创建代理时，直接拿出这个排序好的候选 Advisors。候选 Advisors 排序发生在 Bean 构建这个结论时，我们也可以通过 AopConfig Bean 构建中的堆栈信息验证：\n可以看到，排序是在 Bean 的构建中进行的，而最后排序执行的关键代码位于下面的方法中（参考 ReflectiveAspectJAdvisorFactory#getAdvisorMethods）：\nprivate List\u0026lt;Method\u0026gt; getAdvisorMethods(Class\u0026lt;?\u0026gt; aspectClass) { final List\u0026lt;Method\u0026gt; methods = new ArrayList\u0026lt;\u0026gt;(); ReflectionUtils.doWithMethods(aspectClass, method -\u0026gt; { // Exclude pointcuts if (AnnotationUtils.getAnnotation(method, Pointcut.class) == null) { methods.add(method); } }, ReflectionUtils.USER_DECLARED_METHODS); // 排序 methods.sort(METHOD_COMPARATOR); return methods; } 上述代码的重点是第九行 methods.sort(METHOD_COMPARATOR) 方法。\n我们来查看 METHOD_COMPARATOR 的代码，会发现它是定义在 ReflectiveAspectJAdvisorFactory 类中的静态方法块，代码如下：\nstatic { Comparator\u0026lt;Method\u0026gt; adviceKindComparator = new ConvertingComparator\u0026lt;\u0026gt;( new InstanceComparator\u0026lt;\u0026gt;( Around.class, Before.class, After.class, AfterReturning.class, AfterThrowing.class), (Converter\u0026lt;Method, Annotation\u0026gt;) method -\u0026gt; { AspectJAnnotation\u0026lt;?\u0026gt; annotation = AbstractAspectJAdvisorFactory.findAspectJAnnotationOnMethod(method); return (annotation != null ? annotation.getAnnotation() : null); }); Comparator\u0026lt;Method\u0026gt; methodNameComparator = new ConvertingComparator\u0026lt;\u0026gt;(Method::getName); //合并上面两者比较器 METHOD_COMPARATOR = adviceKindComparator.thenComparing(methodNameComparator); } METHOD_COMPARATOR 本质上是一个连续比较器，由 adviceKindComparator 和 methodNameComparator 这两个比较器通过 thenComparing() 连接而成。\n通过这个案例，我们重点了解 adviceKindComparator 这个比较器，此对象通过实例化 ConvertingComparator 类而来，而 ConvertingComparator 类是 Spring 中较为经典的一个实现。顾名思义，先转化再比较，它构造参数接受以下这两个参数：\n第一个参数是基准比较器，即在 adviceKindComparator 中最终要调用的比较器，在构造函数中赋值于 this.comparator； 第二个参数是一个 lambda 回调函数，用来将传递的参数转化为基准比较器需要的参数类型，在构造函数中赋值于 this.converter。 查看 ConvertingComparator 比较器核心方法 compare 如下：\npublic int compare(S o1, S o2) { T c1 = this.converter.convert(o1); T c2 = this.converter.convert(o2); return this.comparator.compare(c1, c2); } 可知，这里是先调用从构造函数中获取到的 lambda 回调函数 this.converter，将需要比较的参数进行转化。我们可以从之前的代码中找出这个转化工作：\n(Converter\u0026lt;Method, Annotation\u0026gt;) method -\u0026gt; { AspectJAnnotation\u0026lt;?\u0026gt; annotation = AbstractAspectJAdvisorFactory.findAspectJAnnotationOnMethod(method); return (annotation != null ? annotation.getAnnotation() : null); }); 转化功能的代码逻辑较为简单，就是返回传入方法（method）上标记的增强注解（Pointcut,Around,Before,After,AfterReturning 以及 AfterThrowing）：\nprivate static final Class\u0026lt;?\u0026gt;[] ASPECTJ_ANNOTATION_CLASSES = new Class\u0026lt;?\u0026gt;[] { Pointcut.class, Around.class, Before.class, After.class, AfterReturning.class, AfterThrowing.class}; protected static AspectJAnnotation\u0026lt;?\u0026gt; findAspectJAnnotationOnMethod(Method method) { for (Class\u0026lt;?\u0026gt; clazz : ASPECTJ_ANNOTATION_CLASSES) { AspectJAnnotation\u0026lt;?\u0026gt; foundAnnotation = findAnnotation(method, (Class\u0026lt;Annotation\u0026gt;) clazz); if (foundAnnotation != null) { return foundAnnotation; } } return null; } 经过转化后，我们获取到的待比较的数据其实就是注解了。而它们的排序依赖于 ConvertingComparator 的第一个参数，即最终会调用的基准比较器，以下是它的关键实现代码：\nnew InstanceComparator\u0026lt;\u0026gt;( Around.class, Before.class, After.class, AfterReturning.class, AfterThrowing.class) 构造方法也是较为简单的，只是将传递进来的 instanceOrder 赋予了类成员变量，继续查看 InstanceComparator 比较器核心方法 compare 如下，也就是最终要调用的比较方法：\npublic int compare(T o1, T o2) { int i1 = getOrder(o1); int i2 = getOrder(o2); return (i1 \u0026lt; i2 ? -1 : (i1 == i2 ? 0 : 1)); } 一个典型的 Comparator，代码逻辑按照 i1、i2 的升序排列，即 getOrder() 返回的值越小，排序越靠前。\n查看 getOrder() 的逻辑如下：\nprivate int getOrder(@Nullable T object) { if (object != null) { for (int i = 0; i \u0026lt; this.instanceOrder.length; i++) { //instance 在 instanceOrder 中的“排号” if (this.instanceOrder[i].isInstance(object)) { return i; } } } return this.instanceOrder.length; } 返回当前传递的增强注解在 this.instanceOrder 中的序列值，序列值越小，则越靠前。而结合之前构造参数传递的顺序，我们很快就能判断出：最终的排序结果依次是 Around.class, Before.class, After.class, AfterReturning.class, AfterThrowing.class。\n到此为止，答案也呼之欲出：this.instanceOrder 的排序，即为不同类型增强的优先级，排序越靠前，优先级越高。\n结合之前的讨论，我们可以得出一个结论：同一个切面中，不同类型的增强方法被调用的顺序依次为 Around.class, Before.class, After.class, AfterReturning.class, AfterThrowing.class。\n问题修正\n从上述案例解析中，我们知道 Around 类型的增强被调用的优先级高于 Before 类型的增强，所以上述案例中性能统计所花费的时间，包含权限验证的时间，也在情理之中。\n知道了原理，修正起来也就简单了。假设不允许我们去拆分类，我们可以按照下面的思路来修改：\n将 ElectricService.charge() 的业务逻辑全部移动到 doCharge()，在 charge() 中调用 doCharge()； 性能统计只需要拦截 doCharge()； 权限统计增强保持不变，依然拦截 charge()。 ElectricService 类代码更改如下：\n@Service public class ElectricService { @Autowired ElectricService electricService; public void charge() { electricService.doCharge(); } public void doCharge() { System.out.println(\u0026#34;Electric charging ...\u0026#34;); } } 切面代码更改如下：\n//省略 imports @Aspect @Service public class AopConfig { @Before(\u0026#34;execution(* com.spring.puzzle.class6.example1.ElectricService.charge()) \u0026#34;) public void checkAuthority(JoinPoint pjp) throws Throwable { System.out.println(\u0026#34;validating user authority\u0026#34;); Thread.sleep(1000); } @Around(\u0026#34;execution(* com.spring.puzzle.class6.example1.ElectricService.doCharge()) \u0026#34;) public void recordPerformance(ProceedingJoinPoint pjp) throws Throwable { long start = System.currentTimeMillis(); pjp.proceed(); long end = System.currentTimeMillis(); System.out.println(\u0026#34;charge method time cost: \u0026#34; + (end - start)); } } 案例 2：错乱混合同类型增强\n如果同一个切面里的多个增强方法其增强都一样，那调用顺序又如何呢？我们继续看下一个案例。\n这里业务逻辑类 ElectricService 没有任何变化，仅包含一个 charge()：\nimport org.springframework.stereotype.Service; @Service public class ElectricService { public void charge() { System.out.println(\u0026#34;Electric charging ...\u0026#34;); } } 切面类 AspectService 包含两个方法，都是 Before 类型增强。\n第一个方法 logBeforeMethod()，目的是在 run() 执行之前希望能输入日志，表示当前方法被调用一次，方便后期统计。另一个方法 validateAuthority()，目的是做权限验证，其作用是在调用此方法之前做权限验证，如果不符合权限限制要求，则直接抛出异常。这里为了方便演示，此方法将直接抛出异常：\n//省略 imports @Aspect @Service public class AopConfig { @Before(\u0026#34;execution(* com.spring.puzzle.class5.example2.ElectricService.charge())\u0026#34;) public void logBeforeMethod(JoinPoint pjp) throws Throwable { System.out.println(\u0026#34;step into -\u0026gt;\u0026#34;+pjp.getSignature()); } @Before(\u0026#34;execution(* com.spring.puzzle.class5.example2.ElectricService.charge()) \u0026#34;) public void validateAuthority(JoinPoint pjp) throws Throwable { throw new RuntimeException(\u0026#34;authority check failed\u0026#34;); } } 我们对代码的执行预期为：当鉴权失败时，由于 ElectricService.charge() 没有被调用，那么 run() 的调用日志也不应该被输出，即 logBeforeMethod() 不应该被调用，但事实总是出乎意料，执行结果如下：\nstep into -\u0026gt;void com.spring.puzzle.class6.example2.Electric.charge() Exception in thread “main” java.lang.RuntimeException: authority check failed 虽然鉴权失败，抛出了异常且 ElectricService.charge() 没有被调用，但是 logBeforeMethod() 的调用日志却被输出了，这将导致后期针对于 ElectricService.charge() 的调用数据统计严重失真。\n这里我们就需要搞清楚一个问题：当同一个切面包含多个同一种类型的多个增强，且修饰的都是同一个方法时，这多个增强的执行顺序是怎样的？\n案例解析\n你应该还记得上述代码中，定义 METHOD_COMPARATOR 的静态代码块吧。\nMETHOD_COMPARATOR 本质是一个连续比较器，而上个案例中我们仅仅只看了第一个比较器，细心的你肯定发现了这里还有第二个比较器 methodNameComparator，任意两个比较器都可以通过其内置的 thenComparing() 连接形成一个连续比较器，从而可以让我们按照比较器的连接顺序依次比较：\nstatic { //第一个比较器，用来按照增强类型排序 Comparator\u0026lt;Method\u0026gt; adviceKindComparator = new ConvertingComparator\u0026lt;\u0026gt;( new InstanceComparator\u0026lt;\u0026gt;( Around.class, Before.class, After.class, AfterReturning.class, AfterThrowing.class), (Converter\u0026lt;Method, Annotation\u0026gt;) method -\u0026gt; { AspectJAnnotation\u0026lt;?\u0026gt; annotation = AbstractAspectJAdvisorFactory.findAspectJAnnotationOnMethod(method); return (annotation != null ? annotation.getAnnotation() : null); }) //第二个比较器，用来按照方法名排序 Comparator\u0026lt;Method\u0026gt; methodNameComparator = new ConvertingComparator\u0026lt;\u0026gt;(Method::getName); METHOD_COMPARATOR = adviceKindComparator.thenComparing(methodNameComparator); } 我们可以看到，在第 12 行代码中，第 2 个比较器 methodNameComparator 依然使用的是 ConvertingComparator，传递了方法名作为参数。我们基本可以猜测出该比较器是按照方法名进行排序的，这里可以进一步查看构造器方法及构造器调用的内部 comparable()：\npublic ConvertingComparator(Converter\u0026lt;S, T\u0026gt; converter) { this(Comparators.comparable(), converter); } // 省略非关键代码 public static \u0026lt;T\u0026gt; Comparator\u0026lt;T\u0026gt; comparable() { return ComparableComparator.INSTANCE; } 上述代码中的 ComparableComparator 实例其实极其简单，代码如下：\npublic class ComparableComparator\u0026lt;T extends Comparable\u0026lt;T\u0026gt;\u0026gt; implements Comparator\u0026lt;T\u0026gt; { public static final ComparableComparator INSTANCE = new ComparableComparator(); @Override public int compare(T o1, T o2) { return o1.compareTo(o2); } } 答案和我们的猜测完全一致，methodNameComparator 最终调用了 String 类自身的 compareTo()，代码如下：\npublic int compareTo(String anotherString) { int len1 = value.length; int len2 = anotherString.value.length; int lim = Math.min(len1, len2); char v1[] = value; char v2[] = anotherString.value; int k = 0; while (k \u0026lt; lim) { char c1 = v1[k]; char c2 = v2[k]; if (c1 != c2) { return c1 - c2; } k++; } return len1 - len2; } 到这，答案揭晓：如果两个方法名长度相同，则依次比较每一个字母的 ASCII 码，ASCII 码越小，排序越靠前；若长度不同，且短的方法名字符串是长的子集时，短的排序靠前。\n问题修正\n从上述分析我们得知，在同一个切面配置类中，针对同一个方法存在多个同类型增强时，其执行顺序仅和当前增强方法的名称有关，而不是由谁代码在先、谁代码在后来决定。了解了这点，我们就可以直接通过调整方法名的方式来修正程序：\n//省略 imports @Aspect @Service public class AopConfig { @Before(\u0026#34;execution(* com.spring.puzzle.class6.example2.ElectricService.charge())\u0026#34;) public void logBeforeMethod(JoinPoint pjp) throws Throwable { System.out.println(\u0026#34;step into -\u0026gt;\u0026#34;+pjp.getSignature()); } @Before(\u0026#34;execution(* com.spring.puzzle.class6.example2.ElectricService.charge()) \u0026#34;) public void checkAuthority(JoinPoint pjp) throws Throwable { throw new RuntimeException(\u0026#34;authority check failed\u0026#34;); } } 我们可以将原来的 validateAuthority() 改为 checkAuthority()，这种情况下，对增强（Advisor）的排序，其实最后就是在比较字符 l 和 字符 c。显然易见，checkAuthority() 的排序会靠前，从而被优先执行，最终问题得以解决。\nSpring事件常见错误 # 之前介绍了 Spring 依赖注入、AOP 等核心功能点上的常见错误。而作为 Spring 的关键功能支撑，Spring 事件是一个相对独立的点。或许你从没有在自己的项目中使用过 Spring 事件，但是你一定见过它的相关日志。而且在未来的编程实践中，你会发现，一旦你用上了 Spring 事件，往往完成的都是一些有趣的、强大的功能，例如动态配置。那么接下来我就来讲讲 Spring 事件上都有哪些常见的错误。\n案例 1：试图处理并不会抛出的事件\nSpring 事件的设计比较简单。说白了，就是监听器设计模式在 Spring 中的一种实现，参考下图：\n从图中我们可以看出，Spring 事件包含以下三大组件。\n事件（Event）：用来区分和定义不同的事件，在 Spring 中，常见的如 ApplicationEvent 和 AutoConfigurationImportEvent，它们都继承于 java.util.EventObject。 事件广播器（Multicaster）：负责发布上述定义的事件。例如，负责发布 ApplicationEvent 的 ApplicationEventMulticaster 就是 Spring 中一种常见的广播器。 事件监听器（Listener）：负责监听和处理广播器发出的事件，例如 ApplicationListener 就是用来处理 ApplicationEventMulticaster 发布的 ApplicationEvent，它继承于 JDK 的 EventListener，我们可以看下它的定义来验证这个结论： public interface ApplicationListener extends EventListener { void onApplicationEvent(E event); } 当然，虽然在上述组件中，任何一个都是缺一不可的，但是功能模块命名不见得完全贴合上述提及的关键字，例如发布 AutoConfigurationImportEvent 的广播器就不含有 Multicaster 字样。它的发布是由 AutoConfigurationImportSelector 来完成的。\n对这些基本概念和实现有了一定的了解后，我们就可以开始解析那些常见的错误。闲话少说，我们先来看下面这段基于 Spring Boot 技术栈的代码：\n@Slf4j @Component public class MyContextStartedEventListener implements ApplicationListener\u0026lt;ContextStartedEvent\u0026gt; { public void onApplicationEvent(final ContextStartedEvent event) { log.info(\u0026#34;{} received: {}\u0026#34;, this.toString(), event); } } 很明显，这段代码定义了一个监听器 MyContextStartedEventListener，试图拦截 ContextStartedEvent。因为在很多 Spring 初级开发者眼中，Spring 运转的核心就是一个 Context 的维护，那么启动 Spring 自然会启动 Context，于是他们是很期待出现类似下面的日志的：\n2021-03-07 07:08:21.197 INFO 2624 \u0026mdash; [nio-8080-exec-1] c.s.p.l.e.MyContextStartedEventListener : com.spring.puzzle.class7.example1.MyContextStartedEventListener@d33d5a received: org.springframework.context.event.ContextStartedEvent[source=org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@19b56c0, started on Sun Mar 07 07:07:57 CST 2021]\n但是当我们启动 Spring Boot 后，会发现并不会拦截到这个事件，如何理解这个错误呢？\n案例解析\n在 Spring 事件运用上，这是一个常见的错误，就是不假思索地认为一个框架只要定义了一个事件，那么一定会抛出来。例如，在本案例中，ContextStartedEvent 就是 Spring 内置定义的事件，而 Spring Boot 本身会创建和运维 Context，表面看起来这个事件的抛出是必然的，但是这个事件一定会在 Spring Boot 启动时抛出来么？\n答案明显是否定的，我们首先看下要抛出这个事件需要调用的方法是什么？在 Spring Boot 中，这个事件的抛出只发生在一处，即位于方法 AbstractApplicationContext#start 中。\n@Override public void start() { getLifecycleProcessor().start(); publishEvent(new ContextStartedEvent(this)); } 也就是说，只有上述方法被调用，才会抛出 ContextStartedEvent，但是这个方法在 Spring Boot 启动时会被调用么？我们可以查看 Spring 启动方法中围绕 Context 的关键方法调用，代码如下：\npublic ConfigurableApplicationContext run(String... args) { //省略非关键代码 context = createApplicationContext(); //省略非关键代码 prepareContext(context, environment, listeners, applicationArguments, printedBanner); refreshContext(context); //省略非关键代码 return context; } 我们发现围绕 Context、Spring Boot 的启动只做了两个关键工作：创建 Context 和 Refresh Context。其中 Refresh 的关键代码如下：\nprotected void refresh(ApplicationContext applicationContext) { Assert.isInstanceOf(AbstractApplicationContext.class, applicationContext); ((AbstractApplicationContext) applicationContext).refresh(); } 很明显，Spring 启动最终调用的是 AbstractApplicationContext#refresh，并不是 AbstractApplicationContext#start。在这样的残酷现实下，ContextStartedEvent 自然不会被抛出，不抛出，自然也不可能被捕获。所以这样的错误也就自然发生了。\n问题修正\n针对这个案例，有了源码的剖析，我们可以很快找到问题发生的原因，但是修正这个问题还要去追溯我们到底想要的是什么？我们可以分两种情况来考虑。\n假设我们是误读了 ContextStartedEvent 针对这种情况，往往是因为我们确实想在 Spring Boot 启动时拦截一个启动事件，但是我们粗略扫视相关事件后，误以为 ContextStartedEvent 就是我们想要的。针对这种情况，我们只需要把监听事件的类型修改成真正发生的事件即可，例如在本案例中，我们可以修正如下：\n@Component public class MyContextRefreshedEventListener implements ApplicationListener\u0026lt;ContextRefreshedEvent\u0026gt; { public void onApplicationEvent(final ContextRefreshedEvent event) { log.info(\u0026#34;{} received: {}\u0026#34;, this.toString(), event); } } 我们监听 ContextRefreshedEvent 而非 ContextStartedEvent。ContextRefreshedEvent 的抛出可以参考方法 AbstractApplicationContext#finishRefresh，它本身正好是 Refresh 操作中的一步。\nprotected void finishRefresh() { //省略非关键代码 initLifecycleProcessor(); // Propagate refresh to lifecycle processor first. getLifecycleProcessor().onRefresh(); // Publish the final event. publishEvent(new ContextRefreshedEvent(this)); //省略非关键代码 } 假设我们就是想要处理 ContextStartedEvent。 这种情况下，我们真的需要去调用 AbstractApplicationContext#start 方法。例如，我们可以使用下面的代码来让这个事件抛出：\n@RestController public class HelloWorldController { @Autowired private AbstractApplicationContext applicationContext; @RequestMapping(path = \u0026#34;publishEvent\u0026#34;, method = RequestMethod.GET) public String notifyEvent(){ applicationContext.start(); return \u0026#34;ok\u0026#34;; }; } 我们随便找一处来 Autowired 一个 AbstractApplicationContext，然后直接调用其 start() 就能让事件抛出来。\n很明显，这种抛出并不难，但是作为题外话，我们可以思考下为什么要去调用 start() 呢？start() 本身在 Spring Boot 中有何作用？\n如果我们去翻阅这个方法，我们会发现 start() 是 org.springframework.context.Lifecycle 定义的方法，而它在 Spring Boot 的默认实现中是去执行所有 Lifecycle Bean 的启动方法，这点可以参考 DefaultLifecycleProcessor#startBeans 方法来验证：\nprivate void startBeans(boolean autoStartupOnly) { Map\u0026lt;String, Lifecycle\u0026gt; lifecycleBeans = getLifecycleBeans(); Map\u0026lt;Integer, LifecycleGroup\u0026gt; phases = new HashMap\u0026lt;\u0026gt;(); lifecycleBeans.forEach((beanName, bean) -\u0026gt; { if (!autoStartupOnly || (bean instanceof SmartLifecycle \u0026amp;\u0026amp; ((SmartLifecycle) bean).isAutoStartup())) { int phase = getPhase(bean); LifecycleGroup group = phases.get(phase); if (group == null) { group = new LifecycleGroup(phase, this.timeoutPerShutdownPhase, lifecycleBeans, autoStartupOnly); phases.put(phase, group); } group.add(beanName, bean); } }); if (!phases.isEmpty()) { List\u0026lt;Integer\u0026gt; keys = new ArrayList\u0026lt;\u0026gt;(phases.keySet()); Collections.sort(keys); for (Integer key : keys) { phases.get(key).start(); } } } 说起来比较抽象，我们可以去写一个 Lifecycle Bean，代码如下：\n@Component @Slf4j public class MyLifeCycle implements Lifecycle { private volatile boolean running = false; @Override public void start() { log.info(\u0026#34;lifecycle start\u0026#34;); running = true; } @Override public void stop() { log.info(\u0026#34;lifecycle stop\u0026#34;); running = false; } @Override public boolean isRunning() { return running; } } 当我们再次运行 Spring Boot 时，只要执行了 AbstractApplicationContext 的 start()，就会输出上述代码定义的行为：输出 LifeCycle start 日志。\n通过这个 Lifecycle Bean 的使用，AbstractApplicationContext 的 start 要做的事，我们就清楚多了。它和 Refresh() 不同，Refresh() 是初始化和加载所有需要管理的 Bean，而 start 只有在有 Lifecycle Bean 时才有被调用的价值。那么我们自定义 Lifecycle Bean 一般是用来做什么呢？例如，可以用它来实现运行中的启停。这里不再拓展，你可以自己做更深入的探索。\n通过这个案例，我们搞定了第一类错误。而从这个错误中，我们也得出了一个启示：当一个事件拦截不了时，我们第一个要查的是拦截的事件类型对不对，执行的代码能不能抛出它。把握好这点，也就事半功倍了。\n案例 2：监听事件的体系不对\n通过案例 1 的学习，我们可以保证事件的抛出，但是抛出的事件就一定能被我们监听到么？我们再来看这样一个案例，首先上代码：\n@Slf4j @Component public class MyApplicationEnvironmentPreparedEventListener implements ApplicationListener\u0026lt;ApplicationEnvironmentPreparedEvent \u0026gt; { public void onApplicationEvent(final ApplicationEnvironmentPreparedEvent event) { log.info(\u0026#34;{} received: {}\u0026#34;, this.toString(), event); } } 这里我们试图处理 ApplicationEnvironmentPreparedEvent。期待出现拦截事件的日志如下：\n2021-03-07 09:12:08.886 INFO 27064 \u0026mdash; [ restartedMain] licationEnvironmentPreparedEventListener : com.spring.puzzle.class7.example2.MyApplicationEnvironmentPreparedEventListener@2b093d received: org.springframework.boot.context.event.ApplicationEnvironmentPreparedEvent[source=org.springframework.boot.SpringApplication@122b9e6]\n有了案例 1 的经验，首先我们就可以查看下这个事件的抛出会不会存在问题。这个事件在 Spring 中是由 EventPublishingRunListener#environmentPrepared 方法抛出，代码如下：\n@Override public void environmentPrepared(ConfigurableEnvironment environment) { this.initialMulticaster .multicastEvent(new ApplicationEnvironmentPreparedEvent(this.application, this.args, environment)); } 现在我们调试下代码，你会发现这个方法在 Spring 启动时一定经由 SpringApplication#prepareEnvironment 方法调用，调试截图如下：\n表面上看，既然代码会被调用，事件就会抛出，那么我们在最开始定义的监听器就能处理，但是我们真正去运行程序时会发现，效果和案例 1 是一样的，都是监听器的处理并不执行，即拦截不了。这又是为何？\n案例解析\n实际上，这是在 Spring 事件处理上非常容易犯的一个错误，即监听的体系不一致。通俗点说，就是“驴头不对马嘴”。我们首先来看下关于 ApplicationEnvironmentPreparedEvent 的处理，它相关的两大组件是什么？\n广播器：这个事件的广播器是 EventPublishingRunListener 的 initialMulticaster，代码参考如下： public class EventPublishingRunListener implements SpringApplicationRunListener, Ordered { //省略非关键代码 private final SimpleApplicationEventMulticaster initialMulticaster; public EventPublishingRunListener(SpringApplication application, String[] args) { //省略非关键代码 this.initialMulticaster = new SimpleApplicationEventMulticaster(); for (ApplicationListener\u0026lt;?\u0026gt; listener : application.getListeners()) { this.initialMulticaster.addApplicationListener(listener); } } } 监听器：这个事件的监听器同样位于 EventPublishingRunListener 中，获取方式参考关键代码行： this.initialMulticaster.addApplicationListener(listener); 如果继续查看代码，我们会发现这个事件的监听器就存储在 SpringApplication#Listeners 中，调试下就可以找出所有的监听器，截图如下：\n从中我们可以发现并不存在我们定义的 MyApplicationEnvironmentPreparedEventListener，这是为何？\n还是查看代码，当 Spring Boot 被构建时，会使用下面的方法去寻找上述监听器：\nsetListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class)); 而上述代码最终寻找 Listeners 的候选者，参考代码 SpringFactoriesLoader#loadSpringFactories 中的关键行：\n// 下面的 FACTORIES_RESOURCE_LOCATION 定义为 \u0026#34;META-INF/spring.factories\u0026#34;classLoader.getResources(FACTORIES_RESOURCE_LOCATION) : 我们可以寻找下这样的文件（spring.factories），确实可以发现类似的定义：\norg.springframework.context.ApplicationListener=\\ org.springframework.boot.ClearCachesApplicationListener,\\ org.springframework.boot.builder.ParentContextCloserApplicationListener,\\ org.springframework.boot.cloud.CloudFoundryVcapEnvironmentPostProcessor,\\ //省略其他监听器 说到这里，相信你已经意识到本案例的问题所在。我们定义的监听器并没有被放置在 META-INF/spring.factories 中，实际上，我们的监听器监听的体系是另外一套，其关键组件如下：\n广播器：即 AbstractApplicationContext#applicationEventMulticaster； 监听器：由上述提及的 META-INF/spring.factories 中加载的监听器以及扫描到的 ApplicationListener 类型的 Bean 共同组成。 这样比较后，我们可以得出一个结论：我们定义的监听器并不能监听到 initialMulticaster 广播出的 ApplicationEnvironmentPreparedEvent。\n问题修正\n现在就到了解决问题的时候了，我们可以把自定义监听器注册到 initialMulticaster 广播体系中，这里提供两种方法修正问题。\n在构建 Spring Boot 时，添加 MyApplicationEnvironmentPreparedEventListener： @SpringBootApplication public class Application { public static void main(String[] args) { MyApplicationEnvironmentPreparedEventListener myApplicationEnvironmentPreparedEventListener = new MyApplicationEnvironmentPreparedEventListener(); SpringApplication springApplication = new SpringApplicationBuilder(Application.class).listeners(myApplicationEnvironmentPreparedEventListener).build(); springApplication.run(args); } } 使用 META-INF/spring.factories，即在 /src/main/resources 下面新建目录 META-INF，然后新建一个对应的 spring.factories 文件： org.springframework.context.ApplicationListener=\\ com.spring.puzzle.listener.example2.MyApplicationEnvironmentPreparedEventListener 通过上述两种修改方式，即可完成事件的监听，很明显第二种方式要优于第一种，至少完全用原生的方式去解决，而不是手工实例化一个 MyApplicationEnvironmentPreparedEventListener。这点还是挺重要的。\n反思这个案例的错误，结论就是对于事件一定要注意“驴头”（监听器）对上“马嘴”（广播）。\n案例 3：部分事件监听器失效\n通过前面案例的解析，我们可以确保事件在合适的时机被合适的监听器所捕获。但是理想总是与现实有差距，有些时候，我们可能还会发现部分事件监听器一直失效或偶尔失效。这里我们可以写一段代码来模拟偶尔失效的场景，首先我们完成一个自定义事件和两个监听器，代码如下：\npublic class MyEvent extends ApplicationEvent { public MyEvent(Object source) { super(source); } } @Component @Order(1) public class MyFirstEventListener implements ApplicationListener\u0026lt;MyEvent\u0026gt; { Random random = new Random(); @Override public void onApplicationEvent(MyEvent event) { log.info(\u0026#34;{} received: {}\u0026#34;, this.toString(), event); //模拟部分失效 if(random.nextInt(10) % 2 == 1) throw new RuntimeException(\u0026#34;exception happen on first listener\u0026#34;); } } @Component @Order(2) public class MySecondEventListener implements ApplicationListener\u0026lt;MyEvent\u0026gt; { @Override public void onApplicationEvent(MyEvent event) { log.info(\u0026#34;{} received: {}\u0026#34;, this.toString(), event); } } 这里监听器 MyFirstEventListener 的优先级稍高，且执行过程中会有 50% 的概率抛出异常。然后我们再写一个 Controller 来触发事件的发送：\n@RestController @Slf4j public class HelloWorldController { @Autowired private AbstractApplicationContext applicationContext; @RequestMapping(path = \u0026#34;publishEvent\u0026#34;, method = RequestMethod.GET) public String notifyEvent(){ log.info(\u0026#34;start to publish event\u0026#34;); applicationContext.publishEvent(new MyEvent(UUID.randomUUID())); return \u0026#34;ok\u0026#34;; }; } 完成这些代码后，我们就可以使用http://localhost:8080/publishEvent 来测试监听器的接收和执行了。观察测试结果，我们会发现监听器 MySecondEventListener 有一半的概率并没有接收到任何事件。可以说，我们使用了最简化的代码模拟出了部分事件监听器偶尔失效的情况。当然在实际项目中，抛出异常这个根本原因肯定不会如此明显，但还是可以借机举一反三的。那么如何理解这个问题呢？\n案例解析\n这个案例非常简易，如果你稍微有些开发经验的话，大概也能推断出原因：处理器的执行是顺序执行的，在执行过程中，如果一个监听器执行抛出了异常，则后续监听器就得不到被执行的机会了。这里我们可以通过 Spring 源码看下事件是如何被执行的？\n具体而言，当广播一个事件，执行的方法参考 SimpleApplicationEventMulticaster#multicastEvent(ApplicationEvent)：\n@Override public void multicastEvent(final ApplicationEvent event, @Nullable ResolvableType eventType) { ResolvableType type = (eventType != null ? eventType : resolveDefaultEventType(event)); Executor executor = getTaskExecutor(); for (ApplicationListener\u0026lt;?\u0026gt; listener : getApplicationListeners(event, type)) { if (executor != null) { executor.execute(() -\u0026gt; invokeListener(listener, event)); } else { invokeListener(listener, event); } } } 上述方法通过 Event 类型等信息调用 getApplicationListeners 获取了具有执行资格的所有监听器（在本案例中，即为 MyFirstEventListener 和 MySecondEventListener），然后按顺序去执行。最终每个监听器的执行是通过 invokeListener() 来触发的，调用的是接口方法 ApplicationListener#onApplicationEvent。执行逻辑可参考如下代码：\nprotected void invokeListener(ApplicationListener\u0026lt;?\u0026gt; listener, ApplicationEvent event) { ErrorHandler errorHandler = getErrorHandler(); if (errorHandler != null) { try { doInvokeListener(listener, event); } catch (Throwable err) { errorHandler.handleError(err); } } else { doInvokeListener(listener, event); } } private void doInvokeListener(ApplicationListener listener, ApplicationEvent event) { try { listener.onApplicationEvent(event); } catch (ClassCastException ex) { //省略非关键代码 } else { throw ex; } } } 这里我们并没有去设置什么 org.springframework.util.ErrorHandler，也没有绑定什么 Executor 来执行任务，所以针对本案例的情况，我们可以看出：最终事件的执行是由同一个线程按顺序来完成的，任何一个报错，都会导致后续的监听器执行不了。\n问题修正\n怎么解决呢？好办，我提供两种方案给你。\n确保监听器的执行不会抛出异常。 既然我们使用多个监听器，我们肯定是希望它们都能执行的，所以我们一定要保证每个监听器的执行不会被其他监听器影响。基于这个思路，我们修改案例代码如下：\n@Component @Order(1) public class MyFirstEventListener implements ApplicationListener\u0026lt;MyEvent\u0026gt; { @Override public void onApplicationEvent(MyEvent event) { try { // 省略事件处理相关代码 }catch(Throwable throwable){ //write error/metric to alert } } } 使用 org.springframework.util.ErrorHandler 通过上面的案例解析，我们发现，假设我们设置了一个 ErrorHandler，那么就可以用这个 ErrorHandler 去处理掉异常，从而保证后续事件监听器处理不受影响。我们可以使用下面的代码来修正问题：\nSimpleApplicationEventMulticaster simpleApplicationEventMulticaster = applicationContext.getBean(APPLICATION_EVENT_MULTICASTER_BEAN_NAME, SimpleApplicationEventMulticaster.class); simpleApplicationEventMulticaster.setErrorHandler(TaskUtils.LOG_AND_SUPPRESS_ERROR_HANDLER); 其中 LOG_AND_SUPPRESS_ERROR_HANDLER 的实现如下：\npublic static final ErrorHandler LOG_AND_SUPPRESS_ERROR_HANDLER = new LoggingErrorHandler(); private static class LoggingErrorHandler implements ErrorHandler { private final Log logger = LogFactory.getLog(LoggingErrorHandler.class); @Override public void handleError(Throwable t) { logger.error(\u0026#34;Unexpected error occurred in scheduled task\u0026#34;, t); } } 对比下方案 1，使用 ErrorHandler 有一个很大的优势，就是我们不需要在某个监听器中都重复类似下面的代码了：\ntry { //省略事件处理过程 }catch(Throwable throwable){ //write error/metric to alert } 这么看的话，其实 Spring 的设计还是很全面的，它考虑了各种各样的情况。但是 Spring 使用者往往都不会去了解其内部实现，这样就会遇到各种各样的问题。相反，如果你对其实现有所了解的话，也对常见错误有一个感知，则大概率是可以快速避坑的，项目也可以运行得更加平稳顺畅。\n","date":"18 May 2024","permalink":"/posts/language/java/spring/spring-%E7%BC%96%E7%A8%8B%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF50%E4%BE%8B/spirng-core%E7%AF%87/","section":"博客","summary":"Spring Core 常用错误","title":"Spring Core 常用错误"},{"content":"一个HTTP请求的处理过程 # Spring Web 最核心的流程无非就是一个 HTTP 请求的处理过程。这里我以 Spring Boot 的使用为例，以尽量简单的方式带你梳理下。\n首先，回顾下我们是怎么添加一个 HTTP 接口的，示例如下：\n@RestController public class HelloWorldController { @RequestMapping(path = \u0026#34;hi\u0026#34;, method = RequestMethod.GET) public String hi(){ return \u0026#34;helloworld\u0026#34;; }; } 这是我们最喜闻乐见的一个程序，但是对于很多程序员而言，其实完全不知道为什么这样就工作起来了。毕竟，不知道原理，它也能工作起来。\n但是，假设你是一个严谨且有追求的人，你大概率是有好奇心去了解它的。而且相信我，这个问题面试也可能会问到。我们一起来看看它背后的故事。\n其实仔细看这段程序，你会发现一些关键的“元素”：\n请求的 Path：hi 请求的方法：Get 对应方法的执行：hi() 那么，假设让你自己去实现 HTTP 的请求处理，你可能会写出这样一段伪代码：\npublic class HttpRequestHandler{ Map\u0026lt;RequestKey, Method\u0026gt; mapper = new HashMap\u0026lt;\u0026gt;(); public Object handle(HttpRequest httpRequest){ RequestKey requestKey = getRequestKey(httpRequest); Method method = this.mapper.getValue(requestKey); Object[] args = resolveArgsAccordingToMethod(httpRequest, method); return method.invoke(controllerObject, args); }; } 09｜Spring Web URL 解析常见错误 # ","date":"18 May 2024","permalink":"/posts/language/java/spring/spring-%E7%BC%96%E7%A8%8B%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF50%E4%BE%8B/spring-web%E7%AF%87/","section":"博客","summary":"Spring Web 常用错误","title":"Spring Web 常用错误"},{"content":"","date":"18 May 2024","permalink":"/posts/language/java/spring/spring-%E7%BC%96%E7%A8%8B%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF50%E4%BE%8B/","section":"博客","summary":"Spring 编程常见错误50例","title":"Spring 编程常见错误50例"},{"content":"","date":"17 May 2024","permalink":"/posts/architecture/distributed/rpc/","section":"博客","summary":"RPC 的全称是 Remote Procedure Call Protocol，中文名是远程过程调用协议。官方的描述是：一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。","title":"rpc"},{"content":"","date":"17 May 2024","permalink":"/posts/architecture/distributed/rpc/rpc-%E5%AE%9E%E6%88%98%E4%B8%8E%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86/","section":"博客","summary":"精挑细选了 20 多个 RPC 相关的高频场景化问题，帮你搞懂 RPC 核心原理","title":"RPC 实战与核心原理"},{"content":"17 | 异步RPC：压榨单机吞吐量 # 之前我们学习了 RPC 框架的基础架构和一系列治理功能，以及一些与集群管理相关的高级功能，如服务发现、健康检查、路由策略、负载均衡、优雅启停机等等。\n有了这些知识储备，你就已经对 RPC 框架有了较为充分的认识。但如果你想要更深入地了解 RPC，更好地使用 RPC，你就必须从 RPC 框架的整体性能上去考虑问题了。你得知道如何去提升 RPC 框架的性能、稳定性、安全性、吞吐量，以及如何在分布式的场景下快速定位问题等等！那么今天我们就先来讲讲，RPC 框架是如何压榨单机吞吐量的。\n如何提升单机吞吐量？\n在我运营 RPC 的过程中，“如何提升吞吐量”是我与业务团队经常讨论的问题。\n记得之前业务团队反馈过这样一个问题：我们的 TPS 始终上不去，压测的时候 CPU 压到 40%～50% 就再也压不上去了，TPS 也不会提高，问我们这里有没有什么解决方案可以提升业务的吞吐量？\n之后我是看了下他们服务的业务逻辑，发现他们的业务逻辑在执行较为耗时的业务逻辑的基础上，又同步调用了好几个其它的服务。由于这几个服务的耗时较长，才导致这个服务的业务逻辑耗时也长，CPU 大部分的时间都在等待，并没有得到充分地利用，因此 CPU 的利用率和服务的吞吐量当然上不去了。\n那是什么影响到了 RPC 调用的吞吐量呢？\n在使用 RPC 的过程中，谈到性能和吞吐量，我们的第一反应就是选择一款高性能、高吞吐量的 RPC 框架，那影响到 RPC 调用的吞吐量的根本原因是什么呢？\n其实根本原因就是由于处理 RPC 请求比较耗时，并且 CPU 大部分的时间都在等待而没有去计算，从而导致 CPU 的利用率不够。这就好比一个人在干活，但他没有规划好时间，并且有很长一段时间都在闲着，当然也就完不成太多工作了。\n那么导致 RPC 请求比较耗时的原因主要是在于 RPC 框架本身吗？事实上除非在网络比较慢或者使用方使用不当的情况下，否则，在大多数情况下，刨除业务逻辑处理的耗时时间，RPC 本身处理请求的效率就算在比较差的情况下也不过是毫秒级的。可以说 RPC 请求的耗时大部分都是业务耗时，比如业务逻辑中有访问数据库执行慢 SQL 的操作。所以说，在大多数情况下，影响到 RPC 调用的吞吐量的原因也就是业务逻辑处理慢了，CPU 大部分时间都在等待资源。\n可以说 RPC 请求的耗时大部分都是业务耗时，比如业务逻辑中有访问数据库执行慢 SQL 的操作。\n弄明白了原因，咱们就可以解决问题了，该如何去提升单机吞吐量？\n这并不是一个新话题，比如现在我们经常提到的响应式开发，就是为了能够提升业务处理的吞吐量。要提升吞吐量，其实关键就两个字：“异步”。我们的 RPC 框架要做到完全异步化，实现全异步 RPC。试想一下，如果我们每次发送一个异步请求，发送请求过后请求即刻就结束了，之后业务逻辑全部异步执行，结果异步通知，这样可以增加多么可观的吞吐量？\n效果不用我说我想你也清楚了。那 RPC 框架都有哪些异步策略呢？\n调用端如何异步？\n说到异步，我们最常用的方式就是返回 Future 对象的 Future 方式，或者入参为 Callback 对象的回调方式，而 Future 方式可以说是最简单的一种异步方式了。我们发起一次异步请求并且从请求上下文中拿到一个 Future，之后我们就可以调用 Future 的 get 方法获取结果。\n就比如刚才我提到的业务团队的那个问题，他们的业务逻辑中调用了好几个其它的服务，这时如果是同步调用，假设调用了 4 个服务，每个服务耗时 10 毫秒，那么业务逻辑执行完至少要耗时 40 毫秒。\n那如果采用 Future 方式呢？\n连续发送 4 次异步请求并且拿到 4 个 Future，由于是异步调用，这段时间的耗时几乎可以忽略不计，之后我们统一调用这几个 Future 的 get 方法。这样一来的话，业务逻辑执行完的时间在理想的情况下是多少毫秒呢？没错，10 毫秒，耗时整整缩短到了原来的四分之一，也就是说，我们的吞吐量有可能提升 4 倍！\n那 RPC 框架的 Future 方式异步又该如何实现呢？\n通过基础篇的学习，我们了解到，一次 RPC 调用的本质就是调用端向服务端发送一条请求消息，服务端收到消息后进行处理，处理之后响应给调用端一条响应消息，调用端收到响应消息之后再进行处理，最后将最终的返回值返回给动态代理。\n这里我们可以看到，对于调用端来说，向服务端发送请求消息与接收服务端发送过来的响应消息，这两个处理过程是两个完全独立的过程，这两个过程甚至在大多数情况下都不在一个线程中进行。那么是不是说 RPC 框架的调用端，对于 RPC 调用的处理逻辑，内部实现就是异步的呢？\n不错，对于 RPC 框架，无论是同步调用还是异步调用，调用端的内部实现都是异步的。\n调用端发送的每条消息都一个唯一的消息标识，实际上调用端向服务端发送请求消息之前会先创建一个 Future，并会存储这个消息标识与这个 Future 的映射，动态代理所获得的返回值最终就是从这个 Future 中获取的；当收到服务端响应的消息时，调用端会根据响应消息的唯一标识，通过之前存储的映射找到对应的 Future，将结果注入给那个 Future，再进行一系列的处理逻辑，最后动态代理从 Future 中获得到正确的返回值。\n所谓的同步调用，不过是 RPC 框架在调用端的处理逻辑中主动执行了这个 Future 的 get 方法，让动态代理等待返回值；而异步调用则是 RPC 框架没有主动执行这个 Future 的 get 方法，用户可以从请求上下文中得到这个 Future，自己决定什么时候执行这个 Future 的 get 方法。\n现在你应该很清楚 RPC 框架是如何实现 Future 方式的异步了。\n如何做到 RPC 调用全异步？\n刚才我讲解了 Future 方式的异步，Future 方式异步可以说是调用端异步的一种方式，那么服务端呢？服务端是否需要异步，有什么实现方式？\n通过基础篇的学习，我们了解到 RPC 服务端接收到请求的二进制消息之后会根据协议进行拆包解包，之后将完整的消息进行解码并反序列化，获得到入参参数之后再通过反射执行业务逻辑。那你有没有想过，在生产环境中这些操作都在哪个线程中执行呢？是在一个线程中执行吗？\n当然不会在一个，对二进制消息数据包拆解包的处理是一定要在处理网络 IO 的线程中，如果网络通信框架使用的是 Netty 框架，那么对二进制包的处理是在 IO 线程中，而解码与反序列化的过程也往往在 IO 线程中处理，那服务端的业务逻辑呢？也应该在 IO 线程中处理吗？原则上是不应该的，业务逻辑应该交给专门的业务线程池处理，以防止由于业务逻辑处理得过慢而影响到网络 IO 的处理。\n这时问题就来了，我们配置的业务线程池的线程数都是有限制的，在我运营 RPC 的经验中，业务线程池的线程数一般只会配置到 200，因为在大多数情况下线程数配置到 200 还不够用就说明业务逻辑该优化了。那么如果碰到特殊的业务场景呢？让配置的业务线程池完全打满了，比如这样一个场景。\n我这里启动一个服务，业务逻辑处理得就是比较慢，当访问量逐渐变大时，业务线程池很容易就被打满了，吞吐量很不理想，并且这时 CPU 的利用率也很低。\n对于这个问题，你有没有想到什么解决办法呢？是不是会马上想到调大业务线程池的线程数？那这样可以吗？有没有更好的解决方式呢？\n我想服务端业务处理逻辑异步是个好方法。\n调大业务线程池的线程数，的确勉强可以解决这个问题，但是对于 RPC 框架来说，往往都会有多个服务共用一个线程池的情况，即使调大业务线程池，比较耗时的服务很可能还会影响到其它的服务。所以最佳的解决办法是能够让业务线程池尽快地释放，那么我们就需要 RPC 框架能够支持服务端业务逻辑异步处理，这对提高服务的吞吐量有很重要的意义。\n那服务端如何支持业务逻辑异步呢？\n这是个比较难处理的问题，因为服务端执行完业务逻辑之后，要对返回值进行序列化并且编码，将消息响应给调用端，但如果是异步处理，业务逻辑触发异步之后方法就执行完了，来不及将真正的结果进行序列化并编码之后响应给调用端。\n这时我们就需要 RPC 框架提供一种回调方式，让业务逻辑可以异步处理，处理完之后调用 RPC 框架的回调接口，将最终的结果通过回调的方式响应给调用端。\n说到服务端支持业务逻辑异步处理，结合我刚才讲解的 Future 方式异步，你有没有想到更好的处理方式呢？其实我们可以让 RPC 框架支持 CompletableFuture，实现 RPC 调用在调用端与服务端之间完全异步。\nCompletableFuture 是 Java8 原生支持的。试想一下，假如 RPC 框架能够支持 CompletableFuture，我现在发布一个 RPC 服务，服务接口定义的返回值是 CompletableFuture 对象，整个调用过程会分为这样几步：\n服务调用方发起 RPC 调用，直接拿到返回值 CompletableFuture 对象，之后就不需要任何额外的与 RPC 框架相关的操作了（如我刚才讲解 Future 方式时需要通过请求上下文获取 Future 的操作），直接就可以进行异步处理； 在服务端的业务逻辑中创建一个返回值 CompletableFuture 对象，之后服务端真正的业务逻辑完全可以在一个线程池中异步处理，业务逻辑完成之后再调用这个 CompletableFuture 对象的 complete 方法，完成异步通知。关键的地方 普通的future与completeFuture最大的不同之处在于 completefuture多了一个异步通知，不是future那样需要轮训直到状态了。 调用端在收到服务端发送过来的响应之后，RPC 框架再自动地调用调用端拿到的那个返回值 CompletableFuture 对象的 complete 方法，这样一次异步调用就完成了。 通过对 CompletableFuture 的支持，RPC 框架可以真正地做到在调用端与服务端之间完全异步，同时提升了调用端与服务端的两端的单机吞吐量，并且 CompletableFuture 是 Java8 原生支持，业务逻辑中没有任何代码入侵性，这是不是很酷炫了？\n18 | 安全体系：如何建立可靠的安全体系？ # 在 RPC 里面该如何提升单机资源的利用率，你要记住的关键点就一个，那就是“异步化”。调用方利用异步化机制实现并行调用多个服务，以缩短整个调用时间；而服务提供方则可以利用异步化把业务逻辑放到自定义线程池里面去执行，以提升单机的 OPS。\n回顾完上一章的重点，我们就切入今天的主题，一起来看看 RPC 里面的安全问题。\n为什么需要考虑安全问题？\n说起安全问题，你可能会想到像 SQL 注入、XSS 攻击等恶意攻击行为，还有就是相对更广义的安全，像网络安全、信息安全等，那在 RPC 里面我们说的安全一般指什么呢？\n我们知道 RPC 是解决应用间互相通信的框架，而应用之间的远程调用过程一般不会暴露在公网，换句话讲就是说 RPC 一般用于解决内部应用之间的通信，而这个“内部”是指应用都部署在同一个大局域网内。相对于公网环境，局域网的隔离性更好，也就相对更安全，所以在 RPC 里面我们很少考虑像数据包篡改、请求伪造等恶意行为。\n那在 RPC 里面我们应该关心什么样的安全问题呢？要搞清楚这个问题，我们可以先看一个完整的 RPC 应用流程。\n我们一般是先由服务提供方定义好一个接口，并把这个接口的 Jar 包发布到私服上去，然后在项目中去实现这个接口，最后通过 RPC 提供的 API 把这个接口和其对应的实现类完成对外暴露，如果是 Spring 应用的话直接定义成一个 Bean 就好了。到这儿，服务提供方就完成了一个接口的对外发布了。\n对于服务调用方来说就更简单了，只要拿到刚才上传到私服上的 Jar 的坐标，就可以把发布到私服的 Jar 引入到项目中来，然后借助 RPC 提供的动态代理功能，服务调用方直接就可以在项目完成 RPC 调用了。\n这里面其实存在一个安全隐患问题，因为私服上所有的 Jar 坐标我们所有人都可以看到，只要拿到了 Jar 的坐标，我们就可以把发布到私服的 Jar 引入到项目中完成 RPC 调用了吗？\n理论上确实是这样，当然我相信在公司内部这种不向服务提供方咨询就直接调用的行为很少发生，而且一般真实业务的接口出入参数都不会太简单，这样不经过咨询只靠调用方自己猜测完成调用的工作效率实在太低了。\n虽然这种靠猜测调用的概率很小，但是当调用方在其它新业务场景里面要用之前项目中使用过的接口，就很有可能真的不跟服务提供方打招呼就直接调用了。这种行为对于服务提供方来说就很危险了，因为接入了新的调用方就意味着承担的调用量会变大，有时候很有可能新增加的调用量会成为压倒服务提供方的“最后一根稻草”，从而导致服务提供方无法正常提供服务，关键是服务提供方还不知道是被谁给压倒的。\n当然你可能会说，这是一个流程问题，我们只要在公司内部规范好调用流程，就可以避免这种问题发生了。\n确实是这样，我们可以通过流程宣贯让我们所有的研发人员达成一个“君子约定”，就是在应用里面每次要用一个接口的时候必须先向服务提供方进行报备，这样确实能在很大程度上避免这种情况的发生。但就 RPC 本身来说，我们是不是可以提供某种功能来解决这种问题呢？毕竟对于人数众多的团队来说，光靠口头约定的流程并不能彻底杜绝这类问题，依然存在隐患，且不可控。\n调用方之间的安全保证\n那在 RPC 里面，我们该怎么解决这种问题呢？\n我们先总结下刚才的问题，根本原因就是服务提供方收到请求后，不知道这次请求是哪个调用方发起的，没法判断这次请求是属于之前打过招呼的调用方还是没有打过招呼的调用方，所以也就没法选择拒绝这次请求还是继续执行。\n问题说明白了就好解决了，我们只需要给每个调用方设定一个唯一的身份，每个调用方在调用之前都先来服务提供方这登记下身份，只有登记过的调用方才能继续放行，没有登记过的调用方一律拒绝。\n这就好比我们平时坐火车，我们拿着身份证去购买火车票，买票成功就类似服务调用方去服务提供方这儿进行登记。当你进站准备上火车的时候，你必须同时出示你的身份证和火车票，这两个就是代表你能上这趟火车的“唯一身份”，只有验证了身份，负责检票的工作人员才会让你上车，否则会直接拒绝你乘车。\n现在方案有了，那在 RPC 里面我们该怎么实现呢？\n首先我们要有一个可以供调用方进行调用接口登记的地方，我们姑且称这个地方为“授权平台”，调用方可以在授权平台上申请自己应用里面要调用的接口，而服务提供方则可以在授权平台上进行审批，只有服务提供方审批后调用方才能调用。但这只是解决了调用数据收集的问题，并没有完成真正的授权认证功能，缺少一个检票的环节。\n既然有了刚搭建的授权平台，而且接口的授权数据也在这个平台上，我们自然就很容易想到是不是可以把这个检票的环节放到这个授权平台上呢？调用方每次发起业务请求的时候先去发一条认证请求到授权平台上，就说：“哥们儿，我能调用这个接口吗？”只有授权平台返回“没问题”后才继续把业务请求发送到服务提供方那去。整个流程如下图所示：\n从使用功能的角度来说，目前这种设计是没有问题的，而且整个认证过程对 RPC 使用者来说也是透明的。但有一个问题就是这个授权平台承担了公司内所有 RPC 请求的次数总和，当公司内部 RPC 使用程度高了之后，这个授权平台就会成为一个瓶颈点，而且必须保证超高可用，一旦这个授权平台出现问题，影响的可就是全公司的 RPC 请求了。\n可能你会说我们可以改进下，我们是不是不需要把这个认证的逻辑放到业务请求过程中，而是可以把这个认证过程挪到初始化过程中呢？这样确实可以在很大程度上减少授权平台的压力，但本质并没有发生变化，还是一个集中式的授权平台。\n我们可以想一个更优雅一点的方案。\n其实调用方能不能调用相关接口，是由服务提供方说了算，我服务提供方认为你是可以的，你就肯定能调，那我们是不是就可以把这个检票过程放到服务提供方里面呢？在调用方启动初始化接口的时候，带上授权平台上颁发的身份去服务提供方认证下，当认证通过后就认为这个接口可以调用。\n现在新的问题又来了，服务提供方验票的时候对照的数据来自哪儿，我总不能又去请求授权平台吧？否则就又会遇到和前面方案一样的问题。\n你还记得我们加密算法里面有一种叫做不可逆加密算法吗？HMAC 就是其中一种具体实现。服务提供方应用里面放一个用于 HMAC 签名的私钥，在授权平台上用这个私钥为申请调用的调用方应用进行签名，这个签名生成的串就变成了调用方唯一的身份。服务提供方在收到调用方的授权请求之后，我们只要需要验证下这个签名跟调用方应用信息是否对应得上就行了，这样集中式授权的瓶颈也就不存在了。\n服务发现也有安全问题？\n好，现在我们已经解决了调用方之间的安全认证问题。那在 RPC 里面，我们还有其它的安全问题吗？\n回到我们上面说的那个完整的 RPC 应用流程里面，服务提供方会把接口 Jar 发布到私服上，以方便调用方能引入到项目中快速完成 RPC 调用，那有没有可能有人拿到你这个 Jar 后，发布出来一个服务提供方呢？这样的后果就是导致调用方通过服务发现拿到的服务提供方 IP 地址集合里面会有那个伪造的提供方。\n当然，这种情况相对上面说的调用方未经过咨询就直接调用的概率会小很多，但为了让我们的系统整体更安全，我们也需要在 RPC 里面考虑这种情况。要解决这个问题的根本就是需要把接口跟应用绑定上，一个接口只允许有一个应用发布提供者，避免其它应用也能发布这个接口。\n服务提供方启动的时候，需要把接口实例在注册中心进行注册登记。我们就可以利用这个流程，注册中心可以在收到服务提供方注册请求的时候，验证下请求过来的应用是否跟接口绑定的应用一样，只有相同才允许注册，否则就返回错误信息给启动的应用，从而避免假冒的服务提供者对外提供错误服务。\n19 | 分布式环境下如何快速定位问题？ # 如何建立可靠的安全体系，关键点就是“鉴权”，我们可以通过统一的鉴权服务动态生成秘钥，提高 RPC 调用的安全性。\n回顾完上一讲的重点，我们就切入今天的主题，一起看看 RPC 在分布式环境下如何快速定位问题。重要性看字面也是不言而喻了，只有准确地定位问题，我们才能更好地解决问题。\n分布式环境下定位问题有哪些困难？\n在此之前，我想先请你想想，在开发以及生产环境运行的过程中，如果遇见问题，我们是如何定位的？\n在开发过程中遇见问题其实很好排查，我们可以用 IDE 在自己本地的开发环境中运行一遍代码，进行 debug，在这个过程中是很容易找到问题的。\n那换到生产环境，代码在线上运行业务，我们是不能进行 debug 的，这时我们就可以通过打印日志来查看当前的异常日志，这也是最简单有效的一种方式了。事实上，大部分问题的定位我们也是这样做的。\n那么如果是在分布式的生产环境中呢？比如下面这个场景：\n我们搭建了一个分布式的应用系统，在这个应用系统中，我启动了 4 个子服务，分别是服务 A、服务 B、服务 C 与服务 D，而这 4 个服务的依赖关系是 A-\u0026gt;B-\u0026gt;C-\u0026gt;D，而这些服务又都部署在不同的机器上。在 RPC 调用中，如果服务端的业务逻辑出现了异常，就会把异常抛回给调用端，那么如果现在这个调用链中有一个服务出现了异常，我们该如何定位问题呢？\n可能你的第一反应仍然是打印日志，好，那就打印日志吧。\n假如这时我们发现服务 A 出现了异常，那这个异常有没有可能是因为 B 或 C 或 D 出现了异常抛回来的呢？当然很有可能。那我们怎么确定在整个应用系统中，是哪一个调用步骤出现的问题，以及是在这个步骤中的哪台机器出现的问题呢？我们该在哪台机器上打印日志？而且为了排查问题，如果要打印日志，我们就必须要修改代码，这样的话我们就得重新对服务进行上线。如果这几个服务又恰好是跨团队跨部门的呢？想想我们要面临的沟通成本吧。\n所以你看，分布式环境下定位问题的难点就在于，各子应用、子服务间有着复杂的依赖关系，我们有时很难确定是哪个服务的哪个环节出现的问题。简单地通过日志排查问题，就要对每个子应用、子服务逐一进行排查，很难一步到位；若恰好再赶上跨团队跨部门，那不死也得去半条命了。\n如何做到快速定位问题？\n明白了难点，我们其实就可以有针对性地去攻克它了。有关 RPC 在分布式环境下如何快速定位问题，我给出两个方法，很实用。\n方法 1：借助合理封装的异常信息\n我们前面说是因为各子应用、子服务间复杂的依赖关系，所以通过日志难定位问题。那我们就想办法通过日志定位到是哪个子应用的子服务出现问题就行了。\n其实，在 RPC 框架打印的异常信息中，是包括定位异常所需要的异常信息的，比如是哪类异常引起的问题（如序列化问题或网络超时问题），是调用端还是服务端出现的异常，调用端与服务端的 IP 是什么，以及服务接口与服务分组都是什么等等。具体如下图所示：\n这样的话，在 A-\u0026gt;B-\u0026gt;C-\u0026gt;D 这个过程中，我们就可以很快地定位到是 C 服务出现了问题，服务接口是 com.demo.CSerivce，调用端 IP 是 192.168.1.2，服务端 IP 是 192.168.1.3，而出现问题的原因就是业务线程池满了。\n由此可见，一款优秀的 RPC 框架要对异常进行详细地封装，还要对各类异常进行分类，每类异常都要有明确的异常标识码，并整理成一份简明的文档。使用方可以快速地通过异常标识码在文档中查阅，从而快速定位问题，找到原因；并且异常信息中要包含排查问题时所需要的重要信息，比如服务接口名、服务分组、调用端与服务端的 IP，以及产生异常的原因。总之就是，要让使用方在复杂的分布式应用系统中，根据异常信息快速地定位到问题。\n以上是对于 RPC 框架本身的异常来说的，比如序列化异常、响应超时异常、连接异常等等。那服务端业务逻辑的异常呢？服务提供方提供的服务的业务逻辑也要封装自己的业务异常信息，从而让服务调用方也可以通过异常信息快速地定位到问题。\n方法 2：借助分布式链路跟踪\n无论是 RPC 框架本身，还是服务提供方提供的服务，只要对异常信息进行合理地封装，就可以让我们在分布式环境下定位问题变得更加容易。那这样是不是就满足我们定位问题的需求了呢？\n我们还是回到前面提过的那个分布式场景：我们搭建了一个分布式的应用系统，它由 4 个子服务组成，4 个服务的依赖关系为 A-\u0026gt;B-\u0026gt;C-\u0026gt;D。\n假设这 4 个服务分别由来自不同部门的 4 个同事维护，在 A 调用 B 的时候，维护服务 A 的同事可能是不知道存在服务 C 和服务 D 的，对于服务 A 来说，它的下游服务只有 B 服务，那这时如果服务 C 或服务 D 出现异常，最终在整个链路中将异常抛给 A 了呢？\n在这种情况下维护服务 A 的同事该如何定位问题呢？\n因为对于 A 来说，它可能是不知道下游存在服务 C 和服务 D 的，所以维护服务 A 的同事会直接联系维护服务 B 的同事，之后维护服务 B 的同事会继续联系下游服务的服务提供方，直到找到问题。可这样做成本很高啊！\n现在我们换个思路，其实我们只要知道整个请求的调用链路就可以了。服务 A 调用下游服务 B，服务 B 又调用了 B 依赖的下游服务，如果维护服务 A 的同事能清楚地知道整个调用链路，并且能准确地发现在整个调用链路中是哪个环节出现了问题，那就好了。\n这就好比我们收发快递，我们可以在平台上看到快递配送的轨迹，实时获知快递在何时到达了哪个站点，这样当我们没有准时地收到快递时，我们马上就能知道快递是在哪里延误了。\n在分布式环境下，要想知道服务调用的整个链路，我们可以用“分布式链路跟踪”。\n先介绍下分布式链路跟踪系统。从字面上理解，分布式链路跟踪就是将一次分布式请求还原为一个完整的调用链路，我们可以在整个调用链路中跟踪到这一次分布式请求的每一个环节的调用情况，比如调用是否成功，返回什么异常，调用的哪个服务节点以及请求耗时等等。\n这样如果我们发现服务调用出现问题，通过这个方法，我们就能快速定位问题，哪怕是多个部门合作，也可以一步到位。\n紧接着，我们再看看在 RPC 框架中是如何整合分布式链路跟踪的？\n分布式链路跟踪有 Trace 与 Span 的概念，什么意思呢，我逐一解释。\nTrace 就是代表整个链路，每次分布式都会产生一个 Trace，每个 Trace 都有它的唯一标识即 TraceId，在分布式链路跟踪系统中，就是通过 TraceId 来区分每个 Trace 的。\nSpan 就是代表了整个链路中的一段链路，也就是说 Trace 是由多个 Span 组成的。在一个 Trace 下，每个 Span 也都有它的唯一标识 SpanId，而 Span 是存在父子关系的。还是以讲过的例子为例子，在 A-\u0026gt;B-\u0026gt;C-\u0026gt;D 的情况下，在整个调用链中，正常情况下会产生 3 个 Span，分别是 Span1（A-\u0026gt;B）、Span2（B-\u0026gt;C）、Span3（C-\u0026gt;D），这时 Span3 的父 Span 就是 Span2，而 Span2 的父 Span 就是 Span1。\nTrace 与 Span 的关系如下图所示：\n分布式链路跟踪系统的实现方式有很多，但它们都脱离不开我刚才说的 Trace 和 Span，这两点可以说非常重要，掌握了这两个概念，其实你就掌握了大部分实现方式的原理。接着我们看看在 RPC 框架中如何利用这两个概念去整合分布式链路跟踪。\nRPC 在整合分布式链路跟踪需要做的最核心的两件事就是“埋点”和“传递”。\n所谓“埋点”就是说，分布式链路跟踪系统要想获得一次分布式调用的完整的链路信息，就必须对这次分布式调用进行数据采集，而采集这些数据的方法就是通过 RPC 框架对分布式链路跟踪进行埋点。\nRPC 调用端在访问服务端时，在发送请求消息前会触发分布式跟踪埋点，在接收到服务端响应时，也会触发分布式跟踪埋点，并且在服务端也会有类似的埋点。这些埋点最终可以记录一个完整的 Span，而这个链路的源头会记录一个完整的 Trace，最终 Trace 信息会被上报给分布式链路跟踪系统。\n那所谓“传递”就是指，上游调用端将 Trace 信息与父 Span 信息传递给下游服务的服务端，由下游触发埋点，对这些信息进行处理，在分布式链路跟踪系统中，每个子 Span 都存有父 Span 的相关信息以及 Trace 的相关信息。\n20 | 详解时钟轮在RPC中的应用 # 在分布式环境下，RPC 框架自身以及服务提供方的业务逻辑实现，都应该对异常进行合理地封装，让使用方可以根据异常快速地定位问题；而在依赖关系复杂且涉及多个部门合作的分布式系统中，我们也可以借助分布式链路跟踪系统，快速定位问题。\n现在，切换到咱们今天的主题，一起看看时钟轮在 RPC 中的应用。\n定时任务带来了什么问题？\n在讲解时钟轮之前，我们先来聊聊定时任务。相信你在开发的过程中，很多场景都会使用到定时任务，在 RPC 框架中也有很多地方会使用到它。就以调用端请求超时的处理逻辑为例，下面我们看一下 RPC 框架是如果处理超时请求的。\n我讲解 Future 的时候说过：无论是同步调用还是异步调用，调用端内部实行的都是异步，而调用端在向服务端发送消息之前会创建一个 Future，并存储这个消息标识与这个 Future 的映射，当服务端收到消息并且处理完毕后向调用端发送响应消息，调用端在接收到消息后会根据消息的唯一标识找到这个 Future，并将结果注入给这个 Future。\n那在这个过程中，如果服务端没有及时响应消息给调用端呢？调用端该如何处理超时的请求？\n没错，就是可以利用定时任务。每次创建一个 Future，我们都记录这个 Future 的创建时间与这个 Future 的超时时间，并且有一个定时任务进行检测，当这个 Future 到达超时时间并且没有被处理时，我们就对这个 Future 执行超时逻辑。\n那定时任务该如何实现呢？\n有种实现方式是这样的，也是最简单的一种。每创建一个 Future 我们都启动一个线程，之后 sleep，到达超时时间就触发请求超时的处理逻辑。\n这种方式吧，确实简单，在某些场景下也是可以使用的，但弊端也是显而易见的。就像刚才我讲的那个 Future 超时处理的例子，如果我们面临的是高并发的请求，单机每秒发送数万次请求，请求超时时间设置的是 5 秒，那我们要创建多少个线程用来执行超时任务呢？超过 10 万个线程，这个数字真的够吓人了。\n别急，我们还有另一种实现方式。我们可以用一个线程来处理所有的定时任务，还以刚才那个 Future 超时处理的例子为例。假设我们要启动一个线程，这个线程每隔 100 毫秒会扫描一遍所有的处理 Future 超时的任务，当发现一个 Future 超时了，我们就执行这个任务，对这个 Future 执行超时逻辑。\n这种方式我们用得最多，它也解决了第一种方式线程过多的问题，但其实它也有明显的弊端。\n同样是高并发的请求，那么扫描任务的线程每隔 100 毫秒要扫描多少个定时任务呢？如果调用端刚好在 1 秒内发送了 1 万次请求，这 1 万次请求要在 5 秒后才会超时，那么那个扫描的线程在这个 5 秒内就会不停地对这 1 万个任务进行扫描遍历，要额外扫描 40 多次（每 100 毫秒扫描一次，5 秒内要扫描近 50 次），很浪费 CPU。\n在我们使用定时任务时，它所带来的问题，就是让 CPU 做了很多额外的轮询遍历操作，浪费了 CPU，这种现象在定时任务非常多的情况下，尤其明显。\n什么是时钟轮？\n这个问题也不难解决，我们只要找到一种方式，减少额外的扫描操作就行了。比如我的一批定时任务是 5 秒之后执行，我在 4.9 秒之后才开始扫描这批定时任务，这样就大大地节省了 CPU。这时我们就可以利用时钟轮的机制了。\n我们先来看下我们生活中用到的时钟。\n很熟悉了吧，时钟有时针、分针和秒针，秒针跳动一周之后，也就是跳动 60 个刻度之后，分针跳动 1 次，分针跳动 60 个刻度，时针走动一步。\n而时钟轮的实现原理就是参考了生活中的时钟跳动的原理。\n在时钟轮机制中，有时间槽和时钟轮的概念，时间槽就相当于时钟的刻度，而时钟轮就相当于秒针与分针等跳动的一个周期，我们会将每个任务放到对应的时间槽位上。\n时钟轮的运行机制和生活中的时钟也是一样的，每隔固定的单位时间，就会从一个时间槽位跳到下一个时间槽位，这就相当于我们的秒针跳动了一次；时钟轮可以分为多层，下一层时钟轮中每个槽位的单位时间是当前时间轮整个周期的时间，这就相当于 1 分钟等于 60 秒钟；当时钟轮将一个周期的所有槽位都跳动完之后，就会从下一层时钟轮中取出一个槽位的任务，重新分布到当前的时钟轮中，当前时钟轮则从第 0 槽位从新开始跳动，这就相当于下一分钟的第 1 秒。\n为了方便你了解时钟轮的运行机制，我们用一个场景例子来模拟下，一起看下这个场景。\n假设我们的时钟轮有 10 个槽位，而时钟轮一轮的周期是 1 秒，那么我们每个槽位的单位时间就是 100 毫秒，而下一层时间轮的周期就是 10 秒，每个槽位的单位时间也就是 1 秒，并且当前的时钟轮刚初始化完成，也就是第 0 跳，当前在第 0 个槽位。\n好，现在我们有 3 个任务，分别是任务 A（90 毫秒之后执行）、任务 B（610 毫秒之后执行）与任务 C（1 秒 610 毫秒之后执行），我们将这 3 个任务添加到时钟轮中，任务 A 被放到第 0 槽位，任务 B 被放到第 6 槽位，任务 C 被放到下一层时间轮的第 1 槽位，如下面这张图所示。\n当任务 A 刚被放到时钟轮，就被即刻执行了，因为它被放到了第 0 槽位，而当前时间轮正好跳到第 0 槽位（实际上还没开始跳动，状态为第 0 跳）；600 毫秒之后，时间轮已经进行了 6 跳，当前槽位是第 6 槽位，第 6 槽位所有的任务都被取出执行；1 秒钟之后，当前时钟轮的第 9 跳已经跳完，从新开始了第 0 跳，这时下一层时钟轮从第 0 跳跳到了第 1 跳，将第 1 槽位的任务取出，分布到当前的时钟轮中，这时任务 C 从下一层时钟轮中取出并放到当前时钟轮的第 6 槽位；1 秒 600 毫秒之后，任务 C 被执行。\n看完了这个场景，相信你对时钟轮的机制已经有所了解了。在这个例子中，时钟轮的扫描周期仍是 100 毫秒，但是其中的任务并没有被过多的重复扫描，它完美地解决了 CPU 浪费的问题。\n这个机制其实不难理解，但实现起来还是很有难度的，其中要注意的问题也很多。具体的代码实现我们这里不展示，这又是另外一个比较大的话题了。有兴趣的话你可以自行查阅下相关源码，动手实现一下。\n时钟轮在 RPC 中的应用\n通过刚才对时钟轮的讲解，相信你可以看出，它就是用来执行定时任务的，可以说在 RPC 框架中只要涉及到定时相关的操作，我们就可以使用时钟轮。\n那么 RPC 框架在哪些功能实现中会用到它呢？\n刚才我举例讲到的调用端请求超时处理，这里我们就可以应用到时钟轮，我们每发一次请求，都创建一个处理请求超时的定时任务放到时钟轮里，在高并发、高访问量的情况下，时钟轮每次只轮询一个时间槽位中的任务，这样会节省大量的 CPU。\n调用端与服务端启动超时也可以应用到时钟轮，以调用端为例，假设我们想要让应用可以快速地部署，例如 1 分钟内启动，如果超过 1 分钟则启动失败。我们可以在调用端启动时创建一个处理启动超时的定时任务，放到时钟轮里。\n除此之外，你还能想到 RPC 框架在哪些地方可以应用到时钟轮吗？还有定时心跳。RPC 框架调用端定时向服务端发送心跳，来维护连接状态，我们可以将心跳的逻辑封装为一个心跳任务，放到时钟轮里。\n这时你可能会有一个疑问，心跳是要定时重复执行的，而时钟轮中的任务执行一遍就被移除了，对于这种需要重复执行的定时任务我们该如何处理呢？在定时任务的执行逻辑的最后，我们可以重设这个任务的执行时间，把它重新丢回到时钟轮里。\n21 | 流量回放：保障业务技术升级的神器 # 时钟轮在 RPC 中的应用，核心原理就一个关键字“分而治之”，我们可以把它用在任何需要高效处理大量定时任务的场景中，最具有代表性的就是在高并发场景下的请求超时检测。\n回顾完上一讲的重点，我们就进入咱们今天的主题，一起看看流量回放在 RPC 里面的应用。\n如果你经常翻阅一些技术文章的话，可能你会不止一次看到过“流量回放”这个词。我简单地介绍一下，所谓的流量就是某个时间段内的所有请求，我们通过某种手段把发送到 A 应用的所有请求录制下来，然后把这些请求统一转发到 B 应用，让 B 应用接收到的请求参数跟 A 应用保持一致，从而实现 A 接收到的请求在 B 应用里面重新请求了一遍。整个过程我们称之为“流量回放”。\n这就好比今晚有场球赛，但我没空看，但我可以利用视频录播技术把球赛录下来，我随时想看都可以拿出来看，画面是一模一样的。\n那在系统开发的过程中，回放功能可以用来做什么呢？\n流量回放可以做什么？\n我个人感觉，在我们日常开发过程中，可以专心致志地写代码、完成业务功能，是件很幸福的事儿，让我比较头疼的是代码开发完成后的测试环节。\n在团队中，我们经常是多个需求并行开发的，在开发新需求的过程中，我们还可能夹杂着应用的重构和拆分。每到这个时候，我们基本很难做到不改动老逻辑，那只要有改动就有可能会存在考虑不周全的情况。如果你比较严谨的话，那可能在开发完成后，你会把项目里面的 TestCase 都跑一遍，并同时补充新功能的 TestCase，只有所有的 TestCase 都跑通后才能安心。\n在代码里面，算小改动的业务需求，这种做法一般不会出问题。但对于大改动的应用，比如应用中很多基础逻辑都被改动过，这时候如果你还是通过已有的 Case 去验证功能的正确性，就很难保证应用上线后不出故障了，毕竟我们靠自己维护的 Case 相对线上运行的真实环境来说还是少了很多。\n这时候我们会向更专业的 QA 测试人员求助，希望他们能从 QA 角度多加入一些 Case。但因为我们改动代码逻辑影响范围比较大，想要圈定一个比较确定的测试范围又很难，坦白讲这时候相对保险的方式就是 QA 把整个项目都回归测试一遍。这种方式已经是在最大程度上避免上线出问题了，但从概率角度上来讲也不是万无一失的，因为线上不仅环境复杂，而且使用场景也并不好评估，还有就是这种方式耗时也很长。\n这就是我认为最让人头疼的原因，靠传统 QA 测试的方式，不仅过程费时，结果也不是完全可靠。那有没有更可靠、更廉价的方案呢？\n传统 QA 测试出问题的根本原因就是，因为改造后的应用在上线后出现跟应用上线前不一致的行为。而我们测试的目的就是为了保证改造后的应用跟改造前应用的行为一致，我们测试 Case 也都是在尽力模拟应用在线上的运行行为，但仅通过我们自己的枚举方式维护的 Case 并不能代表线上应用的所有行为。因此最好的方式就是用线上流量来验证，但是直接把新应用上线肯定是不行的，因为一旦新改造的应用存在问题就可能会导致线上调用方业务受损。\n我们可以换一种思路，我可以先把线上一段时间内的请求参数和响应结果保存下来，然后把这些请求参数在新改造的应用里重新请求一遍，最后比对一下改造前后的响应结果是否一致，这就间接达到了使用线上流量测试的效果。有了线上的请求参数和响应结果后，我们再结合持续集成过程，就可以让我们改动后的代码随时用线上流量进行验证，这就跟我录制球赛视频一样，只要我想看，我随时都可以拿出来重新看一遍。\nRPC 怎么支持流量回放？\n那在实际工作中，我们该怎么实现流量回放呢？\n我们常见的方案有很多，比如像 TcpCopy、Nginx 等。但在线上环境要使用这些工具的时候，我们还得需要找运维团队帮我们把应用安装到应用实例里面，然后再按照你的需求给配置好才能使用，整个过程繁琐而且总数重复做无用功，那有没有更好的办法呢？尤其是在应用使用了 RPC 的情况下。\n在前面我们不止一次说过，RPC 是用来完成应用之间通信的，换句话就是说应用之间的所有请求响应都会经过 RPC。\n既然所有的请求都会经过 RPC，那么我们在 RPC 里面是不是就可以很方便地拿到每次请求的出入参数？拿到这些出入参数后，我们只要把这些出入参数旁录下来，并把这些旁录结果用异步的方式发送到一个固定的地方保存起来，这样就完成了流量回放里面的录制功能。\n有了真实的请求入参之后，剩下的就是怎么把这些请求参数转发到我们要回归测试的应用里面。在 RPC 中，我们把能够接收请求的应用叫做服务提供方，那就是说我们只需要模拟一个应用调用方，把刚才收到的请求参数重新发送一遍到要回归测试的应用里面，然后比对录制拿到的请求结果和新请求的结果，就可以完成请求回放的效果。整个过程如下图所示：\n相对其它现成的流量回放方案，我们在 RPC 里面内置流量回放功能，使用起来会更加方便，并且我们还可以做更多定制，比如在线启停、方法级别录制等个性化需求。\n22 | 动态分组：超高效实现秒级扩缩容 # 上一讲我们介绍了在 RPC 里面怎么支持流量回放，应用在引入 RPC 后，所有的请求都会被 RPC 接管，而我们在 RPC 里面引入回放的原因也很简单，就是想通过线上流量来验证改造后应用的正确性，而线上流量相比手动维护 TestCase 的场景更丰富，所以用线上流量进行测试的覆盖率会更广。\n回顾完上一讲的重点，我们就切入今天的主题，一起看看动态分组在 RPC 里面的应用。\n在调用方复杂的情况下，如果还是让所有调用方都调用同一个集群的话，很有可能会因为非核心业务的调用量突然增长，而让整个集群变得不可用了，进而让核心业务的调用方受到影响。为了避免这种情况发生，我们需要把整个大集群根据不同的调用方划分出不同的小集群来，从而实现调用方流量隔离的效果，进而保障业务之间不会互相影响。\n通过人为分组的方式确实能帮服务提供方硬隔离调用方的流量，让不同的调用方拥有自己独享的集群，从而保障各个调用方之间互不影响。但这对于我们服务提供方来说，又带来了一个新的问题，就是我们该给调用方分配多大的集群才合适呢？\n怎么划分集群的分组？当然，最理想的情况就是给每个调用方都分配一个独立的分组，但是如果在服务提供方的调用方相对比较多的情况下，对于服务提供方来说要维护这些关系还是比较困难的。因此实际在给集群划分分组的时候，我们一般会选择性地合并一些调用方到同一个分组里。这就需要我们服务提供方考虑该怎么合并，且合并哪些调用方？\n因为这个问题并没有统一的标准，所以我当时给的建议就是我们可以按照应用的重要级别来划分，让非核心业务应用跟核心业务应用不要公用一个分组，核心应用之间也最好别用同一个分组。但这只是一个划分集群分组的建议，并没有具体告诉你该如何划分集群大小。换句话就是，你可以按照这个原则去规划设计自己的集群要分多少个组。\n按照上面的原则，我们把整个集群从逻辑上分为不同的分组之后，接下来我们要做的事情就是给每个分组分配相应的机器数量。那每个分组对应的机器数量，我们该怎么计算呢？我相信这个问题肯定难不倒你。在这儿我先分享下我们团队常用的做法，我们一般会先通过压测去评估下服务提供方单台机器所能承受的 QPS，然后再计算出每个分组里面的所有调用方的调用总量。有了这两个值之后，我们就能很容易地计算出这个分组所需要的机器数。\n通过计算分组内所有调用方 QPS 的方式来算出单个分组内所需的机器数，整体而言还是比较客观准确的。但因为每个调用方的调用量并不是一成不变的，比如商家找个网红做个直播卖货，那就很有可能会导致今天的下单量相对昨天有小幅度的上涨。就是因为这些不确定性因素的存在，所以服务提供方在给调用方做容量评估的时候，通常都会在现有调用量的基础上加一个百分比，而这个百分比多半来自历史经验总结。\n总之，就是在我们算每个分组所需要的机器数的时候，需要额外给每个分组增加一些机器，从而让每个小集群有一定的抗压能力，而这个抗压能力取决于给这个集群预留的机器数量。作为服务提供方来说，肯定希望给每个集群预留的机器数越多越好，但现实情况又不允许预留太多，因为这样会增加团队的整体成本。\n分组带来的问题\n通过给分组预留少量机器的方式，以增加单个集群的抗压能力。一般情况下，这种机制能够运行得很好，但在应对大的突发流量时，就会显得有点捉襟见肘了。因为机器成本的原因，我们给每个分组预留的机器数量都不会太多，所以当突发流量超过预留机器的能力的时候，就会让这个分组的集群处于一个危险状态了。\n这时候我们唯一能做的就是给这个分组去扩容新的机器，但临时扩容新机器通常需要一个比较长的时间，而且花的时间越长，业务受影响的范围就越大。\n那有没有更便捷一点的方案呢？前面我们说过，我们在给分组做容量评估的时候，通常都会增加了一些富余。换句话就是，除了当前出问题的分组，其它分组的服务提供方在保障自己调用方质量的同时，还是可以额外承担一些流量的。我们可以想办法快速利用这部分已有的能力。\n但因为我们实现了流量隔离功能，整个集群被我们划分成了不同的分组，所以当前出问题的调用方并不能把请求发送到其它分组的机器上。那可能你会说，既然临时去申请机器进行扩容时间长，那我能不能把上面说的那些富余的机器直接拿过来，把部署在机器上的应用改成出问题的分组，然后进行重启啊？这样出问题的那个分组的服务提供方机器数就会变多了。\n从结果上来看，这样处理确实能够解决问题，但有一个问题就是这样处理的时间还是相对较长的，而且当这个分组的流量恢复后，你还得把临时借过来的机器还回原来的分组。\n问题分析到这儿，我想说，动态分组就可以派上用场了。\n动态分组的应用\n上面的问题，其根本原因就是某个分组的调用方流量突增，而这个分组所预留的空间也不能满足当前流量的需求，但是其它分组的服务提供方有足够的富余能力。但这些富余的能力，又被我们的分组进行了强制的隔离，我们又不能抛弃分组功能，否则老问题就要循环起来了。\n那这样的话，我们就只能在出问题的时候临时去借用其它分组的部分能力，但通过改分组进行重启应用的方式，不仅操作过程慢，事后还得恢复。因此这种生硬的方式显然并不是很合适。\n想一下啊，我们改应用分组然后进行重启的目的，就是让出问题的服务调用方能通过服务发现找到更多的服务提供方机器，而服务发现的数据来自注册中心，那我们是不是可以通过修改注册中心的数据来解决呢？\n我们只要把注册中心里面的部分实例的别名改成我们想要的别名，然后通过服务发现进而影响到不同调用方能够调用的服务提供方实例集合。\n举个例子，服务提供方有 3 个服务实例，其中 A 分组有 2 个实例，B 分组有 1 个实例，调用方 1 调用 A 分组，调用方 2 调用 B 分组。我们把 A 分组里面的一个实例分组在注册中心由 A 分组改为 B 分组，经过服务发现影响后，整个调用拓扑就变成了这样：\n通过直接修改注册中心数据，我们可以让任何一个分组瞬间拥有不同规模的集群能力。我们不仅可以实现把某个实例的分组名改成另外一个分组名，还可以让某个实例分组名变成多个分组名，这就是我们在动态分组里面最常见的两种动作——追加和替换。\n23 | 如何在没有接口的情况下进行RPC调用？ # 应用场景有哪些？\n在 RPC 运营的过程中，让调用端在没有接口 API 的情况下发起 RPC 调用的需求，不只是一个业务方和我提过，这里我列举两个非常典型的场景例子。\n场景一：我们要搭建一个统一的测试平台，可以让各个业务方在测试平台中通过输入接口、分组名、方法名以及参数值，在线测试自己发布的 RPC 服务。这时我们就有一个问题要解决，我们搭建统一的测试平台实际上是作为各个 RPC 服务的调用端，而在 RPC 框架的使用中，调用端是需要依赖服务提供方提供的接口 API 的，而统一测试平台不可能依赖所有服务提供方的接口 API。我们不能因为每有一个新的服务发布，就去修改平台的代码以及重新上线。这时我们就需要让调用端在没有服务提供方提供接口的情况下，仍然可以正常地发起 RPC 调用。\n场景二：我们要搭建一个轻量级的服务网关，可以让各个业务方用 HTTP 的方式，通过服务网关调用其它服务。这时就有与场景一相同的问题，服务网关要作为所有 RPC 服务的调用端，是不能依赖所有服务提供方的接口 API 的，也需要调用端在没有服务提供方提供接口的情况下，仍然可以正常地发起 RPC 调用。\n这两个场景都是我们经常会碰到的，而让调用端在没有服务提供方提供接口 API 的情况下仍然可以发起 RPC 调用的功能，在 RPC 框架中也是非常有价值的。\n怎么做？\nRPC 框架要实现这个功能，我们可以使用泛化调用。那什么是泛化调用呢？我们带着这个问题，先学习下如何在没有接口的情况下进行 RPC 调用。\n在 RPC 调用的过程中，调用端向服务端发起请求，首先要通过动态代理，动态代理可以帮助我们屏蔽 RPC 处理流程，真正地让我们发起远程调用就像调用本地一样。\n那么在 RPC 调用的过程中，既然调用端是通过动态代理向服务端发起远程调用的，那么在调用端的程序中就一定要依赖服务提供方提供的接口 API，因为调用端是通过这个接口 API 自动生成动态代理的。那如果没有接口 API 呢？我们该如何让调用端仍然能够发起 RPC 调用呢？\n所谓的 RPC 调用，本质上就是调用端向服务端发送一条请求消息，服务端接收并处理，之后向调用端发送一条响应消息，调用端处理完响应消息之后，一次 RPC 调用就完成了。那是不是说我们只要能够让调用端在没有服务提供方提供接口的情况下，仍然能够向服务端发送正确的请求消息，就能够解决这个问题了呢？\n没错，只要调用端将服务端需要知道的信息，如接口名、业务分组名、方法名以及参数信息等封装成请求消息发送给服务端，服务端就能够解析并处理这条请求消息，这样问题就解决了。过程如下图所示：\n现在我们已经清楚了解决问题的关键，但 RPC 的调用端向服务端发送消息是需要以动态代理作为入口的，我们现在得继续想办法让调用端发送我刚才讲过的那条请求消息。\n我们可以定义一个统一的接口（GenericService），调用端在创建 GenericService 代理时指定真正需要调用的接口的接口名以及分组名，而 GenericService 接口的 $invoke 方法的入参就是方法名以及参数信息。\n这样我们传递给服务端所需要的所有信息，包括接口名、业务分组名、方法名以及参数信息等都可以通过调用 GenericService 代理的 $invoke 方法来传递。具体的接口定义如下：\nclass GenericService { Object $invoke(String methodName, String[] paramTypes, Object[] params); } 这个通过统一的 GenericService 接口类生成的动态代理，来实现在没有接口的情况下进行 RPC 调用的功能，我们就称之为泛化调用。\n通过泛化调用功能，我们可以解决在没有服务提供方提供接口 API 的情况下进行 RPC 调用，那么这个功能是否就完美了呢？\nRPC 框架可以通过异步的方式提升吞吐量，还有如何实现全异步的 RPC 框架，其关键点就是 RPC 框架对 CompletableFuture 的支持，那么我们的泛化调用是否也可以支持异步呢？\n当然可以。我们可以给 GenericService 接口再添加一个异步方法 $asyncInvoke，方法的返回值就是 CompletableFuture，GenericService 接口的具体定义如下：\nclass GenericService { Object $invoke(String methodName, String[] paramTypes, Object[] params); CompletableFuture\u0026lt;Object\u0026gt; $asyncInvoke(String methodName, String[] paramTypes, Object[] params); } 相信你已经对泛化调用的功能有一定的了解了，那你有没有想过这样一个问题？在没有服务提供方提供接口 API 的情况下，我们可以用泛化调用的方式实现 RPC 调用，但是如果没有服务提供方提供接口 API，我们就没法得到入参以及返回值的 Class 类，也就不能对入参对象进行正常的序列化。这时我们会面临两个问题：\n问题 1：调用端不能对入参对象进行正常的序列化，那调用端、服务端在接收到请求消息后，入参对象又该如何序列化与反序列化呢？\n回顾下如何设计可扩展的 RPC 框架，我们通过插件体系来提高 RPC 框架的可扩展性，在 RPC 框架的整体架构中就包括了序列化插件，我们可以为泛化调用提供专属的序列化插件，通过这个插件，解决泛化调用中的序列化与反序列化问题。\n问题 2：调用端的入参对象（params）与返回值应该是什么类型呢？\n在服务提供方提供的接口 API 中，被调用的方法的入参类型是一个对象，那么使用泛化调用功能的调用端，可以使用 Map 类型的对象，之后通过泛化调用专属的序列化方式对这个 Map 对象进行序列化，服务端收到消息后，再通过泛化调用专属的序列化方式将其反序列成对象。\n24 | 如何在线上环境里兼容多种RPC协议？ # 如何在没有接口的情况下完成 RPC 调用，其关键在于你要理解接口定义在 RPC 里面的作用。除了我们前面说的，动态代理生成的过程中需要用到接口定义，剩余的其它过程中接口的定义只是被当作元数据来使用，而动态代理在 RPC 中并不是一个必须的环节，所以在没有接口定义的情况下我们同样也是可以完成 RPC 调用的。\n回顾完上一讲的重点，咱们就言归正传，切入今天的主题，一起看看如何在线上环境里兼容多种 RPC 协议。\n看到这个问题后，可能你的第一反应就是，在真实环境中为什么会存在多个协议呢？我们说过，RPC 是能够帮助我们屏蔽网络编程细节，实现调用远程方法就跟调用本地一样的体验。大白话说就是，RPC 是能够帮助我们在开发过程中完成应用之间的通信，而又不需要我们关心具体通信细节的工具。\n为什么要支持多协议？\n既然应用之间的通信都是通过 RPC 来完成的，而能够完成 RPC 通信的工具有很多，比如像 Web Service、Hessian、gRPC 等都可以用来充当 RPC 使用。这些不同的 RPC 框架都是随着互联网技术的发展而慢慢涌现出来的，而这些 RPC 框架可能在不同时期会被我们引入到不同的项目中解决当时应用之间的通信问题，这样就导致我们线上的生成环境中存在各种各样的 RPC 框架。\n很显然，这种混乱使用 RPC 框架的方式肯定不利于公司技术栈的管理，最明显的一个特点就是我们维护 RPC 框架的成本越来越高，因为每种 RPC 框架都需要有专人去负责升级维护。\n为了解决早期遗留的一些技术负债，我们通常会去选择更高级的、更好用的工具来解决，治理 RPC 框架混乱的问题也是一样。为了解决同时维护多个 RPC 框架的困难，我们肯定希望能够用统一用一种 RPC 框架来替代线上所有的 RPC 框架，这样不仅能降低我们的维护成本，而且还可以让我们在一种 RPC 上面去精进。\n既然目标明确后，我们该如何实施呢？\n可能你会说这很简单啊，我们只要把所有的应用都改造成新 RPC 的使用方式，然后同时上线所有改造后的应用就可以了。如果在团队比较小的情况下，这种断崖式的更新可能确实是最快的方法，但如果是在团队比较大的情况下，要想做到同时上线所有改造后的应用，暂且不讨论这种方式是否存在风险，光从多个团队同一时间上线所有应用来看，这也几乎是一件不可能做到的事儿\n那对于多人团队来说，有什么办法可以让其把多个 RPC 框架统一到一个工具上呢？我们先看下多人团队在升级过程中所要面临的困难，人数多就意味着要维护的应用会比较多，应用多了之后线上应用之间的调用关系就会相对比较复杂。那这时候如果单纯地把任意一个应用目前使用的 RPC 框架换成新的 RPC 框架的话，就需要让所有调用这个应用的调用方去改成新的调用方式。\n通过这种自下而上的滚动升级方式，最终是可以让所有的应用都切换到统一的 RPC 框架上，但是这种升级方式存在一定的局限性，首先要求我们能够清楚地梳理出各个应用之间的调用关系，只有这样，我们才能按部就班地把所有应用都升级到新的 RPC 框架上；其次要求应用之间的关系不能存在互相调用的情况，最好的情况就是应用之间的调用关系像一颗树，有一定的层次关系。但实际上我们应用的调用关系可能已经变成了网状结构，这时候想再按照这种方式去推进升级的话，就可能寸步难行了。\n为了解决上面升级过程中遇到的问题，你可能还会想到另外一个方案，那就是在应用升级的过程中，先不移除原有的 RPC 框架，但同时接入新的 RPC 框架，让两种 RPC 同时提供服务，然后等所有的应用都接入完新的 RPC 以后，再让所有的应用逐步接入到新的 RPC 上。这样既解决了上面存在的问题，同时也可以让所有的应用都能无序地升级到统一的 RPC 框架上。\n在保持原有 RPC 使用方式不变的情况下，同时引入新的 RPC 框架的思路，是可以让所有的应用最终都能升级到我们想要升级的 RPC 上，但对于开发人员来说，这样切换成本还是有点儿高，整个过程最少需要两次上线才能彻底地把应用里面的旧 RPC 都切换成新 RPC。\n那有没有更好的方式可以让应用上线一次就可以完成新老 RPC 的切换呢？关键就在于要让新的 RPC 能同时支持多种 RPC 调用，当一个调用方切换到新的 RPC 之后，调用方和服务提供方之间就可以用新的协议完成调用；当调用方还是用老的 RPC 进行调用的话，调用方和服务提供方之间就继续沿用老的协议完成调用。对于服务提供方来说，所要处理的请求关系如下图所示：\n怎么优雅处理多协议？\n要让新的 RPC 同时支持多种 RPC 调用，关键就在于要让新的 RPC 能够原地支持多种协议的请求。怎么才能做到？协议的作用就是用于分割二进制数据流。每种协议约定的数据包格式是不一样的，而且每种协议开头都有一个协议编码，我们一般叫做 magic number。\n当 RPC 收到了数据包后，我们可以先解析出 magic number 来。获取到 magic number 后，我们就很容易地找到对应协议的数据格式，然后用对应协议的数据格式去解析收到的二进制数据包。\n协议解析过程就是把一连串的二进制数据变成一个 RPC 内部对象，但这个对象一般是跟协议相关的，所以为了能让 RPC 内部处理起来更加方便，我们一般都会把这个协议相关的对象转成一个跟协议无关的 RPC 对象。这是因为在 RPC 流程中，当服务提供方收到反序列化后的请求的时候，我们需要根据当前请求的参数找到对应接口的实现类去完成真正的方法调用。如果这个请求参数是跟协议相关的话，那后续 RPC 的整个处理逻辑就会变得很复杂。\n当完成了真正的方法调用以后，RPC 返回的也是一个跟协议无关的通用对象，所以在真正往调用方写回数据的时候，我们同样需要完成一个对象转换的逻辑，只不过这时候是把通用对象转成协议相关的对象。\n在收发数据包的时候，我们通过两次转换实现 RPC 内部的处理逻辑跟协议无关，同时保证调用方收到的数据格式跟调用请求过来的数据格式是一样的。整个流程如下图所示：\n","date":"17 May 2024","permalink":"/posts/architecture/distributed/rpc/rpc-%E5%AE%9E%E6%88%98%E4%B8%8E%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86/%E9%AB%98%E7%BA%A7%E7%AF%87/","section":"博客","summary":"RPC 实战与核心原理 - 高级篇","title":"RPC 实战与核心原理 - 高级篇"},{"content":"07 | 架构设计：设计一个灵活的RPC框架 # RPC 架构\n说起架构设计，我相信你一定不陌生。我理解的架构设计呢，就是从顶层角度出发，厘清各模块组件之间数据交互的流程，让我们对系统有一个整体的宏观认识。我们先看看 RPC 里面都有哪些功能模块。\n我们讲过，RPC 本质上就是一个远程调用，那肯定就需要通过网络来传输数据。虽然传输协议可以有多种选择，但考虑到可靠性的话，我们一般默认采用 TCP 协议。为了屏蔽网络传输的复杂性，我们需要封装一个单独的数据传输模块用来收发二进制数据，这个单独模块我们可以叫做传输模块。\n用户请求的时候是基于方法调用，方法出入参数都是对象数据，对象是肯定没法直接在网络中传输的，我们需要提前把它转成可传输的二进制，这就是我们说的序列化过程。但只是把方法调用参数的二进制数据传输到服务提供方是不够的，我们需要在方法调用参数的二进制数据后面增加“断句”符号来分隔出不同的请求，在两个“断句”符号中间放的内容就是我们请求的二进制数据，这个过程我们叫做协议封装。\n虽然这是两个不同的过程，但其目的都是一样的，都是为了保证数据在网络中可以正确传输。这里我说的正确，可不仅指数据能够传输，还需要保证传输后能正确还原出传输前的语义。所以我们可以把这两个处理过程放在架构中的同一个模块，统称为协议模块。\n除此之外，我们还可以在协议模块中加入压缩功能，这是因为压缩过程也是对传输的二进制数据进行操作。在实际的网络传输过程中，我们的请求数据包在数据链路层可能会因为太大而被拆分成多个数据包进行传输，为了减少被拆分的次数，从而导致整个传输过程时间太长的问题，我们可以在 RPC 调用的时候这样操作：在方法调用参数或者返回值的二进制数据大于某个阈值的情况下，我们可以通过压缩框架进行无损压缩，然后在另外一端也用同样的压缩算法进行解压，保证数据可还原。\n传输和协议这两个模块是 RPC 里面最基础的功能，它们使对象可以正确地传输到服务提供方。但距离 RPC 的目标——实现像调用本地一样地调用远程，还缺少点东西。因为这两个模块所提供的都是一些基础能力，要让这两个模块同时工作的话，我们需要手写一些黏合的代码，但这些代码对我们使用 RPC 的研发人员来说是没有意义的，而且属于一个重复的工作，会导致使用过程的体验非常不友好。\n这就需要我们在 RPC 里面把这些细节对研发人员进行屏蔽，让他们感觉不到本地调用和远程调用的区别。假设有用到 Spring 的话，我们希望 RPC 能让我们把一个 RPC 接口定义成一个 Spring Bean，并且这个 Bean 也会统一被 Spring Bean Factory 管理，可以在项目中通过 Spring 依赖注入到方式引用。这是 RPC 调用的入口，我们一般叫做 Bootstrap 模块。\n学到这儿，一个点对点（Point to Point）版本的 RPC 框架就完成了。我一般称这种模式的 RPC 框架为单机版本，因为它没有集群能力。所谓集群能力，就是针对同一个接口有着多个服务提供者，但这多个服务提供者对于我们的调用方来说是透明的，所以在 RPC 里面我们还需要给调用方找到所有的服务提供方，并需要在 RPC 里面维护好接口跟服务提供者地址的关系，这样调用方在发起请求的时候才能快速地找到对应的接收地址，这就是我们常说的“服务发现”。\n但服务发现只是解决了接口和服务提供方地址映射关系的查找问题，这更多是一种“静态数据”。说它是静态数据是因为，对于我们的 RPC 来说，我们每次发送请求的时候都是需要用 TCP 连接的，相对服务提供方 IP 地址，TCP 连接状态是瞬息万变的，所以我们的 RPC 框架里面要有连接管理器去维护 TCP 连接的状态。\n有了集群之后，提供方可能就需要管理好这些服务了，那我们的 RPC 就需要内置一些服务治理的功能，比如服务提供方权重的设置、调用授权等一些常规治理手段。而服务调用方需要额外做哪些事情呢？每次调用前，我们都需要根据服务提供方设置的规则，从集群中选择可用的连接用于发送请求。\n那到这儿，一个比较完善的 RPC 框架基本就完成了，功能也差不多就是这些了。按照分层设计的原则，我将这些功能模块分为了四层，具体内容见图示：\n可扩展的架构\n那 RPC 架构设计出来就完事了吗？当然不，技术迭代谁都躲不过。\n不知道你有没有这样的经历，你设计的一个系统它看上去很完善，也能很好地运行，然后你成功地把它交付给了业务方。有一天业务方有了新的需求，要加入很多新的功能，这时候你就会发现当前架构面临的可就是大挑战了，要修改很多地方才能实现。\n举个例子，假如你设计了一个商品发布系统，早些年我们只能在网上购买电脑、衣服等实物商品，但现在发展成可以在网上购买电话充值卡、游戏点卡等虚拟商品，实物商品的发布流程是需要选择购买区域的，但虚拟商品并没有这一限制。如果你想要在一套发布系统里面同时完成实物和虚拟商品发布的话，你就只能在代码里面加入很多的 if else 判断逻辑，这样是能行，可整个代码就臃肿、杂乱了，后期也极难维护。\n其实，我们设计 RPC 框架也是一样的，我们不可能在开始时就面面俱到。那有没有更好的方式来解决这些问题呢？这就是我们接下来要讲的插件化架构。\n在 RPC 框架里面，我们是怎么支持插件化架构的呢？我们可以将每个功能点抽象成一个接口，将这个接口作为插件的契约，然后把这个功能的接口与功能的实现分离，并提供接口的默认实现。在 Java 里面，JDK 有自带的**SPI（Service Provider Interface）**服务发现机制，它可以动态地为某个接口寻找服务实现。使用 SPI 机制需要在 Classpath 下的 META-INF/services 目录里创建一个以服务接口命名的文件，这个文件里的内容就是这个接口的具体实现类。\n但在实际项目中，我们其实很少使用到 JDK 自带的 SPI 机制，首先它不能按需加载，ServiceLoader 加载某个接口实现类的时候，会遍历全部获取，也就是接口的实现类得全部载入并实例化一遍，会造成不必要的浪费。另外就是扩展如果依赖其它的扩展，那就做不到自动注入和装配，这就很难和其他框架集成，比如扩展里面依赖了一个 Spring Bean，原生的 Java SPI 就不支持。\n加上了插件功能之后，我们的 RPC 框架就包含了两大核心体系——核心功能体系与插件体系，如下图所示：\n这时，整个架构就变成了一个微内核架构，我们将每个功能点抽象成一个接口，将这个接口作为插件的契约，然后把这个功能的接口与功能的实现分离并提供接口的默认实现。这样的架构相比之前的架构，有很多优势。首先它的可扩展性很好，实现了开闭原则，用户可以非常方便地通过插件扩展实现自己的功能，而且不需要修改核心功能的本身；其次就是保持了核心包的精简，依赖外部包少，这样可以有效减少开发人员引入 RPC 导致的包版本冲突问题。\n08 | 服务发现：到底是要CP还是AP？ # 为什么需要服务发现？\n先举个例子，假如你要给一位以前从未合作过的同事发邮件请求帮助，但你却没有他的邮箱地址。这个时候你会怎么办呢？如果是我，我会选择去看公司的企业“通信录”。\n同理，为了高可用，在生产环境中服务提供方都是以集群的方式对外提供服务，集群里面的这些 IP 随时可能变化，我们也需要用一本“通信录”及时获取到对应的服务节点，这个获取的过程我们一般叫作“服务发现”。\n对于服务调用方和服务提供方来说，其契约就是接口，相当于“通信录”中的姓名，服务节点就是提供该契约的一个具体实例。服务 IP 集合作为“通信录”中的地址，从而可以通过接口获取服务 IP 的集合来完成服务的发现。这就是我要说的 RPC 框架的服务发现机制，如下图所示：\n服务注册：在服务提供方启动的时候，将对外暴露的接口注册到注册中心之中，注册中心将这个服务节点的 IP 和接口保存下来。 服务订阅：在服务调用方启动的时候，去注册中心查找并订阅服务提供方的 IP，然后缓存到本地，并用于后续的远程调用。 为什么不使用 DNS？\n既然服务发现这么“厉害”，那是不是很难实现啊？其实类似机制一直在我们身边，我们回想下服务发现的本质，就是完成了接口跟服务提供者 IP 的映射。那我们能不能把服务提供者 IP 统一换成一个域名啊，利用已经成熟的 DNS 机制来实现？\n好，先带着这个问题，简单地看下 DNS 的流程\n如果我们用 DNS 来实现服务发现，所有的服务提供者节点都配置在了同一个域名下，调用方的确可以通过 DNS 拿到随机的一个服务提供者的 IP，并与之建立长连接，这看上去并没有太大问题，但在我们业界为什么很少用到这种方案呢？不知道你想过这个问题没有，如果没有，现在可以停下来想想这样两个问题：\n如果这个 IP 端口下线了，服务调用者能否及时摘除服务节点呢？ 如果在之前已经上线了一部分服务节点，这时我突然对这个服务进行扩容，那么新上线的服务节点能否及时接收到流量呢？ 这两个问题的答案都是：“不能”。这是因为为了提升性能和减少 DNS 服务的压力，DNS 采取了多级缓存机制，一般配置的缓存时间较长，特别是 JVM 的默认缓存是永久有效的，所以说服务调用者不能及时感知到服务节点的变化。\n这时你可能会想，我是不是可以加一个负载均衡设备呢？将域名绑定到这台负载均衡设备上，通过 DNS 拿到负载均衡的 IP。这样服务调用的时候，服务调用方就可以直接跟 VIP 建立连接，然后由 VIP 机器完成 TCP 转发，如下图所示：\n这个方案确实能解决 DNS 遇到的一些问题，但在 RPC 场景里面也并不是很合适，原因有以下几点：\n搭建负载均衡设备或 TCP/IP 四层代理，需求额外成本； 请求流量都经过负载均衡设备，多经过一次网络传输，会额外浪费些性能； 负载均衡添加节点和摘除节点，一般都要手动添加，当大批量扩容和下线时，会有大量的人工操作和生效延迟； 我们在服务治理的时候，需要更灵活的负载均衡策略，目前的负载均衡设备的算法还满足不了灵活的需求。 由此可见，DNS 或者 VIP 方案虽然可以充当服务发现的角色，但在 RPC 场景里面直接用还是很难的。\n基于 ZooKeeper 的服务发现\n那么在 RPC 里面我们该如何实现呢？我们还是要回到服务发现的本质，就是完成接口跟服务提供者 IP 之间的映射。这个映射是不是就是一种命名服务？当然，我们还希望注册中心能完成实时变更推送，是不是像开源的 ZooKeeper、etcd 就可以实现？我很肯定地说“确实可以”。下面我就来介绍下一种基于 ZooKeeper 的服务发现方式。\n整体的思路很简单，就是搭建一个 ZooKeeper 集群作为注册中心集群，服务注册的时候只需要服务节点向 ZooKeeper 节点写入注册信息即可，利用 ZooKeeper 的 Watcher 机制完成服务订阅与服务下发功能，整体流程如下图：\n服务平台管理端先在 ZooKeeper 中创建一个服务根路径，可以根据接口名命名（例如：/service/com.demo.xxService），在这个路径再创建服务提供方目录与服务调用方目录（例如：provider、consumer），分别用来存储服务提供方的节点信息和服务调用方的节点信息。 当服务提供方发起注册时，会在服务提供方目录中创建一个临时节点，节点中存储该服务提供方的注册信息。 当服务调用方发起订阅时，则在服务调用方目录中创建一个临时节点，节点中存储该服务调用方的信息，同时服务调用方 watch 该服务的服务提供方目录（/service/com.demo.xxService/provider）中所有的服务节点数据。 当服务提供方目录下有节点数据发生变更时，ZooKeeper 就会通知给发起订阅的服务调用方。 我所在的技术团队早期使用的 RPC 框架服务发现就是基于 ZooKeeper 实现的，并且还平稳运行了一年多，但后续团队的微服务化程度越来越高之后，ZooKeeper 集群整体压力也越来越高，尤其在集中上线的时候越发明显。“集中爆发”是在一次大规模上线的时候，当时有超大批量的服务节点在同时发起注册操作，ZooKeeper 集群的 CPU 突然飙升，导致 ZooKeeper 集群不能工作了，而且我们当时也无法立马将 ZooKeeper 集群重新启动，一直到 ZooKeeper 集群恢复后业务才能继续上线。\nzookeeper作为注册中心时，当服务节点数量达到一定规模时，会出现性能问题，主要是由于其保证强一致性。\n经过我们的排查，引发这次问题的根本原因就是 ZooKeeper 本身的性能问题，当连接到 ZooKeeper 的节点数量特别多，对 ZooKeeper 读写特别频繁，且 ZooKeeper 存储的目录达到一定数量的时候，ZooKeeper 将不再稳定，CPU 持续升高，最终宕机。而宕机之后，由于各业务的节点还在持续发送读写请求，刚一启动，ZooKeeper 就因无法承受瞬间的读写压力，马上宕机。\n这次“意外”让我们意识到，ZooKeeper 集群性能显然已经无法支撑我们现有规模的服务集群了，我们需要重新考虑服务发现方案。\n基于消息总线的最终一致性的注册中心\n我们知道，ZooKeeper 的一大特点就是强一致性，ZooKeeper 集群的每个节点的数据每次发生更新操作，都会通知其它 ZooKeeper 节点同时执行更新。它要求保证每个节点的数据能够实时的完全一致，这也就直接导致了 ZooKeeper 集群性能上的下降。这就好比几个人在玩传递东西的游戏，必须这一轮每个人都拿到东西之后，所有的人才能开始下一轮，而不是说我只要获得到东西之后，就可以直接进行下一轮了。\n而 RPC 框架的服务发现，在服务节点刚上线时，服务调用方是可以容忍在一段时间之后（比如几秒钟之后）发现这个新上线的节点的。毕竟服务节点刚上线之后的几秒内，甚至更长的一段时间内没有接收到请求流量，对整个服务集群是没有什么影响的，所以我们可以牺牲掉 CP（强制一致性），而选择 AP（最终一致），来换取整个注册中心集群的性能和稳定性。\n那么是否有一种简单、高效，并且最终一致的更新机制，能代替 ZooKeeper 那种数据强一致的数据更新机制呢？\n因为要求最终一致性，我们可以考虑采用消息总线机制。注册数据可以全量缓存在每个注册中心内存中，通过消息总线来同步数据。当有一个注册中心节点接收到服务节点注册时，会产生一个消息推送给消息总线，再通过消息总线通知给其它注册中心节点更新数据并进行服务下发，从而达到注册中心间数据最终一致性，具体流程如下图所示：\n当有服务上线，注册中心节点收到注册请求，服务列表数据发生变化，会生成一个消息，推送给消息总线，每个消息都有整体递增的版本。 消息总线会主动推送消息到各个注册中心，同时注册中心也会定时拉取消息。对于获取到消息的在消息回放模块里面回放，只接受大于本地版本号的消息，小于本地版本号的消息直接丢弃，从而实现最终一致性。 消费者订阅可以从注册中心内存拿到指定接口的全部服务实例，并缓存到消费者的内存里面。 采用推拉模式，消费者可以及时地拿到服务实例增量变化情况，并和内存中的缓存数据进行合并。 为了性能，这里采用了两级缓存，注册中心和消费者的内存缓存，通过异步推拉模式来确保最终一致性。\n另外，你也可能会想到，服务调用方拿到的服务节点不是最新的，所以目标节点存在已经下线或不提供指定接口服务的情况，这个时候有没有问题？这个问题我们放到了 RPC 框架里面去处理，在服务调用方发送请求到目标节点后，目标节点会进行合法性验证，如果指定接口服务不存在或正在下线，则会拒绝该请求。服务调用方收到拒绝异常后，会安全重试到其它节点。\n通过消息总线的方式，我们就可以完成注册中心集群间数据变更的通知，保证数据的最终一致性，并能及时地触发注册中心的服务下发操作。在 RPC 领域精耕细作后，你会发现，服务发现的特性是允许我们在设计超大规模集群服务发现系统的时候，舍弃强一致性，更多地考虑系统的健壮性。最终一致性才是分布式系统设计中更为常用的策略。\n思考题：目前服务提供者上线后会自动注册到注册中心，服务调用方会自动感知到新增的实例，并且流量会很快打到该新增的实例。如果我想把某些服务提供者实例的流量切走，除了下线实例，你有没有想到其它更便捷的办法呢？\n解决这个问题的方法还是有很多的，比如留言中提到的改变服务提供者实例的权重，将权重调整为 0，或者通过路由的方式也可以。\n但解决这个问题最便捷的方式还是使用动态分组，通过业务分组来实现流量隔离。如果业务分组是动态的，我们就可以在管理平台动态地自由调整，那是不是就可以实现动态地流量切换了呢？\n09 | 健康检测：这个节点都挂了，为啥还要疯狂发请求？ # 服务发现的作用就是实时感知集群 IP 的变化，实现接口跟服务集群节点 IP 的映射。在超大规模集群实战中，我们更多需要考虑的是保证最终一致性。其实总结来说，就一关键词，你要记住“推拉结合，以拉为准”。接着昨天的内容，我们再来聊聊 RPC 中的健康检测。\n因为有了集群，所以每次发请求前，RPC 框架会根据路由和负载均衡算法选择一个具体的 IP 地址。为了保证请求成功，我们就需要确保每次选择出来的 IP 对应的连接是健康的，这个逻辑你应该理解。\n但你也知道，调用方跟服务集群节点之间的网络状况是瞬息万变的，两者之间可能会出现闪断或者网络设备损坏等情况，那怎么保证选择出来的连接一定是可用的呢？\n从我的角度看，终极的解决方案是让调用方实时感知到节点的状态变化，这样他们才能做出正确的选择。这个道理像我们开车一样，车有各种各样的零件，我们不可能在开车之前先去挨个检查下他们的健康情况，转而是应该有一套反馈机制，比如今天我的大灯坏了，那中控台就可以给我提示；明天我的胎压不够了，中控台也能够收到提示。汽车中大部分关键零件的状态变化，我作为调用方，都能够第一时间了解。\n那回到 RPC 框架里，我们应该怎么设计这套机制呢？你可以先停下来想想汽车的例子，看看他们是怎么做的。当然，回到我们 RPC 的框架里，这事用专业一点的词来说就是服务的健康检测。今天我们就来详细聊聊这个话题。\n遇到的问题\n在进一步讲解服务健康检测之前，我想先和你分享一个我曾经遇到过的线上问题。\n有一天，我们公司某个业务研发团队的负责人急匆匆跑过来，让我帮他解决个问题。仔细听完他的描述后，我才明白，原来是他们发现线上业务的某个接口可用性并不高，基本上十次调用里总会有几次失败。\n查看了具体的监控数据之后，我们发现只有请求具体打到某台机器的时候才会有这个问题，也就是说，集群中有某台机器出了问题。于是快刀斩乱麻，我建议他们先把这台“问题机器”下线，以快速解决目前的问题。\n但对于我来说，问题并没有结束，我开始进一步琢磨：“接口调用某台机器的时候已经出现不能及时响应了，那为什么 RPC 框架还会继续把请求发到这台有问题的机器上呢？RPC 框架还会把请求发到这台机器上，也就是说从调用方的角度看，它没有觉得这台服务器有问题。”\n就像警察破案一样，为了进一步了解事情的真相，我查看了问题时间点的监控和日志，在案发现场发现了这样几个线索：\n通过日志发现请求确实会一直打到这台有问题的机器上，因为我看到日志里有很多超时的异常信息。 从监控上看，这台机器还是有一些成功的请求，这说明当时调用方跟服务之间的网络连接没有断开。因为如果连接断开之后，RPC 框架会把这个节点标识为“不健康”，不会被选出来用于发业务请求。 深入进去看异常日志，我发现调用方到目标机器的定时心跳会有间歇性失败。 从目标机器的监控上可以看到该机器的网络指标有异常，出问题时间点 TCP 重传数比正常高 10 倍以上。 有了对这四个线索的分析，我基本上可以得出这样的结论：那台问题服务器在某些时间段出现了网络故障，但也还能处理部分请求。换句话说，它处于半死不活的状态。但是（是转折，也是关键点），它还没彻底“死”，还有心跳，这样，调用方就觉得它还正常，所以就没有把它及时挪出健康状态列表。\n到这里，你应该也明白了，一开始，我们为了快速解决问题，手动把那台问题机器下线了。刨根问底之后，我们发现，其实更大的问题是我们的服务检测机制有问题，有的服务本来都已经病危了，但我们还以为人家只是个感冒。\n接下来，我们就来看看服务检测的核心逻辑。\n健康检测的逻辑\n刚刚我们提到了心跳机制，我估计你会想，搞什么心跳，是不是我们把问题复杂化了。当服务方下线，正常情况下我们肯定会收到连接断开的通知事件，在这个事件里面直接加处理逻辑不就可以了？是的，我们前面汽车的例子里检测都是这样做的。但咱们这里不行，因为应用健康状况不仅包括 TCP 连接状况，还包括应用本身是否存活，很多情况下 TCP 连接没有断开，但应用可能已经“僵死了”。\n所以，业内常用的检测方法就是用心跳机制。心跳机制说起来也不复杂，其实就是服务调用方每隔一段时间就问一下服务提供方，“兄弟，你还好吧？”，然后服务提供方很诚实地告诉调用方它目前的状态。\n结合前面的文章，你也不难想出来，服务方的状态一般会有三种情况，一个是我很好，一个是我生病了，一个是没回复。用专业的词来对应这三个状态就是：\n健康状态：建立连接成功，并且心跳探活也一直成功； 亚健康状态：建立连接成功，但是心跳请求连续失败； 死亡状态：建立连接失败。 节点的状态并不是固定不变的，它会根据心跳或者重连的结果来动态变化，具体状态间转换图如下：\n这里你可以关注下几个状态之间的转换箭头，我再给你解释下。首先，一开始初始化的时候，如果建立连接成功，那就是健康状态，否则就是死亡状态。这里没有亚健康这样的中间态。紧接着，如果健康状态的节点连续出现几次不能响应心跳请求的情况，那就会被标记为亚健康状态，也就是说，服务调用方会觉得它生病了。\n生病之后（亚健康状态），如果连续几次都能正常响应心跳请求，那就可以转回健康状态，证明病好了。如果病一直好不了，那就会被断定为是死亡节点，死亡之后还需要善后，比如关闭连接。\n当然，死亡并不是真正死亡，它还有复活的机会。如果某个时间点里，死亡的节点能够重连成功，那它就可以重新被标记为健康状态。\n这就是整个节点的状态转换思路，你不用死记，它很简单，除了不能复活，其他都和我们人的状态一样。当服务调用方通过心跳机制了解了节点的状态之后，每次发请求的时候，就可以优先从健康列表里面选择一个节点。当然，如果健康列表为空，为了提高可用性，也可以尝试从亚健康列表里面选择一个，这就是具体的策略了。\n具体的解决方案\n理解了服务健康检测的逻辑，我们再回到开头我描述的场景里，看看怎么优化。现在你理解了，一个节点从健康状态过渡到亚健康状态的前提是“连续”心跳失败次数必须到达某一个阈值，比如 3 次（具体看你怎么配置了）。\n而我们的场景里，节点的心跳日志只是间歇性失败，也就是时好时坏，这样，失败次数根本没到阈值，调用方会觉得它只是“生病”了，并且很快就好了。那怎么解决呢？我还是建议你先停下来想想。\n你是不是会脱口而出，说改下配置，调低阈值呗。是的，这是最快的解决方法，但是我想说，它治标不治本。第一，像前面说的那样，调用方跟服务节点之间网络状况瞬息万变，出现网络波动的时候会导致误判。第二，在负载高情况，服务端来不及处理心跳请求，由于心跳时间很短，会导致调用方很快触发连续心跳失败而造成断开连接。\n我们回到问题的本源，核心是服务节点网络有问题，心跳间歇性失败。我们现在判断节点状态只有一个维度，那就是心跳检测，那是不是可以再加上业务请求的维度呢？\n起码我当时是顺着这个方向解决问题的。但紧接着，我又发现了新的麻烦：\n调用方每个接口的调用频次不一样，有的接口可能 1 秒内调用上百次，有的接口可能半个小时才会调用一次，所以我们不能把简单的把总失败的次数当作判断条件。 服务的接口响应时间也是不一样的，有的接口可能 1ms，有的接口可能是 10s，所以我们也不能把 TPS 至来当作判断条件。 和同事讨论之后，我们找到了可用率这个突破口，应该相对完美了。可用率的计算方式是某一个时间窗口内接口调用成功次数的百分比（成功次数 / 总调用次数）。当可用率低于某个比例就认为这个节点存在问题，把它挪到亚健康列表，这样既考虑了高低频的调用接口，也兼顾了接口响应时间不同的问题。\n10 | 路由策略：怎么让请求按照设定的规则发到不同的节点上？ # 为什么选择路由策略？\n在前面我们提到过，在真实环境中我们的服务提供方是以一个集群的方式提供服务，这对于服务调用方来说，就是一个接口会有多个服务提供方同时提供服务，所以我们的 RPC 在每次发起请求的时候，都需要从多个服务提供方节点里面选择一个用于发请求的节点。\n既然这些节点都可以用来完成这次请求，那么我们就可以简单地认为这些节点是同质的。这里的同质怎么理解呢？就是这次请求无论发送到集合中的哪个节点上，返回的结果都是一样的。\n既然服务提供方是以集群的方式对外提供服务，那就要考虑一些实际问题。要知道我们每次上线应用的时候都不止一台服务器会运行实例，那上线就涉及到变更，只要变更就可能导致原本正常运行的程序出现异常，尤其是发生重大变动的时候，导致我们应用不稳定的因素就变得很多。\n为了减少这种风险，我们一般会选择灰度发布我们的应用实例，比如我们可以先发布少量实例观察是否有异常，后续再根据观察的情况，选择发布更多实例还是回滚已经上线的实例。\n但这种方式不好的一点就是，线上一旦出现问题，影响范围还是挺大的。因为对于我们的服务提供方来说，我们的服务会同时提供给很多调用方来调用，尤其是像一些基础服务的调用方会更复杂，比如商品、价格等等，一旦刚上线的实例有问题了，那将会导致所有的调用方业务都会受损。\n那对于我们的 RPC 框架来说，有什么的办法可以减少上线变更导致的风险吗？这就不得不提路由在 RPC 中的应用。具体好在哪里，怎么实现，我们接着往下看。\n如何实现路由策略？\n可能你会说，我们可以在上线前把所有的场景都重新测试一遍啊？这也是一种方法，而且测试肯定是上线前的一个重要环节。但以我个人的经验来看，由于线上环境太复杂了，单纯从测试角度出发只能降低风险出现的概率，想要彻底验证所有场景基本是不可能的。\n那如果没法 100% 规避风险，我们还能怎么办？我认为只有一条路可以尝试了，就是尽量减小上线出问题导致业务受损的范围。基于这个思路，我们是不是可以在上线完成后，先让一小部分调用方请求过来进行逻辑验证，待没问题后再接入其他调用方，从而实现流量隔离的效果。那在 RPC 框架里面我们具体该怎么实现呢？\n我们在服务发现那讲讲过，在 RPC 里面服务调用方是通过服务发现的方式拿到了所有服务提供方的 IP 地址，那我们是不是就可以利用这个特点？当我们选择要灰度验证功能的时候，是不是就可以让注册中心在推送的时候区别对待，而不是一股脑的把服务提供方的 IP 地址推送到所有调用方。换句话说就是，注册中心只会把刚上线的服务 IP 地址推送到选择指定的调用方，而其他调用方是不能通过服务发现拿到这个 IP 地址的。\n使用路由策略进行灰度发布（实际中因为注册中心主要是存储配置，所以并不会将路由策略的功能放在注册中心上）\n通过服务发现的方式来隔离调用方请求，从逻辑上来看确实可行，但注册中心在 RPC 里面的定位是用来存储数据并保证数据一致性的。如果把这种复杂的计算逻辑放到注册中心里面，当集群节点变多之后，就会导致注册中心压力很大，而且大部分情况下我们一般都是采用开源软件来搭建注册中心，要满足这种需求还需要进行二次开发。所以从实际的角度出发，通过影响服务发现来实现请求隔离并不划算。上面就是不在注册中心里面添加灰度逻辑的原因。\n我们可以重新回到调用方发起 RPC 调用的流程。在 RPC 发起真实请求的时候，有一个步骤就是从服务提供方节点集合里面选择一个合适的节点（就是我们常说的负载均衡），那我们是不是可以在选择节点前加上“筛选逻辑”，把符合我们要求的节点筛选出来。那这个筛选的规则是什么呢？就是我们前面说的灰度过程中要验证的规则。\n举个具体例子你可能就明白了，比如我们要求新上线的节点只允许某个 IP 可以调用，那我们的注册中心会把这条规则下发到服务调用方。在调用方收到规则后，在选择具体要发请求的节点前，会先通过筛选规则过滤节点集合，按照这个例子的逻辑，最后会过滤出一个节点，这个节点就是我们刚才新上线的节点。通过这样的改造，RPC 调用流程就变成了这样：\n这个筛选过程在我们的 RPC 里面有一个专业名词，就是“路由策略”，而上面例子里面的路由策略是我们常见的 IP 路由策略，用于限制可以调用服务提供方的 IP。使用了 IP 路由策略后，整个集群的调用拓扑如下图所示：\n参数路由\n有了 IP 路由之后，上线过程中我们就可以做到只让部分调用方请求调用到新上线的实例，相对传统的灰度发布功能来说，这样做我们可以把试错成本降到最低。\n但在有些场景下，我们可能还需要更细粒度的路由方式。比如，在升级改造应用的时候，为了保证调用方能平滑地切调用我们的新应用逻辑，在升级过程中我们常用的方式是让新老应用并行运行一段时间，然后通过切流量百分比的方式，慢慢增大新应用承接的流量，直到新应用承担了 100% 且运行一段时间后才能去下线老应用。\n在流量切换的过程中，为了保证整个流程的完整性，我们必须保证某个主题对象的所有请求都使用同一种应用来承接。假设我们改造的是商品应用，那主题对象肯定是商品 ID，在切流量的过程中，我们必须保证某个商品的所有操作都是用新应用（或者老应用）来完成所有请求的响应。\n很显然，上面的 IP 路由并不能满足我们这个需求，因为 IP 路由只是限制调用方来源，并不会根据请求参数请求到我们预设的服务提供方节点上去。\n那我们怎么利用路由策略实现这个需求呢？其实你只要明白路由策略的本质，就不难明白这种参数路由的实现。\n我们可以给所有的服务提供方节点都打上标签，用来区分新老应用节点。在服务调用方发生请求的时候，我们可以很容易地拿到请求参数，也就是我们例子中的商品 ID，我们可以根据注册中心下发的规则来判断当前商品 ID 的请求是过滤掉新应用还是老应用的节点。因为规则对所有的调用方都是一样的，从而保证对应同一个商品 ID 的请求要么是新应用的节点，要么是老应用的节点。使用了参数路由策略后，整个集群的调用拓扑如下图所示：\n相比 IP 路由，参数路由支持的灰度粒度更小，他为服务提供方应用提供了另外一个服务治理的手段。灰度发布功能是 RPC 路由功能的一个典型应用场景，通过 RPC 路由策略的组合使用可以让服务提供方更加灵活地管理、调用自己的流量，进一步降低上线可能导致的风险。\n11 | 负载均衡：节点负载差距这么大，为什么收到的流量还一样？ # 一个需求\n在进入主题之前，我想先和你分享一个需求，这是我们公司的业务部门给我们提的。他们反馈的问题是这样的：有一次碰上流量高峰，他们突然发现线上服务的可用率降低了，经过排查发现，是因为其中有几台机器比较旧了。当时最早申请的一批容器配置比较低，缩容的时候留下了几台，当流量达到高峰时，这几台容器由于负载太高，就扛不住压力了。业务问我们有没有好的服务治理策略？\n这个问题其实挺好解决的，我们当时给出的方案是：在治理平台上调低这几台机器的权重，这样的话，访问的流量自然就减少了。\n但业务接着反馈了，说：当他们发现服务可用率降低的时候，业务请求已经受到影响了，这时再如此解决，需要时间啊，那这段时间里业务可能已经有损失了。紧接着他们就提出了需求，问：RPC 框架有没有什么智能负载的机制？能否及时地自动控制服务节点接收到的访问量？\n这个需求其实很合理，这也是一个比较普遍的问题。确实，虽说我们的服务治理平台能够动态地控制线上服务节点接收的访问量，但当业务方发现部分机器负载过高或者响应变慢的时候再去调整节点权重，真的很可能已经影响到线上服务的可用率了。\n看到这儿，你有没有想到什么好的处理方案呢？接下来，我们就以这个问题为背景，一起看看 RPC 框架的负载均衡。\n什么是负载均衡？\n我先来简单地介绍下负载均衡。当我们的一个服务节点无法支撑现有的访问量时，我们会部署多个节点，组成一个集群，然后通过负载均衡，将请求分发给这个集群下的每个服务节点，从而达到多个服务节点共同分担请求压力的目的。\n负载均衡主要分为软负载和硬负载，软负载就是在一台或多台服务器上安装负载均衡的软件，如 LVS、Nginx 等，硬负载就是通过硬件设备来实现的负载均衡，如 F5 服务器等。负载均衡的算法主要有随机法、轮询法、最小连接法等。\n我刚才介绍的负载均衡主要还是应用在 Web 服务上，Web 服务的域名绑定负载均衡的地址，通过负载均衡将用户的请求分发到一个个后端服务上。\nRPC 框架中的负载均衡\n那 RPC 框架中的负载均衡是不是也是如此呢？和我上面讲的负载均衡，你觉得会有区别吗？\n之前讲过为什么不通过 DNS 来实现“服务发现”，之后我又讲解了为什么不采用添加负载均衡设备或者 TCP/IP 四层代理，域名绑定负载均衡设备的 IP 或者四层代理 IP 的方式。\n我的回答是这种方式会面临这样几个问题：\n搭建负载均衡设备或 TCP/IP 四层代理，需要额外成本； 请求流量都经过负载均衡设备，多经过一次网络传输，会额外浪费一些性能； 负载均衡添加节点和摘除节点，一般都要手动添加，当大批量扩容和下线时，会有大量的人工操作，“服务发现”在操作上是个问题； 我们在服务治理的时候，针对不同接口服务、服务的不同分组，我们的负载均衡策略是需要可配的，如果大家都经过这一个负载均衡设备，就不容易根据不同的场景来配置不同的负载均衡策略了。 我相信看到这儿，你应该已经知道了 RPC 实现的负载均衡所采用的策略与传统的 Web 服务实现负载均衡所采用策略的不同之处了。\nRPC 的负载均衡完全由 RPC 框架自身实现，RPC 的服务调用者会与“注册中心”下发的所有服务节点建立长连接，在每次发起 RPC 调用时，服务调用者都会通过配置的负载均衡插件，自主选择一个服务节点，发起 RPC 调用请求。\nRPC 负载均衡策略一般包括随机权重、Hash、轮询。当然，这还是主要看 RPC 框架自身的实现。其中的随机权重策略应该是我们最常用的一种了，通过随机算法，我们基本可以保证每个节点接收到的请求流量是均匀的；同时我们还可以通过控制节点权重的方式，来进行流量控制。比如我们默认每个节点的权重都是 100，但当我们把其中的一个节点的权重设置成 50 时，它接收到的流量就是其他节点的 1/2。\n这几种负载均衡算法的实现还是很简单的，网上资料也非常多，在这我就不过多介绍了。有什么问题，咱们可以在留言区交流。\n由于负载均衡机制完全是由 RPC 框架自身实现的，所以它不再需要依赖任何负载均衡设备，自然也不会发生负载均衡设备的单点问题，服务调用方的负载均衡策略也完全可配，同时我们可以通过控制权重的方式，对负载均衡进行治理。\n了解完 RPC 框架的负载均衡，现在我们就可以回到这讲最开头业务提的那个需求：有没有什么办法可以动态地、智能地控制线上服务节点所接收到的请求流量？\n现在答案是不是就显而易见了，解决问题的关键就在于 RPC 框架的负载均衡上。对于这个问题，我们当时的方案就是，设计一种自适应的负载均衡策略。\n如何设计自适应的负载均衡？\n我刚才讲过，RPC 的负载均衡完全由 RPC 框架自身实现，服务调用者发起请求时，会通过配置的负载均衡插件，自主地选择服务节点。那是不是只要调用者知道每个服务节点处理请求的能力，再根据服务处理节点处理请求的能力来判断要打给它多少流量就可以了？当一个服务节点负载过高或响应过慢时，就少给它发送请求，反之则多给它发送请求。\n这就有点像日常工作中的分配任务，要多考虑实际情况。当一位下属身体欠佳，就少给他些工作；若刚好另一位下属状态很好，手头工作又不是很多，就多分给他一点。\n那服务调用者节点又该如何判定一个服务节点的处理能力呢？\n这里我们可以采用一种打分的策略，服务调用者收集与之建立长连接的每个服务节点的指标数据，如服务节点的负载指标、CPU 核数、内存大小、请求处理的耗时指标（如请求平均耗时、TP99、TP999）、服务节点的状态指标（如正常、亚健康）。通过这些指标，计算出一个分数，比如总分 10 分，如果 CPU 负载达到 70%，就减它 3 分，当然了，减 3 分只是个类比，需要减多少分是需要一个计算策略的。\n我们又该如果根据这些指标来打分呢？\n这就有点像公司对员工进行年终考核。假设我是老板，我要考核专业能力、沟通能力和工作态度，这三项的占比分别是 30%、30%、40%，我给一个员工的评分是 10、8、8，那他的综合分数就是这样计算的：$10\\times30%+8\\times30%+8\\times40%=8.6$ 分。\n给服务节点打分也一样，我们可以为每个指标都设置一个指标权重占比，然后再根据这些指标数据，计算分数。\n服务调用者给每个服务节点都打完分之后，会发送请求，那这时候我们又该如何根据分数去控制给每个服务节点发送多少流量呢？\n我们可以配合随机权重的负载均衡策略去控制，通过最终的指标分数修改服务节点最终的权重。例如给一个服务节点综合打分是 8 分（满分 10 分），服务节点的权重是 100，那么计算后最终权重就是 80（100*80%）。服务调用者发送请求时，会通过随机权重的策略来选择服务节点，那么这个节点接收到的流量就是其他正常节点的 80%（这里假设其他节点默认权重都是 100，且指标正常，打分为 10 分的情况）。\n到这儿，一个自适应的负载均衡我们就完成了，整体的设计方案如下图所示：\n关键步骤我来解释下：\n添加服务指标收集器，并将其作为插件，默认有运行时状态指标收集器、请求耗时指标收集器。 运行时状态指标收集器收集服务节点 CPU 核数、CPU 负载以及内存等指标，在服务调用者与服务提供者的心跳数据中获取。 请求耗时指标收集器收集请求耗时数据，如平均耗时、TP99、TP999 等。 可以配置开启哪些指标收集器，并设置这些参考指标的指标权重，再根据指标数据和指标权重来综合打分。 通过服务节点的综合打分与节点的权重，最终计算出节点的最终权重，之后服务调用者会根据随机权重的策略，来选择服务节点。 12 | 异常重试：在约定时间内安全可靠地重试 # 为什么需要异常重试？\n我们可以考虑这样一个场景。我们发起一次 RPC 调用，去调用远程的一个服务，比如用户的登录操作，我们会先对用户的用户名以及密码进行验证，验证成功之后会获取用户的基本信息。当我们通过远程的用户服务来获取用户基本信息的时候，恰好网络出现了问题，比如网络突然抖了一下，导致我们的请求失败了，而这个请求我们希望它能够尽可能地执行成功，那这时我们要怎么做呢？\n我们需要重新发起一次 RPC 调用，那我们在代码中该如何处理呢？是在代码逻辑里 catch 一下，失败了就再发起一次调用吗？这样做显然不够优雅吧。这时我们就可以考虑使用 RPC 框架的重试机制。\nRPC 框架的重试机制\n那什么是 RPC 框架的重试机制呢？\n这其实很好理解，就是当调用端发起的请求失败时，RPC 框架自身可以进行重试，再重新发送请求，用户可以自行设置是否开启重试以及重试的次数。\n那这个机制是如何实现的呢？\n调用端在发起 RPC 调用时，会经过负载均衡，选择一个节点，之后它会向这个节点发送请求信息。当消息发送失败或收到异常消息时，我们就可以捕获异常，根据异常触发重试，重新通过负载均衡选择一个节点发送请求消息，并且记录请求的重试次数，当重试次数达到用户配置的重试次数的时候，就返回给调用端动态代理一个失败异常，否则就一直重试下去。\nRPC 框架的重试机制就是调用端发现请求失败时捕获异常，之后触发重试，那是不是所有的异常都要触发重试呢？\n当然不是了，因为这个异常可能是服务提供方抛回来的业务异常，它是应该正常返回给动态代理的，所以我们要在触发重试之前对捕获的异常进行判定，只有符合重试条件的异常才能触发重试，比如网络超时异常、网络连接异常等等。\n了解了 RPC 框架的重试机制，那用户在使用异常重试时需要注意哪些问题呢？\n比如我刚才提的那个调用场景，当网络突然抖动了一下导致请求超时了，但这个时候调用方的请求信息可能已经发送到服务提供方的节点上，也可能已经发送到服务提供方的服务节点上，那如果请求信息成功地发送到了服务节点上，那这个节点是不是就要执行业务逻辑了呢？是的。\n那如果这个时候发起了重试，业务逻辑是否会被执行呢？会的。\n那如果这个服务业务逻辑不是幂等的，比如插入数据操作，那触发重试的话会不会引发问题呢？会的。\n综上，我们可以总结出：在使用 RPC 框架的时候，我们要确保被调用的服务的业务逻辑是幂等的，这样我们才能考虑根据事件情况开启 RPC 框架的异常重试功能。这一点你要格外注意，这算是一个高频误区了。\n通过上述讲解，我相信你已经非常清楚 RPC 框架的重试机制了，这也是现在大多数 RPC 框架所采用的重试机制。\n那看到这儿，你觉得这个机制完善了吗？有没有想到连续重试对请求超时时间的影响？继续考虑这样一个场景：我把调用端的请求超时时间设置为 5s，结果连续重试 3 次，每次都耗时 2s，那最终这个请求的耗时是 6s，那这样的话，调用端设置的超时时间是不是就不准确了呢？\n如何在约定时间内安全可靠地重试？\n我刚才讲到，连续的异常重试可能会出现一种不可靠的情况，那就是连续的异常重试并且每次处理的请求时间比较长，最终会导致请求处理的时间过长，超出用户设置的超时时间。\n解决这个问题最直接的方式就是，在每次重试后都重置一下请求的超时时间。\n当调用端发起 RPC 请求时，如果发送请求发生异常并触发了异常重试，我们可以先判定下这个请求是否已经超时，如果已经超时了就直接返回超时异常，否则就先重置下这个请求的超时时间，之后再发起重试。\n那么解决了因多次异常重试引发的超时时间失效的问题，这个重试机制是不是就完全可靠了呢？\n我们接着考虑，当调用端设置了异常重试策略，发起了一次 RPC 调用，通过负载均衡选择了节点，将请求消息发送到这个节点，这时这个节点由于负载压力较大，导致这个请求处理失败了，调用端触发了重试，再次通过负载均衡选择了一个节点，结果恰好仍选择了这个节点，那么在这种情况下，重试的效果是否受影响了呢？\n当然有影响。因此，我们需要在所有发起重试、负载均衡选择节点的时候，去掉重试之前出现过问题的那个节点，以保证重试的成功率。\n那我们现在再完整地回顾一下，考虑了业务逻辑必须是幂等的、超时时间需要重置以及去掉有问题的服务节点后，这样的异常重试机制，还有没有可优化的地方呢？\n我刚才讲过，RPC 框架的异常重试机制，是调用端发送请求之后，如果发送失败会捕获异常，触发重试，但并不是所有的异常都会触发重试的，只有 RPC 框架中特定的异常才会如此，比如连接异常、超时异常。\n而像服务端业务逻辑中抛回给调用端的异常是不能重试的。那么请你想一下这种情况：服务端的业务逻辑抛给调用端一个异常信息，而服务端抛出这个异常是允许调用端重新发起一次调用的。\n比如这个场景：服务端的业务逻辑是对数据库某个数据的更新操作，更新失败则抛出个更新失败的异常，调用端可以再次调用，来触发服务端重新执行更新操作。那这个时候对于调用端来说，它接收到了更新失败异常，虽然是服务端抛回来的业务异常，但也是可以进行重试的。\n那么在这种情况下，RPC 框架的重试机制需要怎么优化呢？\nRPC 框架是不会知道哪些业务异常能够去进行异常重试的，我们可以加个重试异常的白名单，用户可以将允许重试的异常加入到这个白名单中。当调用端发起调用，并且配置了异常重试策略，捕获到异常之后，我们就可以采用这样的异常处理策略。如果这个异常是 RPC 框架允许重试的异常，或者这个异常类型存在于可重试异常的白名单中，我们就允许对这个请求进行重试。\nRPC 框架是不会知道哪些业务异常能够去进行异常重试的，我们可以加个重试异常的白名单，用户可以将允许重试的异常加入到这个白名单中。当调用端发起调用，并且配置了异常重试策略，捕获到异常之后，我们就可以采用这样的异常处理策略。如果这个异常是 RPC 框架允许重试的异常，或者这个异常类型存在于可重试异常的白名单中，我们就允许对这个请求进行重试。\n所有可能出现的问题，我们排查了一圈下来之后，一个可靠的重试机制就诞生了，如下图所示：\n思考题：在整个 RPC 调用的流程中，异常重试发生在哪个环节？\n在回答这个问题之前，我们先回想下这一讲中讲过的内容。我在讲 RPC 为什么需要异常重试时我说过，如果在发出请求时恰好网络出现问题了，导致我们的请求失败，我们可能需要进行异常重试。从这一点我们可以看出，异常重试的操作是要在调用端进行的。因为如果在调用端发出请求时恰好网络出现问题导致请求失败，那么这个请求很可能还没到达服务端，服务端当然就没办法去处理重试了。\n另外，我还讲过，我们需要在所有发起重试、负载均衡选择节点的时候，去掉重试之前出现过问题的那个节点，以保证重试的成功率。由此可见异常重试的操作应该发生在负载均衡之前，在发起重试的时候，会调用负载均衡插件来选择一个服务节点，在调用负载均衡插件时我们要告诉负载均衡需要刨除哪些有问题的服务节点。\n在整个 RPC 调用的过程中，从动态代理到负载均衡之间还有一系列的操作，如果你研究过开源的 RPC 框架，你会发现在调用端发送请求消息之前还会经过过滤链，对请求消息进行层层的过滤处理，之后才会通过负载均衡选择服务节点，发送请求消息，而异常重试操作就发生在过滤链处理之后，调用负载均衡选择服务节点之前，这样的重试是可以减少很多重复操作的。\n13 | 优雅关闭：如何避免服务停机带来的业务损失？ # 关闭为什么有问题？\n我们知道，在“单体应用”复杂到一定程度后，我们一般会进行系统拆分，也就是时下流行的微服务架构。服务拆分之后，自然就需要协同，于是 RPC 框架就出来了，它用来解决各个子系统之间的通信问题。\n我再倒回来问你一个非常基础的问题？你觉得系统为啥非要拆分呢？从我的角度，如果只说一个原因，我觉得拆分之后我们可以更方便、更快速地迭代业务。那么问题来了，更快速地迭代业务，说人话不就是我会经常更新应用系统，时不时还老要重启服务器吗？\n那具体到我们的 RPC 体系里，你就要考虑，在重启服务的过程中，RPC 怎么做到让调用方系统不出问题呢？\n要想说明白这事，我们先要简述下上线的大概流程：当服务提供方要上线的时候，一般是通过部署系统完成实例重启。在这个过程中，服务提供方的团队并不会事先告诉调用方他们需要操作哪些机器，从而让调用方去事先切走流量。而对调用方来说，它也无法预测到服务提供方要对哪些机器重启上线，因此负载均衡就有可能把要正在重启的机器选出来，这样就会导致把请求发送到正在重启中的机器里面，从而导致调用方不能拿到正确的响应结果。\n在服务重启的时候，对于调用方来说，这时候可能会存在以下几种情况：\n调用方发请求前，目标服务已经下线。对于调用方来说，跟目标节点的连接会断开，这时候调用方可以立马感知到，并且在其健康列表里面会把这个节点挪掉，自然也就不会被负载均衡选中。 调用方发请求的时候，目标服务正在关闭，但调用方并不知道它正在关闭，而且两者之间的连接也没断开，所以这个节点还会存在健康列表里面，因此该节点就有一定概率会被负载均衡选中。 关闭流程\n当然还存在目标服务正在启动的情况，如何优雅地启动我会在下一讲详细地讲，这也是重点。今天我们要聚焦讨论的就是当出现第二种情况的时候，在 RPC 里面怎么避免调用方业务受损。\n这时候你可能会想到，我是不是在重启服务机器前，先通过“某种方式”把要下线的机器从调用方维护的“健康列表”里面删除就可以了，这样负载均衡就选不到这个节点了？你说得一点都没错，但这个具体的“某种方式”是怎么完成呢？\n最没有效率的办法就是人工通知调用方，让他们手动摘除要下线的机器，这种方式很原始也很直接。但这样对于提供方上线的过程来说太繁琐了，每次上线都要通知到所有调用我接口的团队，整个过程既浪费时间又没有意义，显然不能被正常接受。\n这时候，可能你还会想到，RPC 里面不是有服务发现吗？它的作用不就是用来“实时”感知服务提供方的状态吗？当服务提供方关闭前，是不是可以先通知注册中心进行下线，然后通过注册中心告诉调用方进行节点摘除？关闭流程如下图所示：\n这样不就可以实现不通过“人肉”的方式，从而达到一种自动化方式，但这么做就能完全保证实现无损上下线吗？\n如上图所示，整个关闭过程中依赖了两次 RPC 调用，一次是服务提供方通知注册中心下线操作，一次是注册中心通知服务调用方下线节点操作。注册中心通知服务调用方都是异步的，我们在“服务发现”一讲中讲过在大规模集群里面，服务发现只保证最终一致性，并不保证实时性，所以注册中心在收到服务提供方下线的时候，并不能成功保证把这次要下线的节点推送到所有的调用方。所以这么来看，通过服务发现并不能做到应用无损关闭。\n不能强依赖“服务发现”来通知调用方要下线的机器，那服务提供方自己来通知行不行？因为在 RPC 里面调用方跟服务提供方之间是长连接，我们可以在提供方应用内存里面维护一份调用方连接集合，当服务要关闭的时候，挨个去通知调用方去下线这台机器。这样整个调用链路就变短了，对于每个调用方来说就一次 RPC，可以确保调用的成功率很高。大部分场景下，这么做确实没有问题，我们之前也是这么实现的，但是我们发现线上还是会偶尔会出现，因为服务提供方上线而导致调用失败的问题。\n那到底哪里出问题了呢？我后面分析了调用方请求日志跟收到关闭通知的日志，并且发现了一个线索如下：出问题请求的时间点跟收到服务提供方关闭通知的时间点很接近，只比关闭通知的时间早不到 1ms，如果再加上网络传输时间的话，那服务提供方收到请求的时候，它应该正在处理关闭逻辑。这就说明服务提供方关闭的时候，并没有正确处理关闭后接收到的新请求。\n优雅关闭\n知道了根本原因，问题就很好解决了。因为服务提供方已经开始进入关闭流程，那么很多对象就可能已经被销毁了，关闭后再收到的请求按照正常业务请求来处理，肯定是没法保证能处理的。所以我们可以在关闭的时候，设置一个请求“挡板”，挡板的作用就是告诉调用方，我已经开始进入关闭流程了，我不能再处理你这个请求了。\n如果大家经常去银行办理业务，就会很熟悉这个流程。在交接班或者有其他要事情处理的时候，银行柜台工作人员会拿出一个纸板，放在窗口前，上面写到“该窗口已关闭”。在该窗口排队的人虽然有一万个不愿意，也只能换到其它窗口办理业务，因为柜台工作人员会把当前正在办理的业务处理完后正式关闭窗口。\n基于这个思路，我们可以这么处理：当服务提供方正在关闭，如果这之后还收到了新的业务请求，服务提供方直接返回一个特定的异常给调用方（比如 ShutdownException）。这个异常就是告诉调用方“我已经收到这个请求了，但是我正在关闭，并没有处理这个请求”，然后调用方收到这个异常响应后，RPC 框架把这个节点从健康列表挪出，并把请求自动重试到其他节点，因为这个请求是没有被服务提供方处理过，所以可以安全地重试到其他节点，这样就可以实现对业务无损。\n但如果只是靠等待被动调用，就会让这个关闭过程整体有点漫长。因为有的调用方那个时刻没有业务请求，就不能及时地通知调用方了，所以我们可以加上主动通知流程，这样既可以保证实时性，也可以避免通知失败的情况。\n说到这里，我知道你肯定会问，那要怎么捕获到关闭事件呢？\n在我的经验里，可以通过捕获操作系统的进程信号来获取，在 Java 语言里面，对应的是 Runtime.addShutdownHook 方法，可以注册关闭的钩子。在 RPC 启动的时候，我们提前注册关闭钩子，并在里面添加了两个处理程序，一个负责开启关闭标识，一个负责安全关闭服务对象，服务对象在关闭的时候会通知调用方下线节点。同时需要在我们调用链里面加上挡板处理器，当新的请求来的时候，会判断关闭标识，如果正在关闭，则抛出特定异常。\n看到这里，感觉问题已经比较好地被解决了。但细心的同学可能还会提出问题，关闭过程中已经在处理的请求会不会受到影响呢？\n如果进程结束过快会造成这些请求还没有来得及应答，同时调用方会也会抛出异常。为了尽可能地完成正在处理的请求，首先我们要把这些请求识别出来。这就好比日常生活中，我们经常看见停车场指示牌上提示还有多少剩余车位，这个是如何做到的呢？如果仔细观察一下，你就会发现它是每进入一辆车，剩余车位就减一，每出来一辆车，剩余车位就加一。我们也可以利用这个原理在服务对象加上引用计数器，每开始处理请求之前加一，完成请求处理减一，通过该计数器我们就可以快速判断是否有正在处理的请求。\n服务对象在关闭过程中，会拒绝新的请求，同时根据引用计数器等待正在处理的请求全部结束之后才会真正关闭。但考虑到有些业务请求可能处理时间长，或者存在被挂住的情况，为了避免一直等待造成应用无法正常退出，我们可以在整个 ShutdownHook 里面，加上超时时间控制，当超过了指定时间没有结束，则强制退出应用。超时时间我建议可以设定成 10s，基本可以确保请求都处理完了。整个流程如下图所示。\n14 | 优雅启动：如何避免流量打到没有启动完成的节点？ # 接着上一讲的内容，今天我们来聊聊优雅启动。\n是不是很诧异？应用启动居然也要这么“讲究”吗？这就好比我们日常生活中的热车，行驶之前让发动机空跑一会，可以让汽车的各个部件都“热”起来，减小磨损。\n换到应用上来看，原理也是一样的。运行了一段时间后的应用，执行速度会比刚启动的应用更快。这是因为在 Java 里面，在运行过程中，JVM 虚拟机会把高频的代码编译成机器码，被加载过的类也会被缓存到 JVM 缓存中，再次使用的时候不会触发临时加载，这样就使得“热点”代码的执行不用每次都通过解释，从而提升执行速度。\n但是这些“临时数据”，都在我们应用重启后就消失了。重启后的这些“红利”没有了之后，如果让我们刚启动的应用就承担像停机前一样的流量，这会使应用在启动之初就处于高负载状态，从而导致调用方过来的请求可能出现大面积超时，进而对线上业务产生损害行为。\n在上一讲我们说过，在微服务架构里面，上线肯定是频繁发生的，那我们总不能因为上线，就让过来的请求出现大面积超时吧？所以我们得想点办法。既然问题的关键是在于“刚重启的服务提供方因为没有预跑就承担了大流量”，那我们是不是可以通过某些方法，让应用一开始只接少许流量呢？这样低功率运行一段时间后，再逐渐提升至最佳状态。\n这其实就是我今天要和你分享的重点，RPC 里面的一个实用功能——启动预热。\n启动预热\n那什么叫启动预热呢？\n简单来说，就是让刚启动的服务提供方应用不承担全部的流量，而是让它被调用的次数随着时间的移动慢慢增加，最终让流量缓和地增加到跟已经运行一段时间后的水平一样。\n那在 RPC 里面，我们该怎么实现这个功能呢？\n我们现在是要控制调用方发送到服务提供方的流量。我们可以先简单地回顾下调用方发起的 RPC 调用流程是怎样的，调用方应用通过服务发现能够获取到服务提供方的 IP 地址，然后每次发送请求前，都需要通过负载均衡算法从连接池中选择一个可用连接。那这样的话，我们是不是就可以让负载均衡在选择连接的时候，区分一下是否是刚启动不久的应用？对于刚启动的应用，我们可以让它被选择到的概率特别低，但这个概率会随着时间的推移慢慢变大，从而实现一个动态增加流量的过程。\n现在方案有了，我们就可以考虑具体实现了。\n首先对于调用方来说，我们要知道服务提供方启动的时间，这个怎么获取呢？我这里给出两种方法，一种是服务提供方在启动的时候，把自己启动的时间告诉注册中心；另外一种就是注册中心收到的服务提供方的请求注册时间。这两个时间我认为都可以，不过可能你会犹豫我们该怎么确保所有机器的日期时间是一样的？这其实不用太关心，因为整个预热过程的时间是一个粗略值，即使机器之间的日期时间存在 1 分钟的误差也不影响，并且在真实环境中机器都会默认开启 NTP 时间同步功能，来保证所有机器时间的一致性。\n不管你是选择哪个时间，最终的结果就是，调用方通过服务发现，除了可以拿到 IP 列表，还可以拿到对应的启动时间。我们需要把这个时间作用在负载均衡上，上面介绍过一种基于权重的负载均衡，但是这个权重是由服务提供方设置的，属于一个固定状态。现在我们要让这个权重变成动态的，并且是随着时间的推移慢慢增加到服务提供方设定的固定值，整个过程如下图所示：\n通过这个小逻辑的改动，我们就可以保证当服务提供方运行时长小于预热时间时，对服务提供方进行降权，减少被负载均衡选择的概率，避免让应用在启动之初就处于高负载状态，从而实现服务提供方在启动后有一个预热的过程。\n看到这儿，你可能还会有另外一个疑问，就是当我在大批量重启服务提供方的时候，会不会导致没有重启的机器因为扛的流量太大而出现问题？\n关于这个问题，我是这么考虑的。当你大批量重启服务提供方的时候，对于调用方来说，这些刚重启的机器权重基本是一样的，也就是说这些机器被选中的概率是一样的，大家都是一样得低，也就不存在权重区分的问题了。但是对于那些没有重启过的应用提供方来说，它们被负载均衡选中的概率是相对较高的，但是我们可以通过自适应负载的方法平缓地切换，所以也是没有问题的。\n启动预热更多是从调用方的角度出发，去解决服务提供方应用冷启动的问题，让调用方的请求量通过一个时间窗口过渡，慢慢达到一个正常水平，从而实现平滑上线。但对于服务提供方本身来说，有没有相关方案可以实现这种效果呢？\n当然有，这也是我今天要分享的另一个重点，和热启动息息相关，那就是延迟暴露。\n延迟暴露\n我们应用启动的时候都是通过 main 入口，然后顺序加载各种相关依赖的类。以 Spring 应用启动为例，在加载的过程中，Spring 容器会顺序加载 Spring Bean，如果某个 Bean 是 RPC 服务的话，我们不光要把它注册到 Spring-BeanFactory 里面去，还要把这个 Bean 对应的接口注册到注册中心。注册中心在收到新上线的服务提供方地址的时候，会把这个地址推送到调用方应用内存中；当调用方收到这个服务提供方地址的时候，就会去建立连接发请求。\n但这时候是不是存在服务提供方可能并没有启动完成的情况？因为服务提供方应用可能还在加载其它的 Bean。对于调用方来说，只要获取到了服务提供方的 IP，就有可能发起 RPC 调用，但如果这时候服务提供方没有启动完成的话，就会导致调用失败，从而使业务受损。\n那有什么办法可以避免这种情况吗？\n在解决问题前，我们先看下出现上述问题的根本原因。这是因为服务提供方应用在没有启动完成的时候，调用方的请求就过来了，而调用方请求过来的原因是，服务提供方应用在启动过程中把解析到的 RPC 服务注册到了注册中心，这就导致在后续加载没有完成的情况下服务提供方的地址就被服务调用方感知到了。\n这样的话，其实我们就可以把接口注册到注册中心的时间挪到应用启动完成后。具体的做法就是在应用启动加载、解析 Bean 的时候，如果遇到了 RPC 服务的 Bean，只先把这个 Bean 注册到 Spring-BeanFactory 里面去，而并不把这个 Bean 对应的接口注册到注册中心，只有等应用启动完成后，才把接口注册到注册中心用于服务发现，从而实现让服务调用方延迟获取到服务提供方地址（延迟注册的方式，使服务延迟暴露）\n这样是可以保证应用在启动完后才开始接入流量的，但其实这样做，我们还是没有实现最开始的目标。因为这时候应用虽然启动完成了，但并没有执行相关的业务代码，所以 JVM 内存里面还是冷的。如果这时候大量请求过来，还是会导致整个应用在高负载模式下运行，从而导致不能及时地返回请求结果。而且在实际业务中，一个服务的内部业务逻辑一般会依赖其它资源的，比如缓存数据。如果我们能在服务正式提供服务前，先完成缓存的初始化操作，而不是等请求来了之后才去加载，我们就可以降低重启后第一次请求出错的概率。\n那具体怎么实现呢？\n我们还是需要利用服务提供方把接口注册到注册中心的那段时间。我们可以在服务提供方应用启动后，接口注册到注册中心前，预留一个 Hook 过程，让用户可以实现可扩展的 Hook 逻辑。用户可以在 Hook 里面模拟调用逻辑，从而使 JVM 指令能够预热起来，并且用户也可以在 Hook 里面事先预加载一些资源，只有等所有的资源都加载完成后，最后才把接口注册到注册中心。整个应用启动过程如下图所示：\n上面的 Hook 方案需要对不同的接口维护不通的case，维护成本较高。如果仅仅预热JVM，是不是可以在 Spring 容器 refresh 时调用初始化方法。这两个方案效果一样，一样需维护case。或者公司有流量回放工具，回放一部分流量。再或者JVM 可以预热一些代码，这个阿里的JVM可以实现了。\n思考题：在启动预热那部分，我们特意提到过一个问题，就是“当大批量重启服务提供方的时候，会导致请求大概率发到没有重启的机器上，这时服务提供方有可能扛不住”，不知道你是怎么看待这个问题的，是否有好的解决方案呢？\n我们可以考虑在非流量高峰的时候重启服务，将影响降到最低；也可以考虑分批次重启，控制好每批重启的服务节点的数量，当一批服务节点的权重与访问量都到正常水平时，再去重启下一批服务节点。\n15 | 熔断限流：业务如何实现自我保护? # 为什么需要自我保护？\nRPC 是解决分布式系统通信问题的一大利器，而分布式系统的一大特点就是高并发，所以说 RPC 也会面临高并发的场景。在这样的情况下，我们提供服务的每个服务节点就都可能由于访问量过大而引起一系列的问题，比如业务处理耗时过长、CPU 飘高、频繁 Full GC 以及服务进程直接宕机等等。但是在生产环境中，我们要保证服务的稳定性和高可用性，这时我们就需要业务进行自我保护，从而保证在高访问量、高并发的场景下，应用系统依然稳定，服务依然高可用。\n那么在使用 RPC 时，业务又如何实现自我保护呢？\n最常见的方式就是限流了，简单有效，但 RPC 框架的自我保护方式可不只有限流，并且 RPC 框架的限流方式可以是多种多样的。\n我们可以将 RPC 框架拆开来分析，RPC 调用包括服务端和调用端，调用端向服务端发起调用。下面我就分享一下服务端与调用端分别是如何进行自我保护的。\n服务端的自我保护\n我们先看服务端，举个例子，假如我们要发布一个 RPC 服务，作为服务端接收调用端发送过来的请求，这时服务端的某个节点负载压力过高了，我们该如何保护这个节点？\n这个问题还是很好解决的，既然负载压力高，那就不让它再接收太多的请求就好了，等接收和处理的请求数量下来后，这个节点的负载压力自然就下来了。\n那么就是限流吧？是的，在 RPC 调用中服务端的自我保护策略就是限流，那你有没有想过我们是如何实现限流的呢？是在服务端的业务逻辑中做限流吗？有没有更优雅的方式？\n限流是一个比较通用的功能，我们可以在 RPC 框架中集成限流的功能，让使用方自己去配置限流阈值；我们还可以在服务端添加限流逻辑，当调用端发送请求过来时，服务端在执行业务逻辑之前先执行限流逻辑，如果发现访问量过大并且超出了限流的阈值，就让服务端直接抛回给调用端一个限流异常，否则就执行正常的业务逻辑。\n那服务端的限流逻辑又该如何实现呢？\n方式有很多，比如最简单的计数器，还有可以做到平滑限流的滑动窗口、漏斗算法以及令牌桶算法等等。其中令牌桶算法最为常用。\n计数器算法是最简单的一种限流算法。它将时间窗口固定，比如1秒或1分钟，并在这个时间窗口内计数请求。如果请求计数超过了设定的阈值，则拒绝后续的请求直到下一个时间窗口。这种方法实现简单，但可能会出现窗口临界点的请求突增问题，即在时间窗口切换的瞬间，请求量可能会暂时性地两倍于阈值。 滑动窗口算法是对计数器算法的改进。它通过记录每个请求的确切时间来更精确地控制请求的频率。滑动窗口可以视为是连续的多个固定窗口的叠加，每当新的请求到来时，它会计算当前窗口内的请求总数。这种方法比固定窗口算法更平滑，减少了临界点的请求量突增。 漏斗算法将请求想象为水滴，系统像一个漏斗。请求（水滴）以任意速率流入漏斗，而漏斗以固定的速率将水滴排出（处理请求）。如果水滴（请求）填满了漏斗，那么新进的水滴（请求）就会被丢弃。这种算法可以很好地平滑突发流量，确保数据的均匀处理，但在高流量下可能会导致请求的延迟。 令牌桶算法是一种灵活且常用的限流策略。系统会以固定的速率向令牌桶中添加令牌，每个请求到来时必须消耗一个令牌才能被处理。如果令牌桶中没有令牌，则请求要么等待直到获得令牌，要么直接被拒绝。这种方法既可以处理突发请求，也允许在流量较低时积累一定数量的请求处理能力。 我们可以假设下这样一个场景：我发布了一个服务，提供给多个应用的调用方去调用，这时有一个应用的调用方发送过来的请求流量要比其它的应用大很多，这时我们就应该对这个应用下的调用端发送过来的请求流量进行限流。所以说我们在做限流的时候要考虑应用级别的维度，甚至是 IP 级别的维度，这样做不仅可以让我们对一个应用下的调用端发送过来的请求流量做限流，还可以对一个 IP 发送过来的请求流量做限流。\n这时你可能会想，使用方该如何配置应用维度以及 IP 维度的限流呢？在代码中配置是不是不大方便？我之前说过，RPC 框架真正强大的地方在于它的治理功能，而治理功能大多都需要依赖一个注册中心或者配置中心，我们可以通过 RPC 治理的管理端进行配置，再通过注册中心或者配置中心将限流阈值的配置下发到服务提供方的每个节点上，实现动态配置。\n看到这儿，你有没有发现，在服务端实现限流，配置的限流阈值是作用在每个服务节点上的。比如说我配置的阈值是每秒 1000 次请求，那么就是指一台机器每秒处理 1000 次请求；如果我的服务集群拥有 10 个服务节点，那么我提供的服务限流阈值在最理想的情况下就是每秒 10000 次。\n接着看这样一个场景：我提供了一个服务，而这个服务的业务逻辑依赖的是 MySQL 数据库，由于 MySQL 数据库的性能限制，我们是需要对其进行保护。假如在 MySQL 处理业务逻辑中，SQL 语句的能力是每秒 10000 次，那么我们提供的服务处理的访问量就不能超过每秒 10000 次，而我们的服务有 10 个节点，这时我们配置的限流阈值应该是每秒 1000 次。那如果之后因为某种需求我们对这个服务扩容了呢？扩容到 20 个节点，我们是不是就要把限流阈值调整到每秒 500 次呢？这样操作每次都要自己去计算，重新配置，显然太麻烦了。\n我们可以让 RPC 框架自己去计算，当注册中心或配置中心将限流阈值配置下发的时候，我们可以将总服务节点数也下发给服务节点，之后由服务节点自己计算限流阈值，这样就解决问题了吧？\n解决了一部分，还有一个问题存在，那就是在实际情况下，一个服务节点所接收到的访问量并不是绝对均匀的，比如有 20 个节点，而每个节点限流的阈值是 500，其中有的节点访问量已经达到阈值了，但有的节点可能在这一秒内的访问量是 450，这时调用端发送过来的总调用量还没有达到 10000 次，但可能也会被限流，这样是不是就不精确了？那有没有比较精确的限流方式呢？（单机限流存在不准确的问题）\n我刚才讲解的限流方式之所以不精确，是因为限流逻辑是服务集群下的每个节点独立去执行的，是一种单机的限流方式，而且每个服务节点所接收到的流量并不是绝对均匀的。\n我们可以提供一个专门的限流服务，让每个节点都依赖一个限流服务，当请求流量打过来时，服务节点触发限流逻辑，调用这个限流服务来判断是否到达了限流阈值。我们甚至可以将限流逻辑放在调用端，调用端在发出请求时先触发限流逻辑，调用限流服务，如果请求量已经到达了限流阈值，请求都不需要发出去，直接返回给动态代理一个限流异常即可。\n这种限流方式可以让整个服务集群的限流变得更加精确，但也由于依赖了一个限流服务，它在性能和耗时上与单机的限流方式相比是有很大劣势的。至于要选择哪种限流方式，就要结合具体的应用场景进行选择了。\n调用端的自我保护\n刚才我讲解了服务端如何进行自我保护，最简单有效的方式就是限流。那么调用端呢？调用端是否需要自我保护呢？\n举个例子，假如我要发布一个服务 B，而服务 B 又依赖服务 C，当一个服务 A 来调用服务 B 时，服务 B 的业务逻辑调用服务 C，而这时服务 C 响应超时了，由于服务 B 依赖服务 C，C 超时直接导致 B 的业务逻辑一直等待，而这个时候服务 A 在频繁地调用服务 B，服务 B 就可能会因为堆积大量的请求而导致服务宕机。\n由此可见，服务 B 调用服务 C，服务 C 执行业务逻辑出现异常时，会影响到服务 B，甚至可能会引起服务 B 宕机。这还只是 A-\u0026gt;B-\u0026gt;C 的情况，试想一下 A-\u0026gt;B-\u0026gt;C-\u0026gt;D-\u0026gt;……呢？在整个调用链中，只要中间有一个服务出现问题，都可能会引起上游的所有服务出现一系列的问题，甚至会引起整个调用链的服务都宕机，这是非常恐怖的。\n所以说，在一个服务作为调用端调用另外一个服务时，为了防止被调用的服务出现问题而影响到作为调用端的这个服务，这个服务也需要进行自我保护。而最有效的自我保护方式就是熔断。\n我们可以先了解下熔断机制。\n熔断器的工作机制主要是关闭、打开和半打开这三个状态之间的切换。在正常情况下，熔断器是关闭的；当调用端调用下游服务出现异常时，熔断器会收集异常指标信息进行计算，当达到熔断条件时熔断器打开，这时调用端再发起请求是会直接被熔断器拦截，并快速地执行失败逻辑；当熔断器打开一段时间后，会转为半打开状态，这时熔断器允许调用端发送一个请求给服务端，如果这次请求能够正常地得到服务端的响应，则将状态置为关闭状态，否则设置为打开。\n熔断器机制：调用异常多则打开，过段时间半打开，尝试调用服务，一切正常就关闭\n了解完熔断机制，你就会发现，在业务逻辑中加入熔断器其实是不够优雅的。那么在 RPC 框架中，我们该如何整合熔断器呢？\n熔断机制主要是保护调用端，调用端在发出请求的时候会先经过熔断器。我们可以回想下 RPC 的调用流程：\n熔断机制是保护调用端，限流是保护服务端。\n你看图的话，有没有想到在哪个步骤整合熔断器会比较合适呢？\n我的建议是动态代理，因为在 RPC 调用的流程中，动态代理是 RPC 调用的第一个关口。在发出请求时先经过熔断器，如果状态是闭合则正常发出请求，如果状态是打开则执行熔断器的失败策略。\n检查熔断的flag放在动态代理这一步，不是上边说的熔断的逻辑放在这一步，熔断检查的逻辑肯定是要经过调用服务提供方才知道是否调用成功。\n思考题：在使用 RPC 的过程中业务要实现自我保护，针对这个问题你是否还有其他的解决方案？\n在 RPC 调用中无论服务端还是调用端都需要自我保护，服务端自我保护的最简单有效的方式是“限流”，调用端则可以通过“熔断”机制来进行自我保护。\n除了“熔断”和“限流”外，相信你一定听过“降级”这个词。简单来说就是当一个服务处理大量的请求达到一定压力的时候，我们可以让这个服务在处理请求时减少些非必要的功能，从而降低这个服务的压力。\n还有就是我们可以通过服务治理，降低一个服务节点的权重来减轻某一方服务节点的请求压力，达到保护这个服务节点的目的。\n16 | 业务分组：如何隔离流量？ # RPC 中常用的保护手段“熔断限流”，熔断是调用方为了避免在调用过程中，服务提供方出现问题的时候，自身资源被耗尽的一种保护行为；而限流则是服务提供方为防止自己被突发流量打垮的一种保护行为。虽然这两种手段作用的对象不同，但出发点都是为了实现自我保护，所以一旦发生这种行为，业务都是有损的。\n那说起突发流量，限流固然是一种手段，但其实面对复杂的业务以及高并发场景时，我们还有别的手段，可以最大限度地保障业务无损，那就是隔离流量，业务分组。\n为什么需要分组？\n在我们的日常开发中，我们不都提倡让用户使用起来越简单越好吗？如果在接口上再加一个分组维度去管理，不就让事情变复杂了吗？\n实则不然，举个例子。在没有汽车的年代，我们的道路很简单，就一条，行人、洋车都在上边走。那随着汽车的普及以及猛增，我们的道路越来越宽，慢慢地有了高速、辅路、人行道等等。很显然，交通网的建设与完善不仅提高了我们的出行效率，而且还更好地保障了我们行人的安全。\n同样的道理，我们用在 RPC 治理上也是一样的。假设你是一个服务提供方应用的负责人，在早期业务量不大的情况下，应用之间的调用关系并不会复杂，请求量也不会很大，我们的应用有足够的能力扛住日常的所有流量。我们并不需要花太多的时间去治理调用请求过来的流量，我们通常会选择最简单的方法，就是把服务实例统一管理，把所有的请求都用一个共享的“大池子”来处理。这就类似于“简单道路时期”，服务调用方跟服务提供方之间的调用拓扑如下图所示：\n后期因为业务发展丰富了，调用你接口的调用方就会越来越多，流量也会渐渐多起来。可能某一天，一个“爆炸式惊喜”就来了。其中一个调用方的流量突然激增，让你整个集群瞬间处于高负载运行，进而影响到其它调用方，导致它们的整体可用率下降。而这时候作为应用负责人的你，那就得变身“救火队长”了，要想尽各种办法来保证应用的稳定。\n在经过一系列的救火操作后，我们肯定要去想更好的应对办法。那回到问题的根本去看，关键就在于，早期为了管理方便，我们把接口都放到了同一个分组下面，所有的服务实例是以一个整体对外提供能力的。\n但后期因为业务发展，这种粗暴的管理模式已经不适用了，这就好比“汽车来了，我们的交通网也得抓紧建设”一样，让人车分流。此时，道路上的人和车就好比我们应用的调用方，我们可以尝试把应用提供方这个大池子划分出不同规格的小池子，再分配给不同的调用方，而不同小池子之间的隔离带，就是我们在 RPC 里面所说的分组，它可以实现流量隔离。\n怎么实现分组？\n现在分组是怎么回事我们搞清楚了，那放到 RPC 里我们该怎么实现呢？\n既然是要求不同的调用方应用能拿到的池子内容不同，那我们就要回想下服务发现了，因为在 RPC 流程里，能影响到调用方获取服务节点的逻辑就是它了。\n服务调用方是通过接口名去注册中心找到所有的服务节点来完成服务发现的，那换到这里的话，这样做其实并不合适，因为这样调用方会拿到所有的服务节点。因此为了实现分组隔离逻辑，我们需要重新改造下服务发现的逻辑，调用方去获取服务节点的时候除了要带着接口名，还需要另外加一个分组参数，相应的服务提供方在注册的时候也要带上分组参数。\n通过改造后的分组逻辑，我们可以把服务提供方所有的实例分成若干组，每一个分组可以提供给单个或者多个不同的调用方来调用。那怎么分组好呢，有没有统一的标准？\n坦白讲，这个分组并没有一个可衡量的标准，但我自己总结了一个规则可以供你参考，就是按照应用重要级别划分。\n非核心应用不要跟核心应用分在同一个组，核心应用之间应该做好隔离，一个重要的原则就是保障核心应用不受影响。比如提供给电商下单过程中用的商品信息接口，我们肯定是需要独立出一个单独分组，避免受其它调用方污染的。有了分组之后，我们的服务调用方跟服务提供方之间的调用拓扑就如下图所示：\n看上面这图相当于，从实例上就做好了分组的划分。 例如我有两个实例专供调用方 1，两个专供调用方 2。 1的应用压力不会不会影响到 2。 不过前文也提过，除了应用侧，数据库也有可能会互相影响，因此即使做了分组，还是要对每个应用做好限流。 另外除了应用隔离，部署这些实例的机器也要做好隔离。避免物理资源的竞争\n通过分组的方式隔离调用方的流量，从而避免因为一个调用方出现流量激增而影响其它调用方的可用率。对服务提供方来说，这种方式是我们日常治理服务过程中一个高频使用的手段，那通过这种分组进行流量隔离，对调用方应用会不会有影响呢？\n如何实现高可用？\n分组隔离后，单个调用方在发 RPC 请求的时候可选择的服务节点数相比没有分组前减少了，那对于单个调用方来说，出错的概率就升高了。比如一个集中交换机设备突然坏了，而这个调用方的所有服务节点都在这个交换机下面，在这种情况下对于服务调用方来说，它的请求无论如何也到达不了服务提供方，从而导致这个调用方业务受损。\n那有没有更高可用一点的方案呢？回到我们前面说的那个马路例子上，正常情况下我们是必须让车在车道行驶，人在人行道上行走。但当人行道或者车道出现抢修的时候，在条件允许的情况下，我们一般都是允许对方借道行驶一段时间，直到道路完全恢复。\n我们同样可以把这个特性用到我们的 RPC 中，要怎么实现呢？\n在前面我们也说了，调用方应用服务发现的时候，除了带上对应的接口名，还需要带上一个特定分组名，所以对于调用方来说，它是拿不到其它分组的服务节点的，那这样的话调用方就没法建立起连接发请求了。\n因此问题的核心就变成了调用方要拿到其它分组的服务节点，但是又不能拿到所有的服务节点，否则分组就没有意义了。一个最简单的办法就是，允许调用方可以配置多个分组。但这样的话，这些节点对于调用方来说就都是一样的了，调用方可以随意选择获取到的所有节点发送请求，这样就又失去了分组隔离的意义，并且还没有实现我们想要的“借道”的效果。\n所以我们还需要把配置的分组区分下主次分组，只有在主分组上的节点都不可用的情况下才去选择次分组节点；只要主分组里面的节点恢复正常，我们就必须把流量都切换到主节点上，整个切换过程对于应用层完全透明，从而在一定程度上保障调用方应用的高可用。\n思考题：在我们的实际工作中，测试人员和开发人员的工作一般都是并行的，这就导致一个问题经常出现：开发人员在开发过程中可能需要启动自身的应用，而测试人员为了能验证功能，会在测试环境中部署同样的应用。如果开发人员和测试人员用的接口分组名刚好一样，在这种情况下，就可能会干扰其它正在联调的调用方进行功能验证，进而影响整体的工作效率。不知道面对这种情况，你有什么好办法吗？\n我们可以考虑配置不同的注册中心，开发人员将自己的服务注册到注册中心 A 上，而测试人员可以将自己的服务注册到测试专属的注册中心 B 上，这样测试人员在验证功能的时候，调用端会从注册中心 B 上拉取服务节点，开发人员重启自己的服务是影响不到测试人员的。\n如果你使用过或者了解 k8s 的话，你一定知道“命名空间”的概念，RPC 框架如果支持“命名空间”，也是可以解决这一问题的。\n","date":"16 May 2024","permalink":"/posts/architecture/distributed/rpc/rpc-%E5%AE%9E%E6%88%98%E4%B8%8E%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86/%E8%BF%9B%E9%98%B6%E7%AF%87/","section":"博客","summary":"RPC 实战与核心原理 - 进阶篇","title":"RPC 实战与核心原理 - 进阶篇"},{"content":"01 | 核心原理：能否画张图解释下RPC的通信流程？ # 从前，面试的时候，被面试官问过一个问题“你能否给我解释下 RPC 的通信流程”。这问题其实并不难，不过因为我平时都在用各种框架，丹并未停下来思考过框架的原理，所以，我吱唔了半天也没说出所以然来。\n面试官继续引导“你想想，如果没有 RPC 框架，那你要怎么调用另外一台服务器上的接口呢”这问题可深可浅，也特别考验候选人的基本功。如果你是候选人，你会怎么回答呢？今天我就来试着回答下这个问题。\n什么是 RPC？\n我知道你肯定不喜欢听概念，我也是这样，看书的时候一看到概念就直接略过。不过，到后来，我才发现，“定义”是一件多么伟大的事情。当我们能够用一句话把一个东西给定义出来的时候，侧面也说明你已经彻底理解这事了，不仅知道它要解决什么问题，还要知道它的边界。所以，你可以先停下来想想，什么是 RPC\nRPC 的全称是 Remote Procedure Call，即远程过程调用。简单解读字面上的意思，远程肯定是指要跨机器而非本机，所以需要用到网络编程才能实现，但是不是只要通过网络通信访问到另一台机器的应用程序，就可以称之为 RPC 调用了？显然并不够。\n这就好比建在小河上的桥一样连接着河的两岸，如果没有小桥，我们需要通过划船、绕道等其他方式才能到达对面，但是有了小桥之后，我们就能像在路面上一样行走到达对面，并且跟在路面上行走的体验没有区别。所以我认为，RPC 的作用就是体现在这样两个方面：\n屏蔽远程调用跟本地调用的区别，让我们感觉就是调用项目内的方法； 隐藏底层网络通信的复杂性，让我们更专注于业务逻辑。 RPC 通信流程\n理解了什么是 RPC，接下来我们讲下 RPC 框架的通信流程，方便我们进一步理解 RPC。如前面所讲，RPC 能帮助我们的应用透明地完成远程调用，发起调用请求的那一方叫做调用方，被调用的一方叫做服务提供方。为了实现这个目标，我们就需要在 RPC 框架里面对整个通信细节进行封装，那一个完整的 RPC 会涉及到哪些步骤呢？\n我们已经知道 RPC 是一个远程调用，那肯定就需要通过网络来传输数据，并且 RPC 常用于业务系统之间的数据交互，需要保证其可靠性，所以 RPC 一般默认采用 TCP 来传输。我们常用的 HTTP 协议也是建立在 TCP 之上的。\n网络传输的数据必须是二进制数据，但调用方请求的出入参数都是对象。对象是肯定没法直接在网络中传输的，需要提前把它转成可传输的二进制，并且要求转换算法是可逆的，这个过程我们一般叫做“序列化”。\n调用方持续地把请求参数序列化成二进制后，经过 TCP 传输给了服务提供方。服务提供方从 TCP 通道里面收到二进制数据，那如何知道一个请求的数据到哪里结束，是一个什么类型的请求呢？\n在这里我们可以想想高速公路，它上面有很多出口，为了让司机清楚地知道从哪里出去，管理部门会在路上建立很多指示牌，并在指示牌上标明下一个出口是哪里、还有多远。那回到数据包识别这个场景，我们是不是也可以建立一些“指示牌”，并在上面标明数据包的类型和长度，这样就可以正确的解析数据了。确实可以，并且我们把数据格式的约定内容叫做“协议”。大多数的协议会分成两部分，分别是数据头和消息体。数据头一般用于身份识别，包括协议标识、数据大小、请求类型、序列化类型等信息；消息体主要是请求的业务参数信息和扩展属性等。\n根据协议格式，服务提供方就可以正确地从二进制数据中分割出不同的请求来，同时根据请求类型和序列化类型，把二进制的消息体逆向还原成请求对象。这个过程叫作“反序列化”。\n服务提供方再根据反序列化出来的请求对象找到对应的实现类，完成真正的方法调用，然后把执行结果序列化后，回写到对应的 TCP 通道里面。调用方获取到应答的数据包后，再反序列化成应答对象，这样调用方就完成了一次 RPC 调用。\n那上述几个流程就组成了一个完整的 RPC 吗？\n在我看来，还缺点东西。因为对于研发人员来说，这样做要掌握太多的 RPC 底层细节，需要手动写代码去构造请求、调用序列化，并进行网络调用，整个 API 非常不友好。\n那我们有什么办法来简化 API，屏蔽掉 RPC 细节，让使用方只需要关注业务接口，像调用本地一样来调用远程呢？\n如果你了解 Spring，一定对其 AOP 技术很佩服，其核心是采用动态代理的技术，通过字节码增强对方法进行拦截增强，以便于增加需要的额外处理逻辑。其实这个技术也可以应用到 RPC 场景来解决我们刚才面临的问题。\n由服务提供者给出业务接口声明，在调用方的程序里面，RPC 框架根据调用的服务接口提前生成动态代理实现类，并通过依赖注入等技术注入到声明了该接口的相关业务逻辑里面。该代理实现类会拦截所有的方法调用，在提供的方法处理逻辑里面完成一整套的远程调用，并把远程调用结果返回给调用方，这样调用方在调用远程方法的时候就获得了像调用本地接口一样的体验。\n到这里，一个简单版本的 RPC 框架就实现了。我把整个流程都画出来了，供你参考：\nRPC 在架构中的位置\n围绕 RPC 我们讲了这么多，那 RPC 在架构中究竟处于什么位置呢？\n如刚才所讲，RPC 是解决应用间通信的一种方式，而无论是在一个大型的分布式应用系统还是中小型系统中，应用架构最终都会从“单体”演进成“微服务化”，整个应用系统会被拆分为多个不同功能的应用，并将它们部署在不同的服务器中，而应用之间会通过 RPC 进行通信，可以说 RPC 对应的是整个分布式应用系统，就像是“经络”一样的存在。\n那么如果没有 RPC，我们现实中的开发过程是怎样的一个体验呢？\n所有的功能代码都会被我们堆砌在一个大项目中，开发过程中你可能要改一行代码，但改完后编译会花掉你 2 分钟，编译完想运行起来验证下结果可能要 5 分钟，是不是很酸爽？更难受的是在人数比较多的团队里面，多人协同开发的时候，如果团队其他人把接口定义改了，你连编译通过的机会都没有，系统直接报错，从而导致整个团队的开发效率都会非常低下。而且当我们准备要上线发版本的时候，QA 也很难评估这次的测试范围，为了保险起见我们只能把所有的功能进行回归测试，这样会导致我们上线新功能的整体周期都特别长。\n无论你是研发还是架构师，我相信这种系统架构我们肯定都不能接受，那怎么才能解决这个问题呢？\n我们首先都会想到可以采用“分而治之”的思想来进行拆分，但是拆分完的系统怎么保持跟未拆分前的调用方式一样呢？我们总不能因为架构升级，就把所有的代码都推倒重写一遍吧。\nRPC 框架能够帮助我们解决系统拆分后的通信问题，并且能让我们像调用本地一样去调用远程方法。利用 RPC 我们不仅可以很方便地将应用架构从“单体”演进成“微服务化”，而且还能解决实际开发过程中的效率低下、系统耦合等问题，这样可以使得我们的系统架构整体清晰、健壮，应用可运维度增强。\n当然 RPC 不仅可以用来解决通信问题，它还被用在了很多其他场景，比如：发 MQ、分布式缓存、数据库等。下图是我之前开发的一个应用架构图：\n在这个应用中，我使用了 MQ 来处理异步流程、Redis 缓存热点数据、MySQL 持久化数据，还有就是在系统中调用另外一个业务系统的接口，对我的应用来说这些都是属于 RPC 调用，而 MQ、MySQL 持久化的数据也会存在于一个分布式文件系统中，他们之间的调用也是需要用 RPC 来完成数据交互的。\n由此可见，RPC 确实是我们日常开发中经常接触的东西，只是被包装成了各种框架，导致我们很少意识到这就是 RPC，让 RPC 变成了我们最“熟悉的陌生人”。现在，回过头想想，我说 RPC 是整个应用系统的“经络”，这不为过吧？我们真的很有必要学好 RPC，不仅因为 RPC 是构建复杂系统的基石，还是提升自身认知的利器。\n02 | 协议：怎么设计可扩展且向后兼容的协议？ # 一提到协议，你最先想到的可能是 TCP 协议、UDP 协议等等，这些网络传输协议的实现在我看来有点晦涩难懂。虽然在 RPC 中我们也会用到这些协议，但这些协议更多的是对我们上层应用是透明的，我们 RPC 在使用过程中并不太需要关注他们的细节。那我今天要讲的 RPC 协议到底是什么呢？\n可能我举个例子，你立马就明白了。HTTP 协议是不是很熟悉（本讲里面所说的 HTTP 默认都是 1.X）？ 这应该是我们日常工作中用得最频繁的协议了，每天打开浏览器浏览的网页就是使用的 HTTP 协议。那 HTTP 协议跟 RPC 协议又有什么关系呢？看起来他俩好像不搭边，但他们有一个共性就是都属于应用层协议。\n所以我们今天要讲的 RPC 协议就是围绕应用层协议展开的。我们可以先了解下 HTTP 协议，我们先看看它的协议格式是什么样子的。回想一下我们在浏览器里面输入一个 URL 会发生什么？抛开 DNS 解析暂且不谈，浏览器收到命令后会封装一个请求，并把请求发送到 DNS 解析出来的 IP 上，通过抓包工具我们可以抓到请求的数据包，如下图所示：\n协议的作用\n看完 HTTP 协议之后，你可能会有一个疑问，我们为什么需要协议这个东西呢？没有协议就不能通信吗？\n我们知道只有二进制才能在网络中传输，所以 RPC 请求在发送到网络中之前，他需要把方法调用的请求参数转成二进制；转成二进制后，写入本地 Socket 中，然后被网卡发送到网络设备中。\n但在传输过程中，RPC 并不会把请求参数的所有二进制数据整体一下子发送到对端机器上，中间可能会拆分成好几个数据包，也可能会合并其他请求的数据包（合并的前提是同一个 TCP 连接上的数据），至于怎么拆分合并，这其中的细节会涉及到系统参数配置和 TCP 窗口大小。对于服务提供方应用来说，他会从 TCP 通道里面收到很多的二进制数据，那这时候怎么识别出哪些二进制是第一个请求的呢？这就好比让你读一篇没有标点符号的文章，你要怎么识别出每一句话到哪里结束呢？\n很简单啊，我们加上标点，完成断句就好了。\n同理在 RPC 传输数据的时候，为了能准确地“断句”，我们也必须在应用发送请求的数据包里面加入“句号”，这样才能帮我们的接收方应用从数据流里面分割出正确的数据。这个数据包里面的句号就是消息的边界，用于标示请求数据的结束位置。举个具体例子，调用方发送 AB、CD、EF 3 个消息，如果没有边界的话，接收端就可能收到 ABCDEF 或者 ABC、DEF 这样的消息，这就会导致接收的语义跟发送的时候不一致了。\n所以呢，为了避免语义不一致的事情发生，我们就需要在发送请求的时候设定一个边界，然后在收到请求的时候按照这个设定的边界进行数据分割。这个边界语义的表达，就是我们所说的协议。\n如何设计协议？\n理解了协议的作用，我们再来看看在 RPC 里面是怎么设计协议的。可能你会问：“前面你不是说了 HTTP 协议跟 RPC 都属于应用层协议，那有了现成的 HTTP 协议，为啥不直接用，还要为 RPC 设计私有协议呢？”\n这还要从 RPC 的作用说起，相对于 HTTP 的用处，RPC 更多的是负责应用间的通信，所以性能要求相对更高。但 HTTP 协议的数据包大小相对请求数据本身要大很多，又需要加入很多无用的内容，比如换行符号、回车符等；还有一个更重要的原因是，HTTP 协议属于无状态协议，客户端无法对请求和响应进行关联，每次请求都需要重新建立连接，响应完成后再关闭连接。因此，对于要求高性能的 RPC 来说，HTTP 协议基本很难满足需求，所以 RPC 会选择设计更紧凑的私有协议。\n那怎么设计一个私有 RPC 协议呢？\n在设计协议前，我们先梳理下要完成 RPC 通信的时候，在协议里面需要放哪些内容。\n首先要想到的就是我们前面说的消息边界了，但 RPC 每次发请求发的大小都是不固定的，所以我们的协议必须能让接收方正确地读出不定长的内容。我们可以先固定一个长度（比如 4 个字节）用来保存整个请求数据大小，这样收到数据的时候，我们先读取固定长度的位置里面的值，值的大小就代表协议体的长度，接着再根据值的大小来读取协议体的数据，整个协议可以设计成这样：\n但上面这种协议，只实现了正确的断句效果，在 RPC 里面还行不通。因为对于服务提供方来说，他是不知道这个协议体里面的二进制数据是通过哪种序列化方式生成的。如果不能知道调用方用的序列化方式，即使服务提供方还原出了正确的语义，也并不能把二进制还原成对象，那服务提供方收到这个数据后也就不能完成调用了。因此我们需要把序列化方式单独拿出来，类似协议长度一样用固定的长度存放，这些需要固定长度存放的参数我们可以统称为“协议头”，这样整个协议就会拆分成两部分：协议头和协议体。\n在协议头里面，我们除了会放协议长度、序列化方式，还会放一些像协议标示、消息 ID、消息类型这样的参数，而协议体一般只放请求接口方法、请求的业务参数值和一些扩展属性。这样一个完整的 RPC 协议大概就出来了，协议头是由一堆固定的长度参数组成，而协议体是根据请求接口和参数构造的，长度属于可变的，具体协议如下图所示：\n可扩展的协议\n刚才讲的协议属于定长协议头，那也就是说往后就不能再往协议头里加新参数了，如果加参数就会导致线上兼容问题。举个具体例子，假设你设计了一个 88Bit 的协议头，其中协议长度占用 32bit，然后你为了加入新功能，在协议头里面加了 2bit，并且放到协议头的最后。升级后的应用，会用新的协议发出请求，然而没有升级的应用收到的请求后，还是按照 88bit 读取协议头，新加的 2 个 bit 会当作协议体前 2 个 bit 数据读出来，但原本的协议体最后 2 个 bit 会被丢弃了，这样就会导致协议体的数据是错的。\n可能你会想：“那我把参数加在不定长的协议体里面行不行？而且刚才你也说了，协议体里面会放一些扩展属性。”\n没错，协议体里面是可以加新的参数，但这里有一个关键点，就是协议体里面的内容都是经过序列化出来的，也就是说你要获取到你参数的值，就必须把整个协议体里面的数据经过反序列化出来。但在某些场景下，这样做的代价有点高啊！\n比如说，服务提供方收到一个过期请求，这个过期是说服务提供方收到的这个请求的时间大于调用方发送的时间和配置的超时时间，既然已经过期，就没有必要接着处理，直接返回一个超时就好了。那要实现这个功能，就要在协议里面传递这个配置的超时时间，那如果之前协议里面没有加超时时间参数的话，我们现在把这个超时时间加到协议体里面是不是就有点重了呢？显然，会加重 CPU 的消耗。\n所以为了保证能平滑地升级改造前后的协议，我们有必要设计一种支持可扩展的协议。其关键在于让协议头支持可扩展，扩展后协议头的长度就不能定长了。那要实现读取不定长的协议头里面的内容，在这之前肯定需要一个固定的地方读取长度，所以我们需要一个固定的写入协议头的长度。整体协议就变成了三部分内容：固定部分、协议头内容、协议体内容，前两部分我们还是可以统称为“协议头”，具体协议如下：\n最后，我想说，设计一个简单的 RPC 协议并不难，难的就是怎么去设计一个可“升级”的协议。不仅要让我们在扩展新特性的时候能做到向下兼容，而且要尽可能地减少资源损耗，所以我们协议的结构不仅要支持协议体的扩展，还要做到协议头也能扩展。上述这种设计方法来源于我多年的线上经验，可以说做好扩展性是至关重要的，期待这个协议模版能帮你避掉一些坑。\n思考题：在 RPC 里面，我们是怎么实现请求跟响应关联的？\n首先我们要弄清楚为什么要把请求与响应关联。这是因为在 RPC 调用过程中，调用端会向服务端发送请求消息，之后它还会收到服务端发送回来的响应消息，但这两个操作并不是同步进行的。在高并发的情况下，调用端可能会在某一时刻向服务端连续发送很多条消息之后，才会陆续收到服务端发送回来的各个响应消息，这时调用端需要一种手段来区分这些响应消息分别对应的是之前的哪条请求消息，所以我们说 RPC 在发送消息时要请求跟响应关联。\n解决这个问题不难，只要调用端在收到响应消息之后，从响应消息中读取到一个标识，告诉调用端，这是哪条请求消息的响应消息就可以了。在这一讲中，你会发现我们设计的私有协议都会有消息 ID，这个消息 ID 的作用就是起到请求跟响应关联的作用。调用端为每一个消息生成一个唯一的消息 ID，它收到服务端发送回来的响应消息如果是同一消息 ID，那么调用端就可以认为，这条响应消息是之前那条请求消息的响应消息。\n03 | 序列化：对象怎么在网络中传输？ # 首先，我们得知道什么是序列化与反序列化。\n网络传输的数据必须是二进制数据，但调用方请求的出入参数都是对象。对象是不能直接在网络中传输的，所以我们需要提前把它转成可传输的二进制，并且要求转换算法是可逆的，这个过程我们一般叫做“序列化”。 这时，服务提供方就可以正确地从二进制数据中分割出不同的请求，同时根据请求类型和序列化类型，把二进制的消息体逆向还原成请求对象，这个过程我们称之为“反序列化”。\n这两个过程如下图所示：\n总结来说，序列化就是将对象转换成二进制数据的过程，而反序列就是反过来将二进制转换为对象的过程。\n那么 RPC 框架为什么需要序列化呢？还是请你回想下 RPC 的通信流程：\n不妨借用个例子帮助你理解，比如发快递，我们要发一个需要自行组装的物件。发件人发之前，会把物件拆开装箱，这就好比序列化；这时候快递员来了，不能磕碰呀，那就要打包，这就好比将序列化后的数据进行编码，封装成一个固定格式的协议；过了两天，收件人收到包裹了，就会拆箱将物件拼接好，这就好比是协议解码和反序列化。\n所以现在你清楚了吗？因为网络传输的数据必须是二进制数据，所以在 RPC 调用中，对入参对象与返回值对象进行序列化与反序列化是一个必须的过程。\n有哪些常用的序列化？\n那这么看来，你会不会觉得这个过程很简单呢？实则不然，很复杂。我们可以先看看都有哪些常用的序列化，下面我来简单地介绍下几种常用的序列化方式。\nJDK 原生序列化\n如果你会使用 Java 语言开发，那么你一定知道 JDK 原生的序列化，下面是 JDK 序列化的一个例子：\nimport java.io.*; public class Student implements Serializable { //学号 private int no; //姓名 private String name; public int getNo() { return no; } public void setNo(int no) { this.no = no; } public String getName() { return name; } public void setName(String name) { this.name = name; } @Override public String toString() { return \u0026#34;Student{\u0026#34; + \u0026#34;no=\u0026#34; + no + \u0026#34;, name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#39;}\u0026#39;; } public static void main(String[] args) throws IOException, ClassNotFoundException { String home = System.getProperty(\u0026#34;user.home\u0026#34;); String basePath = home + \u0026#34;/Desktop\u0026#34;; FileOutputStream fos = new FileOutputStream(basePath + \u0026#34;student.dat\u0026#34;); Student student = new Student(); student.setNo(100); student.setName(\u0026#34;TEST_STUDENT\u0026#34;); ObjectOutputStream oos = new ObjectOutputStream(fos); oos.writeObject(student); oos.flush(); oos.close(); FileInputStream fis = new FileInputStream(basePath + \u0026#34;student.dat\u0026#34;); ObjectInputStream ois = new ObjectInputStream(fis); Student deStudent = (Student) ois.readObject(); ois.close(); System.out.println(deStudent); } } 我们可以看到，JDK 自带的序列化机制对使用者而言是非常简单的。序列化具体的实现是由 ObjectOutputStream 完成的，而反序列化的具体实现是由 ObjectInputStream 完成的。\n那么 JDK 的序列化过程是怎样完成的呢？我们看下下面这张图：\n序列化过程就是在读取对象数据的时候，不断加入一些特殊分隔符，这些特殊分隔符用于在反序列化过程中截断用。\n头部数据用来声明序列化协议、序列化版本，用于高低版本向后兼容 对象数据主要包括类名、签名、属性名、属性类型及属性值，当然还有开头结尾等数据，除了属性值属于真正的对象值，其他都是为了反序列化用的元数据 存在对象引用、继承的情况下，就是递归遍历“写对象”逻辑 实际上任何一种序列化框架，核心思想就是设计一种序列化协议，将对象的类型、属性类型、属性值一一按照固定的格式写到二进制字节流中来完成序列化，再按照固定的格式一一读出对象的类型、属性类型、属性值，通过这些信息重新创建出一个新的对象，来完成反序列化。\nJSON\nJSON 可能是我们最熟悉的一种序列化格式了，JSON 是典型的 Key-Value 方式，没有数据类型，是一种文本型序列化框架，JSON 的具体格式和特性，网上相关的资料非常多，这里就不再介绍了。\n他在应用上还是很广泛的，无论是前台 Web 用 Ajax 调用、用磁盘存储文本类型的数据，还是基于 HTTP 协议的 RPC 框架通信，都会选择 JSON 格式。\n但用 JSON 进行序列化有这样两个问题，你需要格外注意：\nJSON 进行序列化的额外空间开销比较大，对于大数据量服务这意味着需要巨大的内存和磁盘开销； JSON 没有类型，但像 Java 这种强类型语言，需要通过反射统一解决，所以性能不会太好。 所以如果 RPC 框架选用 JSON 序列化，服务提供者与服务调用者之间传输的数据量要相对较小，否则将严重影响性能。\nHessian\nHessian 是动态类型、二进制、紧凑的，并且可跨语言移植的一种序列化框架。Hessian 协议要比 JDK、JSON 更加紧凑，性能上要比 JDK、JSON 序列化高效很多，而且生成的字节数也更小。\n使用代码示例如下：\nStudent student = new Student(); student.setNo(101); student.setName(\u0026#34;HESSIAN\u0026#34;); //把student对象转化为byte数组 ByteArrayOutputStream bos = new ByteArrayOutputStream(); Hessian2Output output = new Hessian2Output(bos); output.writeObject(student); output.flushBuffer(); byte[] data = bos.toByteArray(); bos.close(); //把刚才序列化出来的byte数组转化为student对象 ByteArrayInputStream bis = new ByteArrayInputStream(data); Hessian2Input input = new Hessian2Input(bis); Student deStudent = (Student) input.readObject(); input.close(); System.out.println(deStudent); 相对于 JDK、JSON，由于 Hessian 更加高效，生成的字节数更小，有非常好的兼容性和稳定性，所以 Hessian 更加适合作为 RPC 框架远程通信的序列化协议。\n但 Hessian 本身也有问题，官方版本对 Java 里面一些常见对象的类型不支持，比如：\nLinked 系列，LinkedHashMap、LinkedHashSet 等，但是可以通过扩展 CollectionDeserializer 类修复； Local 类，可以通过扩展 ContextSerializerFactory 类修复； Byte/Short 反序列化的时候变成 Integer。 以上这些情况，你在实践时需要格外注意。\nProtobuf\nProtobuf 是 Google 公司内部的混合语言数据标准，是一种轻便、高效的结构化数据存储格式，可以用于结构化数据序列化，支持 Java、Python、C++、Go 等语言。Protobuf 使用的时候需要定义 IDL（Interface description language），然后使用不同语言的 IDL 编译器，生成序列化工具类，它的优点是：\n序列化后体积相比 JSON、Hessian 小很多； IDL 能清晰地描述语义，所以足以帮助并保证应用程序之间的类型不会丢失，无需类似 XML 解析器； 序列化反序列化速度很快，不需要通过反射获取类型； 消息格式升级和兼容性不错，可以做到向后兼容。 使用代码示例如下：\n/** * * // IDl 文件格式 * synax = \u0026#34;proto3\u0026#34;; * option java_package = \u0026#34;com.test\u0026#34;; * option java_outer_classname = \u0026#34;StudentProtobuf\u0026#34;; * * message StudentMsg { * //序号 * int32 no = 1; * //姓名 * string name = 2; * } * */ StudentProtobuf.StudentMsg.Builder builder = StudentProtobuf.StudentMsg.newBuilder(); builder.setNo(103); builder.setName(\u0026#34;protobuf\u0026#34;); //把student对象转化为byte数组 StudentProtobuf.StudentMsg msg = builder.build(); byte[] data = msg.toByteArray(); //把刚才序列化出来的byte数组转化为student对象 StudentProtobuf.StudentMsg deStudent = StudentProtobuf.StudentMsg.parseFrom(data); System.out.println(deStudent); Protobuf 非常高效，但是对于具有反射和动态能力的语言来说，这样用起来很费劲，这一点就不如 Hessian，比如用 Java 的话，这个预编译过程不是必须的，可以考虑使用 Protostuff。\nProtostuff 不需要依赖 IDL 文件，可以直接对 Java 领域对象进行反 / 序列化操作，在效率上跟 Protobuf 差不多，生成的二进制格式和 Protobuf 是完全相同的，可以说是一个 Java 版本的 Protobuf 序列化框架。但在使用过程中，我遇到过一些不支持的情况，也同步给你：\n不支持 null； ProtoStuff 不支持单纯的 Map、List 集合对象，需要包在对象里面。 RPC 框架中如何选择序列化？\n我刚刚简单地介绍了几种最常见的序列化协议，其实远不止这几种，还有 Message pack、kryo 等。那么面对这么多的序列化协议，在 RPC 框架中我们该如何选择呢？\n首先你可能想到的是性能和效率，不错，这的确是一个非常值得参考的因素。我刚才讲过，序列化与反序列化过程是 RPC 调用的一个必须过程，那么序列化与反序列化的性能和效率势必将直接关系到 RPC 框架整体的性能和效率。\n那除了这点，你还想到了什么？\n对，还有空间开销，也就是序列化之后的二进制数据的体积大小。序列化后的字节数据体积越小，网络传输的数据量就越小，传输数据的速度也就越快，由于 RPC 是远程调用，那么网络传输的速度将直接关系到请求响应的耗时。\n现在请你再想想，还有什么因素可以影响到我们的选择？\n没错，就是序列化协议的通用性和兼容性。在 RPC 的运营中，序列化问题恐怕是我碰到的和解答过的最多的问题了，经常有业务会向我反馈这个问题，比如某个类型为集合类的入参服务调用者不能解析了，服务提供方将入参类加一个属性之后服务调用方不能正常调用，升级了 RPC 版本后发起调用时报序列化异常了…\n在序列化的选择上，与序列化协议的效率、性能、序列化协议后的体积相比，其通用性和兼容性的优先级会更高，因为他是会直接关系到服务调用的稳定性和可用率的，对于服务的性能来说，服务的可靠性显然更加重要。我们更加看重这种序列化协议在版本升级后的兼容性是否很好，是否支持更多的对象类型，是否是跨平台、跨语言的，是否有很多人已经用过并且踩过了很多的坑，其次我们才会去考虑性能、效率和空间开销。\n还有一点我要特别强调。除了序列化协议的通用性和兼容性，序列化协议的安全性也是非常重要的一个参考因素，甚至应该放在第一位去考虑。以 JDK 原生序列化为例，它就存在漏洞。如果序列化存在安全漏洞，那么线上的服务就很可能被入侵。\n综合上面几个参考因素，现在我们再来总结一下这几个序列化协议。\n我们首选的还是 Hessian 与 Protobuf，因为他们在性能、时间开销、空间开销、通用性、兼容性和安全性上，都满足了我们的要求。其中 Hessian 在使用上更加方便，在对象的兼容性上更好；Protobuf 则更加高效，通用性上更有优势。\nRPC 框架在使用时要注意哪些问题？\n了解了在 RPC 框架中如何选择序列化，那么我们在使用过程中需要注意哪些序列化上的问题呢？\n我刚才讲过，在 RPC 的运营中，我遇到的最多的问题就是序列化问题了，除了早期 RPC 框架本身出现的问题以外，大多数问题都是使用方使用不正确导致的，接下来我们就盘点下这些高频出现的人为问题。\n对象构造得过于复杂：属性很多，并且存在多层的嵌套，比如 A 对象关联 B 对象，B 对象又聚合 C 对象，C 对象又关联聚合很多其他对象，对象依赖关系过于复杂。序列化框架在序列化与反序列化对象时，对象越复杂就越浪费性能，消耗 CPU，这会严重影响 RPC 框架整体的性能；另外，对象越复杂，在序列化与反序列化的过程中，出现问题的概率就越高。 对象过于庞大：我经常遇到业务过来咨询，为啥他们的 RPC 请求经常超时，排查后发现他们的入参对象非常得大，比如为一个大 List 或者大 Map，序列化之后字节长度达到了上兆字节。这种情况同样会严重地浪费了性能、CPU，并且序列化一个如此大的对象是很耗费时间的，这肯定会直接影响到请求的耗时。 使用序列化框架不支持的类作为入参类：比如 Hessian 框架，他天然是不支持 LinkedHashMap、LinkedHashSet 等，而且大多数情况下最好不要使用第三方集合类，如 Guava 中的集合类，很多开源的序列化框架都是优先支持编程语言原生的对象。因此如果入参是集合类，应尽量选用原生的、最为常用的集合类，如 HashMap、ArrayList。 对象有复杂的继承关系：大多数序列化框架在序列化对象时都会将对象的属性一一进行序列化，当有继承关系时，会不停地寻找父类，遍历属性。就像问题 1 一样，对象关系越复杂，就越浪费性能，同时又很容易出现序列化上的问题。 在 RPC 框架的使用过程中，我们要尽量构建简单的对象作为入参和返回值对象，避免上述问题。\n04 | 网络通信：RPC框架在网络通信上更倾向于哪种网络IO模型？ # 在上面讲解了 RPC 框架中的序列化，我们知道由于网络传输的数据都是二进制数据，所以我们要传递对象，就必须将对象进行序列化，而 RPC 框架在序列化的选择上，我们更关注序列化协议的安全性、通用性、兼容性，其次才关注序列化协议的性能、效率、空间开销。承接上一讲，这一讲，我要专门讲解下 RPC 框架中的网络通信，这也是我们在开篇词中就强调过的重要内容。\nRPC 是解决进程间通信的一种方式。一次 RPC 调用，本质就是服务消费者与服务提供者间的一次网络信息交换的过程。服务调用者通过网络 IO 发送一条请求消息，服务提供者接收并解析，处理完相关的业务逻辑之后，再发送一条响应消息给服务调用者，服务调用者接收并解析响应消息，处理完相关的响应逻辑，一次 RPC 调用便结束了。可以说，网络通信是整个 RPC 调用流程的基础。\n常见的网络 IO 模型\n那说到网络通信，就不得不提一下网络 IO 模型。为什么要讲网络 IO 模型呢？因为所谓的两台 PC 机之间的网络通信，实际上就是两台 PC 机对网络 IO 的操作。\n常见的网络 IO 模型分为四种：同步阻塞 IO（BIO）、同步非阻塞 IO（NIO）、IO 多路复用和异步非阻塞 IO（AIO）。在这四种 IO 模型中，只有 AIO 为异步 IO，其他都是同步 IO。\n其中，最常用的就是同步阻塞 IO 和 IO 多路复用，这一点通过了解它们的机制，你会 get 到。至于其他两种 IO 模型，因为不常用，则不作为本讲的重点，有兴趣的话我们可以在留言区中讨论。\n阻塞 IO（blocking IO）\n同步阻塞 IO 是最简单、最常见的 IO 模型，在 Linux 中，默认情况下所有的 socket 都是 blocking 的，先看下操作流程。\n首先，应用进程发起 IO 系统调用后，应用进程被阻塞，转到内核空间处理。之后，内核开始等待数据，等待到数据之后，再将内核中的数据拷贝到用户内存中，整个 IO 处理完毕后返回进程。最后应用的进程解除阻塞状态，运行业务逻辑。\n这里我们可以看到，系统内核处理 IO 操作分为两个阶段——等待数据和拷贝数据。而在这两个阶段中，应用进程中 IO 操作的线程会一直都处于阻塞状态，如果是基于 Java 多线程开发，那么每一个 IO 操作都要占用线程，直至 IO 操作结束。\n这个流程就好比我们去餐厅吃饭，我们到达餐厅，向服务员点餐，之后要一直在餐厅等待后厨将菜做好，然后服务员会将菜端给我们，我们才能享用。\nIO 多路复用（IO multiplexing）\n多路复用 IO 是在高并发场景中使用最为广泛的一种 IO 模型，如 Java 的 NIO、Redis、Nginx 的底层实现就是此类 IO 模型的应用，经典的 Reactor 模式也是基于此类 IO 模型。\n那么什么是 IO 多路复用呢？通过字面上的理解，多路就是指多个通道，也就是多个网络连接的 IO，而复用就是指多个通道复用在一个复用器上。\n多个网络连接的 IO 可以注册到一个复用器（select）上，当用户进程调用了 select，那么整个进程会被阻塞。同时，内核会“监视”所有 select 负责的 socket，当任何一个 socket 中的数据准备好了，select 就会返回。这个时候用户进程再调用 read 操作，将数据从内核中拷贝到用户进程。\n这里我们可以看到，当用户进程发起了 select 调用，进程会被阻塞，当发现该 select 负责的 socket 有准备好的数据时才返回，之后才发起一次 read，整个流程要比阻塞 IO 要复杂，似乎也更浪费性能。但它最大的优势在于，用户可以在一个线程内同时处理多个 socket 的 IO 请求。用户可以注册多个 socket，然后不断地调用 select 读取被激活的 socket，即可达到在同一个线程内同时处理多个 IO 请求的目的。而在同步阻塞模型中，必须通过多线程的方式才能达到这个目的。\n同样好比我们去餐厅吃饭，这次我们是几个人一起去的，我们专门留了一个人在餐厅排号等位，其他人就去逛街了，等排号的朋友通知我们可以吃饭了，我们就直接去享用了。\n为什么说阻塞 IO 和 IO 多路复用最为常用？\n了解完二者的机制，我们就可以回到起初的问题了——我为什么说阻塞 IO 和 IO 多路复用最为常用。对比这四种网络 IO 模型：阻塞 IO、非阻塞 IO、IO 多路复用、异步 IO。实际在网络 IO 的应用上，需要的是系统内核的支持以及编程语言的支持。\n在系统内核的支持上，现在大多数系统内核都会支持阻塞 IO、非阻塞 IO 和 IO 多路复用，但像信号驱动 IO、异步 IO，只有高版本的 Linux 系统内核才会支持。\n在编程语言上，无论 C++ 还是 Java，在高性能的网络编程框架的编写上，大多数都是基于 Reactor 模式，其中最为典型的便是 Java 的 Netty 框架，而 Reactor 模式是基于 IO 多路复用的。当然，在非高并发场景下，同步阻塞 IO 是最为常见的。\n综合来讲，在这四种常用的 IO 模型中，应用最多的、系统内核与编程语言支持最为完善的，便是阻塞 IO 和 IO 多路复用。这两种 IO 模型，已经可以满足绝大多数网络 IO 的应用场景\nReactor模式\n传统I/O服务模型\n模型特点：\n采用阻塞I/O模式获取输入数据 每个连接都需要独立的线程完成数据的输入，业务处理，数据返回 问题分析：\n当并发数很大，就会创建大量线程，占用大量的系统资源 连接创建后，如果当前线程暂时没有数据可读，该线程会阻塞在read操作，造成线程资源浪费 Reactor模式\nReactor模式称为反应器模式或应答者模式，是基于事件驱动的设计模式，拥有一个或多个并发输入源，有一个服务处理器和多个请求处理器，服务处理器会同步的将输入的请求事件以多路复用的方式分发给相应的请求处理器。\nReactor设计模式是一种为处理并发服务请求，并将请求提交到一个或多个服务处理程序的事件设计模式。当客户端请求抵达后，服务处理程序使用多路分配策略，由一个非阻塞的线程来接收所有请求，然后将请求派发到相关的工作线程并进行处理的过程。\n在事件驱动的应用中，将一个或多个客户端的请求分离和调度给应用程序，同步有序地接收并处理多个服务请求。对于高并发系统经常会使用到Reactor模式，用来替代常用的多线程处理方式以节省系统资源并提高系统的吞吐量。\n单Reactor单线程\n优点：模型简单，没有多线程、进程通信和竞争的问题，全部都在一个线程中完成。\n缺点：\n性能问题，只有一个线程，无法发挥多核CPU的性能，Handler在处理某个连接业务时，整个进程无法处理其他连接事件，很容易导致性能瓶颈。\n可靠性问题，线程意外终止或进入死循环，会导致整个系统通信模块不可用，不能接收和处理外部消息，造成节点故障。\n单Reactor多线程\n工作流程\nReactor对象通过select监听客户端请求事件，收到事件后，通过dispatch进行分发。 如果建立连接请求，则Acceptor通过accept处理连接请求，然后创建一个Handler对象处理完成连接后的各种事件。 如果不是连接请求，则由reactor分发调用连接对应的handler来处理。 handler只负责相应事件，不做具体的业务处理，通过read读取数据后，会分发给后面的worker线程池的某个线程处理业务。 worker线程池会分配独立线程完成真正的业务，并将结果返回给handler。 handler收到响应后，通过send分发将结果返回给client。 优点：可以充分利用多核cpu的处理能力 缺点：多线程数据共享和访问比较复杂，rector处理所有的事件的监听和响应，在单线程运行，在高并发应用场景下，容易出现性能瓶颈。 主从Reactor多线程\n工作流程\nReactor主线程MainReactor对象通过select监听连接事件，收到事件后，通过Acceptor处理连接事件。 当Acceptor处理连接事件后，MainReactor将连接分配给SubAcceptor。 SubAcceptor将连接加入到连接队列进行监听，并创建handler进行各种事件处理。 当有新事件发生时，SubAcceptor就会调用对应的handler进行各种事件处理。 handler通过read读取数据，分发给后面的work线程处理。 work线程池分配独立的work线程进行业务处理，并返回结果。 handler收到响应的结果后，再通过send返回给client。 注意：Reactor主线程可以对应多个Reactor子线程，即SubAcceptor。\nReactor模式总结\n3种模式用生活案例来理解\n单reactor单线程，前台接待员、服务员时同一个人，全程为顾客服务。 单reactor多线程，1个前台接待，多个服务员，接待员只负责接待。 主从reactor多线程，多个前台接待，多个服务员。 Reactor模式的优点\n响应块，不必为单个同步时间所阻塞，虽然Reactor本身依然时同步的。 可以最大程度的避免复杂的多线程及同步问题，并且避免多线程/进程的切换开销。 扩展性好，可以方便的通过增加Reactor实例个数来充分利用CPU资源。 复用性好，Reactor模式本身与具体事件处理逻辑无关，具有很高的复用性。 RPC 框架在网络通信上倾向选择哪种网络 IO 模型？\n讲完了这两种最常用的网络 IO 模型，我们可以看看它们都适合什么样的场景。\nIO 多路复用更适合高并发的场景，可以用较少的进程（线程）处理较多的 socket 的 IO 请求，但使用难度比较高。当然高级的编程语言支持得还是比较好的，比如 Java 语言有很多的开源框架对 Java 原生 API 做了封装，如 Netty 框架，使用非常简便；而 GO 语言，语言本身对 IO 多路复用的封装就已经很简洁了。\n而阻塞 IO 与 IO 多路复用相比，阻塞 IO 每处理一个 socket 的 IO 请求都会阻塞进程（线程），但使用难度较低。在并发量较低、业务逻辑只需要同步进行 IO 操作的场景下，阻塞 IO 已经满足了需求，并且不需要发起 select 调用，开销上还要比 IO 多路复用低。\nRPC 调用在大多数的情况下，是一个高并发调用的场景，考虑到系统内核的支持、编程语言的支持以及 IO 模型本身的特点，在 RPC 框架的实现中，在网络通信的处理上，我们会选择 IO 多路复用的方式。开发语言的网络通信框架的选型上，我们最优的选择是基于 Reactor 模式实现的框架，如 Java 语言，首选的框架便是 Netty 框架（Java 还有很多其他 NIO 框架，但目前 Netty 应用得最为广泛），并且在 Linux 环境下，也要开启 epoll 来提升系统性能（Windows 环境下是无法开启 epoll 的，因为系统内核不支持）。\n了解完以上内容，我们可以继续看这样一个关键问题——零拷贝。在我们应用的过程中，他是非常重要的。\n什么是零拷贝？\n刚才讲阻塞 IO 的时候我讲到，系统内核处理 IO 操作分为两个阶段——等待数据和拷贝数据。等待数据，就是系统内核在等待网卡接收到数据后，把数据写到内核中；而拷贝数据，就是系统内核在获取到数据后，将数据拷贝到用户进程的空间中。以下是具体流程：\n应用进程的每一次写操作，都会把数据写到用户空间的缓冲区中，再由 CPU 将数据拷贝到系统内核的缓冲区中，之后再由 DMA（Direct Memory Access，直接内存访问，一种硬件机制，允许外部设备（如网卡、硬盘驱动器等）直接与系统内存进行数据传输，而无需通过CPU进行中转）将这份数据拷贝到网卡中，最后由网卡发送出去。这里我们可以看到，一次写操作数据要拷贝两次才能通过网卡发送出去，而用户进程的读操作则是将整个流程反过来，数据同样会拷贝两次才能让应用程序读取到数据。\n应用进程的一次完整的读写操作，都需要在用户空间与内核空间中来回拷贝，并且每一次拷贝，都需要 CPU 进行一次上下文切换（由用户进程切换到系统内核，或由系统内核切换到用户进程），这样是不是很浪费 CPU 和性能呢？那有没有什么方式，可以减少进程间的数据拷贝，提高数据传输的效率呢？\n这时我们就需要零拷贝（Zero-copy）技术。\n所谓的零拷贝，就是取消用户空间与内核空间之间的数据拷贝操作，应用进程每一次的读写操作，都可以通过一种方式，让应用进程向用户空间写入或者读取数据，就如同直接向内核空间写入或者读取数据一样，再通过 DMA 将内核中的数据拷贝到网卡，或将网卡中的数据 copy 到内核。\n那怎么做到零拷贝？你想一下是不是用户空间与内核空间都将数据写到一个地方，就不需要拷贝了？此时你有没有想到虚拟内存？\n零拷贝有两种解决方式，分别是 mmap+write 方式和 sendfile 方式，mmap+write 方式的核心原理就是通过虚拟内存来解决的。\nmmap+write 方式： 原理：mmap（Memory Mapped Files）通过内存映射文件的方式，将文件直接映射到用户空间的内存地址，这样用户空间的应用程序可以直接操作这段内存，而不需要将数据拷贝到用户空间。当需要进行写操作时，应用程序直接写入这段映射的内存区域，操作系统负责将这部分内存区域的内容写入到对应的文件中。 优点：减少了用户空间和内核空间之间的数据拷贝，降低了CPU的使用率。 代码示例（伪代码）： // 打开文件 int fd = open(\u0026#34;file.txt\u0026#34;, O_RDONLY); // 将文件映射到内存 void *map = mmap(NULL, filesize, PROT_READ, MAP_PRIVATE, fd, 0); // 进行写操作，这里写入的是内存映射区域，而非直接写入文件 write(sockfd, map, filesize); // 解除内存映射 munmap(map, filesize); sendfile 方式： 原理：sendfile 是 Linux 内核提供的一个系统调用，用于在两个文件描述符之间直接传输数据，避免了数据在用户空间和内核空间之间的拷贝。在传输数据时，sendfile 直接将数据从磁盘文件（通过文件描述符）发送到网络套接字（通过另一个文件描述符）。 优点：不仅减少了数据拷贝，还避免了上下文切换，因为整个操作在内核空间完成。 代码示例（伪代码）： // 打开文件 int fd = open(\u0026#34;file.txt\u0026#34;, O_RDONLY); // 获取文件大小 off_t offset = lseek(fd, 0, SEEK_END); // sendfile 系统调用 sendfile(sockfd, fd, \u0026amp;offset, filesize); 9.2 I/O 多路复用：select/poll/epoll\n05 | 动态代理：面向接口编程，屏蔽RPC处理流程 # 在项目中，当我们要使用 RPC 的时候，我们一般的做法是先找服务提供方要接口，通过 Maven 或者其他的工具把接口依赖到我们项目中。我们在编写业务逻辑的时候，如果要调用提供方的接口，我们就只需要通过依赖注入的方式把接口注入到项目中就行了，然后在代码里面直接调用接口的方法 。\n我们都知道，接口里并不会包含真实的业务逻辑，业务逻辑都在服务提供方应用里，但我们通过调用接口方法，确实拿到了想要的结果，是不是感觉有点神奇呢？想一下，在 RPC 里面，我们是怎么完成这个魔术的。\n这里面用到的核心技术就是前面说的动态代理。RPC 会自动给接口生成一个代理类，当我们在项目中注入接口的时候，运行过程中实际绑定的是这个接口生成的代理类。这样在接口方法被调用的时候，它实际上是被生成代理类拦截到了，这样我们就可以在生成的代理类里面，加入远程调用逻辑。\n通过这种“偷梁换柱”的手法，就可以帮用户屏蔽远程调用的细节，实现像调用本地一样地调用远程的体验，整体流程如下图所示：\n实现原理\n动态代理在 RPC 里面的作用，就像是个魔术。现在我不妨给你揭秘一下，我们一起看看这是怎么实现的。之后，学以致用自然就不难了。\n我们以 Java 为例，看一个具体例子，代码如下所示：\n/** * 要代理的接口 */ public interface Hello { String say(); } /** * 真实调用对象 */ public class RealHello { public String invoke(){ return \u0026#34;i\u0026#39;m proxy\u0026#34;; } } /** * JDK代理类生成 */ public class JDKProxy implements InvocationHandler { private Object target; JDKProxy(Object target) { this.target = target; } @Override public Object invoke(Object proxy, Method method, Object[] paramValues) { return ((RealHello)target).invoke(); } } /** * 测试例子 */ public class TestProxy { public static void main(String[] args){ // 构建代理器 JDKProxy proxy = new JDKProxy(new RealHello()); ClassLoader classLoader = ClassLoaderUtils.getCurrentClassLoader(); // 把生成的代理类保存到文件 System.setProperty(\u0026#34;sun.misc.ProxyGenerator.saveGeneratedFiles\u0026#34;,\u0026#34;true\u0026#34;); // 生成代理类 Hello test = (Hello) Proxy.newProxyInstance(classLoader, new Class[]{Hello.class}, proxy); // 方法调用 System.out.println(test.say()); } } 这段代码想表达的意思就是：给 Hello 接口生成一个动态代理类，并调用接口 say() 方法，但真实返回的值居然是来自 RealHello 里面的 invoke() 方法返回值。你看，短短 50 行的代码，就完成了这个功能，是不是还挺有意思的？\n那既然重点是代理类的生成，那我们就去看下 Proxy.newProxyInstance 里面究竟发生了什么？\n一起看下下面的流程图，具体代码细节你可以对照着 JDK 的源码看（上文中有类和方法，可以直接定位），我是按照 1.7.X 版本梳理的。\n在生成字节码的那个地方，也就是 ProxyGenerator.generateProxyClass() 方法里面，通过代码我们可以看到，里面是用参数 saveGeneratedFiles 来控制是否把生成的字节码保存到本地磁盘。同时为了更直观地了解代理的本质，我们需要把参数 saveGeneratedFiles 设置成 true，但这个参数的值是由 key 为“sun.misc.ProxyGenerator.saveGeneratedFiles”的 Property 来控制的，动态生成的类会保存在工程根目录下的 com/sun/proxy 目录里面。现在我们找到刚才生成的 $Proxy0.class，通过反编译工具打开 class 文件，你会看到这样的代码：\npackage com.sun.proxy; import com.proxy.Hello; import java.lang.reflect.InvocationHandler; import java.lang.reflect.Method; import java.lang.reflect.Proxy; import java.lang.reflect.UndeclaredThrowableException; public final class $Proxy0 extends Proxy implements Hello { private static Method m3; private static Method m1; private static Method m0; private static Method m2; public $Proxy0(InvocationHandler paramInvocationHandler) { super(paramInvocationHandler); } public final String say() { try { return (String)this.h.invoke(this, m3, null); } catch (Error|RuntimeException error) { throw null; } catch (Throwable throwable) { throw new UndeclaredThrowableException(throwable); } } public final boolean equals(Object paramObject) { try { return ((Boolean)this.h.invoke(this, m1, new Object[] { paramObject })).booleanValue(); } catch (Error|RuntimeException error) { throw null; } catch (Throwable throwable) { throw new UndeclaredThrowableException(throwable); } } public final int hashCode() { try { return ((Integer)this.h.invoke(this, m0, null)).intValue(); } catch (Error|RuntimeException error) { throw null; } catch (Throwable throwable) { throw new UndeclaredThrowableException(throwable); } } public final String toString() { try { return (String)this.h.invoke(this, m2, null); } catch (Error|RuntimeException error) { throw null; } catch (Throwable throwable) { throw new UndeclaredThrowableException(throwable); } } static { try { m3 = Class.forName(\u0026#34;com.proxy.Hello\u0026#34;).getMethod(\u0026#34;say\u0026#34;, new Class[0]); m1 = Class.forName(\u0026#34;java.lang.Object\u0026#34;).getMethod(\u0026#34;equals\u0026#34;, new Class[] { Class.forName(\u0026#34;java.lang.Object\u0026#34;) }); m0 = Class.forName(\u0026#34;java.lang.Object\u0026#34;).getMethod(\u0026#34;hashCode\u0026#34;, new Class[0]); m2 = Class.forName(\u0026#34;java.lang.Object\u0026#34;).getMethod(\u0026#34;toString\u0026#34;, new Class[0]); return; } catch (NoSuchMethodException noSuchMethodException) { throw new NoSuchMethodError(noSuchMethodException.getMessage()); } catch (ClassNotFoundException classNotFoundException) { throw new NoClassDefFoundError(classNotFoundException.getMessage()); } } } 我们可以看到 $Proxy0 类里面有一个跟 Hello 一样签名的 say() 方法，其中 this.h 绑定的是刚才传入的 JDKProxy 对象，所以当我们调用 Hello.say() 的时候，其实它是被转发到了 JDKProxy.invoke()。到这儿，整个魔术过程就透明了。\n实现过程\n其实在 Java 领域，除了 JDK 默认的 InvocationHandler 能完成代理功能，我们还有很多其他的第三方框架也可以，比如像 Javassist、Byte Buddy 这样的框架。\n单纯从代理功能上来看，JDK 默认的代理功能是有一定的局限性的，它要求被代理的类只能是接口。原因是因为生成的代理类会继承 Proxy 类，但 Java 是不支持多重继承的。\n这个限制在 RPC 应用场景里面还是挺要紧的，因为对于服务调用方来说，在使用 RPC 的时候本来就是面向接口来编程的，这个我们刚才在前面已经讨论过了。使用 JDK 默认的代理功能，最大的问题就是性能问题。它生成后的代理类是使用反射来完成方法调用的，而这种方式相对直接用编码调用来说，性能会降低，但好在 JDK8 及以上版本对反射调用的性能有很大的提升，所以还是可以期待一下的。\n相对 JDK 自带的代理功能，Javassist 的定位是能够操纵底层字节码，所以使用起来并不简单，要生成动态代理类恐怕是有点复杂了。但好的方面是，通过 Javassist 生成字节码，不需要通过反射完成方法调用，所以性能肯定是更胜一筹的。在使用中，我们要注意一个问题，通过 Javassist 生成一个代理类后，此 CtClass 对象会被冻结起来，不允许再修改；否则，再次生成时会报错。\nByte Buddy 则属于后起之秀，在很多优秀的项目中，像 Spring、Jackson 都用到了 Byte Buddy 来完成底层代理。相比 Javassist，Byte Buddy 提供了更容易操作的 API，编写的代码可读性更高。更重要的是，生成的代理类执行速度比 Javassist 更快。\n虽然以上这三种框架使用的方式相差很大，但核心原理却是差不多的，区别就只是通过什么方式生成的代理类以及在生成的代理类里面是怎么完成的方法调用。同时呢，也正是因为这些细小的差异，才导致了不同的代理框架在性能方面的表现不同。因此，我们在设计 RPC 框架的时候，还是需要进行一些比较的，具体你可以综合它们的优劣以及你的场景需求进行选择。\n思考题：如果没有动态代理帮我们完成方法调用拦截，用户该怎么完成 RPC 调用？\n这个问题我们可以参考下 gRPC 框架。gRPC 框架中就没有使用动态代理，它是通过代码生成的方式生成 Service 存根，当然这个 Service 存根起到的作用和 RPC 框架中的动态代理是一样的。\ngRPC 框架用代码生成的 Service 存根来代替动态代理主要是为了实现多语言的客户端，因为有些语言是不支持动态代理的，比如 C++、go 等，但缺点也是显而易见的。如果你使用过 gRPC，你会发现这种代码生成 Service 存根的方式与动态代理相比还是很麻烦的，并不如动态代理的方式使用起来方便、透明。\n06 | RPC实战：剖析gRPC源码，动手实现一个完整的RPC # gRPC 源码\ngRPC 是由 Google 开发并且开源的一款高性能、跨语言的 RPC 框架，当前支持 C、Java 和 Go 等语言，当前 Java 版本最新 Release 版为 1.27.0。gRPC 有很多特点，比如跨语言，通信协议是基于标准的 HTTP/2 设计的，序列化支持 PB（Protocol Buffer）和 JSON，整个调用示例如下图所示：\n如果你想快速地了解一个全新框架的工作原理，我个人认为最快的方式就是从使用示例开始，所以现在我们就以最简单的 HelloWord 为例开始了解。\n在这个例子里面，我们会定义一个 say 方法，调用方通过 gRPC 调用服务提供方，然后服务提供方会返回一个字符串给调用方。\n为了保证调用方和服务提供方能够正常通信，我们需要先约定一个通信过程中的契约，也就是我们在 Java 里面说的定义一个接口，这个接口里面只会包含一个 say 方法。在 gRPC 里面定义接口是通过写 Protocol Buffer 代码，从而把接口的定义信息通过 Protocol Buffer 语义表达出来。HelloWord 的 Protocol Buffer 代码如下所示：\nsyntax = \u0026#34;proto3\u0026#34;; option java_multiple_files = true; option java_package = \u0026#34;io.grpc.hello\u0026#34;; option java_outer_classname = \u0026#34;HelloProto\u0026#34;; option objc_class_prefix = \u0026#34;HLW\u0026#34;; package hello; service HelloService{ rpc Say(HelloRequest) returns (HelloReply) {} } message HelloRequest { string name = 1; } message HelloReply { string message = 1; } 有了这段代码，我们就可以为客户端和服务器端生成消息对象和 RPC 基础代码。我们可以利用 Protocol Buffer 的编译器 protoc，再配合 gRPC Java 插件（protoc-gen-grpc-java），通过命令行 protoc3 加上 plugin 和 proto 目录地址参数，我们就可以生成消息对象和 gRPC 通信所需要的基础代码。如果你的项目是 Maven 工程的话，你还可以直接选择使用 Maven 插件来生成同样的代码。\n发送原理\n生成完基础代码以后，我们就可以基于生成的代码写下调用端代码，具体如下：\npackage io.grpc.hello; import io.grpc.ManagedChannel; import io.grpc.ManagedChannelBuilder; import io.grpc.StatusRuntimeException; import java.util.concurrent.TimeUnit; public class HelloWorldClient { private final ManagedChannel channel; private final HelloServiceGrpc.HelloServiceBlockingStub blockingStub; /** * 构建Channel连接 **/ public HelloWorldClient(String host, int port) { this(ManagedChannelBuilder.forAddress(host, port) .usePlaintext() .build()); } /** * 构建Stub用于发请求 **/ HelloWorldClient(ManagedChannel channel) { this.channel = channel; blockingStub = HelloServiceGrpc.newBlockingStub(channel); } /** * 调用完手动关闭 **/ public void shutdown() throws InterruptedException { channel.shutdown().awaitTermination(5, TimeUnit.SECONDS); } /** * 发送rpc请求 **/ public void say(String name) { // 构建入参对象 HelloRequest request = HelloRequest.newBuilder().setName(name).build(); HelloReply response; try { // 发送请求 response = blockingStub.say(request); } catch (StatusRuntimeException e) { return; } System.out.println(response); } public static void main(String[] args) throws Exception { HelloWorldClient client = new HelloWorldClient(\u0026#34;127.0.0.1\u0026#34;, 50051); try { client.say(\u0026#34;world\u0026#34;); } finally { client.shutdown(); } } } 调用端代码大致分成三个步骤：\n首先用 host 和 port 生成 channel 连接； 然后用前面生成的 HelloService gRPC 创建 Stub 类； 最后我们可以用生成的这个 Stub 调用 say 方法发起真正的 RPC 调用，后续其它的 RPC 通信细节就对我们使用者透明了。 为了能看清楚里面具体发生了什么，我们需要进入到 ClientCalls.blockingUnaryCall 方法里面看下逻辑细节。但是为了避免太多的细节影响你理解整体流程，我在下面这张图中只画下了最重要的部分。\n我们可以看到，在调用端代码里面，我们只需要一行（第 48 行）代码就可以发起一个 RPC 调用，而具体这个请求是怎么发送到服务提供者那端的呢？这对于我们 gRPC 使用者来说是完全透明的，我们只要关注是怎么创建出 stub 对象的就可以了。\n比如入参是一个字符对象，gRPC 是怎么把这个对象传输到服务提供方的呢？只有二进制才能在网络中传输，但是目前调用端代码的入参是一个字符对象，那在 gRPC 里面我们是怎么把对象转成二进制数据的呢？\n回到上面流程图的第 3 步，在 writePayload 之前，ClientCallImpl 里面有一行代码就是 method.streamRequest(message)，看方法签名我们大概就知道它是用来把对象转成一个 InputStream，有了 InputStream 我们就很容易获得入参对象的二进制数据。这个方法返回值很有意思，就是为啥不直接返回我们想要的二进制数组，而是返回一个 InputStream 对象呢？你可以先停下来想下原因，我们会在最后继续讨论这个问题。(避免二次拷贝（序列化＋encode）)\n我们接着看 streamRequest 方法的拥有者 method 是个什么对象？我们可以看到 method 是 MethodDescriptor 对象关联的一个实例，而 MethodDescriptor 是用来存放要调用 RPC 服务的接口名、方法名、服务调用的方式以及请求和响应的序列化和反序列化实现类。\n大白话说就是，MethodDescriptor 是用来存储一些 RPC 调用过程中的元数据，而在 MethodDescriptor 里面 requestMarshaller 是在绑定请求的时候用来序列化方式对象的，所以当我们调用 method.streamRequest(message) 的时候，实际是调用 requestMarshaller.stream(requestMessage) 方法，而 requestMarshaller 里面会绑定一个 Parser，这个 Parser 才真正地把对象转成了 InputStream 对象。\n讲完序列化在 gRPC 里面的应用后，我们再来看下在 gRPC 里面是怎么完成请求数据“断句”的，就是那个问题——二进制流经过网络传输后，怎么正确地还原请求前语义？\n我们在 gRPC 文档中可以看到，gRPC 的通信协议是基于标准的 HTTP/2 设计的，而 HTTP/2 相对于常用的 HTTP/1.X 来说，它最大的特点就是多路复用、双向流，该怎么理解这个特点呢？这就好比我们生活中的单行道和双行道，HTTP/1.X 就是单行道，HTTP/2 就是双行道。\n那既然在请求收到后需要进行请求“断句”，那肯定就需要在发送的时候把断句的符号加上，我们看下在 gRPC 里面是怎么加的？\n因为 gRPC 是基于 HTTP/2 协议，而 HTTP/2 传输基本单位是 Frame，Frame 格式是以固定 9 字节长度的 header，后面加上不定长的 payload 组成，协议格式如下图所示：\n那在 gRPC 里面就变成怎么构造一个 HTTP/2 的 Frame 了。\n现在回看我们上面那个流程图的第 4 步，在 write 到 Netty 里面之前，我们看到在 MessageFramer.writePayload 方法里面会间接调用 writeKnownLengthUncompressed 方法，该方法要做的两件事情就是构造 Frame Header 和 Frame Body，然后再把构造的 Frame 发送到 NettyClientHandler，最后将 Frame 写入到 HTTP/2 Stream 中，完成请求消息的发送。\n接收原理\n讲完 gRPC 的请求发送原理，我们再来看下服务提供方收到请求后会怎么处理？我们还是接着前面的那个例子，先看下服务提供方代码，具体如下：\nstatic class HelloServiceImpl extends HelloServiceGrpc.HelloServiceImplBase { @Override public void say(HelloRequest req, StreamObserver\u0026lt;HelloReply\u0026gt; responseObserver) { HelloReply reply = HelloReply.newBuilder().setMessage(\u0026#34;Hello \u0026#34; + req.getName()).build(); responseObserver.onNext(reply); responseObserver.onCompleted(); } } 上面 HelloServiceImpl 类是按照 gRPC 使用方式实现了 HelloService 接口逻辑，但是对于调用者来说并不能把它调用过来，因为我们没有把这个接口对外暴露，在 gRPC 里面我们是采用 Build 模式对底层服务进行绑定，具体代码如下：\npackage io.grpc.hello; import io.grpc.Server; import io.grpc.ServerBuilder; import io.grpc.stub.StreamObserver; import java.io.IOException; public class HelloWorldServer { private Server server; /** * 对外暴露服务 **/ private void start() throws IOException { int port = 50051; server = ServerBuilder.forPort(port) .addService(new HelloServiceImpl()) .build() .start(); Runtime.getRuntime().addShutdownHook(new Thread() { @Override public void run() { HelloWorldServer.this.stop(); } }); } /** * 关闭端口 **/ private void stop() { if (server != null) { server.shutdown(); } } /** * 优雅关闭 **/ private void blockUntilShutdown() throws InterruptedException { if (server != null) { server.awaitTermination(); } } public static void main(String[] args) throws IOException, InterruptedException { final HelloWorldServer server = new HelloWorldServer(); server.start(); server.blockUntilShutdown(); } } 服务对外暴露的目的是让过来的请求在被还原成信息后，能找到对应接口的实现。在这之前，我们需要先保证能正常接收请求，通俗地讲就是要先开启一个 TCP 端口，让调用方可以建立连接，并把二进制数据发送到这个连接通道里面，这里依然只展示最重要的部分。\n这四个步骤是用来开启一个 Netty Server，并绑定编解码逻辑的，如果你暂时看不懂，没关系的，我们可以先忽略细节。我们重点看下 NettyServerHandler 就行了，在这个 Handler 里面会绑定一个 FrameListener，gRPC 会在这个 Listener 里面处理收到数据请求的 Header 和 Body，并且也会处理 Ping、RST 命令等，具体流程如下图所示：\n在收到 Header 或者 Body 二进制数据后，NettyServerHandler 上绑定的 FrameListener 会把这些二进制数据转到 MessageDeframer 里面，从而实现 gRPC 协议消息的解析。\n那你可能会问，这些 Header 和 Body 数据是怎么分离出来的呢？按照我们前面说的，调用方发过来的是一串二进制数据，这就是我们前面开启 Netty Server 的时候绑定 Default HTTP/2FrameReader 的作用，它能帮助我们按照 HTTP/2 协议的格式自动切分出 Header 和 Body 数据来，而对我们上层应用 gRPC 来说，它可以直接拿拆分后的数据来用。\n思考题：在 gRPC 调用的时候，我们有一个关键步骤就是把对象转成可传输的二进制，但是在 gRPC 里面，我们并没有直接转成二进制数组，而是返回一个 InputStream，你知道这样做的好处是什么吗？\nRPC 调用在底层传输过程中也是需要使用 Stream 的，直接返回一个 InputStream 而不是二进制数组，可以避免数据的拷贝。\n","date":"15 May 2024","permalink":"/posts/architecture/distributed/rpc/rpc-%E5%AE%9E%E6%88%98%E4%B8%8E%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86/%E5%9F%BA%E7%A1%80%E7%AF%87/","section":"博客","summary":"RPC 实战与核心原理 - 基础篇","title":"RPC 实战与核心原理 - 基础篇"},{"content":"avx指令集 网络带宽 kubevirt-crd\n代码仓库：https://github.com/WFUing/ap-vms-ovs\n准备vm # 安装qemu和libvirt、virt-manager， 准备ubuntu镜像，图形化安装即可 配置环境 # 安装Go环境\n有两种方法\nGo 官网推荐的方法 rm -rf /usr/local/go \u0026amp;\u0026amp; curl -OL https://golang.org/dl/go1.22.2.linux-amd64.tar.gz \u0026amp;\u0026amp; tar -C /usr/local -xzf go1.22.2.linux-amd64.tar.gz sudo vim $HOME/.profile ，添加 export PATH=$PATH:/usr/local/go/bin source $HOME/.profile APT 安装 sudo apt install golang-go，可以直接安装，安装的cache在/var/lib/apt，项目目录在/usr/local，具体没有考证过，已使用第一种方法安装了 go version\n安装 nodejs\ninstall volta\n# install Volta curl https://get.volta.sh | bash # install Node volta install node # start using Node node install node\nvolta install node 配置protobuf\nsudo apt-get install pkg-config libczmq-dev cd proto npm install bash protobuf-gen.sh 配置libtensorflow\n# run with root privileges curl -L https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-2.15.0.tar.gz | sudo tar xz --directory /usr/local ldconfig 配置gocv\nhttps://github.com/hybridgroup/gocv https://docs.opencv.org/4.x/d7/d9f/tutorial_linux_install.html git clone https://github.com/hybridgroup/gocv.git cd gocv make install cd /usr/local/go/src/gocv.io/x/gocv go run ./cmd/version/main.go 输出\ngocv version: 0.36.1 opencv lib version: 4.8.0 配置虚拟webcam # 在 Ubuntu 上创建一个虚拟的 webcam 可以通过多种方式实现，但一个比较流行的方法是使用 v4l2loopback 模块，这是一个能够创建虚拟视频设备的 Linux 内核模块。这样的虚拟设备可以用于测试、视频录制、直播或作为真实 webcam 的替代。以下是如何在 Ubuntu 上安装和配置 v4l2loopback 来创建虚拟 webcam 的步骤：\nsudo apt-get update sudo apt-get install v4l2loopback-dkms 创建设备\nsudo modprobe v4l2loopback devices=1 video_nr=10 card_label=\u0026#34;Virtual Webcam\u0026#34; exclusive_caps=1 以图片作为输入\nffmpeg -loop 1 -re -i /media/wds/zhitai/images.jpeg -f v4l2 -vcodec rawvideo -pix_fmt yuv420p /dev/video10 验证webcam输出\nffplay /dev/video10 # 或 fswebcam -d /dev/video0 output.jpg 设置开机启动服务\n配置 v4l2loopback 模块自动加载\n创建模块配置文件\nsudo vim /etc/modules-load.d/v4l2loopback.conf 添加模块加载指令，在打开的文件中输入以下内容\nv4l2loopback 设置模块参数\nsudo vim /etc/modprobe.d/v4l2loopback-options.conf 在文件中添加以下内容来指定你的参数\noptions v4l2loopback devices=1 video_nr=10 card_label=\u0026#34;Virtual Webcam\u0026#34; exclusive_caps=1 创建一个 systemd 服务来播放视频\n创建一个新的 systemd 服务文件\nsudo vim /etc/systemd/system/virtual-webcam.service 编辑并保存服务文件，在服务文件中添加以下内容\n[Unit] Description=Start Virtual Webcam After=network.target [Service] ExecStart=/usr/bin/ffmpeg -stream_loop -1 -re -i /home/ubuntu/video.mp4 -map 0:v -vcodec mjpeg -q:v 2 -f v4l2 /dev/video10 Restart=always RestartSec=5 [Install] WantedBy=multi-user.target -vcodec mjpeg: 指定视频编解码器为 MJPEG。 -q:v 2: 设置视频质量，数值范围通常是 2-31，数值越小质量越高。你可以根据需要调整这个值以获得最佳质量和性能平衡。 -stream_loop -1: 使视频文件循环播放无限次。 -re: 按照原始帧率读取文件，模拟实时数据流。 -i /home/ubuntu/video.mp4: 指定输入文件。 -map 0✌️ 从输入文件中选择视频流。 -vcodec mjpeg: 使用 MJPEG 编解码器，这是一种每帧都完整压缩的视频流格式，基于 JPEG。 -q:v 2: 设置 JPEG 压缩质量，数值越低质量越高。 -f v4l2: 指定输出格式为 Video4Linux2。 /dev/video0: 指定输出设备。 启用并启动服务\nsudo systemctl enable virtual-webcam.service sudo systemctl start virtual-cam.service sudo systemctl restart virtual-cam.service 重启即可\nreboot 虚拟机扩容 # 遇到了再说吧\n在宿主机\n检查原始镜像大小：\nqemu-img info ubuntu24.04-x86-64.qcow2 扩展镜像大小：\nqemu-img resize ubuntu24.04-x86-64.qcow2 +30G 扩展镜像文件后，你需要在镜像的文件系统内部也进行扩展以利用新增的空间。这通常需要挂载镜像并使用文件系统专用的工具，比如 resize2fs（对于 ext3/ext4 文件系统）。\n进入虚拟机\n查找可用的物理存储设备，使用 lsblk 或 fdisk -l 查找未分配的磁盘或分区。\n查看 LVM 配置和状态\nsudo pvs # 查看物理卷 sudo vgs # 查看卷组 sudo lvs # 查看逻辑卷 调整卷组和逻辑卷大小\nsudo lvextend -L +10G /dev/mapper/vgname-lvname # 或者扩展到所有可用空间 sudo lvextend -l +100%FREE /dev/mapper/vgname-lvname 配置 open-vswitch # 力荐 使用Open-vSwitch创建虚拟网络这篇博文，写的很全\n在宿主机上下载\nsudo apt install openvswitch-switch systemctl start openvswitch-switch.service systemctl enable openvswitch-switch.service 这边列出 ovs-vsctl 的常用命令\n添加网桥：ovs-vsctl add-br br0 列出所有网桥：ovs-vsctl list-br 判断网桥是否存在：ovs-vsctl br-exists br0 将物理网卡挂接到网桥：ovs-vsctl add-port br0 eth0 列出网桥中的所有端口：ovs-vsctl list-ports br0 列出所有挂接到网卡上的网桥：ovs-vsctl port-to-br eth0 查看OVS 状态：ovs-vsctl show 查看OVS 的所有Interface、Port 等：ovs-vsctl list (Interface|Port) 或 ovs-vsctl list Port ens37 删除网桥上已经挂接的网口：vs-vsctl del-port br0 eth0 删除网桥：ovs-vsctl del-br br0 配置无密码sudo\n如果你需要频繁地运行需要超级用户权限的命令（如 ip, ovs-vsctl, ovs-ofctl），有几种方法可以方便地管理这些权限\n通过 which \u0026lt;指令名\u0026gt; 查找指令目录\n# 使用 visudo 命令以安全方式编辑 sudoers 文件 sudo visudo # 在打开的文件中添加以下行，替换 \u0026#39;username\u0026#39; 为你的用户名 username ALL=(ALL) NOPASSWD: ALL # 或者为特定命令配置无密码 sudo： username ALL=(ALL) NOPASSWD: /usr/sbin/ip, /usr/bin/ovs-vsctl, /usr/bin/ovs-ofctl 启用 Linux 系统的 IP 转发功能\nsudo sysctl -w net.ipv4.ip_forward=1 echo \u0026#34;net.ipv4.ip_forward = 1\u0026#34; | sudo tee -a /etc/sysctl.conf 创建ovs网桥\nif ! sudo ovs-vsctl br-exists ovsbr0; then sudo ovs-vsctl add-br ovsbr0 fi # 删除网桥 sudo ovs-vsctl del-br ovsbr0 配置ovs网桥的ip地址\nsudo ip addr add 192.168.100.1/24 dev ovsbr0 sudo ip addr del 192.168.100.1/24 dev ovsbr0 将虚拟机加入到网桥以后，可以在先查看下端口信息 sudo ovs-vsctl show。\n若端口已经加入，则可以实现选择在虚拟机里设置静态ip或者动态ip\n手动配置 IP 地址\n如果你的网络环境支持静态 IP 或者你已经有指定的 IP 地址范围，你可以手动为 ens3 设置 IP 地址。在虚拟机中执行以下命令：\n很多时候的问题都是接口没有激活，得先up一下\nsudo ip addr add 192.168.100.10/24 dev ens3 sudo ip link set ens3 up 此外，你还需要设置默认网关（通常是你的网络的路由器地址）：\nsudo ip route add default via 192.168.100.1 可以写成开机自启动，开机的时候挂载一个hdd的iso\n使用 vim 创建脚本文件\nsudo vim /usr/local/bin/mount_and_apply_netplan.sh #!/bin/bash mount /dev/sr0 /mnt cp /mnt/network-config /etc/netplan/01-netcfg.yaml netplan apply 使脚本可执行\nsudo chmod +x /usr/local/bin/mount_and_apply_netplan.sh 创建 systemd 服务单元文件\nsudo vim /etc/systemd/system/mount_netplan.service [Unit] Description=Mount sr0 and apply netplan configuration After=network.target [Service] Type=oneshot ExecStart=/usr/local/bin/mount_and_apply_netplan.sh RemainAfterExit=true [Install] WantedBy=multi-user.target 启用并启动服务\nsudo systemctl daemon-reload sudo systemctl enable mount_netplan.service sudo systemctl start mount_netplan.service 使用 DHCP 自动获取 IP 地址\n在宿主机上安装dhcp服务器\nsudo apt update sudo apt install isc-dhcp-server 配置 DHCP 服务器\n编辑 DHCP 配置文件 /etc/dhcp/dhcpd.conf，添加适用于你的网络环境的配置。例如，如果你的 bridge 接口名为 ovsbr0，且想要分配的 IP 范围在 192.168.100.50 到 192.168.100.99 之间：\nsudo vim /etc/dhcp/dhcpd.conf 添加内容\nsubnet 192.168.100.0 netmask 255.255.255.0 { range 192.168.100.50 192.168.100.99; option routers 192.168.100.1; option subnet-mask 255.255.255.0; option domain-name-servers 8.8.8.8, 8.8.4.4; default-lease-time 600; max-lease-time 7200; } 指定 DHCP 服务的网络接口\nsudo vim /etc/default/isc-dhcp-server 添加或修改以下行，指定你的 bridge 接口：\nINTERFACES=\u0026#34;ovsbr0\u0026#34; 启动 DHCP 服务\nsudo systemctl restart isc-dhcp-server sudo systemctl enable isc-dhcp-server 配置虚拟机\n在新版的ubuntu中，Netplan 是默认的网络配置工具：\n找到 Netplan 的配置文件，通常在 /etc/netplan/ 目录下。打开这个文件进行编辑：\nsudo vim /etc/netplan/01-netcfg.yaml 确保配置如下，以允许 ens3 接口使用 DHCP：\nnetwork: version: 2 renderer: networkd ethernets: ens3: dhcp4: true 应用配置更改\nsudo netplan apply 老版需要修改 /etc/network/interfaces\nsudo vim /etc/network/interfaces 确保有以下行以启用 DHCP：\nauto ens3 iface ens3 inet dhcp 重启网络服务以应用更改：\nsudo systemctl restart networking 配置开机设置网络的服务\n名称为 wfuing_network_init\nsudo vim /etc/wfuing_network_init.sh 在编辑器中输入以下内容：\n#!/bin/bash mkdir -p /mnt mount /dev/sr0 /mnt cp /mnt/network-config /etc/netplan/01-netcfg.yaml netplan apply 给脚本执行权限：\nsudo chmod +x /etc/wfuing_network_init.sh 创建 systemd 服务文件：\nsudo vim /etc/systemd/system/wfuing_network_init.service 在编辑器中添加以下内容：\n[Unit] Description=Initialize network settings at startup [Service] Type=oneshot ExecStart=/etc/wfuing_network_init.sh RemainAfterExit=yes [Install] WantedBy=multi-user.target 启用并启动服务：\nsudo systemctl enable wfuing_network_init.service sudo systemctl start wfuing_network_init.service 限制端口流量\n定义 QoS 和队列\n# 创建 QoS 规则并同时定义两个队列 sudo ovs-vsctl -- --id=@qos create QoS type=linux-htb other-config:max-rate=250000 queues:1=@q1 queues:2=@q2 -- \\ --id=@q1 create Queue other-config:min-rate=160000 other-config:max-rate=250000 -- \\ --id=@q2 create Queue other-config:min-rate=160000 other-config:max-rate=200000 \\ -- set Port YOUR-BRIDGE-PORT qos=@qos 在这个例子中：\nYOUR-BRIDGE-PORT 是需要应用带宽限制的端口名称。 max-rate 是最大速率限制，单位是 bps（比特每秒），这里 250000 对应250kbps。 min-rate 是最小速率限制，同样单位是 bps，这里 160000 对应160kbps。 创建了两个队列：@q1 用于 Multi-tone（限制为250kbps），@q2 用于 Single-tone（限制为200kbps）。 应用 QoS 到特定端口，修改上面的命令中的 YOUR-BRIDGE-PORT 来指定正确的端口。\n完成配置后，可以使用以下命令来检查 QoS 设置是否已正确应用：\nsudo ovs-vsctl list qos sudo ovs-vsctl list queue libvirt操作 # 查看和管理虚拟机 列出所有虚拟机: virsh list --all 启动虚拟机: virsh start \u0026lt;vm_name\u0026gt; 关闭虚拟机: virsh shutdown \u0026lt;vm_name\u0026gt; 强制关闭虚拟机 (类似于断电): virsh destroy \u0026lt;vm_name\u0026gt; 重启虚拟机: virsh reboot \u0026lt;vm_name\u0026gt; 暂停虚拟机: virsh suspend \u0026lt;vm_name\u0026gt; 恢复暂停的虚拟机: virsh resume \u0026lt;vm_name\u0026gt; 管理虚拟机快照 创建快照: virsh snapshot-create-as \u0026lt;vm_name\u0026gt; \u0026lt;snapshot_name\u0026gt; 列出所有快照: virsh snapshot-list \u0026lt;vm_name\u0026gt; 恢复快照: virsh snapshot-revert \u0026lt;vm_name\u0026gt; \u0026lt;snapshot_name\u0026gt; 删除快照: virsh snapshot-delete \u0026lt;vm_name\u0026gt; \u0026lt;snapshot_name\u0026gt; 配置和资源管理 查看虚拟机配置: virsh dumpxml \u0026lt;vm_name\u0026gt; 编辑虚拟机配置: virsh edit \u0026lt;vm_name\u0026gt; 设置虚拟机自动启动: virsh autostart \u0026lt;vm_name\u0026gt; 取消虚拟机自动启动: virsh autostart --disable \u0026lt;vm_name\u0026gt; 网络管理 列出所有网络: virsh net-list --all 启动一个网络: virsh net-start \u0026lt;network_name\u0026gt; 停止一个网络: virsh net-destroy \u0026lt;network_name\u0026gt; 创建网络: virsh net-create \u0026lt;xml_file\u0026gt; 编辑网络配置: virsh net-edit \u0026lt;network_name\u0026gt; 存储管理 列出所有存储池: virsh pool-list --all 创建存储池: virsh pool-create \u0026lt;xml_file\u0026gt; 删除存储池: virsh pool-destroy \u0026lt;pool_name\u0026gt; 查看存储池信息: virsh pool-info \u0026lt;pool_name\u0026gt; 如果要在启动一个pool路径下的虚拟机\n需要给 pool 路径上所有的文件夹目录赋权限，才能使用\nsudo chown -R libvirt-qemu:kvm /home/wfuing/test/images/ sudo chmod -R 775 /home/wfuing/test/images/ sudo chmod -R 775 /home/wfuing/test sudo chmod -R 775 /home/wfuing # 将一个用户加入组 sudo gpasswd -a username groupname # 查看用户的群组信息 id username # 列出群组成员 getent group groupname virsh console\n如果要 连接到console，需要在配置文件中先加上\n\u0026lt;serial type=\u0026#39;pty\u0026#39;\u0026gt; \u0026lt;target port=\u0026#39;0\u0026#39;/\u0026gt; \u0026lt;/serial\u0026gt; \u0026lt;console type=\u0026#39;pty\u0026#39;\u0026gt; \u0026lt;target type=\u0026#39;serial\u0026#39; port=\u0026#39;0\u0026#39;/\u0026gt; \u0026lt;/console\u0026gt; 然后在虚拟机中启动\nsystemctl enable serial-getty@ttyS0.service systemctl start serial-getty@ttyS0.service resource \u0026#34;null_resource\u0026#34; \u0026#34;enable_ip_forwarding\u0026#34; { provisioner \u0026#34;local-exec\u0026#34; { command = \u0026lt;\u0026lt;EOF sudo sysctl -w net.ipv4.ip_forward=1 echo \u0026#34;net.ipv4.ip_forward = 1\u0026#34; | sudo tee -a /etc/sysctl.conf EOF } } # 确保创建桥接和设置桥接 IP 地址的资源在启用 IP 转发之后执行 resource \u0026#34;null_resource\u0026#34; \u0026#34;create_ovs_bridge\u0026#34; { depends_on = [null_resource.enable_ip_forwarding] provisioner \u0026#34;local-exec\u0026#34; { command = \u0026lt;\u0026lt;EOF if ! sudo ovs-vsctl br-exists ovsbr0; then sudo ovs-vsctl add-br ovsbr0 fi EOF } provisioner \u0026#34;local-exec\u0026#34; { when = destroy command = \u0026#34;sudo ovs-vsctl del-br ovsbr0\u0026#34; } } resource \u0026#34;null_resource\u0026#34; \u0026#34;setup_bridge_ip\u0026#34; { depends_on = [null_resource.create_ovs_bridge] provisioner \u0026#34;local-exec\u0026#34; { command = \u0026#34;sudo ip addr add 192.168.100.1/24 dev ovsbr0\u0026#34; } provisioner \u0026#34;local-exec\u0026#34; { when = destroy command = \u0026#34;sudo ip addr del 192.168.100.1/24 dev ovsbr0\u0026#34; } } data \u0026#34;template_file\u0026#34; \u0026#34;user_data\u0026#34; { count = var.vm_count template = file(\u0026#34;${path.module}/user_data.yaml\u0026#34;) vars = { name = \u0026#34;vm-${count.index}\u0026#34; } } 运行 Actor # go run head/main.go head --addr \u0026lt;head_addr\u0026gt; --port \u0026lt;head_port\u0026gt; go run worker/main.go --addr \u0026lt;worker_addr\u0026gt; --remote_addr \u0026lt;head_addr\u0026gt; --remote_port \u0026lt;head_port\u0026gt; ","date":"8 May 2024","permalink":"/posts/architecture/virtualization/ap-vm/","section":"博客","summary":"actor-platform 智能部署","title":"actor-platform 智能部署"},{"content":"安装 Anaconda # Anaconda 简化了 Ubuntu 上 Python 及其相关库的安装和管理。安装 Anaconda 非常简单，首先使用以下命令更新本地包管理器：\nsudo apt update 更新完成后，继续执行以下部分中说明的步骤。\n第 1 步：下载最新版本的 Anaconda\n要下载 Anaconda，请转至 https://www.anaconda.com/download/success ，\n记下您要下载的版本的 URL，然后执行以下步骤：\n导航到终端中的/tmp目录。该目录通常用于临时文件存储。 cd /tmp 使用curl 命令下载安装程序： curl -O [URL] 该 -O 参数指示以与 URL curl 中的文件相同的名称保存下载的文件。将 URL 替换为开发人员页面中的 URL。在这种情况下，它是：\ncurl -O https://repo.anaconda.com/archive/Anaconda3-2024.02-1-Linux-x86_64.sh 我用的是图形化界面，可以直接点击下载。\ncurl 如果您的系统上尚未安装，请运行 sudo apt install curl\n使用校验和工具验证下载文件在下载过程中没有被更改或损坏： sha256sum Anaconda3-2024.02-1-Linux-x86_64.sh 将输出与Anaconda 文档中相应的校验和（或散列）进行比较。\n如果校验和相同，则下载的 Anaconda 安装程序文件与官方来源提供的原始文件相同。此验证过程确认文件在下载过程中没有被篡改或损坏。\n第2步：运行Anaconda安装脚本\nAnaconda 安装程序是一个bash 脚本。因此，运行 Anaconda 安装程序会执行一系列处理安装过程的bash 命令和脚本。\n要安装 Anaconda，请执行以下步骤：\n运行bash脚本： bash Anaconda3-2024.02-1-Linux-x86_64.sh 出现许可协议。使用 Enter 查看协议并 yes 在底部键入以表示同意。 安装程序提示用户接受默认位置或安装到其他位置。使用默认路径，除非您有特定需要更改它。 安装完成，确定启动时是否自动初始化conda。yes除非您有特殊原因不这样做，否则请在提示后键入。 安装完成后，关闭并重新打开 shell 以确认更改生效。默认情况下，Anaconda 基础环境处于激活状态，这意味着它被设置为 shell 中的活动 Python 环境。\n第 3 步：激活并测试安装\n在 Anaconda 安装过程中，安装程序会向.bashrc文件添加行以更新系统的 PATH 变量。要使更改生效，请运行：\nsource ~/.bashrc 该命令没有输出。使用 conda 命令测试安装：\nconda info 如果该命令返回有关 conda 安装的信息且没有任何错误，则表明 Anaconda 已在系统上正确安装和配置。\n更新 Anaconda # 更新 Anaconda 可确保安装最新的包管理器和分发版本。要在 Ubuntu 上更新 Anaconda，请执行以下步骤：\n更新conda包管理器： conda update conda 该命令更新到包管理器的最新版本，包括错误修复、性能改进和新功能。\n使用以下命令更新 Anaconda 发行版： conda update anacondaCopied! 这会将 Anaconda 发行版中包含的所有软件包更新为最新版本。\n卸载 Anaconda # 如果您不再使用 Anaconda，请执行以下步骤将其卸载：\n安装anaconda clean软件包： conda install anaconda-clean Anaconda clean 是一个清理与 Anaconda 安装相关的不必要文件和目录的工具。\n使用以下命令删除缓存、日志和临时文件： anaconda-clean 对于每个提示，键入 y 或 n\n使用以下命令删除整个 Anaconda 目录： rm -rf ~/anaconda3 该命令没有输出，但会删除 Anaconda 安装目录及其所有内容，包括文件、目录和子目录。此动作不可逆转。\n通过编辑.bashrc文件从 PATH 中删除 Anaconda 。为此，请打开选择的文本编辑器。 vim ~/.bashrc 向下滚动到文件末尾并删除 Anaconda 块。保存并退出文件。\n这四个步骤将从系统中删除 Anaconda。\n创建虚拟环境 # 在Ubuntu系统上使用Anaconda创建虚拟环境，可以按照以下步骤进行操作：\n打开终端，输入以下命令进入Anaconda环境： source ~/anaconda3/bin/activate 创建一个新的虚拟环境，例如，创建一个名为“myenv”的环境，使用以下命令： conda create --name myenv # 在创建虚拟环境时，可以指定Python版本，例如，使用以下命令创建Python 3.8版本的环境 conda create --name myenv python=3.8 激活虚拟环境，使用以下命令： conda activate myenv 在虚拟环境中安装所需的软件包和库，例如，安装 numpy 和 pandas，可以使用以下命令： # 激活虚拟环境后，使用pip安装的软件包将被安装到虚拟环境中。如果要在虚拟环境中安装conda软件包，请使用conda install命令 conda install numpy pandas 完成后，可以使用以下命令退出虚拟环境： conda deactivate 删除虚拟环境，可以使用以下命令： conda remove --name myenv --all ","date":"2 May 2024","permalink":"/posts/skills/ubuntu-anaconda/","section":"博客","summary":"Ubuntu 上的 Anaconda 配置。","title":"Ubuntu-Anaconda"},{"content":"","date":"2 May 2024","permalink":"/posts/skills/","section":"博客","summary":"你的生产力来自于生产力工具，如何提高效率是一个难题。","title":"生产力工具使用小技巧"},{"content":"查看cpu架构\nARCH=$(uname -s | tr A-Z a-z)-$(uname -m | sed \u0026#39;s/x86_64/amd64/\u0026#39;) || windows-amd64.exe echo ${ARCH} Github 上的clash已经下架了，现在可以在这个网站下载\nhttps://www.clash.la/releases/ 命令行安装 # 下载clash\nsudo snap install clash 一般不是最新版本 去release下载对应版本的clash，我这里是 clash-linux-amd64-v1.18.0 很奇怪，直接用wget报错\n可以先下载再上传\nrsync -avz /path/to/local/folder username@remote_host:/path/to/remote/folder rsync 是一个非常强大的文件和文件夹复制工具，常用于备份和镜像操作，它支持本地和远程文件传输。\n-a（archive）：这个选项是指以归档模式传输，它保留符号链接、文件权限、用户组信息、时间戳等。 -v（verbose）：详细模式，输出更多的信息。 -z（compress）：在传输过程中进行数据压缩，这可以提高传输速度，尤其是在带宽受限的情况下。 /path/to/local/folder：本地文件夹的路径。 username@remote_host：远程服务器的用户名和地址。 /path/to/remote/folder：远程服务器上目的地文件夹的路径。 gunzip clash-linux-amd64-v1.18.0.gz mv clash-linux-amd64-v1.18.0 clash 然后就可以直接\nchmod +x clash ./clash -d iggfeed ├── clash ├── configs │ ├── i2ec │ │ ├── cache.db │ │ ├── config.yaml │ │ └── Country.mmdb │ ├── iggfeed │ │ ├── cache.db │ │ ├── config.yaml │ │ └── Country.mmdb │ ├── iggfeed.yaml │ └── proxy.i2ec.top.yaml docker 安装 # version: \u0026#39;3\u0026#39; services: # Clash clash: image: dreamacro/clash:latest container_name: clash volumes: - /home/wds/clash/configs/iggfeed.yaml:/root/.config/clash/config.yaml ports: - \u0026#34;7890:7890/tcp\u0026#34; - \u0026#34;7890:7890/udp\u0026#34; - \u0026#34;9090:9090\u0026#34; restart: unless-stopped clash-dashboard: image: hitian/clash-dashboard container_name: clash-dashboard ports: - \u0026#34;7880:80\u0026#34; restart: unless-stopped 如果要使用clash-dashboard，则必须将clash的控制接口9090端口开放访问。\n请检查订阅的配置文件中external-controller这一项是0.0.0.0:9090，否则dashboard无法获取和控制clash配置信息，一般这里默认是127.0.0.1\n添加控制配置密码（用于dashboard）\n# 在external-controller下一行 secret: \u0026#39;你的密码\u0026#39; # 直接使用 Docker 官方的 Compose 插件 DOCKER_CONFIG=${DOCKER_CONFIG:-$HOME/.docker} mkdir -p $DOCKER_CONFIG/cli-plugins curl -SL https://github.com/docker/compose/releases/download/v2.2.3/docker-compose-linux-x86_64 -o $DOCKER_CONFIG/cli-plugins/docker-compose chmod +x $DOCKER_CONFIG/cli-plugins/docker-compose docker compose up --build -d 或者\nv2ray下载 #!/bin/bash docker run -d \\ --restart=always \\ --privileged \\ --network=host \\ --name v2raya \\ -e V2RAYA_LOG_FILE=/tmp/v2raya.log \\ -e V2RAYA_V2RAY_BIN=/usr/local/bin/v2ray \\ -v /lib/modules:/lib/modules:ro \\ -v /etc/resolv.conf:/etc/resolv.conf \\ -v /etc/v2raya:/etc/v2raya \\ mzz2017/v2raya 图形化界面安装 # 一般个人电脑请选择 x64-linux\n解压缩\ntar -xzvf Clash.for.Windows-0.20.39-x64-linux.tar.gz 命令行工具 tar 非常适合处理 .tar.gz（也称为 tarball）格式的压缩文件。\nx：表示解压缩。 z：表示处理通过 gzip 进行压缩的数据。 v：表示在解压缩过程中显示详细输出（verbose）。 f：表示后面跟着的是文件名。 可以使用 -C 选项，把文件解压到特定的目录。\n进入到解压后的文件夹，在当前终端运行命令\n也可以直接双击 cfw 文件，即可打开\n设置订阅链接\n这个就是八仙过海，各显神通了！\n系统配置代理 # 配置浏览器代理（使用 SwitchyOmega）\nSwitchyOmega 是一款流行的浏览器扩展，用于快速切换代理设置。它主要用于 Google Chrome 和 Firefox 浏览器。以下是如何在浏览器中安装和配置 SwitchyOmega：\n安装 SwitchyOmega:\nChrome: 访问 Chrome 网上应用店，搜索 “SwitchyOmega” 并安装。 Firefox: 访问 Firefox Add-ons 网站，搜索 “SwitchyOmega” 并安装。 系统设置\n打开设置-网络-网络代理（齿轮），将http/https代理指向本机的clash默认端口7890（clash界面启动页可以修改Port）\n","date":"1 May 2024","permalink":"/posts/skills/ubuntu-clash/","section":"博客","summary":"ubuntu上配置clash","title":"ubuntu-配置clash"},{"content":" Terraform + libvirt + Open vSwitch terraform-provider-openvswitch 使用Open-vSwitch创建虚拟网络 Understanding Open vSwitch: Part 1 Understanding Open vSwitch: Part 2 Open vSwitch with KVM SPICE ","date":"30 April 2024","permalink":"/posts/architecture/virtualization/ovs/","section":"博客","summary":"Open vSwitch","title":"Open vSwitch"},{"content":"Terraform 是基础设施配置自动化方面的首选 IaC 工具。尽管 Ansible 主要是一个配置工具，但它也可以完成这项工作。\nKVM 是一个开源 I 类管理程序，它使用 Linux 内核和 libvirt 库进行虚拟化。高性能、低成本和 Linux 原生特性使其成为众多产品中的一个不错的选择。还有基于这些优势构建的企业级虚拟化软件产品，例如Red Hat Virtualization和Proxmox。\nTerraform 设计有支持资源创建的提供程序和模块。 terraform-libvirt-provider\n该场景由一个具有三层的典型环境组成。第一个是应用程序负载均衡器层，第二个是应用程序层，第三个是数据库层。无需在虚拟化软件 UI 上通过鼠标单击来配置所有三层，而是开发动态设置，其中对虚拟机的所有必需规格（例如 CPU、内存和网络配置）进行参数化。\n为了简单起见，该模拟没有部署应用程序。使用 cloudinit 资源就可以很容易地拥有一个。然而，在配置后，Ansible 可能是完成此任务的更好选择。\n使用 Terraform 等 IaC 工具的主要优点是，无论数量多少，上述拓扑中的虚拟机的配置时间都不到 30 秒。此外，任何层中的任何虚拟机都可以通过更改几个变量来添加或删除。此外，现有虚拟机的规格可以以同样的实用性进行更改。下面提供了实时屏幕截图以供说明。\n录屏\ngithub - terraform-kvm Resources\n在 Libvirt 和 KVM 上使用 Terraform 进行自动配置 — DevOps 日记 使用 Terraform 进行 AWS ASG-ALB-EC2 部署 — DevOps 日记 Ansible 与 Terraform 揭秘 ","date":"29 April 2024","permalink":"/posts/architecture/virtualization/terraform-libvirt-kvm/","section":"博客","summary":"无论是公共云还是本地环境，当今的企业基础设施比以往任何时候都需要更多的自动化。幸运的是，Terraform 和 Ansible 等基础设施即代码 (IaC) 工具为这两者提供了解决方案。本文提供了一个场景，其中存在基于 KVM 的本地虚拟环境并利用 Terraform 进行自动配置。","title":"在 Libvirt 和 KVM 上使用 Terraform 进行自动配置"},{"content":" 使用 Ray 扩展 AI：综合指南 Ray documentation 提供了详细的指南、API 参考和教程。 Ray\u0026rsquo;s GitHub repository是为项目做出贡献、了解其开发或只是探索代码的首选来源。 使用 Ray 编写您的第一个分布式 Python 应用程序 ","date":"27 April 2024","permalink":"/posts/ai/ray/","section":"博客","summary":"Ray是一个开源项目，为构建分布式应用程序提供了简单、通用的 API。它广泛应用于机器学习和数据处理领域的扩展应用。Ray 因其能够以最少的代码更改将 Python 应用程序从单机扩展到大型集群而闻名。","title":"Ray"},{"content":" Debian 与 Ubuntu：哪个 Linux 发行版最适合您？ Ubuntu 与 Debian：哪个更好？ ","date":"27 April 2024","permalink":"/posts/reviews/os/debian/","section":"博客","summary":" Debian 与 Ubuntu：哪个 Linux 发行版最适合您？ Ubuntu 与 Debian：哪个更好？ ","title":"Debian"},{"content":"","date":"27 April 2024","permalink":"/tags/debian-%E9%87%87%E7%94%A8-linux-%E5%86%85%E6%A0%B8%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E7%94%B1-linus-torvalds-%E5%88%9B%E5%BB%BA%E7%9A%84%E5%85%8D%E8%B4%B9%E8%BD%AF%E4%BB%B6%E5%8C%85%E5%B9%B6%E5%BE%97%E5%88%B0%E5%85%A8%E7%90%83%E6%95%B0%E7%99%BE%E5%90%8D%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E6%94%AF%E6%8C%81/","section":"Tags","summary":"","title":"Debian 采用 Linux 内核，这是一个由 Linus Torvalds 创建的免费软件包，并得到全球数百名程序员的支持。"},{"content":"","date":"27 April 2024","permalink":"/posts/reviews/os/","section":"博客","summary":"操作系统（Operating System，简称OS）是计算机系统的核心软件，负责管理和协调硬件资源、提供用户与计算机的接口、执行和监视程序。它控制文件管理、内存分配、多任务处理、网络通信等关键功能，以确保计算机的稳定运行。操作系统充当应用程序和硬件之间的中介，提供便捷的方式来执行任务、访问资源，并确保系统的安全性和效率。不同的操作系统包括Windows、macOS、Linux等，适用于各种计算设备。","title":"Operating System"},{"content":"操作系统上的程序分为两种，\n一种是用户态的程序，例如 Word、Excel 等； 一种是内核态的程序，例如内核代码、驱动程序等。 为了区分内核态和用户态，CPU 专门设置四个特权等级 0、1、2、3 来做这个事情。大牛们在写 Linux 内核的时候，如果用户态程序做事情，就将扳手掰到第 3 等级，一旦要申请使用更多的资源，就需要申请将扳手掰到第 0 等级，内核才能在高权限访问这些资源，申请完资源，返回到用户态，扳手再掰回去。这个程序一直非常顺利地运行着，直到虚拟机出现了。\n虚拟化是一个描述运行软件的广义计算机术语，通常体现为在单一系统上运行多个操作系统，这些操作系统同时运行，而每个操作系统又是相互独立的。\n每一个操作系统都有一个内核 kernel ，而这个 kernel 是需要对 CPU 的 ring0 进行操作的。如果没有任何限制，运行多台虚拟机则会出现多个操作系统同时抢占 CPU。所以运行多台虚拟机就需要一个调度程序去进行调度，它被称为 hypervisor（虚拟机控制软件）。\n通过这样处理以后，我们就得到了虚拟机，而虚机运行在 ring3 环。反过来说。在 ring3 称为用户模式下的 全虚拟化 虚拟机，而 ring2 使用 hypervisor 实现资源调度；多台同时运行在 ring0 下的 半虚拟化 虚拟机容易崩。\n虚拟化类型 # 虚拟化主要分为三种类型：完全虚拟化、半虚拟化和软件虚拟化。\n完全虚拟化使用系统 CPU 的硬件特性为虚拟客户提供底层物理系统的完全抽象。 完全虚拟化(full virtualization)：允许未修改的操作系统和软件在虚拟机上的运行，就像直接在真实硬件上的运行。 配备虚拟外围设备后,虚拟机环境看起来就像是裸机一样。物理硬件的访问权限由 hypervisor 控制，因此虚拟机不会互相干扰。 半虚拟化使用呈现给虚拟客户的软件和数据结构的集合，要求客户中的软件修改以使用半虚拟化环境。 半虚拟化(Paravirtualization)：允许hypervisor为客户机操作系统提供特殊接口,以便其更有效地与hypervisor进行通信。通常,这需要对客户机操作系统或特殊hypervisor 感知型驱动程序的安装进行修改。 半虚拟化应用一系列呈现给虚拟机的软件和数据结构，需要客机中的软件修改以使用半虚拟环境。 半虚拟化包含整个内核，就像 Xen 准虚拟机，或者虚拟化 I/O 设备的驱动程序一样。 软件虚拟化使用较慢的二进制转换和其他仿真技术来运行未经修改的操作系统。像直接使用 QEMU 这种仿真技术。 Red Hat Enterprise Linux 不支持软件虚拟化。 原生虚拟化(native virtualization)或硬件辅助完全虚拟化(hardware-assisted fullvirtualization)允许几乎所有的代码都可未经更改地由CPU直接运行，从而提高效率。 hypervisior 只需在代码在使用干涉系统管理程序的状态或其支持环境的敏感指令时介入 当今32位和64位x86处理器上的Intel虚拟化技术(VT-x)和AMD虚拟化(AMD-V)的发展使硬件辅助完全虚拟化成为可能。利用处理器的硬件特性，向客机提供底层实体系统的总抽象。这创建了新的虚拟系统，被称为一个虚拟机（virtual machine），它允许客机操作系统在无需修改的情况下运行。客机操作系统和任何在客机虚拟机器中的应用并不会察觉出虚拟化环境并正常运作。 **原生虚拟化，性能更好。**允许几乎所有的代码都可未经更改地由 CPU 直接运行，从而提高效率。\n也是工程师不断的在改进的地方，要更好的支持原生虚拟化。让虚拟化的速度和物理机一样，无论是 CPU 还是 IO or RAM。\n硬件辅助虚拟化技术（HVM） # 为了让虚拟机达到原生虚拟化的目的，需要使用硬件辅助虚拟化技术。\n在第一代技术中，支持Intel VT/AMD-V的CPU上提供新处理器指令，可将CPU置入新的执行模式。为硬件辅助虚拟机执行指令时，CPU将切换到非root或客户机模式，在该模式下虚拟机的内核能够以级别0运行，而用户空间能够以级别3运行。\n对于虚拟机内核来讲，只要将标志位设为虚拟机状态，我们就可以直接在 CPU 上执行大部分的指令，不需要虚拟化软件在中间转述，除非遇到特别敏感的指令，才需要将标志位设为物理机内核态运行，这样大大提高了效率。\n这样操作以后，就出现了 CPU 快而 RAM 慢的情况。\n第二代 x86 硬件虚拟化支持内存管理单元(MMU)虚拟化。通常,CPU需要花费很多周期来处理内存页面与虚拟机的映射。MMU 虚拟化允许将工作负载转移到特殊硬件,从而提高性能。Intel将此技术称为扩展页表(ERT),在Nehalem微型架构处理器中引入。AMD将此技术称为快速虚拟化索引(RVi), 于2007年在四核Opteron处理器中引入。\n这样 CPU 和 RAM 就都解决掉了。\n第三代x86 硬件虚拟化支持集中于I/0虚拟化。主板芯片集上的使能技术为安全PCI穿透,允许将物理PCI设备直接连接到虚拟机。该技术可为虚拟机提供近似原生的I/O性能。在Intel中,这称为直接1/O虚拟化技术(VT-d);在AMD中,称为1/0虚拟化技术(AMD- Vi)(最初称为IOMMU)\n之前，网卡 nic 还是要通过 hypervsor 交给虚拟机来用 vm ，到了千兆上不去。现在，新技术 SR-IOV 的网卡划48个口，跨过 hypervisor 直接给 vm 调用。很多通道的处理，直接交给网卡自己来做。通过这种技术，就解决了 IO 性能问题。\n虚拟化组件 # KVM\nKVM（英文 Kernel-based Virtual Machine 的缩写） 来说，其是一款支持虚拟机技术，而且是 linux 内核中的一个功能模块。它在 linux2.6.20 之后的任何 linux 分支中都被支持。\nKVM 是基于内核的虚拟机，直接集成到Linux内核中。它允许 Linux 内核在裸机上运行,并直接自行充当 hypervisor。 KVM 项目由 Qumranet(现属于红帽) 于2006年10月启动。从 RHEL 5.4 开始,在 x86-64 架构上运行的红帽企业 Linux 中完全支持 KVM KVM 设计性能十分优异，hypervisor 所需的许多功能都已通过 Linux 内核实施,如处理器调度、内存管理、物理设备驱动程序等 KVM 需要芯片支持虚拟化技术（英特尔的 VT 扩展或者 AMD 的 AMD-V 扩展） 对于是否支持也可以通过命令行查看：egrep '(vmx|svm)' --color=always /proc/cpuinfo 在 bios 中是默认设置不打开该功能的，要去 bios 设置其为 enable 如果有任何内容则说明当期硬件架构是支持 kvm 的，否则就不支持。 Q：为什么还要 yum install libvirt*\nA：操作系统装以后，天生就可以安装虚拟机了。但是，内核支持虚拟化，不代表应用就能直接访问虚拟化。应用需要调用 lib 来使用虚拟化，这是免费的。\nQ：英特尔的 VT 扩展或者 AMD 的 AMD-V 扩展 是做什么的？\nA：因为完全虚拟化是非常慢的，所以要使用硬件辅助虚拟化技术 Intel-VT，AMD-V，所以需要 CPU 硬件开启这个标志位，一般在 BIOS 里面设置。当确认开始了标志位之后，通过 KVM，GuestOS 的 CPU 指令不用经过 Qemu 转译，直接运行，大大提高了速度。所以，KVM 在内核里面需要有一个模块，来设置当前 CPU 是 Guest OS 在用，还是 Host OS 在用。\nlibvirt\nlibvirt 程序包是一个与 虚拟机监控程序 相独立的 虚拟化应用程序接口，它可以与操作系统的一系列虚拟化性能进行交互。\nlibvirt 是一个管理虚拟化平台的工具包，可从 C、Python、Perl、Go 等访问。\nlibvirt 项目旨在为运行在不同虚拟机管理程序技术上的虚拟管理工具提供长期稳定的 C API。\n也有 Python 写好的 libvirt-python 软件包，来调用 libvirt 库接口，从而方便自己的应用程序更好的使用虚拟化功能。\nlibvirt 程序包在 GNU 较宽松公共许可证下，可作为免费软件使用。\nRed Hat Enterprise Linux 7 支持 libvirt 以及其包括的基于 libvirt 的工具作为默认虚拟化管理（如 Red Hat Enterprise Virtualization 管理），例如 virt-manager 与 virsh 命令行管理工具。\nlibvirt 的主要功能：\n一个稳定的通用层来安全地管理主机上的虚拟机。 一个管理本地系统和连网主机的通用接口。 提供 API 来列举、监测和使用管理节点上的可用资源，其中包括 CPU、内存、储存、网络和非一致性内存访问（NUMA）分区。 部署、创建、修改、监测、控制、迁移以及停止虚拟机操作都需要这些 API。 尽管 libvirt 可同时访问多个主机，但 API 只限于单节点操作。 管理工具可以位于独立于主机的物理机上，并通过安全协议和主机进行交流。 从该图可以看出，libvirt 的设计理念，是面向驱动的架构设计。对任何一种虚拟机技术都开发设计相对于该技术的驱动。在 libvirt api 之上会有很多个 driver，对于每一种虚拟机技术都会有一种 driver，用来充当该虚拟机技术与 libvirt 之间的包装接口。如此设计就可以避免 libvirt 需要设计各种针对不同虚拟机技术的接口，它主要关注底层的实现，提供对外接口调用，而不同的虚拟机技术通过调用 libvirt 提供的接口来完成自己所需要的功能。不同虚拟机技术就可以使用不同驱动，而且相互直接不会影响，方便扩展。而且 libvirt 提供了多种语言的编程接口，可以直接通过编程，调用 libvirt 提供的对外接口实现对虚拟机的操作。\nQEMU\nQEMU 是一个通用的开源机器模拟器和虚拟器。QEMU 有整套的虚拟机实现，由以下部分构成\n处理器模拟器（x86、IBM Z、PowerPC、Sparc） 模拟的设备（显卡、网卡、硬盘、鼠标） 用于将被模拟设备连接到相关主机设备的通用设备 被模拟计算机（PC、Power Mac）的说明 调试程序 用来与模拟器交互的用户界面 QEMU 是一个用户空间的进程，需要通过特定的接口才能调用到 KVM 模块提供的功能。\n从 QEMU 角度来看，虚拟机运行期间，QEMU 通过 KVM 模块提供的系统调用接口进行内核设置，由 KVM 模块负责将虚拟机置于处理器的特殊模式运行。QEMU 使用了 KVM 模块的虚拟化功能，为自己的虚拟机提供硬件虚拟化加速以提高虚拟机的性能。\n当用作虚拟器时，QEMU 通过直接在主机 CPU 上执行来宾代码来实现接近原生的性能。在 Xen 管理程序下执行或在 Linux 中使用 KVM 内核模块时，QEMU 支持虚拟化。使用 KVM 时，QEMU 可以虚拟化 x86、服务器和嵌入式 PowerPC、64 位 POWER、S390、32 位和 64 位 ARM 以及 MIPS 客户机。\nQ：KVM 和 QEMU 有什么区别？\nA：QEMU 使用仿真；KVM 使用处理器扩展 (HVM) 进行虚拟化。\nQ：QEMU 和 KVM 是独立的吗？\nA：是，但不完全是。KVM 模块的职责就是打开并初始化 VMX 功能，提供相应的接口以支持虚拟机的运行。 QEMU（quick emulator) 本身并不包含或依赖 KVM 模块，而 是一套由 Fabrice Bellard 编写的模拟计算机的自由软件。 QEMU 虚拟机 是一个纯软件的实现，可以在没有 KVM 模块的情况下独立运行，但是性能比较低。\nQ：那为何还会有 qemu-kvm 这个名词？\nA：单纯的使用 Qemu 属于完全虚拟化，性能特别低。所以 Qemu 将 KVM 整合进来，将有关 CPU 指令的部分交由内核模块来做，就是 qemu-kvm (qemu-system-XXX)。另外 Qemu 还会模拟其他的硬件，如网络和硬盘。同样，全虚拟化的方式也会影响这些设备的性能。这个时候就需要让 GuestOS 知道自己是虚拟机，需要加载特殊的半虚拟化驱动来提高性能。总之 qemu-kvm 就是这样的一种技术。它补充了 kvm 技术的不足，而且在性能上对 kvm 进行了优化。\nQ：QEMU 和 KVM 到底有什么联系？\nA：KVM 只是内核模块，用户并没法直接跟内核模块交互，需要借助用户空间的管理工具，而这个工具就是 QEMU。KVM 和 QEMU 相辅相成，QEMU 通过 KVM 达到了硬件虚拟化的速度，而 KVM 则通过 QEMU 来模拟设备。简单直接的理解就是：QEMU 是个计算机模拟器，而 KVM 为计算机的模拟提供加速功能。\nQ：QEMU 又和 libvirt 有什么关系？\nA：对于 KVM 来说。其匹配的用户空间工具并不仅仅只有 QEMU 作为唯一选择，还有 RedHat 开发的 libvirt、virsh、virt-manager 等。\n虚拟化细节 # 1、内存\n内存管理和内核同页合并(KSM):\n允许在进程间共享相同的内存页面 对于同一个模板部署出来的虚拟机有用，对异构的没有效果。 KVM 使用该机制来允许内存过量使用。在两个相似的虚拟机中,内存中很有可能有许多相同页面。KSM允许主机扫描虚拟机内存来查找相同页面,并将它们整合成一个共享内存页面,从而减少物理内存消耗 同一个虚拟机模板运行10个虚拟机，大量页是相同的，可以合并成一份。如果修改了一些东西，发生了变化就多占用一些。 2、存储\n虚拟机的储存从虚拟机使用的物理储存中提取。它通过使用半虚拟化或仿真块设备驱动与虚拟机相连。\n2.1. 储存池\n储存池（storage pool）即一个由 “libvirt” 管理的文件、目录或储存设备，其目的是为虚拟机提供储存空间。储存池被分隔为存储卷（volume），可以用来存储虚拟机镜像或附加到虚拟机作为额外额存储。多个客机可共享同一储存池，允许储存资源得到更好分配。\n本地储存池：本地储存池直接连接到主机服务器。它们包括本地目录、直接连接的磁盘、物理分区和本地设备上的 LVM 卷组。本地储存池对开发、测试及不需要迁移或具有大量虚拟机的小型部署十分有用。因为本地储存池不支持实时迁移，所有它可能不适用于某些生产环境。 网络（共享）储存池：网络储存池包括在网络上使用标准协议共享的储存设备。使用 virt-manager 在主机间进行虚拟机的迁移需要网络储存，但是当使用 virsh 迁移时，它是可选的。网络储存池由 libvirt 进行管理。 2.2. 储存卷\n储存池进一步划分为储存卷（storage volume）。储存卷是物理分区、LVM 逻辑卷、基于文件的磁盘镜像及其它由 libvirt 控制的储存形式的抽象层。不论基于何种硬件，储存卷会作为本地储存设备呈现给虚拟机。\n2.3. 仿真储存设备\n虚拟机可以被提供一系列经主机仿真的储存设备。每种储存设备都适用于特定的使用情况。具有可以选择不同种类储存设备的功能，可以使灵活性以及与客机操作系统的兼容性达到最大化。\nvirtio-scsi：virtio-scsi 是为使用大量磁盘或高级储功能（如 TRIM）的客机推荐使用的半虚拟化设备。使用除 Red Hat Enterprise Linux 7 以外操作系统的客机可能需要安装相应的客机驱动。 virtio-blk：virtio-blk 是适用于向客机提供镜像文件的半虚拟化储存设备。virtio-blk 可以为虚拟机提供最好的磁盘 I/O 性能，但比 virtio-scsi 的功能少。 IDE：IDE 是推荐给不支持 virtio 驱动的旧客机用的。IDE 性能不如 virtio-scsi 或 virtio-blk，但它与不同系统广泛兼容。 CD-ROM：ATAPI CD-ROM 与 virtio-scsi CD-ROM 都能向客机提供 ISO 文件或主机 CD-ROM 驱动。virtio-scsi CD-ROM 可以与安装了 virtio-scsi 驱动的客机一同使用。ATAPI CD-ROM 兼容性广泛但性能较低。 USB 存储设备和软盘：如需要可移动介质，可使用 USB 存储设备和软盘。USB 存储设备由于其较大的容量比软盘更受欢迎。 AHCI：仿真 AHCI（高级主机控制器接口，Advanced Host Controller Interface）总线是 IDE 的一种替代品，它的特征增多、性能提高，包括与串行 ATA（SATA）设备交流。AHCI 作为一种技术预览包括在 Red Hat Enterprise Linux 7.1 中。 2.4 主机存储\n磁盘镜像可以储存在一系列和主机相连的本地或远程存储中。\n镜像文件：镜像文件储存在主机文件系统中。它可以储存在本地文件系统中，如 ext4 或 xfs；或网络文件系统中，如 NFS 。例如 libguestfs 这样的工具，能管理、备份及监控文件。KVM 上的磁盘镜像格式包括： raw：raw 镜像文件指不包含附加元数据的磁盘内容。假如主机文件系统允许，raw 文件可以是预分配（pre-allocated）或稀疏（sparse）。稀疏文件根据需求分配主机磁盘空间，因此它是一种精简配置形式（thin provisioning）。预分配文件的所有空间需要被预先分配，但它比稀疏文件性能好。当对磁盘 I/O 性能要求非常高，而且通常不需要通过网络传输镜像文件时，可以使用 raw 文件。 qcow2：qcow2 镜像文件提供许多高级磁盘镜像特征，如快照、压缩及加密。它们可以用来代表通过模板镜像创建的虚拟机。因为只有虚拟机写入的扇区部分才会分配在镜像中，所以 qcow2 文件的网络传输效率较高。Red Hat Enterprise Linux 7.0 及更新版本支持 qcow2 v3 镜像文件格式。 LVM 卷：逻辑卷可用于磁盘镜像，并使用系统的 LVM 工具进行管理。 由于它使用更简单的块储存模式，LVM 比文件系统的性能更高。LVM 精简配置为 LVM 卷提供快照和高效的空间使用，它可以作为 qcow2 的一种替代选择。 主机设备：主机设备如物理 CD-ROM、原始磁盘或 LUN 都可以提供给客机。这使得 SAN 或 iSCSI LUN 还有本地 CD-ROM 都可以提供给客机所用。在 SAN 而不是主机上进行储存管理时，可以使用主机设备。 分布式存储系统：Gluster 卷可用作磁盘镜像。它提供了高效的、使用网络的集群存储。Red Hat Enterprise Linux 7 包括在 GlusterFS 上对磁盘镜像的原生支援。这使 KVM 主机可以直接从 GlusterFS 卷引导虚拟机镜像，并使用 GlusterFS 卷中的镜像作为虚拟机的数据磁盘。与 GlusterFS FUSE 相比，KVM 原生支持性能更好。 数据恢复\nLinux 意外操作后如何进行数据抢救\n虚拟化嵌套\n从 Red Hat Enterprise Linux 7.5 开始，嵌套虚拟化作为 KVM 来宾虚拟机的技术预览版提供。借助此功能，在物理主机（ 0 级或 L0 级）上运行的来宾虚拟机（也称为 1 级或 L1 ）可以充当管理程序，并创建自己的来宾虚拟机 ( L2 )。\n嵌套虚拟化在各种场景中都很有用，例如在受限环境中调试管理程序以及在有限数量的物理资源上测试更大的虚拟部署。但是，请注意，在生产用户环境中不支持或不推荐使用嵌套虚拟化，它主要用于开发和测试。\n嵌套虚拟化依赖于主机虚拟化扩展来运行，它不应与使用 QEMU Tiny Code Generator (TCG) 仿真在虚拟环境中运行来宾混淆，红帽企业 Linux 不支持这种仿真。\n按照以下步骤启用、配置和开始使用嵌套虚拟化：\n启用：默认情况下禁用该功能。要启用它，请在 L0 主机物理机上使用以下过程。 对于英特尔：\n检查您的主机系统上是否可以使用嵌套虚拟化。 $ cat /sys/module/kvm_intel/parameters/nested 如果此命令返回 Y 或 1，则启用该功能。\n如果命令返回 0 or N，请使用步骤 ii 和 iii。\n卸载 kvm_intel 模块：\n#modprobe -r kvm_intel 激活嵌套功能：\n#modprobe kvm_intel nested=1 嵌套功能现在仅在 L0 主机下一次重新引导之前启用。要永久启用它，请将以下行添加到 /etc/\u0026gt; modprobe.d/kvm.conf 文件中：\noption kvm_intel nested=1 对于 AMD：\n检查您的系统上是否可以使用嵌套虚拟化：\n$cat /sys/module/kvm_amd/parameters/nested 如果此命令返回 Y 或 1 ，则启用该功能。 如果命令返回 0 or N，请使用步骤 ii 和 iii。 卸载 kvm_amd 模块\n#modprobe -r kvm_amd 激活嵌套功能\n#modprobe kvm_amd nested=1 嵌套功能现在仅在 L0 主机下一次重新引导之前启用。要永久启用它，请将以下行添加到 /etc/\u0026gt; modprobe.d/kvm.conf 文件中：\noption kvm_intel nested=1 使用以下方法之一为嵌套虚拟化 配置 L1 虚拟机：\n虚拟管理器：打开预期来宾的 GUI，然后单击显示虚拟硬件详细信息图标。选择Processor菜单，在Configuration部分中，输入host-passthroughModel字段（不要使用下拉选择），然后单击Apply。 域 XML：将以下行添加到来宾的域 XML 文件中：\u0026lt;cpu mode='主机直通'/\u0026gt; 如果来宾的 XML 配置文件已经包含一个 \u0026lt;cpu\u0026gt; 元素，则重写它。 要开始使用嵌套虚拟化，请在 L1 来宾中安装 L2 来宾。为此，请遵循与安装 L1 来宾时相同的过程 - 有关详细信息，请参阅\n工具的使用（已熟练使用） # virsh：virsh 是一个用于监控系统程序和客户机虚拟机器的命令行接口（CLI）工具。virsh 命令行工具建立在 libvirt 管理 API，并作为可选择的一个运行方式来替代 qemu-kvm 命令和图形界面的 virt-manager 应用。无特权的用户以只读的方式使用 virsh 命令；有根用户权限的用户可以使用该命令的所有功能。virsh 是一个对虚拟环境的管理任务进行脚本化的理想工具。另外，virsh 工具是 virsh 客机域的一个主要管理接口，可以用于创造、暂停和关闭域，或罗列现有域。这一工具作为 libvirt-client 软件包中的一部分被安装。\nvirsh list \u0026ndash;all virsh destroy servername virsh start servername virsh snapshot-create virsh reboot virsh shutdown virsh dumpxml servername virsh define 使用一个xml文件来创建虚拟机 virsh create 使用一个xml文件来创建并启动虚拟机 virt-install：virt-install 是一个命令行工具，它提供了一种将操作系统配置到虚拟机中的简单方法。virt-install 是一个用来配置新的虚拟机器的命令行工具。它通过使用连续的控制台、SPICE 或 VNC 客户 / 服务器成对图形，支持基于文本和图形的安装。安装介质可以是本地的，或已有的远程 NFS、HTTP 或 FTP 服务器。考虑到便捷的自动化安装，还可以通过配置此工具实现在无需人工参与的情况下运行，并在安装完成时快速启动客机。此工具以 python-virtinst 软件包的一部分进行安装。\nvirt-manager：virt-manager应用程序是一个桌面用户界面，用于通过 libvirt 管理虚拟机。它主要针对KVM虚拟机，但也管理Xen和LXC（Linux 容器）。它提供了正在运行的域、它们的实时性能和资源利用率统计信息的摘要视图。向导可以创建新域，以及配置和调整域的资源分配和虚拟硬件。嵌入式 VNC 和 SPICE 客户端查看器向来宾域提供完整的图形控制台。virt-manager 是一个用于管理虚拟机器的简单的图形工具。它所提供的功能用以控制现有机器寿命周期、储备新机器、管理虚拟网络、访问虚拟机器的图形控制台并查看性能数据。这个工具包括在同名的软件包中，称为 virt-manager。\nvirt-viewer：virt-viewer是一个轻量级的 UI 界面，用于与虚拟客户操作系统的图形显示进行交互。它可以显示 VNC 或 SPICE，并使用 libvirt 查找图形连接详细信息。Virt Viewer 用于使用 SPICE 远程桌面协议访问 KVM 虚拟机。它是用于 KVM 虚拟化解决方案（如 Proxmox）的 VMware 远程控制台 (VMRC) 的替代方案。Virt Viewer 具有许多高级功能，例如 VMware Remote Console (VMRC)。\nResources # libvirt and QEMU 基础篇 libvirt and QEMU 进阶篇 ","date":"26 April 2024","permalink":"/posts/architecture/virtualization/overview/","section":"博客","summary":"虚拟化是一个描述运行软件的广义计算机术语，通常体现为在单一系统上运行多个操作系统，这些操作系统同时运行，而每个操作系统又是相互独立的。","title":"Virtualization"},{"content":"简介 # VirtualBox 与 QEMU\nVirtualBox 提供用户友好的 GUI，适合想要以简单方式在桌面上运行虚拟机的用户。相比之下，QEMU 提供更多的控制和灵活性，但通常需要命令行交互。两者之间的选择取决于您的具体用例和偏好。\n两者之间存在一些关键差异：\nEmulation：QEMU 可以仿真各种硬件，包括 CPU、内存控制器和设备。另一方面，VirtualBox 主要设计用于虚拟化 x86 和 AMD64 系统。 Performance：QEMU 通常比 VirtualBox 更快，尤其是在运行模拟系统时。 Compatibility：QEMU 比 ​​VirtualBox 更兼容更广泛的客户操作系统。 QEMU 的优势\nVersatility: QEMU 可以模拟各种硬件并运行各种客户操作系统。 Performance: QEMU 通常比其他虚拟化软件更快，尤其是在运行模拟系统时。 Compatibility: 与其他虚拟化软件相比，QEMU 与更广泛的客户操作系统兼容。 Open source: QEMU 是免费的开源软件，这意味着它会不断得到改进和更新。 QEMU 的缺点\nComplex Configuration: QEMU 的命令行界面和配置文件对于初学者来说可能会令人望而生畏。 Performance Overhead: 与 KVM 等虚拟化解决方案相比，仿真往往会产生更多的性能开销。 Limited GUI: QEMU 的默认界面是基于命令行的，这可能不像 VirtualBox 的 GUI 那样用户友好。 QEMU vs. KVM\nKVM (Kernel-based Virtual Machine) 是一种内置于 Linux 内核中的虚拟化技术。它使用硬件虚拟化功能在单个主机系统上运行多个虚拟机。 QEMU 和 KVM 可以一起使用，提供更高效、更强大的虚拟化解决方案。QEMU可以用来模拟KVM不支持的硬件，并且KVM可以用来为虚拟机提供更好的性能。 Use QEMU # 1. 更新系统\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y 2. 检查CPU和虚拟化是否开启：\nLC_ALL=C lscpu | grep Virtualization 如果是 intel 处理器，它应该带来以下输出：\n如果是 AMD 处理器，它应该会为您提供以下输出：\nVirtualization: AMD-V egrep -c \u0026#39;(vmx|svm)\u0026#39; /proc/cpuinfo 如果输出大于 0，则 CPU 虚拟化已启用。否则，您需要在系统 BIOS 中启用它。\n3. 安装QEMU\n使用 sudo apt install qemu 即可在 Ubuntu 中安装 QEMU，安装完成后会有很多以 qemu- 开头的命令，如：\nqemu-system-i386：用于模拟 32 位的 80386 硬件环境 qemu-system-x86_64：用于模拟 64 位的 x86 硬件环境 qemu-system-arm：用于模拟 ARM 硬件环境 这些命令又可以分为以 qemu-system- 开头和以 qemu- 开头，\n以 qemu-system- 开头的用于在模拟的硬件环境上运行整个系统； 以 qemu- 开头的用于在模拟的硬件环境上运行某个程序，而非整个系统。 qemu-kvm libvirt-clients libvirt-daemon-system bridge-utils virtinst libvirt-daemon 4. 启用并启动libvirtd服务\nsudo systemctl enable --now libvirtd 5. 安装 Virt-Manager\nsudo apt install virt-manager -y Virt-Manager 是一个用于管理虚拟机的 GUI 工具。\n6. 使用 QEMU\n安装 QEMU 后，您就可以开始使用它来创建和运行虚拟机。\n要创建新的虚拟机，可以使用以下命令：\nqemu-kvm -name \u0026lt;vm_name\u0026gt; -m \u0026lt;memory_size\u0026gt; -cdrom \u0026lt;iso_file\u0026gt; 这将创建一个具有指定名称、内存大小和 ISO 文件的新虚拟机。\n使用 QEMU 模拟一个硬件环境并运行整个系统的命令格式为\nqemu [options] [disk_image] 其中 disk_image 即硬盘镜像文件。其常用的参数如下：\n-hda file：使用 file 作为硬盘 0 的镜像文件。 -m megs：设定虚拟内存为 megs M 字节，默认为 128 M 字节。 -smp n：设置为有 n 个 CPU 的 SMP 系统。 值得注意的是，QEMU 的启动需要有图形界面，若未安装图形界面，则会报错：\nCould not initialize SDL(No available video device) - exiting 若要无图形界面启动需要加参数 -nographic。\n要启动虚拟机，可以使用以下命令：\nqm launch \u0026lt;vm_name\u0026gt; 要停止虚拟机，可以使用以下命令：\nqm stop \u0026lt;vm_name\u0026gt; Resources\n什么是 QEMU QEMU 小记 在QEMU上的虚拟机中运行Ubuntu # 首先，我们需要获取将在虚拟机中运行的 Linux 发行版的iso映像。在此示例中，我们将使用 Ubuntu 24.04 LTS，您可以在此链接中找到它（https://ubuntu.com/download/desktop）\niso文件下载完成后，使用QEMU创建一个虚拟硬盘映像，例如创建一个大小为20GB的qcow2格式硬盘映像：\nqemu-img create -f qcow2 ubuntu-vm.qcow2 20G 启动虚拟机\nqemu-system-x86_64 \\ -m 2048 \\ -cpu max \\ -smp 2 \\ -boot d \\ -drive file=ubuntu-vm.qcow2,format=qcow2 \\ -cdrom ubuntu-22.04.4-live-server-amd64.iso \\ -vnc :1 \\ -net nic \\ -net user,hostfwd=tcp::2222-:22 \\ -nographic -m 2048：分配2048 MB内存给虚拟机。 -cpu host：使用与宿主机相同的CPU型号。 -smp 2：分配2个CPU核心。 -boot d：从光驱启动，即ISO文件。 -drive：指定硬盘文件和格式。 -cdrom：指定ISO文件路径。 -vnc :1：开启VNC服务，在5901端口监听（因为VNC端口是5900+N，这里N=1）。 -net nic：创建一个网络接口。 -net user：使用用户模式网络，包括端口转发设置，这里把宿主机的2222端口转发到虚拟机的22端口，以便SSH访问。 -nographic：不启用图形输出。 qemu的标准选项\n# qemu的标准选项主要涉及指定主机类型、CPU模式、NUMA、软驱设备、光驱设备及硬件设备等。 -name name\t# 虚拟机名称 -M machine\t# 指定要模拟的主机类型，如standard PC，ISA-only PC或Intel-Mac等，可以使用“qemu-kvm -M ?”获取所支持的所有类型 -m megs\t# 设定虚拟机的RAM大小 -cpu model\t# 设定CPU模型，如coreduo、qemu64等，可以使用\u0026#34;qemu-kvm -cpu ?\u0026#34;获取所支持的所有模型 -smp n\t# 设定模拟的SMP架构中CPU的个数 [,cores=cores]\t# 每个CPU的核心数 [,threads=threads] # 线程数 [,sockets=sockets] # CPU的socket数目 [,maxcpus=maxcpus] # 用于指定热插入的CPU个数上限 -numa 非一致内存访问 -numa opts：指定模拟多节点的numa设备 -fda file： -fdb file：使用指定文件(file)作为软盘镜像，file为/dev/fd0表示使用物理软驱 -hda file： -hdb file： -hdc file： -hdd file：使用指定file作为硬盘镜像 -cdrom file：使用指定file作为CD-ROM镜像，需要注意的是-cdrom和-hdc不能同时使用：将file指定为/dev/cdrom可以直接使用物理光驱 -drive\t# 定义一个硬盘设备：可用子选项有很多 file=/path/to/somefile\t# 硬盘映像文件 if=interface\t# 硬盘设备接口类型 ide、scsi、sd、virtio（半虚拟化） index=index\t# 设定同一种控制器类型中不同设备的索引号，即标识号 media=media\t# 定义介质类型为硬盘还是光盘disk、cdrom snapshot=snapshot\t# 指定当前硬盘设备是否支持快照功能：on或off cache=cache\t# 定义如何使用物理机缓存来访问块数据，其可用值有none、writeback、unsafe和writethrough四个 format=format\t# 指定映像文件的格式，具体格式可参见qemu-img命令 -boot [order=drives][,once=drives][,menu=on|off]\t# 定义启动设备的引导次序，每种设备使用一个字符表示：不同的架构所支持的设备及其表示字符不尽相同，在x86 PC架构上，a、b表示软驱，c表示第一个光驱设备，n-p表示网络适配器，默认为硬盘设备。例如：-boot order=dc,once=d qemu显示选项\n显示选项用于定义虚拟机启动后的显示接口相关类型及属性等。\nSDL -sdl\t# 启用SDL VNC -vnc display [option，option]\t# 默认情况下，qemu使用SDL显示VGA输出；使用-vnc选项，可以让qemu监听在vnc上，并将VGA输出重定向至vnc会话，使用此选项时，必须使用-k选项指定键盘布局类型;其中有许多子选项，具体请参考qemu的手册 display 1、host:N\t# N为控制台号 192.168.1.1:1\t# 5900为起始端口 2、unix:/path/to/socket_file\t# 监听在套接字 3、none\t# 不显示 option password\t# 连接时需要验证密码，设定密码通过monitor接口使用change reverse\t# “反向”连接至某处于监听状态的vncview上 -vga type\t# 指定要仿真的VGA接口类型，常见的类型有： cirrus: Cirrus Logic GD5446显示卡 std：带有Bochs VBI扩展的标准VGA显示卡 vmware：VMware SVGA-II兼容的显示适配器 qxl：QXL半虚拟化显示卡：与VGA兼容，在Guest中安装qxl驱动后能以很好的方式工作，在使用spice协议时推荐使用此类型 none：禁用VGA卡 -monitor stdio\t# 在标准输入输出上显示monitor界面 -nographic\t# 默认情况下，qemu使用SDL来显示VGA输出，而此选项用于禁止图形接口，此时，qemu类似一个简单的命令行程序，其仿真串口设备将被重定向到控制台 -curses\t# 禁止图形接口，并使用curses/ncurses作为交互接口 -alt-grab\t# 使用Ctrl+Alt+Shift组合键释放鼠标 -ctrl-grab\t# 使用右Ctrl键释放鼠标 -spice option[,option[,...]]\t# 启用spice远程桌面协议：其中有许多子选项，具体请参照qemu-kvm手册。 网络属性相关选项\nnic #定义网络接口 -net nic [,vlan=n,macaddr=n,model=type,name=name,addr=addr,vectors=v]\t# 创建一个新的网卡设备并连接至vlan n中：PC架构上默认的NIC为e1000，macaddr用于为其制定mac地址，name用于指定一个在监控时显示的网上设备名称；qemu可以模拟多个类型的网卡设备，如virtio、i82557b、i82559er、ne2k_isa、pcnet、rtl8139、e1000、smc91c111、lance及mcf_fec等；不过，不同平台架构上，其支持的类型可能只包含前述列表中的一部分，可以使用qemu-system-x86_64 -net nic,model=?来获取当前平台支持的类型。 vlan\t# vlan号 macaddr\t# mac地址(mac 默认不变) model\t# e1000 virtio name\t# 设备名 addr\t# ip地址 tap #nic管理虚拟机中的接口，tap就是管理宿主机上的对应接口 -net tap[,vlan=n][,name=name][,fd=h][,ifname=name][,script=file][,downscript=dfile]\t# 通过物理机的TAP网络接口连接至vlan n中，使用script=file指定的脚本（默认为/etc/qemu-ifup）来配置当前网络接口，并使用downscript=file指定的脚本(默认为/etc/qemu-ifdown)来撤销接口配置；使用script=no和downscript=no可分别用来禁止执行脚本。 user -net user[,option][,option][,...]：在用户模式配置网络栈，其不依赖于管理权限；有效选项有： vlan=n\t# 连接至vlan n，默认n=0 name=name\t# 指定接口的显示名称，常用于监控模式中 net=addr[/mask]\t# 设定GuestOS中可见的IP网络，掩码可选，默认为10.0.2.0/8 host=addr\t# 指定GuestOS中看到的物理机的IP地址，默认为指定网络中的第二个，即x.x.x.2 dhcpstart=addr\t# 指定DHCP服务地址池中16个地址的起始IP，默认为第16个至第31个，即x.x.x.16-x.x.x.31 dns=addr\t# 指定GuestOS可见的dns服务器地址，默认为GuestOS网络中的第3个地址，即x.x.x.3 tftp=dir\t# 激活内置的tftp服务器，并使用指定的dir作为tftp服务器的默认根目录 bootfile=file\t# BOOTP文件名称，用于实现网络引导GuestOS,如：qemu -hda linux.img -boot n -net user,tftp=/tftpserver/pub,bootfile=/pexlinux.0 kvm的网络模型\n1、隔离模型 使用bridge连接各个虚拟机但不关联物理网卡 2、nat模型 在路由模型上添加nat规则 iptables 3、路由模型 在隔离模型的基础之上添加一个虚拟网卡，开启路由转发功能。 需要虚拟机指定虚拟网卡的ip为网关 需要在要通信的主机或路由添加回复报文的路由条目 4、桥接模型 在隔离模型的bridge上添加物理网卡 将物理网卡变为bridge，将原来的IP放到一张虚拟网卡并添加到桥上 dhcp 服务器 namespace 名称空间 手动创建bridge\nyum install bridge-utils #安装工具包 rpm -ql bridge-utils #查看utils释放的文件 brctl -h #查看帮助 brctl addbr br0 #添加网桥 ifconfig -a #查看全部接口 brctl stp br0 off #关闭生成树 ip link set br0 up #启动br0设备 ip addr del 192.168.1.50/24 dev ens33 #拆除物理网卡ip ip addr add 192.168.1.50/24 dev br0 #添加ip ip a #ip是否添加成功 ping 192.168.1.50 #检查ip可用 ip link set dev ens33 master br0 #物理网卡加入桥接设备 brctl show #查看是否加入桥 常用的 virsh 命令选项\nResources\n如何在Ubuntu上安装QEMU/KVM创建虚拟机 在QEMU（快速模拟器）上的虚拟机中运行Ubuntu 在 Ubuntu 上安装和使用 Qemu 制作自动安装Ubuntu22.04-server的iso镜像以及安装过程详解 Linux实现KVM+QEMU+libvirt的虚拟机环境 并使用virsh对虚拟机进行管理 全自动化系统安装 Introduction to autoinstall ubuntu20.04打包iso镜像自动安装 kubevirt\n如何使用 KubeVirt 将旧虚拟机集成到 Kubernetes 上的容器管道中 使用 Kubevirt 在 Kubernetes 集群中部署 VM 其他 # Resources\nDocker 和 QEMU：加速边缘计算开发和优化代码编译的强大组合 ","date":"26 April 2024","permalink":"/posts/architecture/virtualization/qemu/","section":"博客","summary":"QEMU（快速模拟器）是一个免费的开源机器模拟器和虚拟器，可以在单个主机系统上运行各种客户操作系统和体系结构。与传统的虚拟化解决方案不同，QEMU 不仅仅是一个虚拟机管理程序；它是一个模拟器，可以在各种主机平台上运行客户操作系统。它是用于软件开发、测试和部署的强大工具。","title":"QEMU"},{"content":"","date":"25 April 2024","permalink":"/posts/reviews/24%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0/","section":"博客","summary":"不打没有准备的仗！！实习过程中的复习。","title":"实习复习"},{"content":"简介 # 使用 Kubernetes 进行虚拟机编排似乎非常奇怪和令人困惑。考虑如何在 Kubernetes 集群内配置虚拟机。是否会在 pods 内运行？或者沿着 pod ？这很令人困惑，而且似乎不可能。但是，我必须向您透露，使用 KubeVirt 进行配置是完全可能且容易的。\nKubeVirt 是 CNCF 的一个开源沙箱项目，它是一个 Kubernetes 插件，使用户能够同时安排传统虚拟机工作负载和容器工作负载。通过使用自定义资源定义（CRD）和其他 Kubernetes 功能，KubeVirt 无缝扩展现有 Kubernetes 集群，以提供一组可用于管理虚拟机的虚拟化 API。\nKubeVirt 技术满足了已经采用或想要采用 Kubernetes 但现有的基于虚拟机的工作负载无法轻松容器化的开发团队的需求。更具体地说，该技术提供了一个统一的开发平台，开发人员可以在该平台上构建、修改和部署驻留在通用共享环境中的应用程序容器和虚拟机中的应用程序。其好处是广泛而显著的。依赖现有基于虚拟机的工作负载的团队能够快速容器化应用程序。通过将虚拟化工作负载直接放置在开发工作流程中，团队可以随着时间的推移分解它们，同时仍然根据需要利用剩余的虚拟化组件。\n提供统一的 API 来管理常规 Kubernetes 工作负载和虚拟机。 Kubernetes 客户端将能够管理虚拟磁盘和 qemu/kvm 虚拟机，同时受到 Kubernetes RBAC、配额、通过众所周知的 API 等的约束。与其他虚拟化工具相比，Kubevit 不需要 Kubernetes 最终用户能够登录 vmware/vsphere。\nResources # https://medium.com/@arbnair97/virtual-machine-orchestration-in-kubernetes-using-kubevirt-91bd0e81a5bd ","date":"25 April 2024","permalink":"/posts/architecture/virtualization/k8s/kubevirt/","section":"博客","summary":"使用 KubeVirt 在 Kubernetes 中进行虚拟机编排。","title":"使用 KubeVirt 在 Kubernetes 中进行虚拟机编排"},{"content":"关闭防火墙 sudo ufw disable\n前置知识：\ndocker： 清华源下载docket-ce containerd：https://containerd.io/downloads/ apt install constanerd containerd设置代理\n我的版本是这样的，\n这里以通过 systemd 安装的 containerd 为例。\ncontainerd 的配置一般位于 /etc/containerd/config.toml 下，service 文件位于：/lib/systemd/system/containerd.service 配置 Proxy 可以通过 service 环境变量方式配置，具体如下：\n修改 /etc/containerd/config.toml 文件\n# 1 # 阿里云镜像 sandbox_image = \u0026#34;registry.k8s.io/pause:3.8\u0026#34; 改为 sandbox_image = \u0026#34;registry.aliyuncs.com/k8sxio/pause:3.8\u0026#34; # 2 SystemdCgroup = false 修改为 SystemdCgroup = true # 3 # 配置镜像加速 # 上下级配置,缩进两个空格 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.mirrors] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.mirrors.\u0026#34;docker.io\u0026#34;] endpoint = [\u0026#34;https://bqr1dr1n.mirror.aliyuncs.com\u0026#34;] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.mirrors.\u0026#34;k8s.gcr.io\u0026#34;] endpoint = [\u0026#34;https://registry.aliyuncs.com/k8sxio\u0026#34;] 创建或编辑文件：/lib/systemd/system/containerd.service.d/http-proxy.conf\n[Service] Environment=\u0026#34;HTTP_PROXY=http://127.0.0.1:7890\u0026#34; Environment=\u0026#34;HTTPS_PROXY=http://127.0.0.1:7890\u0026#34; Environment=\u0026#34;NO_PROXY=localhost,127.0.0.0/8,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,.svc,.cluster.local,.ewhisper.cn,\u0026lt;nodeCIDR\u0026gt;,\u0026lt;APIServerInternalURL\u0026gt;,\u0026lt;serviceNetworkCIDRs\u0026gt;,\u0026lt;etcdDiscoveryDomain\u0026gt;,\u0026lt;clusterNetworkCIDRs\u0026gt;,\u0026lt;platformSpecific\u0026gt;,\u0026lt;REST_OF_CUSTOM_EXCEPTIONS\u0026gt;\u0026#34; 这里有个推荐 NO_PROXY 配置：\n本地地址和网段：localhost 和 127.0.0.1 或 127.0.0.0/8 Kubernetes 的默认域名后缀：.svc 和 .cluster.local Kubernetes Node 的网段甚至所有应该不用 proxy 访问的 node 网段：\u0026lt;nodeCIDR\u0026gt; APIServer 的内部 URL: \u0026lt;APIServerInternalURL\u0026gt; Service Network: \u0026lt;serviceNetworkCIDRs\u0026gt; （如有）etcd 的 Discovery Domain: \u0026lt;etcdDiscoveryDomain\u0026gt; Cluster Network: \u0026lt;clusterNetworkCIDRs\u0026gt; 其他特定平台相关网段(如 DevOps, Git/制品仓库。..): \u0026lt;platformSpecific\u0026gt; 其他特定 NO_PROXY 网段：\u0026lt;REST_OF_CUSTOM_EXCEPTIONS\u0026gt; 常用内网网段： 10.0.0.0/8 172.16.0.0/12 192.168.0.0/16 系统设置代理\nsudo vim ~/.bashrc ... function setproxy() { export http_proxy=proxy.i2ec.top:21087 export https_proxy=proxy.i2ec.top:21087 export no_proxy=*.i2ec.top,*.local,localhost,127.0.0.1,127.0.0.0/8,172.16.0.0/12,10.0.0.0/8,192.168.0.0/16 } function unsetproxy() { unset http_proxy unset https_proxy unset no_proxy } # setproxy() export PATH=\u0026#34;$PATH:$HOME/tools/bin\u0026#34; alias src=\u0026#34;source ~/.bashrc\u0026#34; alias k=kubectl alias kg=\u0026#34;kubectl get\u0026#34; alias kd=\u0026#34;kubectl describe\u0026#34; 如果你一开始安装过k8s，并且有原来的文件残留，你可以用这个过程把它的文件先删光\nsudo apt remove kubelet kubeadm kubectl sudo rm -rf .kube/config sudo rm -rf /etc/kubernetes/ sudo rm -r /etc/cni/net.d/ sudo rm -rf /var/lib/etcd/ sudo rm -rf /var/lib/kubelet/ 可以用清华源，也可以用阿里源，这里使用阿里云的镜像构建 k8s\nsudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add - 之后将阿里云的镜像地址写到 sources.list 当中\nsudo vim /etc/apt/sources.list.d/kubernetes.list # 写入下列内容 deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main 退出后更新软件包，下载 k8s\nsudo apt-get update sudo apt-get install -y kubelet=1.27.0-00 kubeadm=1.27.0-00 kubectl=1.27.0-00 初始化k8s\n这次初始化遇到三个问题\n首先是 [WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet\n需要把虚拟存储交换关掉\n(base) wds@wds-NucBox-K8:~$ sudo swapoff -a (base) wds@wds-NucBox-K8:~$ sudo vim /etc/fstab (base) wds@wds-NucBox-K8:~$ sudo mount -a (base) wds@wds-NucBox-K8:~$ free -h total used free shared buff/cache available 内存： 28Gi 4.6Gi 532Mi 241Mi 23Gi 23Gi 交换： 0B 0B 0B 然后是 [ERROR FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables does not exist，这是之前配置的 br_netfilter 没有启动，运行下面的命令即可。\n(base) wds@wds-NucBox-K8:~$ sudo modprobe br_netfilter (base) wds@wds-NucBox-K8:~$ echo 1 | sudo tee /proc/sys/net/bridge/bridge-nf-call-iptables 1 这个问题同理 [ERROR FileContent--proc-sys-net-ipv4-ip_forward]: /proc/sys/net/ipv4/ip_forward contents are not set to 1\n(base) wds@wds-NucBox-K8:~$ echo 1 | sudo tee /proc/sys/net/ipv4/ip_forward 1 然后可以先拉取镜像\n(base) wds@wds-NucBox-K8:~$ sudo kubeadm config images list I0503 08:02:18.239000 90461 version.go:256] remote version is much newer: v1.30.0; falling back to: stable-1.27 W0503 08:02:19.105381 90461 images.go:80] could not find officially supported version of etcd for Kubernetes v1.27.13, falling back to the nearest etcd version (3.5.7-0) registry.k8s.io/kube-apiserver:v1.27.13 registry.k8s.io/kube-controller-manager:v1.27.13 registry.k8s.io/kube-scheduler:v1.27.13 registry.k8s.io/kube-proxy:v1.27.13 registry.k8s.io/pause:3.9 registry.k8s.io/etcd:3.5.7-0 registry.k8s.io/coredns/coredns:v1.10.1 (base) wds@wds-NucBox-K8:~$ sudo kubeadm config images pull --image-repository registry.aliyuncs.com/google_containers 若是遇到下面的问题\n你可以用我上面的命令，用--image-repository option来指定拉去的仓库\n然后初始化 kubeadm init\n安装网络插件calico\n可能会用到 TIFERA\n下面给出 calico.yaml，需要的可以直接 copy\nkubectl apply -f calico.yaml 修改containd的网络配置\nsudo vim /etc/cni/net.d/10-containerd-net.conflist 从 An example containerd configuration file copy 相关的内容\n{ \u0026#34;cniVersion\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;containerd-net\u0026#34;, \u0026#34;plugins\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;bridge\u0026#34;: \u0026#34;cni0\u0026#34;, \u0026#34;isGateway\u0026#34;: true, \u0026#34;ipMasq\u0026#34;: true, \u0026#34;promiscMode\u0026#34;: true, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34;, \u0026#34;ranges\u0026#34;: [ [{ \u0026#34;subnet\u0026#34;: \u0026#34;10.88.0.0/16\u0026#34; }], [{ \u0026#34;subnet\u0026#34;: \u0026#34;2001:db8:4860::/64\u0026#34; }] ], \u0026#34;routes\u0026#34;: [ { \u0026#34;dst\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34; }, { \u0026#34;dst\u0026#34;: \u0026#34;::/0\u0026#34; } ] } }, { \u0026#34;type\u0026#34;: \u0026#34;portmap\u0026#34;, \u0026#34;capabilities\u0026#34;: {\u0026#34;portMappings\u0026#34;: true}, \u0026#34;externalSetMarkChain\u0026#34;: \u0026#34;KUBE-MARK-MASQ\u0026#34; } ] } sudo systemctl restart containerd.service 等待，安装完成\nkubectl get nodes worker01 -o yaml | code - 搜索 taint\n然后把污点删除 kubectl taint node worker01 node-role.kubernetes.io/control-plane-\n查看cpu架构\nARCH=$(uname -s | tr A-Z a-z)-$(uname -m | sed \u0026#39;s/x86_64/amd64/\u0026#39;) || windows-amd64.exe echo ${ARCH} Resources\nhttps://www.cnblogs.com/xuweiweiwoaini/p/13884112.html PXE 批量部署 使用kubeadm部署Kubernetes 1.28 二进制部署企业级K8S 1.28.3+集群实战 Ubuntu22.04 安装 K8S 1.27.1 Kubernetes 概述 在公司代理后面安装 Kubernetes ","date":"25 April 2024","permalink":"/posts/architecture/virtualization/k8s/k8s_deployment/","section":"博客","summary":"k8s 安装过程","title":"k8s 安装"},{"content":"","date":"25 April 2024","permalink":"/tags/kubevela/","section":"Tags","summary":"","title":"Kubevela"},{"content":"Beta分布是一个概率的概率分布，其范围限制在0和1之间。例如，我们可以使用它来建模概率：\n广告的点击率； 网站上购买的客户的转化率； 读者为您的博客点赞的可能性； 特朗普赢得一次竞选的可能性； 乳腺癌女性的5年生存机会； 依此类推。 让我们暂时忽略系数 $\\frac{1}{Beta(\\alpha, \\beta)}$ ，只看分子 $x^{\\alpha - 1}(1-x)^{\\beta - 1}$，因为 $\\frac{1}{Bata(\\alpha, \\beta)}$ 只是使该函数积分为 $1$ 的规格化常数。然后，分子中的术语 $-x$ 表示某的幂乘以 $1-x$ 表示某的幂看起来很熟悉。\n当我们从二项式分布的角度来看时，Beta分布的直觉就起作用了。\n二项式和Beta之间的区别在于，前者对成功次数(x)进行建模，而后者对成功概率(p)进行建模。 换句话说，概率是二项式的参数；在Beta中，概率是一个随机变量。\n解释 $\\alpha$、$\\beta$\n你能想到的 $\\alpha-1$ 作为成功的数目和 $\\beta-1$ 作为失败的次数，就像ñ＆NX二项式条款。 您可以选择 $\\alpha$ 和 $\\beta$ 参数，但是您认为它们应该是。如果你觉得成功的概率是非常高的，比方说 $90％$，设置 $90 \\alpha$ 和 $10 \\beta$。否则，$\\beta$ 为90，$\\alpha$ 为10。 随着 $\\alpha$ 变大（成功事件越多），概率分布的大部分将向右移动，而β的增加则使分布向左移动（更多的失败）。 同样，如果我们同时确定 $\\alpha$ 和 $\\beta$ 都增加，则分布将变窄。\n2. 例子：概率的概率\n假设某人同意与您约会的可能性是，$\\beta$ 分布为 $\\alpha = 2$，$\\beta = 8$。您的成功率大于 $50％$ 的概率是多少？\n$P(X \u0026gt; 0.5)= 1- CDF(0.5)= 0.01953$ 对不起，这很低。\n爱荷华大学的Bognar博士为Beta分布构建了demo，我发现它实用且美观。您可以试验不同的α和β值，并可视化形状变化。\n3. 为什么我们使用Beta分布？\n如果我们只希望概率分布对概率建模，那么 $(0,1)$ 上的任何任意分布都将起作用。创建一个应该很容易。只需取任何不会在 $0$ 到 $1$ 之间爆炸并保持正值的函数，然后将其从 $0$ 积分到 $1$ ，然后简单地将该函数除以该结果即可。您刚刚获得了可用于对概率进行建模的概率分布。在那种情况下，为什么我们坚持在任意概率分布上使用beta分布？\nBeta分布有何特别之处？\nBeta分布是贝叶斯推断中伯努利，二项式，负二项式和几何分布(似乎是涉及成功与失败的分布)的共轭先验。\n使用共轭先验计算后验非常方便，因为您可以避免贝叶斯推理中涉及的太多的数值计算。\n共轭先验\n讨论概率分布的一个重要原因是，现实生活中有很多数据可以使用这些模型来模拟。对于给定的一个数据集合 $x_1,…,x_n$ 我们希望这个数据集合来自于某个随机变量 $X$ ，并且这个随机变量具有概率分布 $P(X)$。找到 $P(X)$ 的过程叫做密度估计（density estimation）。需要强调的是密度估计问题是一个病态问题，因为世界上的概率密度函数不计其数，能够给出观测集合 $x_1,…,x_n$ 的概率密度函数也是如此之多。任何一个在 $x_1,…,x_n$ 处非零的密度函数 $P(X)$ 都可能是候选。选择一个合适的 $P(X)$ 是模型选择问题，在机器学习领域经常遇见。\nhttps://zhuanlan.zhihu.com/p/149964631 Β分布 ","date":"25 April 2024","permalink":"/posts/ai/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/beta%E5%88%86%E5%B8%83/","section":"博客","summary":"Beta分布是一个概率的概率分布。","title":"Beta分布"},{"content":"","date":"25 April 2024","permalink":"/posts/ai/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/","section":"博客","summary":"机器学习的数学基础涵盖概率论、统计学、线性代数、微积分等领域。这些工具用于建模、分析数据和优化算法，助力于从数据中学习和预测。","title":"数学基础"},{"content":"瑞士数学家雅克·伯努利(Jacques Bernoulli, 1654～1705)首次研究独立重复试验(每次成功率为$p$ )。在他去世后的第 $8$ 年(1713年)，他侄子尼克拉斯出版了伯努利的著作《推测术》。在书中，伯努利指出了如果这样的试验次数足够大，那么成功次数所占的比例以概率 $1$ 接近 $p$。 雅克·伯努利是这个最著名的数学家庭的第一代。在后来的三代里，一共有 $8$ 到 $12$ 个伯努利，在概率论、统计学和数学上做出了杰出的基础性贡献。\n伯努利分布在一次试验中，事件 $A$ 出现的概率为 $p$，不出现的概率为 $q=1-p$。若以 $\\beta$ 记事件 $A$ 出现的次数，则 $\\beta$ 仅取 $0, 1$ 两值，相应的概率分布为：\n$$ P(\\beta = k) = \\begin{cases} p \u0026amp; \\text{if } k = 1 \\ 1-p \u0026amp; \\text{if } k = 0 \\end{cases} $$\n其中 $k \\in {0, 1}$。\n二项分布是指在只有两个结果的 $n$ 次独立的伯努利试验中，所期望的结果出现次数的概率。在单次试验中，结果A出现的概率为 $p$ ，结果B出现的概率为 $q$，$p+q=1$。那么在 $n=10$ ，即 $10$ 次试验中，结果A出现 $0$ 次、$1$ 次、……、$10$ 次的概率各是多少呢？这样的概率分布呈现出什么特征呢？这就是二项分布所研究的内容。\n还是先举个例子吧。\n掷一枚硬币(怎么老是硬币？小学的时候就讲了)出现正面和反面的概率各为 $0.5$ ，那么掷 $1$ 次，出现正面的概率肯定是 $0.5$ 。掷 $2$ 次、掷 $3$ 次呢？\n掷 $2$ 次出现的结果有 $4$ 个，正正、正反、反正、反反。因为 $p=0.5$，所以每个结果出现的概率是 $0.5×0.5=0.25$，那正面出现 $2$ 次、$1$ 次、$0$ 次的概率分别是 $0.25$ 、$0.5$ 、$0.25$ 。\n掷3次出现的结果有8个，正正正、正正反、正反正、正反反、反正正、反正反、反反正、反反反。每个结果出现的概率是 $0.5×0.5×0.5=0.125$ ，那正面出现 $3$ 次、$2$ 次、$1$ 次、$0$ 次的概率分别是 $0.125$、$0.375$、$0.375$、$0.125$。\n统计学家们总结出了计算概率的一般公式\n$$ b(x,n,p) = C_n^x p^x q^{n-x} $$\n其中 $b$ 表示二项分布的概率，$n$ 表示试验次数，$x$ 表示出现某个结果的次数。是组合，表示在 $n$ 次试验中出现 $x$ 次结果的可能的次数。如 $10$ 次试验，出现 $0$ 次正面的次数有 $1$ 次，出现 $1$ 次正面的次数有 $10$ 次，……，出现 $5$ 次正面的次数有 $252$ 次，等等。其计算也有一个通式：\n$$ C_n^x=\\frac{n \\times (n-1) \\times \u0026hellip; \\times (n-x-1)}{x \\times (x-1) \\times \u0026hellip; \\times 1} $$\n也可以写成\n$$ C_n^x=\\frac{n!}{(n-x)!x!} $$\n如果这个公式你算不好，就查下面的杨辉三角形吧，每一行的数字是上一行相邻两个数字的和。在下图中，每一行表达的是 $(a+b)^n$ 展开式的各项系数，下图列出了 $n=0,1,\u0026hellip;,16$ 时展开式中各项的系数。\n需要特别提醒的是：二项分布是建立在有放回抽样的基础上的，也就是抽出一个样品测量或处理完后再放回去，然后抽下一个。在实际的工作中通常我们很少会这样抽，一般都属于无放回抽样，这时候需要用超几何分布来计算概率。在一般的教课书上都会要求，\n当总体的容量 $N$ 不大时，要用超几何分布来计算， 如果 $N$ 很大而 $n$ 很小，则可以用二项分布来近似计算，也就是可以将无放回抽样近似看出有放回抽样。至于 $n$ 要小到什么程度，有的书上说 $n/N$ 小于 $0.1$ 就可以了，有的书上则要求小于 $0.05$。 在很多工厂里，通常都会跟零件供应商约定供货合格率，并对每批供货进行抽检，就是所谓的 $IQC$ 。设约定的合格品率为 $97%$ ，如果每批随机抽 $10$ 件，那么抽出 $1$ 件不合格时，整批的零件的合格率是不是达不到 $97%$？\n根据题意，$p=0.97$，$n=10$，$x=9$，据此算出 $10$ 个样品中有 $9$ 个合格品的概率是\n$$ b(9,10,0.97) = C_{10}^9 \\times 0.97^9 \\times 0.03^1 = 0.228 $$\n反过来，如果考虑不合格品率，$p=0.03$，$n=10$，$x=1$，据此计算出 $10$ 个样品中有 $1$ 个不合格品的概率是\n$$ b(1,10,0.03) = C_{10}^1 \\times 0.03^1 \\times 0.97^9 = 0.228 $$\n结果是一样的。由此可见，$10$ 个样品中有 $1$ 个不合格品的概率还是很大的，因此不能说这批零件不合格。\n那抽出 $2$ 个不合格的呢？同样可以算出\n$$ b(2,10,0.03) = C_{10}^2 \\times 0.03^2 \\times 0.97^8 = 0.032 $$\n概率非常小，而且抽出超过 $2$ 个以上不合格品的概率会更小，因此如果 $10$ 个样品中有 $2$ 个或以上的不合格品，则整批的零件合格率肯定达不到 $97%$ ，可以整批退货。\n如果约定的合格率是 $99.5%$ ，则出现 $0$ 个、$1$ 个、$2$ 个不合格品的概率分别为 $0.951$ 、$0.0478$ 、$0.001$，如此 $10$ 个只要抽出 $1$ 个不合格品就可以整批退货了。\n有人会问，到底应该抽多少样呢？这在GB/T2828里有明确规定，限于篇幅，这里只介绍其中一种最简单的应用原理，具体应用时大家可以去查国标。\n假设你与供应商约定的接收合格率是 $99%$ ，即 $AQL(接收质量限)=0.01$ ，本批的总数量是 $1000$ 只，只做一般性的检验，查国标可得抽样量为 $80$ ； $Ac=2$，即抽到2个及以下不合格品可接收该批； $Re=3$，即抽到3个及以上不合格品则拒绝接收。 限于人力物力，你可能无法抽这么多的样，根据该供应商以往的表现，你制定了两种抽样方案，\n一种是抽 $20$ 个，不合格品为 $0$ 接收，大于 $0$ 退回； 另一种是抽 $50$ 个，不合格品不超过 $1$ 则接收，大于 $1$ 则退回。 我们来看看，如果这批来料合格率只有 $98%$ ，按照这两种抽样方案以及国标的方案，你接收的概率有多大。为了方便我们用Excel来算。\n方案一\n方案二\n国标方案\n这几种方案接收的概率都不小，这就是抽样检验带来的风险。如果实际批合格率低于约定合格率，仍被接收的风险属于使用者风险。\n反过来，如果批合格率高于约定合格率，如99.5%，那有多大的可能性拒绝该批呢？我们也可以用二项分布来计算。\n我们可以看到，即使实际合格率高于约定，仍然存在拒收的风险，虽然这个风险并不大，通常这一类的风险叫做生产者风险。\n根据不同的批合格率，可以计算出每一种抽样方案的两类风险，画出OC曲线。用方案一画出的OC曲线如下：\n图中横坐标为实际的批不合格率，纵坐标为接收概率，曲线下方为接收概率，上方为拒收概率，可以看出即使来料不合格率远高于约定，接收的概率还是很大的。黄色的矩形框称为理想曲线，理想的情况下，批不合格率低于约定肯定接收，而超过约定则肯定拒收，但这种理想曲线是不可能达到的，只能尽可能接近。\n下面我们再看看三种抽样方案的OC曲线之间的对比。\n三种方案各有优劣势，但国标方案的下降趋势要比另外两种要快，更接近理想曲线。当然你也可以试一下其它的抽样方案，有可能会找到更好的。\n确定抽样方案不是靠拍脑袋来决定的，需要对抽样方案进行比较深入的研究，找到最恰当的抽样方案。\n另外，关于抽样问题要具体问题具体分析，如果供应商质量控制能力很强，可以放宽检验甚至免检(可以将此作为供应商的激励措施，这也是我在客户那里极力推动的，虽然这项政策最终是依据国内一个质量大腕的建议制定的，我仍然觉得非常高兴)；如果供应商质量控制能力很差，就需要加严检验。有时要控制误检，有时要控制漏检，这要看成本与收益。我曾经服务的一家客户对一个零件专门配10个人进行全检，就是为了防止漏检，因为必须要100%合格，否则因为漏检造成客户的索赔是承受不起的。\n本文所描述的仅仅是国标中最基本、最简单的应用，当遇到各种复杂的情况时，要想到去参考国标。\n最后再说一下二项分布的正态近似。在大样本的情况下，二项分布的计算会很麻烦，这时可以采用正态分别来近似，其条件是np和n(1-p)都大于5。采用正态分布的参数为：\n$$ \\mu=np, \\sigma=\\sqrt{np(1-p)} $$\n","date":"24 April 2024","permalink":"/posts/ai/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/%E4%BA%8C%E9%A1%B9%E5%88%86%E5%B8%83/","section":"博客","summary":"二项分布相关。","title":"二项分布"},{"content":"","date":"23 April 2024","permalink":"/posts/reviews/24%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0/%E5%8D%8E%E4%B8%BA/","section":"博客","summary":"不打没有准备的仗！！实习过程中的复习。","title":"华为"},{"content":"","date":"23 April 2024","permalink":"/posts/ai/online-ml/","section":"博客","summary":"在线学习(Online Learning)是一种特殊的算法类型，能够逐步接收数据，并实时更新模型，以适应新数据，涉及到数据流和实时学习的概念。","title":"Online Machine Learning"},{"content":"Online Learning是工业界比较常用的机器学习算法，在很多场景下都能有很好的效果。\n机器学习纯粹主义者可能会嘲笑实时学习在线算法的想法。训练模型可能会以多种不同的方式出现错误：算法本身可能不合适，模型可能无法很好地泛化，学习率可能错误，正则化可能太低或太高……这样的例子不胜枚举。当无法保证会发生什么时，我们到底为什么要尝试立即学习？\n答案很简单：无论模型有多好，或者输入了多少数据，模型仍然是环境的不完美表示。为了立即做出最佳决策，我们不能拥有一个只知道昨天发生的事情的模型。\nimport numpy as np from sklearn import linear_model n_samples, n_features = 1, 500 y = np.random.randn(n_samples) X = np.random.randn(n_samples, n_features) clf = linear_model.SGDRegressor() # 该fit()方法完成了所有的训练魔法，产生了一个我们可以用于预测的模型 clf.fit(X, y) import time start_time = time.time() # 还有一种partial_fit()方法，可以对小批量数据进行增量训练。 clf.partial_fit(X, y) elapsed_time = time.time() - start_time print(elapsed_time) Online Learning（也称增量学习或核外学习）并不是一种模型，而是一种模型的训练方法，Online Learning能够根据线上反馈数据，实时快速地进行模型调整，使得模型及时反映线上的变化，提高线上预测的准确率。Online Learning的流程包括：将模型的预测结果展现给用户，然后收集用户的反馈数据，再用来训练模型，形成闭环的系统。如下图所示：\nOnline Learning有点像自动控制系统，但又不尽相同，二者的区别是：Online Learning的优化目标是整体的损失函数最小化，而自动控制系统要求最终结果与期望值的偏差最小。\n传统的训练方法，模型上线后，更新的周期会比较长（一般是一天，效率高的时候为一小时），这种模型上线后，一般是静态的（一段时间内不会改变），不会与线上的状况有任何互动，假设预测错了，只能在下一次更新的时候完成更正。Online Learning训练方法不同，会根据线上预测的结果动态调整模型。如果模型预测错误，会及时做出修正。因此，Online Learning能够更加及时地反映线上变化。\n如上图所示，Online Learning训练过程也需要优化一个目标函数（红框标注的），但是和其他的训练方法不同，Online Learning要求快速求出目标函数的最优解，最好是能有解析解。\nOnline Learning 的实现方法 # 前面说到Online Learning要求快速求出目标函数的最优解。要满足这个要求，一般的做法有两种：\nFTRL（Follow The Regularized Leader） BPR（Bayesian Probit Regression） McMahan H B, Holt G, Sculley D, et al. Ad Click Prediction: a View from the Trenches. Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD). 2013.\nGraepel T, Candela J Q, Borchert T,et al. Web-Scale Bayesian Click-Through Rate Prediction for Sponsored Search Advertising in Microsoft’s Bing Search Engine. Proceedings of the 27th International Conference on Machine Learning ICML. 2010.\n下面就详细介绍这两种做法的思路。\nBayesian Online Learning # 贝叶斯方法能够比较自然地导出Online Learning的训练方法：给定参数先验，根据反馈计算后验，将其作为下一次预测的先验，然后再根据反馈计算后验，如此进行下去，就是一个Online Learning的过程，如下图所示。\n举个例子， 我们做一个抛硬币实验，估算硬币正面的概率 $\\mu$ 。我们假设 $\\mu$ 的先验满足 $p(\\mu)=Beta(\\alpha,\\beta)$ 。\n对于观测值 $Y=1$ ，代表是正面，我们可以算的后验 $p(\\mu | Y=1)=Beta(\\alpha +1,\\beta)$ 对于观测值 $Y=0$ ，代表是反面，我们可以算的后验 $p(\\mu | Y=0)=Beta(\\alpha,\\beta + 1)$ 补充 Beta分布\nResources # 什么是在线机器学习？ ","date":"23 April 2024","permalink":"/posts/ai/online-ml/%E4%BB%80%E4%B9%88%E6%98%AF%E5%9C%A8%E7%BA%BF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/","section":"博客","summary":"Online Learning是工业界比较常用的机器学习算法，在很多场景下都能有很好的效果。Online Learning并不是一种模型，而是一种模型的训练方法，Online Learning能够根据线上反馈数据，实时快速地进行模型调整，使得模型及时反映线上的变化，提高线上预测的准确率。Online Learning的流程包括：将模型的预测结果展现给用户，然后收集用户的反馈数据，再用来训练模型，形成闭环的系统。","title":"什么是在线机器学习"},{"content":"","date":"22 April 2024","permalink":"/posts/reviews/24%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0/%E9%98%BF%E9%87%8C%E7%B3%BB/","section":"博客","summary":"不打没有准备的仗！！实习过程中的复习。","title":"alibaba"},{"content":"","date":"22 April 2024","permalink":"/posts/reviews/24%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0/%E9%98%BF%E9%87%8C%E7%B3%BB/%E8%9A%82%E8%9A%81/","section":"博客","summary":"不打没有准备的仗！！实习过程中的复习。","title":"蚂蚁金服"},{"content":"编程题 # 前两题速通了，第三题有一些问题。脑子真的抽了，没做出来。\n题目概述\n第一行给你三个数 n a b，n代表第二行的数组个数，第二行给你n个int值，你可以把这个数组染成黑色或者白色，要求黑色的总和加起来要是 a 的倍数，白色总和要是 b 的倍数，请问有多少种可能性，黑色和可以为0，白色和也可以为0\n示例\n输入\n4 1 2 1 2 3 5 输出\n8 import java.util.*; public class SubsetColorings { public static void main(String[] args) { int[] nums = {1, 2, 3, 5}; int a = 1; int b = 2; System.out.println(\u0026#34;Number of valid colorings: \u0026#34; + countColorings(nums, a, b)); } public static int countColorings(int[] nums, int a, int b) { int count = 0; int n = nums.length; // 生成所有子集 List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; subsets = generateSubsets(nums, n); // 遍历所有子集，确定符合条件的黑白分组 for (List\u0026lt;Integer\u0026gt; subset : subsets) { int blackSum = sum(subset); // 子集和作为黑色组 int whiteSum = Arrays.stream(nums).sum() - blackSum; // 剩余的作为白色组 if (blackSum % a == 0 \u0026amp;\u0026amp; whiteSum % b == 0) { count++; } } return count; } private static List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; generateSubsets(int[] nums, int n) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; subsets = new ArrayList\u0026lt;\u0026gt;(); // 按位掩码来生成所有子集 for (int i = 0; i \u0026lt; (1 \u0026lt;\u0026lt; n); i++) { List\u0026lt;Integer\u0026gt; subset = new ArrayList\u0026lt;\u0026gt;(); for (int j = 0; j \u0026lt; n; j++) { if ((i \u0026amp; (1 \u0026lt;\u0026lt; j)) != 0) { subset.add(nums[j]); } } subsets.add(subset); } return subsets; } private static int sum(List\u0026lt;Integer\u0026gt; list) { int sum = 0; for (int num : list) { sum += num; } return sum; } } 超时间\npublic class OptimizedColorings { public static void main(String[] args) { int[] nums = {1, 2, 3, 5}; int a = 1; int b = 2; System.out.println(\u0026#34;Number of valid colorings: \u0026#34; + countColorings(nums, a, b)); } public static int countColorings(int[] nums, int a, int b) { // 初始化dp数组 int[][][] dp = new int[nums.length + 1][a][b]; dp[0][0][0] = 1; // 初始条件：没有元素时，只有一种方案（全部元素不选择） // 更新dp数组 for (int idx = 1; idx \u0026lt;= nums.length; idx++) { int num = nums[idx - 1]; for (int i = 0; i \u0026lt; a; i++) { for (int j = 0; j \u0026lt; b; j++) { // 不选择当前元素，继承之前的状态 dp[idx][i][j] = dp[idx - 1][i][j]; // 选择当前元素为黑色 dp[idx][(i + num) % a][j] += dp[idx - 1][i][j]; // 选择当前元素为白色 dp[idx][i][(j + num) % b] += dp[idx - 1][i][j]; } } } // 因为黑色和白色的和均需为0（模a和模b），我们返回dp[nums.length][0][0] return dp[nums.length][0][0]; } } ","date":"20 April 2024","permalink":"/posts/reviews/24%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0/%E9%98%BF%E9%87%8C%E7%B3%BB/%E8%9A%82%E8%9A%81/%E7%AC%94%E8%AF%954.20/","section":"博客","summary":"阿里的笔试有单选和多选，但是编程题给人的感觉不是很难。","title":"蚂蚁金服笔试4.20"},{"content":"","date":"19 April 2024","permalink":"/posts/devoops/","section":"博客","summary":"DevOps 集文化理念、实践和工具于一身，它强调团队授权、跨团队沟通和协作以及技术自动化，其最终目标是优化质量和交付","title":"DevOops"},{"content":"","date":"19 April 2024","permalink":"/tags/git/","section":"Tags","summary":"","title":"Git"},{"content":"📚 资料 # 官方资源 Git 官网 Git Github 模板 gitignore 模板 - .gitignore 文件模板 gitattributes 模板 - .gitattributes 文件模板 github-cheat-sheet - git 命令简略图表 Git 教程 Learn Git branching - 交互式教程 Git 官方推荐教程 - Scott Chacon 的 Git 书。 git-flight-rules git-tips Git 中文教程 廖雪峰的 Git 教程 有关 git 的学习资源 文章 Git Cookbook Git 奇技淫巧 Git 风格指南 Git 在团队中的最佳实践\u0026ndash;如何正确使用 Git Flow Git 工具 guis - Git 官网展示的客户端工具列表。 gogs - 极易搭建的自助 Git 服务。 gitflow - 应用 fit-flow 模型的工具。 firstaidgit.io 一个可搜索的最常被问到的 Git 的问题 git-extra-commands - 一堆有用的额外的 Git 脚本 git-extras - GIT 工具集 \u0026ndash; repo summary, repl, changelog population, author commit percentages and more git-fire - git-fire 是一个 Git 插件，用于帮助在紧急情况下添加所有当前文件, 做提交(committing), 和推(push)到一个新分支(阻止合并冲突)。 git-tips - Git 小提示 git-town - 通用，高级 Git 工作流支持！ GUI 客户端 GitKraken - 豪华的 Git 客户端 Windows, Mac \u0026amp; Linux git-cola - 另外一个 Git 客户端 Windows \u0026amp; OS X GitUp - 一个新的 Git 客户端，在处理 Git 的复杂性上有自己的特点 gitx-dev - 图形化的 Git 客户端 OS X Source Tree - 免费的图形化 Git 客户端 Windows \u0026amp; OS X Tower - 图形化 Git 客户端 OS X(付费) git cheat sheet github-git-cheat-sheet 📖 内容 # ","date":"19 April 2024","permalink":"/posts/devoops/git/","section":"博客","summary":"Git 是一个开源的分布式版本控制系统。","title":"Git"},{"content":" git config --global --unset https.https://github.com.proxy git config --global --unset http.https://github.com.proxy ","date":"19 April 2024","permalink":"/posts/devoops/git/gnutls_handshake/","section":"博客","summary":"Git 帮助手册","title":"跳坑 gnutls_handshake() failed: The TLS connection was non-properly terminated."},{"content":"","date":"19 April 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/","section":"博客","summary":"刷题过程中记录的算法技巧","title":"算法技巧"},{"content":"高精度乘法 # import java.util.*; public class Main { public static String mulNums(String a, String b) { // Create an array to store multiplication results int[] res = new int[a.length() + b.length() - 1]; // Populate the result array with products of digits for (int i = 0; i \u0026lt; a.length(); i++) { for (int j = 0; j \u0026lt; b.length(); j++) { res[i + j] += (a.charAt(i) - \u0026#39;0\u0026#39;) * (b.charAt(j) - \u0026#39;0\u0026#39;); } } // Convert the result array into the final result string StringBuilder ret = new StringBuilder(); int carry = 0; for (int i = res.length - 1; i \u0026gt;= 0; i--) { int num = res[i] + carry; ret.insert(0, (char) (num % 10 + \u0026#39;0\u0026#39;)); carry = num / 10; } // Add remaining carry if present if (carry \u0026gt; 0) ret.insert(0, (char) (carry + \u0026#39;0\u0026#39;)); return ret.toString(); } public static void main(String[] args) { System.out.println(mulNums(\u0026#34;12345\u0026#34;, \u0026#34;67890111\u0026#34;)); } } 高精度除法 # import java.math.BigInteger; public class HighPrecisionDivision { public static String divideNumbers(String dividend, String divisor) { // Convert strings to BigInteger for handling large numbers BigInteger bigDividend = new BigInteger(dividend); BigInteger bigDivisor = new BigInteger(divisor); // Perform division and get the quotient BigInteger quotient = bigDividend.divide(bigDivisor); // Return the quotient as a string return quotient.toString(); } public static void main(String[] args) { String dividend = \u0026#34;12345678901234567890\u0026#34;; String divisor = \u0026#34;12345\u0026#34;; System.out.println(\u0026#34;Quotient: \u0026#34; + divideNumbers(dividend, divisor)); } } public class HighPrecisionDivision { public static String divide(String dividend, String divisor, int scale) { // 移除小数点并记录小数点扩展的位数 int scaleDividend = dividend.indexOf(\u0026#39;.\u0026#39;); int scaleDivisor = divisor.indexOf(\u0026#39;.\u0026#39;); int decimalPlaces = 0; if (scaleDividend != -1) { decimalPlaces += (dividend.length() - scaleDividend - 1); dividend = dividend.replace(\u0026#34;.\u0026#34;, \u0026#34;\u0026#34;); } if (scaleDivisor != -1) { decimalPlaces -= (divisor.length() - scaleDivisor - 1); divisor = divisor.replace(\u0026#34;.\u0026#34;, \u0026#34;\u0026#34;); } // 处理结果的小数点位置 decimalPlaces -= scale; // 用于收集结果的StringBuilder StringBuilder result = new StringBuilder(); // 高精度的除法核心 StringBuilder part = new StringBuilder(); int index = 0; while (index \u0026lt; dividend.length()) { part.append(dividend.charAt(index)); while (new BigInteger(part.toString()).compareTo(new BigInteger(divisor)) \u0026lt; 0 \u0026amp;\u0026amp; index \u0026lt; dividend.length() - 1) { part.append(dividend.charAt(++index)); if (result.length() != 0) result.append(\u0026#34;0\u0026#34;); } BigInteger[] divisionResult = new BigInteger(part.toString()).divideAndRemainder(new BigInteger(divisor)); result.append(divisionResult[0].toString()); part = new StringBuilder(divisionResult[1].toString()); index++; } // 处理余数部分以生成所需精度的小数 if (scale \u0026gt; 0) { result.append(\u0026#39;.\u0026#39;); for (int i = 0; i \u0026lt; scale; i++) { part.append(\u0026#34;0\u0026#34;); BigInteger[] divisionResult = new BigInteger(part.toString()).divideAndRemainder(new BigInteger(divisor)); result.append(divisionResult[0].toString()); part = new StringBuilder(divisionResult[1].toString()); } } // 调整小数点的最终位置 if (decimalPlaces \u0026gt; 0) { result.append(\u0026#34;0\u0026#34;.repeat(decimalPlaces)); } else if (decimalPlaces \u0026lt; 0) { result.insert(result.length() + decimalPlaces, \u0026#39;.\u0026#39;); } return result.toString(); } public static void main(String[] args) { String dividend = \u0026#34;12345.678\u0026#34;; String divisor = \u0026#34;12.345\u0026#34;; String result = divide(dividend, divisor, 10); // 设置小数点后的精度为10位 System.out.println(\u0026#34;Result: \u0026#34; + result); } } 高精度加减法 # public class HighPrecisionArithmetic { // 高精度加法 public static String add(String num1, String num2) { StringBuilder result = new StringBuilder(); int carry = 0; // 进位 // 对齐两个字符串的长度 int maxLen = Math.max(num1.length(), num2.length()); num1 = String.format(\u0026#34;%\u0026#34; + maxLen + \u0026#34;s\u0026#34;, num1).replace(\u0026#39; \u0026#39;, \u0026#39;0\u0026#39;); num2 = String.format(\u0026#34;%\u0026#34; + maxLen + \u0026#34;s\u0026#34;, num2).replace(\u0026#39; \u0026#39;, \u0026#39;0\u0026#39;); // 从右向左逐位相加 for (int i = maxLen - 1; i \u0026gt;= 0; i--) { int digit1 = num1.charAt(i) - \u0026#39;0\u0026#39;; int digit2 = num2.charAt(i) - \u0026#39;0\u0026#39;; int sum = digit1 + digit2 + carry; carry = sum / 10; result.append(sum % 10); } // 如果最后还有进位，需要添加到结果的最前面 if (carry != 0) { result.append(carry); } return result.reverse().toString(); } // 高精度减法 public static String subtract(String num1, String num2) { // 确定哪个数字大，以决定结果的正负 boolean negative = false; if (num1.length() \u0026lt; num2.length() || (num1.length() == num2.length() \u0026amp;\u0026amp; num1.compareTo(num2) \u0026lt; 0)) { String temp = num1; num1 = num2; num2 = temp; negative = true; // 结果为负数 } StringBuilder result = new StringBuilder(); int borrow = 0; // 借位 // 对齐两个字符串的长度 int maxLen = Math.max(num1.length(), num2.length()); num1 = String.format(\u0026#34;%\u0026#34; + maxLen + \u0026#34;s\u0026#34;, num1).replace(\u0026#39; \u0026#39;, \u0026#39;0\u0026#39;); num2 = String.format(\u0026#34;%\u0026#34; + maxLen + \u0026#34;s\u0026#34;, num2).replace(\u0026#39; \u0026#39;, \u0026#39;0\u0026#39;); // 从右向左逐位相减 for (int i = maxLen - 1; i \u0026gt;= 0; i--) { int digit1 = num1.charAt(i) - \u0026#39;0\u0026#39;; int digit2 = num2.charAt(i) - \u0026#39;0\u0026#39; + borrow; if (digit1 \u0026lt; digit2) { digit1 += 10; borrow = 1; } else { borrow = 0; } result.append(digit1 - digit2); } // 移除结果前端的零 while (result.length() \u0026gt; 1 \u0026amp;\u0026amp; result.charAt(result.length() - 1) == \u0026#39;0\u0026#39;) { result.deleteCharAt(result.length() - 1); } if (negative) { result.append(\u0026#39;-\u0026#39;); } return result.reverse().toString(); } public static void main(String[] args) { System.out.println(\u0026#34;Addition: \u0026#34; + add(\u0026#34;1234567890\u0026#34;, \u0026#34;987654321\u0026#34;)); System.out.println(\u0026#34;Subtraction: \u0026#34; + subtract(\u0026#34;1234567890\u0026#34;, \u0026#34;987654321\u0026#34;)); } } ","date":"19 April 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E9%AB%98%E7%B2%BE%E5%BA%A6%E7%AE%97%E6%B3%95/","section":"博客","summary":"高精度算法详情。","title":"高精度算法"},{"content":"","date":"18 April 2024","permalink":"/posts/reviews/24%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0/%E8%85%BE%E8%AE%AF/","section":"博客","summary":"不打没有准备的仗！！实习过程中的复习。","title":"鹅厂"},{"content":"","date":"16 April 2024","permalink":"/posts/ai/ai-ci-cd/","section":"博客","summary":"AI的CI/CD是指在人工智能开发过程中采用持续集成（Continuous Integration，CI）和持续部署（Continuous Deployment，CD）的实践。这种方法使得AI模型可以实现自动化测试和部署，从而加快迭代速度，提高软件质量和稳定性。CI负责将代码更改频繁地合并到主分支，而CD则自动将代码部署到生产环境，确保了开发的高效与高质。","title":"AI Model CI/CD"},{"content":"运行教程指南 # Databricks：开始使用 MLflow 的最简单方法是使用 Databricks 提供的托管 MLflow 服务。这里有两个单独的选项，一个对于 Databricks 客户来说是最方便的，另一个是任何人都可以免费使用的。\nDatabricks 社区版页面 登录社区版后，您将看到如下所示的登录页面：\n自己托管 MLflow 服务器：如果您想要使用自己的托管 MLflow 服务器，则只需将 MLflow 跟踪 URI 设置为指向您的服务器即可。如果连接有其他配置或访问限制，请联系管理 MLflow 部署的小组以获取更多信息。\n要直接设置 MLflow 跟踪 URI（假设您没有设置额外的安全验证），您需要做的就是在笔记本中：\nmlflow.set_tracking_uri( \u0026#34;http://\u0026lt;your-mlflow-server\u0026gt;:\u0026lt;the port number that is configured to accept traffic\u0026gt;\u0026#34; ) 入门指南 # MLflow Tracking 快速入门 # MLflow Tracking 是 MLflow 的主要服务组件之一，可以用来\n记录模型的训练统计数据（损失、准确性等）和超参数 记录（保存）模型以供以后检索 使用 MLflow 模型注册表注册模型以启用部署 加载模型并将其用于推理 Step 1 - Get MLflow\npip install mlflow Step 2 - Start a Tracking Server\nUsing a Managed MLflow Tracking Server : 有关使用托管 MLflow 跟踪服务器的选项的详细信息，包括如何使用托管 MLflow 创建免费的 Databricks Community Edition 帐户。但是使用这些仓库需要用到 Google Cloud 相关的产品，只有14天的免费日期，之后就要付款 (Optional) Run a local Tracking Server : 我们将启动一个本地 MLflow 跟踪服务器，我们将连接到该服务器来记录本快速入门的数据。从终端运行 mlflow server --host 127.0.0.1 --port 8080 ","date":"16 April 2024","permalink":"/posts/ai/ai-ci-cd/mlflow/","section":"博客","summary":"运行教程指南 # Databricks：开始使用 MLflow 的最简单方法是使用 Databricks 提供的托管 MLflow 服务。这里有两个单独的选项，一个对于 Databricks 客户来说是最方便的，另一个是任何人都可以免费使用的。","title":"MLflow 入门"},{"content":"MLOps这个市场的相关供应商不多也不少，找了一圈，一共找到了300多家！下面是找到的全球的MLOps平台和ML工具总览，列出那些专门针对机器学习任务和目标的项目。\n力荐 MLOop 技术栈\n关键统计数据 # 一共分析了300多个平台和工具，其中大约220个是活跃项目。 在整个欧洲，只能辨认出4个MLOps平台（MLReef、Hopsworks、Valohai（芬兰）和Polyaxon（德国柏林））。 大多数平台和工具涵盖 ML 生命周期中2-3个任务。 MLOps的生命周期 # 下图展示了哪些工具和平台为ML生命周期中的各个任务和流程提供服务。 为了更好地概览，我们将生命周期分为4个主要的ML领域：\n数据管理：主要是所有以机器学习为重点的任务，用于探索、管理和创建数据（集）。 建模：主要是从数据处理、训练模型、验证模型等所有流水线（Pipeline）相关的任务。 持续部署：主要是与MLOps的“Ops”部分相关的所有任务，包括启动、监控和保护生产模型。 计算和资源管理： 主要是与计算和资源管理相关的所有功能 注意：\n一个提供的工具可以在ML生命周期中多个任务中发现。 我们通过查看他们的网站、演示视频和动手测试来尽可能地确定他们的产品。\nMLOps平台对比 # 我们想要更深入地分析工具，将他们定位为 ML 生命周期中涵盖更广泛任务的全面的平台。这种区分需要基于可识别和具体的指标进行，以避免随意选择。我们认为，MLOps 平台需要满足如下几点：\n至少涵盖 ML 生命周期中的 5 个任务， 至少存在于 2 个主要领域（例如数据管理 + 建模）， 将自己定位为 MLOps 平台。 根据这一标准，在220多个确定的平台和工具中，只有 18 个是 MLOps 平台。\n审查特征 # 下一个有趣的问题是：这些平台之间的区别是什么？\n我们决定最好的方法不是列出特定的特性和功能，而是列出关于它们的产品的更通用的能力。我们提出以下“软”标准，在我们看来，这些标准定义了一个出色的 MLOps 平台：\n整体性：MLOps 平台需要涵盖广泛的 ML 任务可供选择。图中长条的规则为：一个表示覆盖一个主要领域，两个表示覆盖两个主要领域，三个表示覆盖三个主要领域。 协作：MLOps 中的一项关键任务是协作，并且随着 ML 解决方案越来越多地嵌入到组织中，协作变得越来越重要。我们根据在数据处理、建模和管理运行时环境的流水线（Pipeline）上共享和并行工作的可能性来定义协作。规则：一个长条表示覆盖三个部分中的其中一个。 可重现性：在 ML 中重现整个增值链非常重要，因为它能够让我们理解预测并增加对已部署模型的信心。我们将可重现性定义为数据、源代码和超参数以及环境配置的跟踪和版本控制。规则：每个可变主题（例如数据、代码和超参数、运行时环境）表示一个长条。 社区：随着越来越多的数据集、基于代码的函数和库可用，我们认为访问社区内容越来越重要。GitHub 一直是一个很好的代码来源，Kaggle 和其他代码/项目托管平台也是如此。尽管如此，我们希望看到在MLOps平台中直接利用社区协同增效作用。规则：MLOps平台内的每个可共享 ML 元素对应一个条（对于团队外部来说）。 托管：在哪里可以使用该平台？仅限云、自托管或仅限本地。规则：主要的三种可能的使用方式中的每一种都对应一个长条。 数据连通性：本节描述了平台内数据获取的可能性。我们确定了四种：在平台上、通过数据源（数据连接器）、第三方应用程序的直接 API 以及通过直接访问数据库。 规则：前面提到的每种数据连接类型都有一个长条（限制为 3 个长条）。 专业水平：随着越来越多的新手进入 ML 市场，我们认为在平台的一般可操作性方面易于使用变得越来越重要。 最后一个特征更难评估，因为它涉及许多不同的方面（UI/UX、工作流机制、帮助文档、一般概念等）。这部分的讨论更加开放，但我们尽量做到客观。规则：一个长条表示仅限于专家，两个长条表示适用于专家和高级用户，三个长条表示适用于专家、高级和初学者。 深入审查MLOps平台 # 以下部分将仔细研究上面列出的 MLOps 平台。 在接下来的概览分析中，我们还将包括我们找到的每个工具和平台的部分（但现在这有点太多了！）。\nMLReef（开源）\n描述：MLReef 是一个开源 MLOps 平台，为机器学习项目提供托管。 它建立在您的团队或社区制作的可重用 ML 模块之上，以提高快速迭代和易于采用的能力。 它基于 git 来促进并发工作流、更高效、协作和可重复的 ML 开发，包含四个主要部分： 数据管理：完全版本化的数据托管和处理基础设施 发布代码存储库：容器化和版本化的脚本存储库，保证数据流水线中的使用不可变 实验管理：实验跟踪、环境和结果管理 MLOps：ML/DL作业的流水线（pipeline）和编排解决方案（k8s/云/裸机） 开源仓库：https://gitlab.com/mlreef/mlreef 和 https://github.com/MLReef/mlreef Databricks\n描述：一个开放、简单的平台，用于存储和管理您的所有数据并支持您的所有分析和 AI 用例。 开源仓库：https://github.com/databricks H2O（开源）\n描述：H2O.ai 是 H2O 的创建者，H2O 是领先的开源机器学习和人工智能平台，受到全球 14,000 家企业的数据科学家的信赖。 我们的愿景是通过我们屡获殊荣的数据科学平台Driverless AI，让每个人的智能大众化。 开源仓库：github.com/h2oai Iguazio\n描述：Iguazio数据科学平台将人工智能项目转化为现实世界的业务成果。 使用 MLOps 和端到端自动化机器学习流水线（Pileline）来加速和扩展你的AI应用程序的开发、部署和管理。 开源仓库：github.com/iguazio/ Iguazio MLOps 平台：自动化并加速数据科学工作流程，使概念从开发到部署顺利完成。 关键组件：\n特征平台：实时和批量数据的自动化离线和在线特征工程。 实时服务流水线：使用实时serverless技术快速开发可扩展数据和 ML 流水线。 监控 \u0026amp; 重新训练：无代码数据和模型监控、漂移检测和自动再训练。 将CI/CD用于ML：使用主流的 ML、Git 和 CI/CD 框架，跨代码、数据和模型集成 CI/CD。 Hopsworks（新增，开源）\n描述：Hopsworks 2.0 是一个用于 ML 模型开发和运行的开源平台，可用作本地平台（开源或企业版）以及 AWS 和 Azure 上的托管平台。 开源仓库：https://github.com/logicalclocks/hopsworks Algorithmia\n描述：Algorithmia 是机器学习运维 (MLOps) 软件，用于管理现有操作流程中 ML 生命周期的所有阶段。 快速、安全且经济高效地将模型投入生产。 开源仓库：https://github.com/algorithmiaio Allegro AI（开源，ClearML）\n描述：端到端企业级平台，供数据科学家、数据工程师、DevOps 和管理人员管理整个机器学习和深度学习产品生命周期。 开源仓库：https://github.com/allegroai Valohai⭐️（商业化，来自芬兰）\n描述：训练、评估、部署、重复。 Valohai 是唯一一个从数据提取到模型部署自动化的 MLOps 平台。 开源仓库：github.com/valohai Amazon SageMaker\n描述：Amazon SageMaker 通过汇集专为 ML 构建的广泛功能集，帮助数据科学家和开发人员快速准备、构建、训练和部署高质量的机器学习 (ML) 模型。 开源仓库：https://github.com/aws/amazon-sagemaker-examples Pachyderm（开源）\n描述：用于托管和管理的Pachydrm适用于希望获得Pachydrm所能提供的一切的用户，无需亲自管理基础设施。借助 Hub，您可以对数据进行版本控制、部署端到端流水线（Pipeline）等。 几乎不需要任何设置，而且是免费的！ 开源仓库：https://github.com/pachyderm/pachyderm Dataiku（商业化）\n描述：Dataiku 是一个使数据访问大众化并使企业能够以以人为中心的方式构建自己的 AI 之路的平台。 注意：它们仅限于表格数据。 开源仓库：github.com/dataiku Alteryx\n描述：从数据到发现再到决策——只需几分钟。并且自动化和优化业务效果的分析。 开源仓库：github.com/alteryx Domino Data Lab\n描述：让您的数据科学团队使用他们喜欢的工具，并将它们整合到一个企业级平台中，使他们能够花更多时间解决关键业务问题。 开源仓库：https://github.com/dominodatalab Google Cloud Platform\n描述：利用 Google Cloud 对开源、多云和混合云的承诺，避免供应商锁定（lock-in）并加快开发速度。 在整个组织中实现更明智的决策。 开源仓库：https://github.com/GoogleCloudPlatform/ OpenML\n描述：由于机器学习正在增强我们理解自然和建设更美好未来的能力，因此让研究、教育和行业中的每个人都透明且易于访问是至关重要的。 OpenML（Open Machine Learning）项目是一项范围广泛的动作，旨在为机器学习构建一个开放、有组织的在线生态系统。 开源仓库：github.com/openml MLflow⭐️（开源）\n描述：MLflow 是一个简化机器学习开发的平台，包括跟踪实验、将代码打包为能够可复现的运行以及共享和部署模型。 MLflow 提供了一组轻量级 API，可与任何现有的机器学习应用程序或库（TensorFlow、PyTorch、XGBoost 等）一起使用，无论您当前在何处运行 ML 代码（例如在notebooks、独立应用程序或云中）。 开源仓库：github.com/mlflow SAS\n描述：通过一个单一的、集成的、协作的解决方案解决最复杂的分析问题——现在它有了自动化建模 API。 开源仓库：github.com/sassoftware Polyaxon⭐️（来自德国柏林，开源，针对k8s的机器学习平台）\n描述：使用生产级 MLOps 工具重现、自动化和扩展您的数据科学工作流。 开源仓库：github.com/polyaxon Microsoft Azure\n描述：无限的数据和分析能力。是的，无限。在专为数据和分析构建的云上获得无与伦比的洞察、大规模和性价比。 开源仓库：github.com/Azure DagsHub（git+dvc+mlflow）\n描述：DagsHub 是一个供数据科学家和机器学习工程师对其数据、模型、实验和代码进行版本控制的平台。 它允许您和您的团队轻松共享、审查和重复使用您的工作，为机器学习提供 GitHub 体验。DagsHub 基于流行的开源工具和格式构建，可以轻松与您已经使用的工具集成。 官方网站：dagshub.com/ Neu.ro⭐️\n描述：Neuro MLOps 平台，用于在公有云、混合云和私有云上进行全周期 ML/DL 应用程序开发和部署。在多年为一系列企业客户开发创新 AI 解决方案的工作中，Neuro 获得了第一手了解构建和维护构建、部署和扩展 AI 解决方案所需的基础设施和工具所涉及的挑战。 这个 AI 基础设施现在被称为：MLOps。 我们相信构建和部署 ML 模型应该像使用 DevOps 的软件工程一样简单、快速和安全。 \u0026mdash; MLOps.community\nNeuro 平台使用户能够自动执行从数据收集到准备、训练、测试和部署的 ML 管道。 使用 Neuro，打包、扩展、微调、检测和持续交付的步骤都可以完全自动化，解决每个组织的两个主要挑战：上市时间和资源管理。\n开箱即用的 Neu.ro 启动了一个成熟的机器学习开发环境，您需要的所有工具都触手可及。它将开源和专有工具集成到面向客户端的系统中。具体组件如下：\n# Lifecycle Component Tool 1 Data labeling Label Studio 2 Data management DVC 3 Development environment VSCode, Jupyter 4 Remote debugging VSCode remote debugger 5 Code management Git 6 Experiment tracking MLflow 7 Hyperparameter tuning NNI 8 Distributed training Neu.ro 9 Metadata management MLflow 10 Model management MLflow 11 Deployment Seldon Core 12 Testing Locust 13 Monitoring Prometheus + Grafana 14 Interpretation Seldon Alibi 15 Pipelines orchestration Neu.ro 16 Resource orchestration Neu.ro 17 Access control orchestration Neu.ro 官方网站：neu.ro/mlops/ 所有的MLOps和ML工具与平台 # ML 生命周期中的这个主要领域侧重于管理数据。 我们决定将它作为一个单独的部分，因为它有许多方面位于“建模”部分之外。\n数据探索和管理 # 帮助您管理、探索、存储和组织数据的工具和平台。\nAlgorithmia Alluxio Amazon Redshift Amundsen Cohesity Aparavi AtScale Cazena Cloudera Clearsky Databricks Datagrok Dataiku Delta Lake Datera Dremio Druid Elastifile Erwin Excelero Fluree Gemini Hammerspace Hudi HYCU Imply Komprise Kyvos MLReef Microsoft Azure Milvus Octopai Openml Parquet Pilosa Presto Qri Rubrik Spark Tamr Waterline Data Whylabs Vearch Vexata Yellowbrick 数据标注 # 支持您标记数据以创建训练数据集的工具。\n4SmartMachines Amazon Sage Maker - Data Labeling Appen Dataturks Defined Workflows Doccano Figure Eight iMerit Labelbox Prodigy Playment Scale Segments Snorkel Supervisely 数据流 # 用于将大量数据直接加载到数据流水线（pipelines）中的数据流服务和工具。\nAmazon Kinesis Alluxio Ares DB Confluent Flink Google Cloud Dataflow Hudi Kafka Microsoft Azure Stream Analytics Storm Striim Valohai 数据版本控制 # 下面提供了一些数据版本控制的工具和平台。这是特别有价值的，因为数据对模型的表现是不可分割的一部分。检视数据变更和数据治理对于模型完全再现性至关重要。\nDagshub Databricks Dataiku Dolt DVC Floydhub MLReef Pachyderm Qri Waterline Data 数据隐私 # 数据隐私包含匿名化、加密、高度安全的数据存储和其他保护数据隐私的方法。\nAmnesia AirCloak Data Anon Celantur Mostly AI PySyft Tumult 数据质量检查 # 确保数据健康的方法。\nArize Great Expectations Naveego Whylabs 建模 # ML 生命周期中的这个主要领域侧重于创建 ML 模型。 这包括与创建模型直接相关的所有步骤，例如准备数据、特征工程、实验跟踪直至模型管理。 可以说，这个阶段是所有魔法发生的地方。\nNotebook / ML 代码管理\n帮助您管理、探索、存储和组织Notebook或机器学习操作的工具和平台。 我们明确没有包括 SCM 平台，例如 GitHub 或 Gitlab，因为它们并不是专门针对 ML（尽管他们完全能够托管这些功能）。\nAmazon Sagemaker Dagshub Databricks Dataiku Deepnote Domino Data Labs Floydhub Google Colab H2O Kaggle MLReef Openml Polyaxon Pachyderm Valohai Weights and Biases 数据处理和可视化\n专门的数据处理（例如数据清理、格式化、预处理等）和可视化流水线（pipelines），旨在分析大量数据。 我们明确排除了简单的数据表示，例如在表格数据中显示数据分布（有很多工具可以做到这一点）。\nAlteryx Ascend IO Google Colab Dask Dataiku Databricks Dotdata Flyte Gluent Koalas Iguazio Imply Incorta Mlflow Kyvos MLReef Modin Naveego Openml Pachyderm Pilosa Presto SAS Snorkel SQLflow Starburst Turi Create Vaex Valohai Weights and Biases 特征工程\n专门的特征工程和特征存储平台和工具。\nAmazon Sagemaker Dotdata Feast Featuretools Pachyderm ScribbleData Tecton TSfresh MLReef Iguazio 模型训练 # 这些工具和平台具有专门的流水线（pipeline）和功能来训练机器学习模型。\nAlteryx Amazon Sagemaker Iguazio Microsoft Azure Google Cloud Platform Google Colab Databricks Dataiku Domino Data Labs Dotscience Floydhub Flyte Horovod IBM Watson Ludwig Kaggle MLReef H2O Metaflow Mlflow Paperspace PerceptiLabs Snorkel Turi Create Valohai SAS Anyscale Pachyderm 实验跟踪 # 提供跟踪、比较和记录模型训练指标的工具和平台。\nAllegro AI Amazon Sagemaker Comet ML Dagshub Dataiku Datarobot Datmo Domino Data Labs Floydhub Google Cloud Platform H2O Ludwig Iguazio Mlflow MLReef Neptune AI Openml Polyaxon Spell Valohai Weights and Biases 模型/超参数优化 # 允许您为模型搜索理想超参数的工具和平台（例如，包括贝叶斯或网格搜索、性能优化等）\nAlteryx Amazon Sagemaker Angel Comet ML Datarobot Hyperopt Polyaxon Sigopt Spell Tune Optuna Talos 自动机器学习 # 自动机器学习，也称为AutoML，是基于架构、数据和超参数自动化找到理想模型配置的过程。 AutoML 是一种更高级的模型优化方法，但并不总是适用。\nDatarobot DeterminedAI Dotdata Google Cloud Platform H2O Iguazio Tazi Transmogrify 模型管理 # 模型管理包括模型存储、工件制品管理和模型版本控制。\nAlgorithmia Allegro AI Amazon Sagemaker Databricks Dataiku DeterminedAI Dockship Domino Data Labs Dotdata Floydhub Gluon Google Cloud Platform H2O Huggingface IBM Watson Iguazio Mlflow Modzy Perceptilabs SAS Turi Create Valohai Verta 模型评估 # 此任务涉及测量模型的预测性能。它还包括测量所需的计算资源、延迟检查和漏洞。\nArize Dawnbench MLperf Streamlit Tensorboard Whylabs 模型可解释性 # 通过分析深度学习模型的架构、权重分布与测试数据、热力图等来消除（尤其是）深度学习模型的黑盒综合症。这些工具为模型可解释性提供了专用功能。\nAmazon Sagemaker Clarify Fiddler InterpretML Lucid Perceptilabs Shap Tensorboard 持续部署 # ML 生命周期中的这个主要领域侧重于将经过训练的模型投入生产。\n数据流管理\n这些工具让您可以在推理过程中管理和自动化数据流过程（新数据进入后会发生什么？），衡量性能和安全问题。\nAlluxio Spark Ascend IO Kafka Dataiku Dotdata HYCU Prefect 特征转换\n类似于模型训练期间的过程，但现在在推理任务期间。 随着新数据的进入，需要对其进行转换以适应模型训练过的输入数据。 这些工具允许您创建应用于生产模型的特征转换。\nFeast Featuretools ScribbleData Tecton Iguazio 监控\n模型性能监控非常重要，因为数据分布或计算性能的偏差可能会对业务逻辑和流程产生直接影响。\nAlgorithmia Amazon Sagemaker Arize Dataiku Datadog Datatron Datarobot Domino Data Labs Dotscience Fiddler H2O Iguazio Losswise Snorkel Unravel Valohai Verta Whylabs 模型合规性和审计\n此任务涉及提供模型来源的透明度。\nAlgorithmia SAS H2O 模型部署和服务\n集成模型部署功能的工具和平台。\nAmazon Sagemaker Aible Algorithmia Allegro AI Clipper Core ML Cortex Dataiku Datatron Datmo Domino Data Labs Dotdata Dotscience Floydhub Fritz AI Google Cloud Platform IBM Watson Iguazio Kubeflow Mlflow Modzy OctoML Paperspace Prediction IO SAS Seldon Spell Streamlit H2O Valohai Verta 模型验证（Model validation） # 模型验证是一组旨在验证模型是否按预期执行的过程和活动。模型有效性应在操作上（即通过确定模型输出是否与观察到的数据一致）和概念上（即通过确定模型背后的理论和假设是否合理）进行评估。\nArize Datatron Fiddler Lucid MLperf SAS Streamlit 模型序列化格式 # 调整模型以与其他框架、库或语言兼容。\nMMdnn ONNX Plaidml 计算管理 # ML 生命周期中的这个主要领域包括管理计算基础设施。 这一点尤其重要，因为机器学习有时需要大量的存储和计算资源。\n计算和数据基础设施（服务器）\n这些组织为您的 ML 项目提供所需的能力（在硬件方面）。\nGoogle Cloud Platform IBM Watson Amazon AWS Microsoft Azure Cloudera Paperspace Kamatera Linode Cloudways Liquidweb Digitalocean Vultr 环境管理\n自定义脚本管理基础环境需要包、库和运行时环境。 以下工具和平台将帮助您管理基础环境。\nConda Databricks Datmo Mahout MLReef 资源分配\n以下工具和平台支持管理不同的资源（如计算实例、存储卷等）。 此外，还管理一些包括预算和团队优先权等，以控制支出。\nAmazon Sagemaker Algorithmia Google Cloud Platform MLReef Databricks Microsoft Azure Dataiku DeterminedAI Floydhub IBM Watson Polyaxon Spell Amazon Sagemaker Valohai Allegro AI 模型服务和计算任务扩展\n这些工具提供对已部署模型和计算任务的弹性扩展。\nAmazon Sagemaker Argo Microsoft Azure Datadog Datatron Datmo Google Cloud Platform TensorRT Seldon TVM 安全与隐私\n这些工具可让您在将模型部署到生产环境时管理隐私话题（例如，GDPR合规性）并提高安全水平。\nAlgorithmia Clever Hans Datadog Modzy PySyft Tumult Resources # https://juejin.cn/post/7035761114650509319 Global MLOps and ML tools landscape ","date":"16 April 2024","permalink":"/posts/ai/ai-ci-cd/%E5%85%A8%E7%90%83%E7%9A%84mlops%E5%92%8Cml%E5%B7%A5%E5%85%B7%E6%A6%82%E8%A7%88/","section":"博客","summary":"对于人工智能领域的任何人来说，MLOps 一词是解决所有问题的一个神奇词汇。 它结合了所有与机器学习相关的任务，从管理、处理和可视化数据、运行和跟踪实验，到将创建的模型投入到生产，理想情况下是大规模、合规和安全的。它定义了实施 ML 工作以创建基于AI的应用程序和服务的过程。","title":"全球的MLOps和ML工具概览"},{"content":" mlops-zoomcamp youtube courses Learn MLOps principles and take your projects from the notebook to production in 9 weeks 目录\n第 1 周：简介和先决条件 第 2 周：实验跟踪和模型管理 第 3 周：编排和 ML 管道 第 4 周：模型部署 第 5 周：模型监控 第 6 周：最佳实践 第 7、8、9 周：项目 第 1 周：简介和先决条件\n技术：Docker、AWS 重点：第 1 周致力于设置您将在整个课程中使用的关键工具和技术，并向您介绍 MLOps 的概念以及我们为什么需要使用该概念。 第 2 周：实验跟踪和模型管理\n技术：MLFlow 重点：第 2 周涵盖实验跟踪，以存储和组织有关实验的相关信息。例如，模型的输入数据、源代码、模型架构参数以及相应的输出。 第 3 周：编排和 ML 管道\n技术：Mage 重点：第 3 周的重点是创建用于训练机器学习模型的生产就绪管道。这意味着管道可以以完全自动化的方式轻松复制和重新运行。 第 4 周：模型部署\n技术：Flask、Docker、MLflow、Mage、AWS Lambda \u0026amp; AWS Kinesis 重点：第 4 周向您介绍模型部署的三种方法，并演示如何使用每种方法。 第 5 周：模型监测\n技术：Prometheus, Evidently AI, and Grafana 重点：第 5 周是关于监控机器学习模型，包括服务运行状况、模型性能、数据质量和完整性以及数据漂移和概念漂移。 第 6 周: 最佳实践\n技术：Python, Docker, Localstack, Github Actions 重点：第 6 周总结了最佳实践，例如单元测试、集成测试、检查代码质量以及使用 CI/CD 和 GitHub Actions 进行自动化部署。 第 7/8/9 周: 项目\n持续时间：2 周用于开发，1 周用于同行评审 目标：该项目的重点是应用您获得的技能从头开始构建数据工程管道。完成这个实践项目不仅可以验证您的技能，还可以增强您的作品集，从而在求职中提供竞争优势。 同行评审：要完成该项目，您需要评估至少三位同行的项目。如果不这样做，您的项目将被标记为不完整。 项目要求：\n选择感兴趣的数据集 在选定的数据集上训练模型并跟踪您的实验 开发模型训练管道 以批处理、Web 服务或流格式部署模型 监控模型的性能 遵守最佳实践 ","date":"16 April 2024","permalink":"/posts/ai/ai-ci-cd/mlops-zoomcamp/","section":"博客","summary":"第 1 周，简介和先决条件；第 2 周：实验跟踪和模型管理；第 3 周：编排和 ML 管道；第 4 周：模型部署；第 5 周：模型监控；第 6 周：最佳实践；第 7、8、9 周：项目。","title":"MLOPS-ZOOMCAMP"},{"content":" Ikram, Abdullah and Ullah Tabassam. “MLOps: A Step Forward to Enterprise Machine Learning.” ArXiv abs/2305.19298 (2023): n. pag. ","date":"15 April 2024","permalink":"/posts/ai/ai-ci-cd/mloops/","section":"博客","summary":"机器学习运营 (MLOps) 正在成为寻求利用 AI 和 ML 模型优势的企业的一个非常重要的部分。这项研究详细回顾了 MLOps、其优点、困难、演变以及 MLOps 框架、Docker、GitHub actions 和 Kubernetes 等重要的底层技术。详细解释了 MLOps 工作流程，包括模型设计、部署和操作，以及模型和数据探索和部署所需的各种工具。本文还重点介绍了使用各种成熟度级别的自动化管道进行 ML 项目的端到端生产，其中至少完全没有自动化，最高的是具有完整的 CI/CD 和 CT 功能。此外，还使用用于对象检测服务的企业级 MLOps 项目的详细示例来解释该技术在现实场景中的工作流程。为此，托管来自 TensorFlow 2 Model Zoo 的预训练模型的 Web 应用程序被打包并部署到互联网，以确保系统可扩展、可靠并针对企业级部署进行优化。","title":"MLOps: A Step Forward to Enterprise Machine Learning"},{"content":" Mátyás K K, MOROZ-DUBENCO C, Florentin B. Facilitating Model Training With Automated Techniques[J]. Studia Universitatis Babeș-Bolyai Informatica, 2023: 53-68. 在过去的十年中，人工智能 (AI) 已从一个流行词发展成为一项最先进的技术，为欺诈检测、医疗保健、预测性维护、能源管理和零售等各个领域提供可靠的解决方案。人工智能应用程序现在被用作企业核心流程中的独立解决方案。然而，随着对人工智能的需求呈指数级增长，曾经被视为学术实验的传统人工智能模型训练流程现在已成为一个后勤问题。需要迁移到可以自动化所有步骤的可靠流程，从清理原始数据集到训练模型并将其暴露给可以在现实场景中使用它的其他服务。\n此外，人们还强调为没有人工智能背景的软件开发人员创建用户友好界面的必要性，这可以遵循灰盒原则，方便维护封装在模块中的人工智能流程。这些要求已经在软件工程领域得到解决，特别是在 Web 编程中，使用持续集成/持续部署 (CI/CD) 机制和 Web 服务。这篇论文的方法可以实现数据预处理的自动化，训练各种人工智能模型并选择性能最佳的模型，将该模型部署为供第三方使用的 Web 服务，同时确保服务的高可用性和可扩展性。\n问题 # 在工业环境中开发基于机器学习的解决方案包括两个步骤：构建模型并将其部署到生产中。虽然第一部分可能更有趣，但第二部分花费的时间最长——模型构建和优化后的所有任务。根据 Gartner 的数据，85% 的大数据项目失败的前五个因素之一是部署的复杂性。\nGartner. https://www.gartner.com/en. Accessed: June 16, 2023.\nAlgorithmia 的“2021 年企业机器学习状况”报告称，64% 的组织至少需要一个月的时间来部署机器学习模型，而 38% 的组织的数据科学家花费超过 50% 的时间来部署机器学习模型生产。\nAlgorithmia. 2021 state pf enterprise ml. https://info.algorithmia.com/hubfs/ 2020/Reports/2021-Trends-in-ML/Algorithmia_2021_enterprise_ML_trends.pdf. Accessed: June 9, 2023.g\n根据 Ashmore，工业环境中基于机器学习的解决方案的开发可以分为四个阶段。\nAshmore, R., Calinescu, R., and Paterson, C. Assuring the machine learning lifecycle: Desiderata, methods, and challenges. ACM Computing Surveys (CSUR) 54, 5 (2021), 1–39.\n每个阶段都可以分为更小的步骤，Paleyes 提出了与每个步骤相关的各种问题和关注点，如下所述。\nPaleyes, A., Urma, R.-G., and Lawrence, N. D. Challenges in deploying machine learning: a survey of case studies. ACM Computing Surveys 55, 6 (2022), 1–29.\n数据管理 Data Management 模型学习 Model Learning 模型验证 Model Validation 模型部署 Model Deployment 数据管理 Data Management # 数据收集、汇集和理解。尤其是在大型生产环境中，问题可能会出现，在这种环境中，使用“单一责任”的原则，应用程序通常被构建为多个相互通信的服务，这很容易导致数据被不同的服务存储在不同的位置和形式。\nMartin, R. C. The single responsibility principle. The principles, patterns, and practices of Agile Software Development (2002), 149–154.\n数据收集后必须进行清理，据 Kim 的研究，数据清理是导致专家怀疑其工作质量的主要原因。\nKim, M., Zimmermann, T., DeLine, R., and Begel, A. Data scientists in software teams: State of the art and challenges. IEEE Transactions on Software Engineering 44, 11 (2017), 1024–1038.\n数据不足、有偏差、有噪声、不相关或不平衡可能导致模型拟合不足或过度。此外，如果必须将多个数据源集成到一个数据源中，它们在模式、约定或存储和访问数据的过程方面可能会有所不同。一旦数据经过预处理，可能需要进行增强，因为在现实生活中，数据往往是未标记的，并且由于监督学习技术需要标记数据进行训练，因此可以为大量数据分配标签的过程，这是一项乏味的任务。最后，必须对标记数据进行分析，以发现偏差或意外的分布变化。此步骤中特别具有挑战性的一个领域是数据分析的可视化，因为可用于有效执行此任务的工具很少。\n模型学习 Model Learning # 首先，需要选择最适合当前问题的模型。Wagstaff 表示在选择在资源受限的环境中使用的模型时，复杂性是最重要的因素之一。\nWagstaff, K. L., Doran, G., Davies, A., Anwar, S., Chakraborty, S., Cameron, M., Daubar, I., and Phillips, C. Enabling onboard detection of events of scientific interest for the europa clipper spacecraft. In Proceedings of the 25th acm sigkdd international conference on knowledge discovery \u0026amp; data mining (2019), pp. 2191–2201.\n在实践中，选择更简单的模型，例如决策树、随机森林、主成分分析等，而不是深度学习或强化学习技术，因为它们需要更少的资源，并且还可以减少开发和部署时间。此外，根据应用领域，能够解释机器学习模型的输出甚至可以超过其性能。 然后，必须训练模型。此步骤通常需要增加计算资源，从而导致成本增加。Sharir 表明 BERT 模型的训练成本至少为 5 万美元，这对于许多公司来说是无法承受的。\nSharir, O., Peleg, B., and Shoham, Y. The cost of training nlp models: A concise overview. arXiv preprint arXiv:2004.08900 (2020).\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).\n更重要的是，在选择超参数以找到模型的最佳设置时可能会出现挑战。在最坏的情况下，超参数优化任务的规模可能呈指数级增长，从而导致严峻的计算挑战。在谈论深度学习技术时，这个过程可能非常昂贵且占用大量资源。\n模型验证 Model Validation # 模型训练完成后，必须对其进行验证。此阶段涉及定义模型的要求，不仅应优先考虑提高模型性能，还应考虑用于在生产环境中评估和监控模型的业务驱动指标。除了数学正确性或错误界限之外，所有要求的验证都是必要的，包括遵守业务定义的监管框架。在现实环境中测试模型对于确保质量来说是理想的选择，但这可能会带来安全性和扩展方面的挑战。\n模型部署 Model Deployment # 最后，在工业环境中开发基于机器学习的解决方案的最后阶段是模型的部署。必须实现模型才能使用它，并且需要构建用于运行模型的基础设施。由于机器学习模型可以明确依赖于外部数据，因此许多工程反模式（例如校正级联）在使用 ML 的软件中广泛存在。\nSculley, D., Holt, G., Golovin, D., Davydov, E., Phillips, T., Ebner, D., Chaudhary, V., Young, M., Crespo, J.-F., and Dennison, D. Hidden technical debt in machine learning systems. Advances in neural information processing systems 28 (2015).\n一旦系统部署到生产中，就会进行维护。系统必须受到监控，但这个过程在机器学习社区中仍处于早期阶段，监控机器学习模型的整体性能仍然是一个悬而未决的问题。然而，当现有模型需要适应新数据并将新模型工件交付生产时，最大的挑战就出现了。虽然软件工程通过持续交付解决了这个问题，但机器学习问题变得更加复杂，因为与仅更改代码的常规软件产品不同，ML解决方案可以在三个方面进行更改：数据、模型和代码。\n自动化人工智能训练的进展 # 最近，人工智能已成为一项尖端技术，在不同行业拥有众多用例。人工智能模型已被用作问题的新颖解决方案、辅助系统或完全替代人类干预。然而，这种需求的增长也暴露了一些挑战，例如可靠的模型训练、部署和实时生产消费机制的必要性。大型云服务提供商迅速发现了这一需求，并通过提供一整套促进大规模人工智能模型训练和部署的工具来满足这一需求。此外，还集成了现代 CI/CD 机制，以确保可扩展的解决方案满足高可用性、性能和日志记录的需求。\nLi, W., Chen, J., and Huang, W. A survey on continuous integration, delivery and deployment tools. Journal of Systems and Software 147 (2018), 1–15.\nAzureML 是 Microsoft 开发的基于云的机器学习平台，提供了针对各种用例实施人工智能模型所需的一整套工具。\nMicrosoft. Azure machine learning. https://azure.microsoft.com/en-us/services /machine-learning/, 2021. Accessed: February 7, 2023.\n凭借其与不同人工智能模型架构集成的能力，它已被多家大公司用作从培训到部署的整个管道的可信生态系统。 例如，美国运通使用 AzureML 开发用于欺诈检测的应用程序，Mediktor 将其用于医疗保健解决方案以检查症状，E-ON 将其用于管理太阳能电池板农场的能源并预测能源解决方案，Belfius 使用它来帮助检测欺诈和金钱洗钱方面，Cognizant 和 Claro 利用 AzureML 个性化并改善用户的学习体验，Epiroc 在其帮助下推进制造创新 [19]。\nMicrosoft. Microsoft customer stories. Microsoft Azure Blog (January 2022).\nAzureML 场景介绍\u0026hellip;\n自动化模型训练过程 # 管道架构 Pipelines architecture # 在架构头脑风暴过程中，最重要的考虑因素之一是从客户端接收各种数据集并随后触发管道的业务需求。这些数据集来自不同的仓库，采用不同的格式，包括 SQL 数据库、NoSQL 数据库和文件。因此，需要一个层来抽象训练数据的来源并允许管道交互，提供一个公开简单数据下载操作以及结果上传的接口。这种方法使我们能够提供一个无缝且灵活的管道，可以处理各种数据源，同时还有助于将多个数据源集成到管道中。在这种特殊情况下，我们可以认为这些模块将接收数据集作为输入，并通过应用的数据处理操作输出它。\n上图概述了拟议的管道，该管道代表了 CI/CD 部署中的软件工程原理与 AI 模型训练过程之间的最先进组合。定义数据接口后，我们继续开发一系列模块，对训练人工智能模型过程中所需的最先进的操作进行编码。每个模块都是通过接口定义的，每个模型都有输入和输出，从而可以在所需的管道内进行无缝组装。为了启动管道，我们首先为最知名的数据处理操作定义模块，从数据清理（即删除空值、重复数据删除和填充不完整数据）、数据增强（即分类数据的单热编码）开始 列或数据标准化）和列选择（即，如果需要，仅选择特定列）。通过这个过程，我们的目标是建立一个灵活且可扩展的管道，能够执行各种数据处理操作，以促进人工智能模型的有效和高效的训练。\n数据清理过程之后，我们管道的下一步涉及模型训练。该阶段由一个伞形模块组成，该模块封装了各种子模型，每个子模型代表不同类型的模型，例如神经网络、逻辑回归、决策树等。该模块启动了所有这些模型的训练，随后选择了具有最高指标的模型，该模型是根据其准确性确定的。该管道模块的输出是一个灵活的二进制文件，可以在模型部署的后续阶段进行部署。\n为了将经过训练的二进制模型暴露给公司软件堆栈的其他组件，我们实现了一种允许输入新数据和检索推断结果的机制。遵循 Web 开发的最佳实践，我们决定使用微服务，因为它们具有高可用性、版本控制和可扩展性的功能。\nAzureML Pipelines # Microsoft AzureML 平台代表了基于云的技术的重大进步，为用户提供了一系列用于数据处理、模型训练和版本控制以及大规模部署的功能。鉴于我们的架构要求和项目需求，AzureML 被证明是非常适合我们工作的解决方案。\nAzureML 的管道设计器功能是一种低代码/无代码解决方案，用于开发用于模型训练和预测的机器学习管道。可视化组件支持管道的配置，只需对底层转换和机器学习算法有有限的了解即可执行管道配置。此外，其预先构建的基础设施和精心设计的环境使我们能够以最小的维护开销运行这些管道，而其多功能的功能套件使我们能够为我们的项目开发必要的模块。 AzureML 平台中的可视化管道设计器和具有策划环境的托管基础​​设施为寻求简化 AI 工作流程并提高运营效率的组织提供了创新且高效的解决方案。\nMicrosoft AzureML 通过实施 Azure 数据集提供了另一个重要的接口，用于管理客户端使用的各种数据集。此功能允许从不同来源（包括内部和外部数据存储库）加载表格格式的数据集。此外，Azure 数据集可以进行版本控制，并且其创建可以触发关联管道的执行。\n上图中所示的演示管道以 Azure 数据集开始，该数据集充当后续数据处理步骤的主要输入。具体来说，管道执行一系列数据处理模块，包括列选择、缺失数据删除和数据分割。数据分割模块将输入数据集划分为单独的训练集和测试集，随后将其输入到训练模块中以进行模型开发。每个模块都包含可自定义的属性，可以根据特定的用户需求进行调整。例如，数据分割模块中分配给训练集和测试集的数据的百分比可以根据需要进行配置。\n在初始数据处理步骤之后，管道中的下一组模块涉及多个人工智能模型的训练，特别是决策树系列的回归模型。每个模型都通过训练模型模块使用指定的训练集进行训练，然后通过评分模型模块使用测试集进行测试。在此阶段结束时，使用评估模型模块来比较和评估每个训练模型的性能，最终目标是确定最佳模型。然后可以使用建立的阈值来确定性能最佳的模型是否满足业务案例的要求。如果模型超过预定义的阈值，则可以将其部署在生产环境中使用。因此，通过使用这些模块，AzureML 可以高效开发、测试和评估多个 AI 模型，最终目标是为给定任务确定最佳解决方案。\nAzureML 的灵活性和可扩展性通过其开放的内置组件库得到了进一步体现，该库由充满活力和支持的社区维护。对于许多用例，内置组件满足开发机器学习模型的必要要求。但是，在需要特定和专有步骤的情况下，开发人员可以利用 Python 编程语言和 azureml 库来实现自定义构建的组件。在与外部客户合作的背景下，我们设计和开发了一系列神经网络（特别是 LSTM 和 GRU 模型 ），这些网络经过定制和优化，以满足合作者的需求。然后，这些模型被部署为我们开发的 AzureML 管道中的自定义组件。虽然我们实现的模块和组件是根据我们的特定用例量身定制的，但它们仍然遵循总体原则，即让模型根据各自的准确性水平进行竞争以确定获胜者。通过提供使用预定义组件和实施定制组件的能力，AzureML 为 AI 开发的性能和可维护性提供了显着的提升，即使对于非专业开发人员也是如此。\nHochreiter, S., and Schmidhuber, J. Long short-term memory. Neural computation 9, 8 (1997), 1735–1780.\nChung, J., Gulcehre, C., Cho, K., and Bengio, Y. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 (2014).\n自动化模型训练和部署 # 为了实现模型训练和部署的强大生态系统，必须拥有一个结构良好的自动化管道，该管道可以响应一系列事件，根据预定义的阈值验证模型，并仅在阈值达到时将模型部署初始化为新版本超过了。为了确保流程顺利高效，拟议的管道必须遵循 CI/CD 原则，它提供了一组用于实时训练机器学习模型的协议，并使用 AzureML 提供的基础设施以高可用性和可扩展性部署它们。通过遵循 CI/CD 的原则，可以构建管道以简化模型开发流程、降低管理成本并最大化 AI 开发流程的投资回报。一般来说，CI/CD 原则对于确保模型训练和部署管道的稳健性和可扩展性至关重要，从而促进模型的快速部署，同时最大限度地减少对整个工作流程的干扰。\n上图描述了在 Microsoft AzureML 生态系统中使用 CI/CD 进行模型训练的建议自动化机制。每当与管道关联的 Azure 数据集发生更改时，都会触发 CI/CD 系统，指示新数据集可用于训练。然后，系统会提取最新版本的数据集并启动管道。如果管道中的组件位于层次结构中的同一级别，则它们将被并行化，从而允许同时执行所有模型训练，从而节省时间和资源，从而节省成本。\n在所提出的 CI/CD 管道的训练后阶段，最佳模型的准确性将与阈值进行比较，如果未达到阈值，将触发通知管理员对训练集进行额外的微调或修改组件（例如，通过添加更多清洁方法）。另一方面，如果超过阈值，最佳模型将被打包为二进制模型，封装到 Docker 化的 Web 微服务中。\nMerkel, D. Docker: lightweight linux containers for consistent development and deployment. Linux journal 2014, 239 (2014), 2.\n该微服务将作为服务部署在 AzureML 基础设施上，它将通过端点公开模型。此过程确保模型可以随时被其他服务使用。\n在 AzureML 的上下文中，打包训练模型的机制可以通过使用两个关键组件来实现，即 Web 服务输入模块和 Web 服务输出模块，如下图所示。\n这些组件使 AzureML 能够识别输出火车模型部分的目的是封装在 Web 服务中。Web 服务输入组件负责接收一组输入参数，然后将这些参数传递给负责处理输入并将其提供给模型进行推理的组件。然后，模型输出被传递到 Web 服务输出组件，该组件又将结果返回到发起请求的服务。\nCI/CD 管道完成后，生成的 Web 服务可以位于 AzureML 端点部分，该部分提供与模型交互的所有必要详细信息。除了支持微服务的部署之外，AzureML 还包含一系列在部署后阶段特别有用的功能，确保模型的高可用性和性能。我们认为其中两个内置功能非常有用：容错 - AzureML 保证至少一个 Web 服务实例始终处于运行状态 - 以及自动扩展 - 在需求较高的情况下， Web 服务可以扩展到通过负载均衡器管理的多个实例。\n用于模型训练、验证和部署的底层基础设施可以配置为在 CPU 或 GPU 上运行。通常，GPU 在训练阶段是必不可少的。虽然 GPU 虚拟机比配备 CPU 的虚拟机贵得多，但权衡模型训练所需的时间与每小时成本可能会产生涉及 GPU 实例的经济高效的解决方案。\n建议使用 AzureML 管道进行模型训练和部署的解决方案是一种自动化机制，从将新数据集版本上传到关联的 Azure 数据集开始，一直持续到模型部署的最后一步。这种方法提供了自主且直观的维护过程的好处，因为管道的每个步骤都是根据定义的逻辑自动执行的。该管道及其部署机制适用于实时环境中使用的Web应用程序，满足可用性、易于维护和调试以及快速执行等要求。监控系统还可以快速识别和解决任何问题。由于所有这些，所提出的解决方案为自动化模型训练和部署过程提供了全面且可靠的解决方案。\n使用经过训练的模型 # 本节介绍如何将 AzureML 中的模型部署为微服务，以及使用 Swagger 实现 Web 服务或 API 的标准化。 部署模型后，可以通过 HTTP 请求访问它，并且可以在 AzureML 端点部分找到有关 Web 服务的所有相关信息，例如 REST 端点、身份验证类型和监视日志。 此外，通过 AzureML Pipelines 创建的 Web 服务包含其自己的文档，其中提供了有关现有 Web 服务端点、其预期输入结构和预期输出结构的详细信息。 Swagger 是一种用于标准化 Web 服务或 API 的流行标准，它在 AzureML 中实现，以支持与语言无关的解决方案。 因此，任何具有 HTTP 框架的编程语言都可以向模型发出请求，并期望输出遵循其 Swagger 描述的结构。\nSwagger: The world’s most popular framework for apis. https://swagger.io, 2022. Accessed: Feb. 7, 2022.\n","date":"14 April 2024","permalink":"/posts/ai/ai-ci-cd/%E5%88%A9%E7%94%A8%E8%87%AA%E5%8A%A8%E5%8C%96%E6%8A%80%E6%9C%AF%E4%BF%83%E8%BF%9B%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/","section":"博客","summary":"自动化人工智能（AI）模型训练已成为自动化领域的一项重大挑战。从原始数据到模型部署的完整管道需要定义强大的流程，以确保公开模型的服务的效率。本文介绍了一种通用架构，用于使用 Microsoft Azure 机器学习 (AzureML) CI/CD 工具自动执行数据准备、模型训练、模型选择以及将模型部署为 Web 服务以供第三方使用。我们利用带有预定义和自定义模块的 AzureML 管道进行了实际实验，证明了它可以集成到任何生产应用程序中。我们还成功地将这种架构集成到专为工业预测而设计的实际产品中。这一实际实施证明了我们方法的有效性和适应性，表明其满足不同培训需求的潜力。","title":"利用自动化技术促进模型训练"},{"content":"","date":"14 April 2024","permalink":"/graduation-project/","section":"Graduation-Projects","summary":"","title":"Graduation-Projects"},{"content":" Al-Najim A, Al-Amoudi A, Ooishi K, et al. Intelligent maintenance recommender system[C]//2022 7th International Conference on Data Science and Machine Learning Applications (CDMA). IEEE, 2022: 212-218. 推荐引擎是智能系统，可根据用户的兴趣向特定用户（个人或公司）推荐最合适的项目（产品或服务）。由于兴趣和倾向是未来选择的有效指示，推荐分析通常基于有关项目、用户以及服务提供商和用户之间先前交互的相关信息。推荐系统的目标是通过评估用户的活动和/或其他用户的行为来预测用户的偏好和兴趣，从而提供个性化服务。\nJ. Lu, D. Wu, M. Mao, W. Wang, and G. Zhang, “Recommender system application developments: A survey,” Decis. Support Syst., vol. 74, pp. 12–32, 2015, doi: 10.1016/j.dss.2015.03.008.\n推荐系统在质量和效率方面显示出显着的改进，这为服务提供商和用户层面的决策过程增加了价值。\nPortugal, P. Alencar, and D. Cowan, “The use of machine learning algorithms in recommender systems: A systematic review,” Expert Syst. Appl., vol. 97, no. November 2015, pp. 205–227, 2018, doi: 10.1016/j.eswa.2017.12.020.\n例如，推荐系统在降低搜索和选择产品的交易成本、增加电子商务业务的收入方面发挥着至关重要的作用。为了最大化用户利益，需要采用准确、高效的技术为用户提供适当、可靠的推荐。\nF. O. Isinkaye, Y. O. Folajimi, and B. A. Ojokoh, “Recommendation systems: Principles, methods and evaluation,” Egypt. Informatics J., vol. 16, no. 3, pp. 261–273, 2015, doi: 10.1016/j.eij.2015.06.005.\n如今，推荐系统已应用于许多领域，例如 Netflix [4] 等视频服务、亚马逊 [5] 等在线商店以及 Twitter [6] 等社交媒体。\nC. A. Gomez-Uribe and N. Hunt, “The netflix recommender system: Algorithms, business value, and innovation,” ACM Trans. Manag. Inf. Syst., vol. 6, no. 4, 2015, doi: 10.1145/2843948.\nG. Linden, B. Smith, and J. York, “Amazon.com recommendations: Item-to-item collaborative filtering,” IEEE Internet Comput., vol. 7, no. 1, pp. 76–80, 2003, doi: 10.1109/MIC.2003.1167344.\nN. Ben-Lhachemi and E. H. Nfaoui, “Using tweets embeddings for hashtag recommendation in twitter,” Procedia Comput. Sci., vol. 127, pp. 7–15, 2018, doi: 10.1016/j.procs.2018.01.092.\n在工业领域，维护是确保工厂过程安全、可靠、高效的关键操作之一。工厂维护团队的关键绩效指标（KPI）是更短的计划外停机时间。为了减少与预防性维护相关的停机时间和维护成本，公司已转向主要基于高级分析和大数据的预测性维护，以便能够识别故障的根本原因并更有效地预测故障何时发生。另一方面，一旦发生突发资产故障，维护团队应迅速制定并执行维护计划。然而，规划需要时间和经验，因为失败的情况各不相同，可供选择的选项也很多。维护团队需要从选项中选择适当的维护操作。如果选择了错误的维护操作，恢复和返工时间会使停机时间增加更多。因此维护动作的准确性非常重要。最近，维护作业推荐系统的实施开始引起业界的关注。\n文献中发现的相关工作之一是由 Ying Huang 等人提出的。作者提出了一个推荐系统，为给定的维护请求推荐最合适的维护文档。该系统使用推荐引擎概念，根据与历史数据的相似性、专家知识和聚类技术来推荐合适的维护文档。\nYing Huang, Mickaël Gardoni, Amadou Coulibaly, et al. \u0026ldquo;A decision support system designed for personalized maintenance recommendation based on proactive maintenance\u0026rdquo;, In ICED, 2009\nMin 开发了一种基于协同过滤的机器学习推荐系统，旨在减少 oil well 设施的意外停机。开发的推荐系统利用客户之间的相似性来预测未来的购买并提出产品推荐。该模型能够为客户减少约 250 万美元的成本，并将每位客户每年的意外设备故障减少 1.7%。\nA. Min, “Reducing Oil Well Downtime with a Machine Learning Recommender System Reducing Oil Well Downtime with a Machine Learning Recommender System,” 2020.\nWang 等人提出了一种基于矩阵分解技术的轴承状态识别协同过滤推荐系统。通过该方法对外环上不同位置的故障和不同类型的故障进行了分析。系统经过测试，准确率达到90%以上。\nG. Wang, Y. He, Y. Peng, and H. Li, “Bearing Fault Identification Method Based on Collaborative Filtering Recommendation Technology,” Shock Vib., vol. 2019, 2019, doi: 10.1155/2019/7378526.\nDas 在构建了基于交替最小二乘非负矩阵分解（ALS NMF）技术的协同过滤推荐系统来预测工业设备中的问题类型。系统在测试集上实现的准确率为 49.3%。尽管系统的准确度较低，但它能够识别设备中一些未知的问题类型。\nS. Das, “Maintenance action recommendation using collaborative filtering,” Int. J. Progn. Heal. Manag., vol. 4, no. 2, pp. 1–7, 2013.\nAnthony 提出了一种维护推荐系统，使用计算机化维护管理系统（CMMS）数据作为文本信息，使用能源管理和控制系统（EMCS）数据作为传感器数据。文本数据已使用扩展布尔模型进行处理。推荐引擎比较文本问题描述符和传感器数据描述符，以确定先前的维护操作和即将到来的维护操作之间的相似程度。\nB. Anthony, “Design of a Maintenance and Operations Recommender,” Ensemble, vol. 15, no. 4, pp. 250–260, 2013.\nSun 等人提出了一种基于时间序列的推荐系统来预测石油钻井行业项目的安全检查顺序。系统预测每个项目的安全检查次数，并对其进行排序，生成安全检查流程。当系统进行测试时，发现它能够提供更好的安全检查程序。\nS. B. Sun, X. L. Dong, L. Zhang, Z. J. Jing, and F. Min, “Time series based interactive recommendation for petroleum drilling safety check,” Proc. - Int. Conf. Mach. Learn. Cybern., vol. 1, no. 1, pp. 13–18, 2016, doi: 10.1109/ICMLC.2016.7860870.\n识别故障的根本原因只是维护活动的第一步。然而，为了在行业中执行适当的维护操作，需要更多有关所需准备、设备、工具和技能的信息，以遵守维护程序和安全要求。虽然文献中的其他论文旨在利用推荐系统来识别故障根本原因，但本研究活动旨在开发一个推荐系统来推荐所需的维护程序。除了识别故障根本原因之外，我们的系统还可以自动建议必要的维护计划，包括所需的维护操作、准备活动工作、安全要求以及适合给定故障的任何其他操作。因此，维护团队可以在短时间内确定适当的维护准备操作。该系统可以增加的另一个价值是知识积累和向年轻成员的转移。\n推荐系统 # 本节介绍推荐系统的两种方法，它们利用人工智能技术来推荐针对给定缺陷采取的维护操作。第一种方法是单阶段推荐器，它根据新缺陷与数据集中其他缺陷之间的相似性来推荐维护操作。然后，推荐系统根据最相似的缺陷建议合适的维护操作。第二种方法是多阶段推荐系统，其中推荐集是逐步生成的。在每次迭代中，我们推荐一个项目/特征，然后该项目/特征将用作输入的一部分来估计下一个项目/特征，依此类推，直到推荐所有所需的特征。\n数据集 # 使用的数据集是从一家化工厂过去十年生成的实际维护数据库中收集的。数据集中的每一行对应一个唯一的缺陷，其中列对应缺陷特征和相关的维护操作。在这个系统中，我们读取五个不同的输入：位置、优先级、团队、问题描述和详细描述。输出将是故障类型、根本原因、维修、组件、所需时间、所需人员数量以及必要的准备措施列表，如 TABLE I 所示。\n单阶段推荐系统 # 该系统由三个模块组成。\n第一个模块是数据准备模块，系统从外部 CSV 文件读取数据并构建评级表以将其传递到下一阶段。 第二个模块是推荐引擎模块，系统读取新缺陷的信息以及前一个模块传递过来的评级表，构建新缺陷与历史数据中每个缺陷的相似度矩阵，并输出前五名类似的缺陷。 第三个模块是维护动作提取模块。在该模块中，系统从原始数据集中提取前一个模块传递的前五个相似缺陷所采取的维护操作，并将其推荐给用户。 下图说明了系统的工作流程。\n1. 数据预处理\n第一个模块是数据准备模块，其中构建评级表并将其传递给推荐引擎。\n评级表由三列组成，分别包含以下信息：用户ID、项目信息和评级。\n在此项目中，我们将每个缺陷视为唯一用户，因此第一列将包含缺陷 ID。第二列包含将根据数据类型进行处理的项目信息，可将其分为两组。第一组包含类别输入，其中包括位置、优先级和团队。 第二列将填充该组可能的特征值（例如位置 1、位置 2、优先级 1、优先级 2、团队 1、团队 2\u0026hellip;等）。而包括文本输入（简要和详细描述）的第二组将通过插入指示两个输入的标签在评级表上进行处理（即，在第二列中，我们将插入一个用于简要描述的标签，另一个用于详细描述的标签）的描述）。 第三列包含将根据相应功能的数据类型分配的评级值。如果数据类型是分类的，则评级值为 5 或 1。值 5 表示特定缺陷的真实项目对。例如，如果分配给位置 3 上的缺陷 2 的评级为 5，则这意味着缺陷 2 发生在位置 3。而其他位置上的缺陷 2 的评级将为 1，因为缺陷 2 未发生在其他位置。另一方面，文本输入具有一个评级值，该评级值表示新缺陷的文本输入与数据集中其他缺陷之间的相似度，该相似度是使用余弦相似度计算的。 余弦算法的输入是一个术语文档矩阵，该矩阵是通过称为术语频率 - 逆文档频率 (TFIDF) 的信息检索算法生成的。 术语-文档矩阵由包含所有缺陷的关键术语的行和包含缺陷标识符（缺陷 ID）的列组成。 单元格内是每个单词的频率，按输入大小加权。 TF-IDF算法由以下公式计算\nt：文档 d 中某个单词出现的次数 n：文档总数 df：包含给定单词的文档数量 2. 推荐引擎\n在此模块中，推荐引擎是根据前一个模块的评级表构建的，以查找最相似的缺陷。一般来说，推荐系统会预测新用户可以给项目的评分，以推荐评分最高的项目。然而，由于缺少评级反馈，因此无法在系统中应用评级预测。因此，我们取消了最后一步，即评级预测，并仅找到与新缺陷最相似的缺陷。使用余弦相似度计算相似度，找出新缺陷与数据集中每个缺陷之间的距离，并构建相似度矩阵。两个向量之间的余弦相似度可以由下式给出：\n从这个矩阵中，我们选择前五个相似的缺陷并将它们传递到最后一个模块。 在最后一个模块中，我们从原始数据集中提取前五个相似缺陷所采取的维护操作以进行推荐，从而结束该过程。\n多级推荐系统 # ","date":"14 April 2024","permalink":"/posts/devoops/%E6%99%BA%E8%83%BD%E8%BF%90%E7%BB%B4/%E6%99%BA%E8%83%BD%E7%BB%B4%E6%8A%A4%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/","section":"博客","summary":"推荐引擎的技术已经在亚马逊、Netflix等不同领域证明了其性能。本文讨论了推荐引擎概念在工业领域，特别是在维护操作中的使用。如今，工厂维护团队需要针对突然的资产故障制定维护计划，以减少计划外的生产停机。然而，规划需要花费大量时间，因为根据故障情况和资产环境，需要从许多选项中选择适当的维护对策。因此，我们尝试针对故障情况提出可靠的对策，以缩短规划时间。在这项工作中，我们提出了两种基于人工智能技术的维护推荐系统方法来推荐维护操作。第一种方法是单级推荐系统，它读取操作员输入的缺陷信息及其描述，以针对历史数据中发现的类似缺陷推荐维护操作。第二种方法是多阶段推荐系统，其中系统首先估计一个维护属性，该属性用作下一阶段的输入来估计下一个维护属性。最后，我们将使用过去的维护报告来评估建议的准确性，该报告包含缺陷情况和过去实际采取的维护措施。我们发现多级系统在准确性方面优于单级系统，并且多级系统可能通过维护行动建议帮助维护团队应对突发资产故障。","title":"智能维护推荐系统"},{"content":"","date":"14 April 2024","permalink":"/posts/devoops/%E6%99%BA%E8%83%BD%E8%BF%90%E7%BB%B4/","section":"博客","summary":"随着信息技术的发展，企业IT运维管理的发展大致经历了四个阶段：手工阶段、工具和自动化阶段、平台阶段和智能运维阶段。","title":"智能运维"},{"content":" Wang X, Su Y, Li Q, et al. Research on intelligent operation and maintenance management method of enterprise it[C]//Journal of Physics: Conference Series. IOP Publishing, 2021, 1732(1): 012059. 问题：随着现代企业IT相关软硬件系统变得越来越庞大、越来越复杂，这意味着运维的主要对象庞大、复杂、多样，运维的边界不断扩大，运维数据已经日益量化，传统的人工运维管理模式已经逐渐无法适应。\n智能运维研究现状 # 随着信息技术的发展，企业IT运维管理的发展大致经历了四个阶段：手工阶段、工具和自动化阶段、平台阶段和智能运维阶段。\nGartner于2016年提出了人工智能IT运维（AIOps）的概念，并预测AIOps的全球部署率将从2017年的10%增长到2020年的50%。\nGartner.https//blogs.gartner.com/andrew-learner/2017/08/09/aiops-platforms/Liu D， Zhao Y ， Xu H ， et al.Opprentice ： Towards Practical and Automatic Anomaly Detection Through Machine Learning[C]//Proceedings of the 2015 Internet Measurement Conference. New York： ACM Press，2015：211-224.\n目前，众多科研机构（如清华大学NetMan智能运维实验室等）、互联网企业（如阿里巴巴、百度、京东金融等）、大型金融机构（如交通银行、银行等）中国等）、技术厂商（如Splunk、IBM、华为等）均走在智能运维工程应用的前沿，构建了运维大数据平台、智能分析等实际应用和决策、自动化工具，取得了良好的运维效果。\n2019年，清华大学裴丹教授预测了AIOps产业的生态趋势。他指出：“各行业都在尝试落地AIOps，这为AIOps的方向提供了良好的产业基础。‘产、学、研、用’。各方都在积极跟进，形成AIOps生态圈。”\nPEI Dan, ZHANG Shenglin, PEI Changhua. Intelligent Operation and Maintenance Based on Machine Learning[J]. Communications of CCF, 2017, 13(12): 67-73.\n运维管理智能技术总体框架 # 对于企业IT来说，传统IT工具仍然发挥作用，充分保护企业的初期投资。智能运维管理是将人工智能技术融入到运维系统中，以大数据和机器学习为基础，从多种数据源收集海量数据（包括日志、业务数据、系统数据等）用于运维管理。实时或离线分析挖掘，从海量运维数据中自动学习总结规则，辅助运维人员在动态复杂的场景条件下做出高效决策[1]。准确的决策和判断，从而实现更好更快的决策和任务过程自动化。\n因此，运维管理本质上是一个观察、判断、决策、执行的循环迭代过程。期望能够快速识别异常，准确定位故障，及时找出根本原因，及时响应和治疗，实现优质高效的愿景目标。从技术角度来看，智能运维管理基于多维度、海量时序KPI、文本日志、告警信息等，直接感知和采集各类关键运维数据。\nZHU Haiqi, JIANG Feng, Research and Analysis of Anomaly Detection Technology for Operation and Maintenance Data in the Era of Artificial Intelligence, NetInfo Security. 2019, Vol. 19 Issue (11),pp 24-35\n加工后符合一定的规格和质量要求；基于大数据之上，利用机器学习等人工智能算法进行分析挖掘，根据具体运维场景分析确定运维事件的根源和传播关系，给出相应的决策建议，最后借助自动化工具直接进行运维操作。运维管理智能技术整体框架如下图所示。\n为了构建运维领域的知识图谱，需要对运维管理的知识来源进行梳理。这些知识包含在结构化（如报警事件、监控数据等）、半结构化（如配置管理信息、日志等）和非结构化（如各种规范、手册、案例）数据中。相应的数据，获取语料库进行采集和提取。然后进行相关知识存储、知识建模与表示、知识补充与融合、知识计算等，完成运维领域知识图谱的构建。以运维领域知识图谱为支撑，重点收集运维场景中相关历史数据和实时数据，研究和实现智能故障预测、智能故障分析、智能运维等。维护调度等关键落地场景技术方法。\n基于海量数据的智能故障预测方法 # 故障预测是运维管理中最基本的活动。传统运维通常在发现异常后采用运维方式进行处理，服务质量和用户体验往往受到不同程度的影响。如果将运维后的被动处理转变为事前的主动预测，用户体验和运维效率将得到大幅提升\n基本思想 # 基于海量数据的智能故障预测方法依托人工智能的机器学习和深度学习能力，通过信息系统运维数据的实际采集和模拟生成部分训练数据，提取并分析看不见的特征故障事件发生前阶段前台业务异常情况的分析。\nJiahao Bu, Ying Liu, Shenglin Zhang,Weibin Meng, Qitong Liu, Xiaotian Zhu, and Dan Pei. Rapid deployment of anomaly detection models for large number of emerging kpi streams. In IEEE IPCCC, 2018.\n网络指标下降、后台服务中断突变等常见特征，对主要、重大、一般、异常故障级别进行分类，并有效预测失败。\n主要步骤 # 智能故障预测方法利用机器学习和深度学习，挖掘故障与异常之间的深层关联特征，对系统承载的应用数据进行分析，借助数据分析技术建立预测模型，客观、准确地捕捉故障的预兆。 - 故障症状，进而预测故障。具体方法步骤如下图所示。\n首先，离线历史数据分析挖掘系统性能指标趋势和客观数据的深度关联特征，如运行环境、业务量突发事件、重大安全任务等。然后对性能指标进行在线实时监控，利用训练好的特征规则进行匹配推理，分析性能趋势，及时预测系统性能指标趋势。智能故障预测通过离线历史数据的训练得到预测模型。上线部署后，通过定期收集数据进行性能测试。同时，利用训练好的模型进行故障预测和推理。如果出现不可预测的故障，则汇总故障时间点之前的阶段数据作为训练输入，重新训练预测模型，不断迭代优化。\n基于业务特征的智能故障分析方法 # 故障分析是运维管理中最常见的活动。企业信息系统故障表现出多样性，如性能报警、KPI异常或业务故障等。\nShenglin Zhang, Ying Liu, Weibin Meng, Zhiling Luo, Jiahao Bu, Sen Yang, Peixian Liang,Dan Pei, Jun Xu, Yuzhi Zhang, et al. Prefix: Switch failure prediction in datacenter networks. Proceedings of the ACM on Measurement and Analysis of Computing Systems,2(1):2, 2018.\nADEREMI O, ANDRONICUS A A. A Survey of Machine-learning and Nature-inspired Based Credit Card Fraud Detection Techniques[J]. International Journal of System Assurance Engineering and Management, 2017, 8(2): 937–953.\n单一的故障报警已无法反映准确的故障信息，给运维人员带来困难。依靠经验判断和快速准确的定位。\n主要思想 # 智能故障分析方法以业务承载的系统特征为切入点，综合分析企业信息系统网络、硬件、软件、数据、云平台等运维数据，进行故障预测和诊断。问题分析，弥补传统运维方式的缺陷。通过采用智能故障追踪技术，可以从多样化的告警中提取出共同特征，快速定位故障点，降低运维难度，提高运维效率。\n主要步骤 # 智能故障分析方法基于大数据分析和人工智能特征挖掘，基于系统中网络等软硬件及业务隶属关系进行综合多维度历史数据分析，如：KPI、报警、性能、配置、日志、故障解决历史、运维工单历史数据等，挖掘人工经验无法总结的潜在特征和规则，输出实际运维中故障事件和特征匹配的规则库过程中，根据故障特征自动匹配故障规则进行诊断，并给出判断和处理建议。同时可以结合智能工单管理技术，实现故障精准定位并触发运维资源调度。具体方法步骤如下图所示。\n智能故障诊断是故障分析的关键步骤，主要包括诊断规则库的生成和诊断规则的运行。\nKWON D, KIM H, KIM J, et al. A Survey of Deep Learning-based Network Anomaly Detection[J]. Cluster Computing, 2019, 22(1): 949-961.\n其中，基于AI学习的诊断规则库的生成包括多维度历史数据采集，并基于历史数据、人工智能模型算法实现的特征和规则挖掘数据。诊断规则的运行包括系统监控、实时故障和报警信息获取、匹配规则库、根本原因分析、智能故障诊断以及关联运维系统发出运维需求。\nLITJENS G, KOOI T, BEJNORDI B E, et al. A Survey on Deep Learning in Medical Image Analysis[J]. Medical Image Analysis, 2017, 42: 60-88.[12] MOHAMMADI M, ALA A F, SAMEH S, et al. Deep Learnin\n运维后，反转运维的有效性，对现有的规则体系进行修改和强化，进行自学习和自优化。\n任务驱动的智能运维调度方法 # 运维调度是运维管理中最关键的活动。在执行运维任务后，特别是定位故障后，需要尽快安排运维人员处置，调度各类运维力量，完成网络、硬件、软件、数据、安全和云平台。处置效果的好坏直接决定运维效率。\n基本思想 # 智能运维调度方法通过建立科学规范的工单体系、流转流程和流转机制，将系统运维活动量化为工单内容，有效连接运维人员和运维人员活动通过工单形成以任务为基础的系统。驱动智能运维调度流程，缩短处理时间，提高响应效率。\n主要步骤 # 智能运维调度技术包括故障工单预警和基于AI学习的实时智能调度。具体方法步骤如下图所示。故障工单预警从历史工单信息中提取与故障发生相关的特征向量，借助多项式拟合、神经网络等AI经典预测算法生成故障模型​​。通过该模型，提取当前特征来预测发生故障的可能性，为主动运维提供策略参考。实时智能调度利用遗传算法根据工单类型、需求、优先级、备件、故障位置等自动规划满足全局最优调度的路径，实现实时调度。\nResource # [1]Gartner.https//blogs.gartner.com/andrew-learner/2017/08/09/aiops-platforms/Liu D， Zhao Y ， Xu H ， et al.Opprentice ： Towards Practical and Automatic Anomaly Detection Through Machine Learning[C]//Proceedings of the 2015 Internet Measurement Conference. New York： ACM Press，2015：211-224. [2]PEI Dan, ZHANG Shenglin, PEI Changhua. Intelligent Operation and Maintenance Based on Machine Learning[J]. Communications of CCF, 2017, 13(12): 67-73. [3]ZHU Haiqi, JIANG Feng, Research and Analysis of Anomaly Detection Technology for Operation and Maintenance Data in the Era of Artificial Intelligence, NetInfo Security. 2019, Vol. 19 Issue (11),pp 24-35 [4]Song Hai-tao , Wei Da-wei , Tang Guang-ming ，et a1. Anoma1y detection of single user behaviors based on pattern ming [J ] . Journal of Chinese Comput er Systems ,2016 ,37(2) :221226. [5]Jiahao Bu, Ying Liu, Shenglin Zhang,Weibin Meng, Qitong Liu, Xiaotian Zhu, and Dan Pei. Rapid deployment of anomaly detection models for large number of emerging kpi streams. In IEEE IPCCC, 2018. [6]Shenglin Zhang, Ying Liu, Weibin Meng, Zhiling Luo, Jiahao Bu, Sen Yang, Peixian Liang,Dan Pei, Jun Xu, Yuzhi Zhang, et al. Prefix: Switch failure prediction in datacenter networks. Proceedings of the ACM on Measurement and Analysis of Computing Systems,2(1):2, 2018. [7]ADEREMI O, ANDRONICUS A A. A Survey of Machine-learning and Nature-inspired Based Credit Card Fraud Detection Techniques[J]. International Journal of System Assurance Engineering and Management, 2017, 8(2): 937–953. [8]KWON D, KIM H, KIM J, et al. A Survey of Deep Learning-based Network Anomaly Detection[J]. Cluster Computing, 2019, 22(1): 949-961. [9]LITJENS G, KOOI T, BEJNORDI B E, et al. A Survey on Deep Learning in Medical Image Analysis[J]. Medical Image Analysis, 2017, 42: 60-88.[12] MOHAMMADI M, ALA A F, SAMEH S, et al. Deep Learnin ","date":"14 April 2024","permalink":"/posts/devoops/%E6%99%BA%E8%83%BD%E8%BF%90%E7%BB%B4/%E4%BC%81%E4%B8%9Ait%E6%99%BA%E8%83%BD%E8%BF%90%E7%BB%B4%E7%AE%A1%E7%90%86%E6%96%B9%E6%B3%95%E7%A0%94%E7%A9%B6/","section":"博客","summary":"随着人工智能技术对各行各业的影响逐渐加深，企业IT运维管理也正在向更加智能化、先进的方向发生变化。为了改变企业IT运维管理方法单一、手段匮乏等实际问题，本文对企业IT智能运维管理技术的整体框架进行了分析研究，深入剖析了企业IT智能运维管理技术的关键智能化方法。故障预测、故障分析、运维调度等运维管理，为有效提升企业IT智能化运维水平和系统保障能力提供支撑。","title":"企业IT智能运维管理方法研究"},{"content":"","date":"14 April 2024","permalink":"/posts/ai/%E5%9B%BE%E8%A7%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/","section":"博客","summary":"机器学习（Machine learning）是人工智能的子集，是实现人工智能的一种途径，但并不是唯一的途径。它是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能的学科。大概在上世纪80年代开始蓬勃发展，诞生了一大批数学统计相关的机器学习模型。","title":"图解机器学习"},{"content":"","date":"14 April 2024","permalink":"/posts/ai/%E5%9B%BE%E8%A7%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E4%B8%8E%E5%B9%BF%E5%91%8A%E8%AE%A1%E7%AE%97/","section":"博客","summary":"推荐与广告计算是利用算法分析大量数据，以预测用户的兴趣和行为模式，进而向他们展示个性化的内容或产品广告。这些技术广泛应用于电商、社交媒体和在线视频平台，帮助企业提升用户体验和经济效益。通过机器学习模型，系统能够不断学习和优化，实现更准确的用户画像和推荐结果。","title":"推荐与广告计算"},{"content":"转载自 https://www.showmeai.tech/article-detail/60 本文为笔者自学的资料，读者若需要请支持原版。\n一、多目标优化介绍 # 什么是多目标优化场景\n多目标排序是推荐排序系统中常见的技术实现，在很多推荐与排序常见中，有多个业务目标，找到一种综合排序方法使得多个目标都达到整体最优，能有最好的总体收益。\n为什么需要多目标优化\n为什么需要多目标排序呢，在实际互联网的推荐系统产品中，大多数用户反馈都不是直接评分，而是各式各样的隐式反馈，比如说用户的点击、收藏、分享、观看时长、下单购买等。 在评估用户满意度与设定优化目标时，可能有一些偏差：\n全局偏差/Global bias：不同目标表达不同的偏好程度。 电商场景中，购买行为表达的偏好高于点击浏览和收藏 新闻场景中，浏览时长超过20s这个行为表达的偏好高于点击 短视频场景，完播行为表达的偏好高于点击 物品偏差/Item bias：单个目标衡量不全面。 信息流产品中，标题党增加点击率，但降低满意度 短视频场景中，悬念设计提升完播率，但需要观看下一个引发用户更多操作的不满 自媒体资讯产品，鼓励转发率，可能会提升『转发保平安』等恶性操作 用户偏差/User bias：⽤户表达满意度的⽅式不同 信息流产品中，用户有 深度阅读、点赞、收藏 等不同表达满意的方式 短视频场景中，用户有 点赞、收藏、转发 等不同表达满意的方式 下图为部分互联网产品下，用户包含信息反馈的行为路径。\n在上述众多互联网业务中，工程师优化和提升的目标可能是多个，比如，短视频推荐任务，既要点击率又要完播率；电商排序，既要点击率又要转化率。\n多目标优化的难点\n多目标优化的主要技术实现是在推荐系统的『排序』环节完成，如下所示的信息流推荐中，排序环节影响最终展示结果，进而对目标效果影响最为直接。\n排序环节多使用CTR预估（click through rate prediction）技术来完成，业界有非常成熟的机器学习与深度学习技术方法与方案。但是，应用在多目标学习优化中，有五大难点：\n部分目标数据稀疏，模型准确率低。 比如在电商产品中，用户下单行为显著稀疏于点击行为，下单的标签正样本数量和比例都偏小 在线服务计算量⼤。通常多目标优化的模型有着更为复杂的模型结构，在线预估时，计算复杂度也更高；实时推荐任务需要有短响应时间和高并发支撑的稳定性，技术复杂度高一些。 多个目标间重要性难以量化。在追求点击率又追求完播率的短视频中，这两个target如何量化权衡重要度？ 分数融合的超参难以学习。很多建模方案中，我们会量化得到不同的目标score，但最终融合时，涉及到的融合计算超参数不容易通过业务直接敲定，也没有合适的方法让模型学习 Label较为模糊。很多业务中，连标签本身也是模糊的，比如资讯类产品中的阅读时长多长算长? 多目标vs多任务\n实际技术解决方案中，有几个非常相似的概念，分别是 多任务、多目标、多类别，他们的定义和关联如下图所示：\n在我们这里提到的推荐多目标优化中，其实不同的目标也对应不同的 task。 比如电商场景下，在推荐的排序阶段进行CTR建模，对同一输入样本同时预估点击率、转化率多个目标，在这个场景下，我们认为多目标多任务优化可以采用同一套方法。\n我们经常使用联合训练Joint-train的模式进行多目标优化学习，公式如下：\n$$ L=min_{\\theta} \\sum^{T}_{t=1} \\alpha ^t L^t (\\theta ^{sh}, \\theta ^{t}) $$\n$\\theta ^ t$ 是任务 $t$ 的独享参数，总 $Loss$ 是每个子任务对应 $Loss$ 的带权求和。 二、多目标学习与共享参数 # 多任务多目标学习的实现，我们现在多采用『共享』机制完成，可以在不同任务的模型参数和特征共享两方面做设计。\n模型架构方面：在深度学习网络中可以共享embedding特征，或者共享中间层的某些隐藏单元，也可以是模型某一层或者最后一层的结果，并且共享之外的部分各自独立。在模型的设计中，层间关系自由组合搭配。 特征组合方面：多个任务可以采用不同的特征组合，有的特征只属于模型架构的某个部分，有些特征整个模型都可以使用。 典型的一些参数共享机制\n参数的硬共享机制（基于参数的共享，Parameter Based）：基于参数的共享是多目标学习最常用的方法。在深度学习的网络中，通过共享特征和特征的embedding以及隐藏层的网络架构，在最后一层通过全连接+softmax的方式来区分不同任务，最后做一个线性融合来实现多目标排序。 参数的软共享机制（基于约束的共享，Regularization Based）：参数的软共享机制，每个任务都有自己的参数和模型结构，可以选择哪些共享哪些不共享。最后通过正则化的方式，来拉近模型参数之间的距离，例如使用 L2 进行正则化。 多目标优化的4种结果\n实际多目标优化，在采用共享机制设计的各种模型结构后，可能有『Well Done』、『还不错』、『不理想』、『无法接受』 这 $4$ 种不同的结果：\n『Well Done』：最好的状态，所有share任务实现共同提升。 『还不错』：其次的状态，所有任务不降低，至少一个任务提升。如果是 主任务 + 辅助任务 的搭配，能够实现牺牲辅助任务达到主任务提升的效果，也是 well done。 『不理想』：跷跷板现象，任务有涨有跌。 『无法接受』：负迁移现象，所有任务都不如从前。 三、多目标学习两大优化方向 # 为了能够更好地『共享参数』，让同个模型中多个任务和谐共存、相辅相成、相得益彰，研究界有两大优化方向，分别是：\n网络结构优化，设计更好的参数共享位置与方式 优化策略提升，设计更好的优化策略以提升优化 $Loss$ 过程中的多任务平衡 优化方向1：网络结构设计。网络结构设计方向思考哪些参数共享，在什么位置，如何共享等。 优化方向2：优化方法与策略。多目标优化策略从loss与梯度的视角去思考任务与任务之间的关系。平衡loss体量（Magnitude），调节loss更新速度（velocity），优化Gradient更新方向（direction）。在微观层面缓解梯度冲突，参数撕扯，在宏观层面达到多任务的平衡优化。 四、优化方向1：网络结构优化 # 4.1 总体思想与演进思路 # 网络结构设计是目前多任务研究和应用的主要焦点，它主要思考哪些参数共享，在什么位置，如何共享。优秀合理的共享网络结构对于最终效果提升作用巨大。\n近年来网络结构设计经历了 Share Bottom 公式 MMoE 公式 PLE 的典型结构变迁，重要的业界顶尖企业研究人员发表的多任务网络结构设计论文包括：\nShare Bottom：早期一直在使用的方式，参数共享（hard或者soft）的方式来对多任务建模。 2018 Google MMOE：将hard的参数共享变成多个 expert，通过门控来控制不同loss对每个expert的影响。 2019 Google SNR：借助简单的 NAS（Neural Architecture Search），对 Sub-Network 进行组合，为不同目标学习各自的网络结构。 2020 腾讯 PLE：在 MMOE 的基础上增加了各任务独有的 Expert。 下图中对早期的Share Bottom，MMoE，PLE三种典型网络结构及对应的动机和公式做了总结。\nShared Bottom 公式 MMoE：MMoE将shared bottom分解成多个Expert，然后通过门控网络自动控制不同任务对这些Expert的梯度贡献。 MMoE 公式 PLE：PLE在MMoE的基础上又为每个任务增加了自有的Expert，仅由本任务对其梯度更新。 ","date":"14 April 2024","permalink":"/posts/ai/%E5%9B%BE%E8%A7%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E4%B8%8E%E5%B9%BF%E5%91%8A%E8%AE%A1%E7%AE%97/%E5%A4%9A%E7%9B%AE%E6%A0%87%E4%BC%98%E5%8C%96%E5%8F%8A%E5%BA%94%E7%94%A8/","section":"博客","summary":"多目标排序是推荐排序系统中常见的技术实现，在很多推荐与排序常见中，有多个业务目标，找到一种综合排序方法使得多个目标都达到整体最优，能有最好的总体收益。","title":"推荐与广告计算 | 多目标优化介绍"},{"content":"转载自 https://www.showmeai.tech/article-detail/186 本文为笔者自学的资料，读者若需要请支持原版。\n在机器学习领域，对模型的测量和评估至关重要。选择与问题相匹配的评估方法，能帮助我们快速准确地发现在模型选择和训练过程中出现的问题，进而对模型进行优化和迭代。本文我们系统地讲解一下机器学习模型评估相关知识。\n1. 模型评估的目标 # 模型评估的目标是选出泛化能力强的模型完成机器学习任务。实际的机器学习任务往往需要进行大量的实验，经过反复调参、使用多种模型算法（甚至多模型融合策略）来完成自己的机器学习问题，并观察哪种模型算法在什么样的参数下能够最好地完成任务。\n泛化能力强的模型能很好地适用于未知的样本，模型的错误率低、精度高。机器学习任务中，我们希望最终能得到准确预测未知标签的样本、泛化能力强的模型。\n但是我们无法提前获取「未知的样本」，因此我们会基于已有的数据进行切分来完成模型训练和评估，借助于切分出的数据进行评估，可以很好地判定模型状态（过拟合 or 欠拟合），进而迭代优化。\n在建模过程中，为了获得泛化能力强的模型，我们需要一整套方法及评价指标。\n评估方法：为保证客观地评估模型，对数据集进行的有效划分实验方法。 性能指标：量化地度量模型效果的指标。 2. 离线与在线实验方法 # 进行评估的实验方法可以分为「离线」和「在线」两种。\n1）离线实验方法\n模型评估通常指离线试验。原型设计（Prototyping）阶段及离线试验方法，包含以下几个过程：\n使用历史数据训练一个适合解决目标任务的一个或多个机器学习模型。 对模型进行验证（Validation）与离线评估（Offline Evaluation）。 通过评估指标选择一个较好的模型。 2）在线实验方法\n除了离线评估之外，其实还有一种在线评估的实验方法。由于模型是在老的模型产生的数据上学习和验证的，而线上的数据与之前是不同的，因此离线评估并不完全代表线上的模型结果。因此我们需要在线评估，来验证模型的有效性。\n在线实验有一个杰出代表，那就是 $A/B Test$。\n$A/B Test$ 是目前在线测试中最主要的方法。$A/B Test$ 是为同一个目标制定两个方案让一部分用户使用 $A$ 方案，另一部分用户使用 $B$ 方案，记录下用户的使用情况，看哪个方案更符合设计目标。如果不做 $A/B$ 实验直接上线新方案，新方案甚至可能会毁掉你的产品。\n3）评估指标\n在离线评估中，经常使用准确率（Accuracy）、查准率（Precision）、召回率（Recall）、ROC、AUC、PRC等指标来评估模型。\n在线评估与离线评估所用的评价指标不同，一般使用一些商业评价指标，如用户生命周期值（Customer Lifetime value）、广告点击率（Click Through Rate）、用户流失率（Customer Churn Rate）等标。\n我们将常见的评估指标汇总如下：\n3. 常见模型评估方法介绍 # 下面我们来了解一下模型评估方法，主要涉及到对完整数据集不同的有效划分方法，保证我们后续计算得到的评估指标是可靠有效的，进而进行模型选择和优化。\n1）留出法（Hold-out）\n留出法是机器学习中最常见的评估方法之一，它会从训练数据中保留出验证样本集，这部分数据不用于训练，而用于模型评估。\n完整的数学定义如下：\n对于一个机器学习问题，通常有数据集 $D$（用于训练模型），但还需要评估模型，因此不能把整个 $D$ 用于训练，因为拿训练过的数据再去评估必然无效。那么最基本的方法就是留出法：把 $D$ 划分为两部分，训练集 $S$ 和测试集 $T$，其中 $S\\cup T=D$ ，$S\\cap T=\\empty$。\n下面是留出法数据划分的注意点：\n随机划分不一定能保证有效性，因为如果 $T$ 中正好只取到某一种特殊类型数据，从而带来了额外的误差。此时处理方法要视具体情况而定，如当数据明显的分为有限类时，可以采用分层抽样方式选择测试数据，保证数据分布比例的平衡。 单次划分不一定能得到合适的测试集，一般多次重复「划分 - 训练 - 测试求误差」的步骤，取误差的平均值。 划分的验证集，太大或者太小都不合适，常用做法是选择 $1/5~1/3$ 左右数据当作验证集用于评估。 2）交叉验证法（Cross Validation）\n留出法的数据划分，可能会带来偏差。在机器学习中，另外一种比较常见的评估方法是交叉验证法—— $K$ 折交叉验证对 $K$ 个不同分组训练的结果进行平均来减少方差。\n因此模型的性能对数据的划分就不那么敏感，对数据的使用也会更充分，模型评估结果更加稳定，可以很好地避免上述问题。\n3）自助法（Bootstrap）\n部分场景下，数据量较少，很难通过已有的数据来估计数据的整体分布（因为数据量不足时，计算的统计量反映不了数据分布），这时可以使用 Bootstrap 自助法。\nBootstrap 是一种用小样本估计总体值的一种非参数方法，在进化和生态学研究中应用十分广泛。Bootstrap通过有放回抽样生成大量的伪样本，通过对伪样本进行计算，获得统计量的分布，从而估计数据的整体分布。\n有了有效的模型评估方法，我们还需要量化的度量标准来精准评估与判断。下文归纳了分类与回归问题的各类评估指标。\n4. 回归问题常用的评估指标 # 回归类问题场景下，我们会得到连续值的预测结果，比对标准答案，我们有 MAE、MSE、RMSE 等评估指标（准则）可以衡量预测结果相对实际情况的偏离程度，它们的取值越小说明回归模型的预测越准，模型性能越好。如下图所示：\n1）平均绝对误差 MAE\n平均绝对误差（Mean Absolute Error，MAE），又叫平均绝对离差，是所有标签值与回归模型预测值的偏差的绝对值的平均。\n优点：直观地反映回归模型的预测值与实际值之间的偏差。准确地反映实际预测误差的大小。不会出现平均误差中误差符号不同而导致的正负相互抵消。 缺点：不能反映预测的无偏性（估算的偏差就是估计值的期望与真实值的差值。无偏就要求估计值的期望就是真实值）。 $$ MAE=\\frac{1}{m}\\sum^m_{i=1}|f(x_i)-y_i| $$\n2）平均绝对百分误差 MAPE\n虽然平均绝对误差能够获得一个评价值，但是你并不知道这个值代表模型拟合是优还是劣，只有通过对比才能达到效果。当需要以相对的观点来衡量误差时，则使用MAPE。\n平均绝对百分误差（Mean Absolute Percentage Error，MAPE）是对 MAE 的一种改进，考虑了绝对误差相对真实值的比例。\n优点：考虑了预测值与真实值的误差。考虑了误差与真实值之间的比例。 $$ MAPE=\\frac{100}{m}\\sum^m_{i=1}|\\frac{y_i-f(x_i)}{y_i}| $$\n在某些场景下，如房价从 $5K$ 到 $50K$ 之间，$5K$ 预测成 $10K$ 与 $50K$ 预测成 $45K$ 的差别是非常大的，而平均绝对百分误差考虑到了这点。\n3）均方误差 MSE\nMAE虽能较好衡量回归模型的好坏，但是绝对值的存在导致函数不光滑，在某些点上不能求导。可以考虑将绝对值改为残差的平方，就得到了均方误差。\n均方误差（Mean Square Error，MSE）相对于平均绝对误差而言，均方误差求的是所有标签值与回归模型预测值的偏差的平方的平均。\n优点：准确地反映实际预测误差的大小。放大预测偏差较大的值。比较不同预测模型的稳定性。 缺点：不能反映预测的无偏性。 $$ MSE=\\frac{1}{m}\\sum^m_{i=1}(f(x_i)-y_i)^2 $$\n4）均方根误差 RMSE\n均方根误差（Root-Mean-Square Error，RMSE），也称标准误差，是在均方误差的基础上进行开方运算。RMSE会被用来衡量观测值同真值之间的偏差。\n$$ RMSE=\\sqrt{MSE}=\\sqrt{\\frac{1}{m}\\sum^m_{i=1}(f(x_i)-y_i)^2} $$\n5）决定系数\n决定系数 $R$ 平方与之前介绍的三个指标有所不同，它表征的是因变量 $y$ 的变化中有多少可以用自变量 $x$ 来解释，是回归方程对观测值拟合程度的一种体现。\n$R$ 平方越接近 $1$，说明回归模型的性能越好，即能够解释大部分的因变量变化。\n优点：用于定量描述回归模型的解释能力。 缺点：没有考虑特征数量变化的影响。无法比较特征数目不同的回归模型。\n$$ R^2=\\frac{SSR}{SST} $$\nSSR：Sum of Squares of the Regression，即预测数据与原始数据均值之差的平方和，反映的是模型相对原始数据均值的离散程度。 SST：Total Sum of Squares，即原始数据和均值之差的平方和，反映的是原始数据相对均值的离散程度。 SSE：Sum of Squares for Error，残差平方和，原始数据和预测数据之差的平方和。 6）校正决定系数\n在利用 $R$ 平方来评价回归方程的优劣时，随着自变量个数的不断增加，$R$ 平方将不断增大。而校正决定系数则可以消除样本数量和特征数量的影响。\n优点：在决定系数 $R$ 平方的基础上考虑了特征个数的影响。比较变量数不同的模型。 $$ R^2_{adjusted}=1-\\frac{(1-R^2)(m-1)}{m-n-1} $$\n5. 回归评估指标适用场景分析 # 在熟悉了回归问题的各种评价指标后，再来看看各自适用的具体场景以及优缺点。\nMAE、MSE、RMSE 均存在求平均的操作（包括R的平方也可以认为有此操作，只是因为分子分母的约分导致求平均的操作不明显），而取均值是为了消除样本数量的影响，使得评估指标的大小不会太依赖于样本数量，而是更多地反映模型的误差。\n校正之后的决定系数在此基础上消除了样本数量和特征数量的影响，自变量越多，校正决定系数就会对自变量进行处罚，所以一般校正决定系数小于决定系数，它能更好地反映模型的质量，可以用来选择不同特征数量的回归模型。\n6. 分类问题常用的评估指标 # 分类问题是机器学习领域最常见的大类问题，有很多场景可以划归到分类问题的解决范畴。下面我们梳理一下分类问题的主要评估指标（Evaluation Metrics）。\n1）混淆矩阵\n在人工智能中，混淆矩阵（Confusion Matrix）是非常有效的评估模式，特别用于监督学习（在无监督学习中一般叫做匹配矩阵）。典型的混淆矩阵构成如下图所示：\n每一列代表了预测类别，每一列的总数表示预测为该类别的数据的数目。 每一行代表了数据的真实归属类别，每一行的数据总数表示该类别的数据实例的数目。 True Positive（TP）：真实值为Positive，预测值为Positive。 False positive（FP）：真实值为Negative，预测值为Negative。 False Negative（FN）：真实值为Negative，预测值为Positive。 True Negative（TN）：真实值为Positive，预测值为Negative。 很多评估指标可以基于混淆矩阵计算得到，如下图所示：\n2）Accuracy 精确率\n对于分类问题，精确率（Accuracy）指分类正确的样本数占样本总数的比例，是最常用的指标，可以总体上衡量一个预测的性能。一般情况（数据类别均衡）下，模型的精度越高，说明模型的效果越好。\n$$ Accuracy=\\frac{TP+TN}{TP+FN+FP+TN} $$\n下图中的公式有问题\n但是在数据类别严重不均衡的情况下，这个评估指标并不合理，比如发病率 $0.1%$ 的医疗场景下，如果只追求 Accuracy，模型可以把所有人判定为没有病的正常人，Accuracy高达 $99.9%$，但这个模型实际是不可用的。为了更好地应对上述问题，衍生出了一系列其他评估指标。例如：\n宁愿漏掉，不可错杀：在识别垃圾邮件的场景中可能偏向这一种思路，因为不希望很多的正常邮件被误杀，这样会造成严重的困扰。因此，查准率（Precision）将是一个被侧重关心的指标。 宁愿错杀，不可漏掉：在金融风控领域大多偏向这种思路，希望系统能够筛选出所有有风险的行为或用户，然后交给人工鉴别，漏掉一个可能造成灾难性后果。因此，查全率（Recall）将是一个被侧重关心的指标。 3） Precision 查准率\nPrecision （查准率），又称正确率、准确率，表示在模型识别为正类的样本中，真正为正类的样本所占的比例。一般情况下，查准率越高，说明模型的效果越好。\n$$ Precision=\\frac{TP}{TP+FP} $$\n4） Recall 查全率\nRecall （查全率），又称召回率，表示的是，模型正确识别出为正类的样本的数量占总的正类样本数量的比值。一般情况下，Recall 越高，说明有更多的正类样本被模型预测正确，模型的效果越好。\n$$ Recall=\\frac{TP}{TP+FN} $$\n**5）$F_\\beta-Score$ 和 $F_1-Score$ **\n理论上来说，Precision 和 Recall 都是越高越好，但更多时候它们两个是矛盾的，经常无法保证二者都很高。此时，引入一个新指标 $F\\beta - Score$，用来综合考虑 Precision 与 Recall。\n$$ F_\\beta = (1+\\beta^2) \\times \\frac{Precision \\times Recall}{\\beta^2 \\times Precision + Recall} $$\n需要根据不同的业务场景来调整 $\\beta$ 值：\n$\\beta = 1$ 时，$F_\\beta-Score$ 就是 $F_1-Score$，综合平等考虑 Precision 和 Recall 的评估指标，当 $F_1$ 值较高时则说明模型性能较好。 $$ F_1 = \\frac{2 Precision \\times Recall}{ Precision + Recall} $$\n$\\beta \u0026lt; 1$ 时，更关注 Precision。 $\\beta \u0026gt; 1$ 时，更关注 Recall。 6）ROC\n除了前面介绍的Accuracy、Precision 与 Recall，还有一些其他的度量标准，如使用 True Positive Rate（TPR，真正例率）和False Positive Rate（FPR，假正例率）两个指标来绘制 ROC 曲线。\n$$ TPR=\\frac{TP}{TP+FN} $$\n$$ FPR=\\frac{FP}{FP+TN} $$\n算法对样本进行分类时，都会有置信度，即表示该样本是正样本的概率。\n比如，$90%$ 的概率认为样本 $A$ 是正例，$1%$ 的概率认为样本 $B$ 是正例。通过选择合适的阈值，比如 $50%$ ，对样本进行划分，概率大于 $50%$ 的就认为是正例，小于 $50%$ 的就是负例。\n通过置信度可以对所有样本进行降序排序，再逐个样本地选择阈值，比如排在某个样本之前的都属于正例，该样本之后的都属于负例。每一个样本作为划分阈值时，都可以计算对应的 TPR 和 FPR，那么就可以绘制 ROC 曲线。\nROC曲线（Receiver Operating Characteristic Curve）全称是「受试者工作特性曲线」。综合考虑了概率预测排序的质量，体现了学习器在不同任务下的「期望泛化性能」的好坏，反映了TPR和FPR随阈值的变化情况。\nROC曲线越接近左上角，表示该分类器的性能越好。也就是说模型在保证能够尽可能地准确识别小众样本的基础上，还保持一个较低的误判率，即不会因为要找出小众样本而将很多大众样本给误判。\n一般来说，如果ROC是光滑的，那么基本可以判断没有太大的overfitting。\n7）AUC\nROC曲线的确能在一定程度上反映模型的性能，但它并不是那么方便，因为曲线靠近左上方这个说法还比较主观，不够定量化，因此还是需要一个定量化的标量指标来反映这个事情。ROC曲线的AUC值恰好就做到了这一点。\nAUC（Area Under ROC Curve）是 ROC 曲线下面积，其物理意义是，正样本的预测结果大于负样本的预测结果的概率，本质是AUC反应的是分类器对样本的排序能力。\nAUC值越大，就能够保证ROC曲线越靠近左上方。\n8）PRC\n与 ROC 曲线的思想类似，根据 Precision 和 Recall，也提出了一种 Precision-Recall 曲线。\n同样是通过置信度就可以对所有样本进行降序排序，再逐个样本地选择阈值，比如排在某个样本之前的都属于正例，该样本之后的都属于负例。每一个样本作为划分阈值时，都可以计算对应的 Precision 和 Recall，那么就可以绘制PR曲线。\n9）小结\n7. 二分类评估指标适用场景 # 在不同的业务场景中，Precision 和 Recall 的侧重不一样：\n对于癌症预测、地震预测这类业务场景，人们更关注模型对正类的预测能力和敏感度，因此模型要尽可能提升 Recall，甚至不惜降低 Precision。 而对于垃圾邮件识别等场景中，人们更难以接受FP（把正常邮件识别为垃圾邮件，影响工作），因此模型可以适度降低 Recall 以便获得更高的 Precision。我们可以通过调节 中 的大小来控制 Precision 和 Recall 的侧重程度。 1）评价指标分析\n对于这些评价指标的选择，有如下的一些经验：\nAccuracy适用于正负样本比例相差不大的情况的结果评估。 Precision 和 Recall 适用于正负样本差异很大的情况，Precision 不能用于抽样情况下的效果评估，Recall 不受抽样影响。 负样本的数量远远大于正样本的数据集里，PRC 更能有效衡量分类器的好坏。 AUC 计算主要与排序有关，所以它对排序敏感，而对预测分数没那么敏感。 2）垃圾邮件识别\n垃圾邮件占用网络带宽、侵犯收件人的隐私权、骗人钱财等，已经对现实社会造成了危害。一般来说，凡是未经用户许可就强行发送到用户的邮箱中的任何电子邮件都可称作是垃圾邮件，这是一个典型的二分类问题。\n「把垃圾文件识别为正常文件」和「把正常文件识别为垃圾文件」，二者相比，、我们显然更能容忍前者，因此模型可以适度降低 Recall 以便获得更高的 Precision。\n3）金融风控\n再来看个金融风控的例子，首先需要明确一点，正常客户的数量一般来说是远远大于风险客户的，这是个样本不均衡问题。互联网金融公司风控部门的主要工作是利用机器模型抓取坏客户。\n根据前面对 Precision 、 Recall 以及PR曲线的介绍，知道，Precision 和 Recall 往往都是相互牵制的，很难同时达到一个很高的水平。所以在这个案例中，同样需要根据业务场景来衡量这两个指标的重要性。\n互联网金融公司要扩大业务量，尽量多的吸引好客户，此时风控部门就会提高阈值，从而提高模型的查准率 Precision，同时，也会放进一部分坏客户，导致查全率 Recall 下降。\n如果公司坏账扩大，公司缩紧业务，尽可能抓住更多的坏客户，此时风控部门需要不惜一切代价降低损失，守住风险底线，因此会降低阈值，从而提高模型的查全率 Recall，但是这样会导致一部分好客户误抓，从而降低模型的查准率 Precision。\n可以通过调节 $F_\\beta-Score$ 中 $\\beta$ 的大小来控制 Precision 和 Recall 的侧重程度。$\\beta \u0026lt; 1$ ，重视查准率；$\\beta \u0026gt; 1$ ，重视查全率。\n8. 样本均衡与采样 # 首先看看什么是分类任务中的样本不均衡问题，以及如何解决样本不均衡问题。\n1）样本均衡问题\n在学术研究与教学中，很多算法都有一个基本假设，那就是数据分布是均匀的。当把这些算法直接应用于实际数据时，大多数情况下都无法取得理想的结果，因为实际数据往往分布得很不均匀，都会存在「长尾现象」。\n多数样本数量多，信息量大，容易被模型充分学习，模型容易识别这类样本 少数样本数量少，信息量少，模型没有充分学习到它们的特征，很难识别这类样本 解决这一问题的基本思路是，让正负样本在训练过程中拥有相同的话语权（比如利用采样与加权等方法）。样本类别不均衡的情况下，最常见的处理方式是「数据采样」与「样本加权」，详细介绍如下：\n2）数据采样\n（1）欠采样 / 下采样\n欠采样技术是将数据从原始数据集中移除。\n从多数类集合中筛选样本集E。 将这些样本从多数类集合中移除。 （2）过采样 / 上采样\n随机过采样：\n首先在少数类集合中随机选中一些少数类样本。 然后通过复制所选样本生成样本集合 。 将它们添加到少数类集合中来扩大原始数据集从而得到新的少数类集合。 我们也有一些少类别样本合成技术方法，比如机器学习中有SMOTE算法通过合成新样本完成过采样，缓解样本类别不均衡问题。\n（3）不同采样方法的比较\n下采样的缺点显而易见，那就是最终的训练集丢失了数据，模型只学到了总体模式的一部分。而SMOTE算法为每个小众样本合成相同数量的新样本，但这也带来一些潜在的问题：\n一方面是增加了类之间重叠的可能性，即通过算法生成的小众样本并不一定是合理的小众样本。 另一方面是生成一些没有提供有益信息的样本。 3）加权\n除了上采样和下采样这种采样方式以外，还可以通过加权的方式来解决数据不均衡问题，即对不同类别分错的代价不同，对于小众样本，如果分错了会造成更大的损失。这种方法的难点在于设置合理的权重，实际应用中一般让各个分类间的加权损失值近似相等。当然这并不是通用法则，还是需要具体问题具体分析。\n","date":"13 April 2024","permalink":"/posts/ai/%E5%9B%BE%E8%A7%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95%E4%B8%8E%E5%87%86%E5%88%99/","section":"博客","summary":"在机器学习领域，对模型的测量和评估至关重要。选择与问题相匹配的评估方法，能帮助我们快速准确地发现在模型选择和训练过程中出现的问题，进而对模型进行优化和迭代。本文我们系统地讲解一下机器学习模型评估相关知识。","title":"图解机器学习 | 机器学习基础知识"},{"content":"转载自 https://www.showmeai.tech/article-detail/185 本文为笔者自学的资料，读者若需要请支持原版。\n1. 机器学习概述 # 人工智能（Artificial intelligence）是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。它是一个笼统而宽泛的概念，人工智能的最终目标是使计算机能够模拟人的思维方式和行为。大概在上世纪50年代开始兴起，但是受限于数据和硬件设备等限制，当时发展缓慢。\n机器学习（Machine learning）是人工智能的子集，是实现人工智能的一种途径，但并不是唯一的途径。它是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能的学科。大概在上世纪80年代开始蓬勃发展，诞生了一大批数学统计相关的机器学习模型。\n深度学习（Deep learning）是机器学习的子集，灵感来自人脑，由人工神经网络（ANN）组成，它模仿人脑中存在的相似结构。在深度学习中，学习是通过相互关联的「神经元」的一个深层的、多层的「网络」来进行的。「深度」一词通常指的是神经网络中隐藏层的数量。大概在2012年以后爆炸式增长，广泛应用在很多的场景中。\n让我们看看国外知名学者对机器学习的定义：\n机器学习研究的是计算机怎样模拟人类的学习行为，以获取新的知识或技能，并重新组织已有的知识结构，使之不断改善自身。从实践的意义上来说，机器学习是在大数据的支撑下，通过各种算法让机器对数据进行深层次的统计分析以进行「自学」，使得人工智能系统获得了归纳推理和决策能力\n通过经典的「垃圾邮件过滤」应用，我们再来理解下机器学习的原理，以及定义中的T、E、P分别指代什么。\n机器学习三要素\n机器学习三要素包括数据、模型、算法。这三要素之间的关系，可以用下面这幅图来表示：\n1. 数据\n数据驱动：数据驱动指的是我们基于客观的量化数据，通过主动数据的采集分析以支持决策。与之相对的是经验驱动，比如我们常说的「拍脑袋」。\n2. 模型\u0026amp;算法\n模型：在AI数据驱动的范畴内，模型指的是基于数据X做决策Y的假设函数，可以有不同的形态，计算型和规则型等。 算法：指学习模型的具体计算方法。统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后需要考虑用什么样的计算方法求解最优模型。通常是一个最优化的问题。 机器学习发展历程\n人工智能一词最早出现于1956年，用于探索一些问题的有效解决方案。1960年，美国国防部借助「神经网络」这一概念，训练计算机模仿人类的推理过程。\n2010年之前，谷歌、微软等科技巨头改进了机器学习算法，将查询的准确度提升到了新的高度。而后，随着数据量的增加、先进的算法、计算和存储容量的提高，机器学习得到了更进一步的发展。\n机器学习核心技术\n分类：应用以分类数据进行模型训练，根据模型对新样本进行精准分类与预测。 聚类：从海量数据中识别数据的相似性与差异性，并按照最大共同点聚合为多个类别。 异常检测：对数据点的分布规律进行分析，识别与正常数据及差异较大的离群点。 回归：根据对已知属性值数据的训练，为模型寻找最佳拟合参数，基于模型预测新样本的输出值。 机器学习基本流程\n机器学习工作流（WorkFlow）包含数据预处理（Processing）、模型学习（Learning）、模型评估（Evaluation）、新样本预测（Prediction）几个步骤。\n数据预处理：输入（未处理的数据 + 标签）→处理过程（特征处理+幅度缩放、特征选择、维度约减、采样）→输出（测试集 + 训练集）。 模型学习：模型选择、交叉验证、结果评估、超参选择。 模型评估：了解模型对于数据集测试的得分。 新样本预测：预测测试集。 机器学习应用场景\n作为一套数据驱动的方法，机器学习已广泛应用于数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别和机器人等领域。\n智能医疗：智能假肢、外骨骼、医疗保健机器人、手术机器人、智能健康管理等。 人脸识别：门禁系统、考勤系统、人脸识别防盗门、电子护照及身份证，还可以利用人脸识别系统和网络，在全国范围内搜捕逃犯。 机器人的控制领域：工业机器人、机械臂、多足机器人、扫地机器人、无人机等。 2. 机器学习基本名词 # 监督学习（Supervised Learning）：训练集有标记信息，学习方式有分类和回归。 无监督学习（Unsupervised Learning）：训练集没有标记信息，学习方式有聚类和降维。 强化学习（Reinforcement Learning）：有延迟和稀疏的反馈标签的学习方式。 示例/样本：上面一条数据集中的一条数据。 属性/特征：「色泽」「根蒂」等。 属性空间/样本空间/输入空间X：由全部属性张成的空间。 特征向量：空间中每个点对应的一个坐标向量。 标记：关于示例结果的信息，如（（色泽=青绿，根蒂=蜷缩，敲声=浊响），好瓜），其中「好瓜」称为标记。 分类：若要预测的是离散值，如「好瓜」，「坏瓜」，此类学习任务称为分类。 假设：学得模型对应了关于数据的某种潜在规律。 真相：潜在规律自身。 学习过程：是为了找出或逼近真相。 泛化能力：学得模型适用于新样本的能力。一般来说，训练样本越大，越有可能通过学习来获得具有强泛化能力的模型。 3. 机器学习算法分类 # 机器学习算法依托的问题场景\n机器学习在近30多年已发展为一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、计算复杂性理论等多门学科。机器学习理论主要是设计和分析一些让计算机可以自动「学习」的算法。\n机器学习算法从数据中自动分析获得规律，并利用规律对未知数据进行预测。机器学习理论关注可以实现的、行之有效的学习算法。很多推论问题属于无程序可循难度，所以部分的机器学习研究是开发容易处理的近似算法。\n机器学习最主要的类别有：监督学习、无监督学习和强化学习。\n监督学习：从给定的训练数据集中学习出一个函数，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求是包括输入和输出，也可以说是特征和目标。训练集中的目标是由人标注的。常见的监督学习算法包括回归分析和统计分类。 无监督学习：与监督学习相比，训练集没有人为标注的结果。常见的无监督学习算法有生成对抗网络（GAN）、聚类。 强化学习：通过观察来学习做成如何的动作。每个动作都会对环境有所影响，学习对象根据观察到的周围环境的反馈来做出判断。 分类问题\n分类问题是机器学习非常重要的一个组成部分。它的目标是根据已知样本的某些特征，判断一个新的样本属于哪种已知的样本类。分类问题可以细分如下：\n二分类问题：表示分类任务中有两个类别新的样本属于哪种已知的样本类。 多类分类（Multiclass classification）问题：表示分类任务中有多类别。 多标签分类（Multilabel classification）问题：给每个样本一系列的目标标签。 了解更多机器学习分类算法：KNN算法、逻辑回归算法、朴素贝叶斯算法、决策树模型、随机森林分类模型、GBDT模型、XGBoost模型、支持向量机模型等。\n回归问题\n了解更多机器学习回归算法：决策树模型、随机森林分类模型、GBDT模型、回归树模型、支持向量机模型等。\n聚类问题\n降维问题\n4. 机器学习模型评估与选择 # 机器学习与数据拟合\n机器学习最典型的监督学习为分类与回归问题。分类问题中，我们学习出来一条「决策边界」完成数据区分；在回归问题中，我们学习出拟合样本分布的曲线。\n训练集与数据集\n我们以房价预估为例，讲述一下涉及的概念。\n训练集（Training Set）：帮助训练模型，简单的说就是通过训练集的数据让确定拟合曲线的参数。 测试集（Test Set）：为了测试已经训练好的模型的精确度。 当然，test set这并不能保证模型的正确性，只是说相似的数据用此模型会得出相似的结果。因为在训练模型的时候，参数全是根据现有训练集里的数据进行修正、拟合，有可能会出现过拟合的情况，即这个参数仅对训练集里的数据拟合比较准确，这个时候再有一个数据需要利用模型预测结果，准确率可能就会很差。\n经验误差\n在训练集的数据上进行学习。模型在训练集上的误差称为「经验误差」（Empirical Error）。但是经验误差并不是越小越好，因为我们希望在新的没有见过的数据上，也能有好的预估结果。\n过拟合\n过拟合，指的是模型在训练集上表现的很好，但是在交叉验证集合测试集上表现一般，也就是说模型对未知样本的预测表现一般，泛化（Generalization）能力较差。\n如何防止过拟合呢？一般的方法有Early Stopping、数据集扩增（Data Augmentation）、正则化、Dropout等。\n正则化：指的是在目标函数后面添加一个正则化项，一般有L1正则化与L2正则化。L1正则是基于L1范数，即在目标函数后面加上参数的L1范数和项，即参数绝对值和与参数的积项。 数据集扩增：即需要得到更多的符合要求的数据，即和已有的数据是独立同分布的，或者近似独立同分布的。一般方法有：从数据源头采集更多数据、复制原有数据并加上随机噪声、重采样、根据当前数据集估计数据分布参数，使用该分布产生更多数据等。 DropOut：通过修改神经网络本身结构来实现的。 Dropout 是一种在训练神经网络模型时常用的正则化技术。它由 Hinton 和他的学生 Srivastava 在 2014 年提出。Dropout 的主要思想是在训练过程中随机地丢弃（即不激活）网络中的一部分神经元。这样做可以防止模型对训练数据过拟合，从而提高模型在新数据上的泛化能力。 在具体实施时，每个神经元都有一定概率被临时从网络中移除，这个概率是一个超参数，通常设置在 20%-50%。这意味着网络的每次前向传播都是在略微不同的网络结构上进行的。由于这种随机性，模型被迫学习更加鲁棒的特征，因为它不能依赖于任何一个神经元的存在。 在测试时，所有的神经元都会被保留，但它们的输出会根据在训练时被丢弃的概率进行缩放，以此保持网络总的活动水平与训练时相似。这种方法的效果已在多种网络架构和任务中得到验证，是深度学习中非常流行的一种技术。 偏差\n偏差（Bias），它通常指的是模型拟合的偏差程度。给定无数套训练集而期望拟合出来的模型就是平均模型。偏差就是真实模型和平均模型的差异。\n简单模型是一组直线，平均之后得到的平均模型是一条直的虚线，与真实模型曲线的差别较大（灰色阴影部分较大）。因此，简单模型通常高偏差 。\n复杂模型是一组起伏很大波浪线，平均之后最大值和最小组都会相互抵消，和真实模型的曲线差别较小，因此复杂模型通常低偏差（见黄色曲线和绿色虚线几乎重合）。\n方差\n方差（Variance），它通常指的是模型的平稳程度（简单程度）。简单模型的对应的函数如出一辙，都是水平直线，而且平均模型的函数也是一条水平直线，因此简单模型的方差很小，并且对数据的变动不敏感。\n复杂模型的对应的函数千奇百怪，毫无任何规则，但平均模型的函数也是一条平滑的曲线，因此复杂模型的方差很大，并且对数据的变动很敏感。\n偏差与方差的平衡\n性能度量指标\n性能度量是衡量模型泛化能力的数值评价标准，反映了当前问题（任务需求）。使用不同的性能度量可能会导致不同的评判结果。\n1. 回归问题\n关于模型「好坏」的判断，不仅取决于算法和数据，还取决于当前任务需求。回归问题常用的性能度量指标有：平均绝对误差、均方误差、均方根误差、R平方等。\n平均绝对误差（Mean Absolute Error，MAE），又叫平均绝对离差，是所有标签值与回归模型预测值的偏差的绝对值的平均。 平均绝对百分误差（Mean Absolute Percentage Error，MAPE）是对MAE的一种改进，考虑了绝对误差相对真实值的比例。 均方误差（Mean Square Error，MSE）相对于平均绝对误差而言，均方误差求的是所有标签值与回归模型预测值的偏差的平方的平均。 均方根误差（Root-Mean-Square Error，RMSE），也称标准误差，是在均方误差的基础上进行开方运算。RMSE会被用来衡量观测值同真值之间的偏差。 R平方，决定系数，反映因变量的全部变异能通过目前的回归模型被模型中的自变量解释的比例。比例越接近于1，表示当前的回归模型对数据的解释越好，越能精确描述数据的真实分布。 2. 分类问题\n分类问题常用的性能度量指标包括错误率（Error Rate）、精确率（Accuracy）、查准率（Precision）、查全率（Recall）、F1、ROC曲线、AUC曲线和R平方等。\n错误率：分类错误的样本数占样本总数的比例。 精确率：分类正确的样本数占样本总数的比例。 查准率（也称准确率），即在检索后返回的结果中，真正正确的个数占你认为是正确的结果的比例。 查全率（也称召回率），即在检索结果中真正正确的个数，占整个数据集（检索到的和未检索到的）中真正正确个数的比例。 F1是一个综合考虑查准率与查全率的度量，其基于查准率与查全率的调和平均定义：即：F1度量的一般形式-Fβ，能让我们表达出对查准率、查全率的不同偏好。 ROC曲线（Receiver Operating Characteristic Curve）全称是「受试者工作特性曲线」。综合考虑了概率预测排序的质量，体现了学习器在不同任务下的「期望泛化性能」的好坏。ROC曲线的纵轴是「真正例率」（TPR），横轴是「假正例率」（FPR）。\nAUC（Area Under ROC Curve）是ROC曲线下面积，代表了样本预测的排序质量。\n从一个比较高的角度来认识AUC：仍然以异常用户的识别为例，高的AUC值意味着，模型在能够尽可能多地识别异常用户的情况下，仍然对正常用户有着一个较低的误判率（不会因为为了识别异常用户，而将大量的正常用户给误判为异常。\n评估方法\n我们手上没有未知的样本，如何可靠地评估？关键是要获得可靠的「测试集数据」（Test Set），即测试集（用于评估）应该与训练集（用于模型学习）「互斥」。\n常见的评估方法有：留出法（Hold-out）、交叉验证法（ Cross Validation）、自助法（Bootstrap）。\n留出法（Hold-out）是机器学习中最常见的评估方法之一，它会从训练数据中保留出验证样本集，这部分数据不用于训练，而用于模型评估。\n机器学习中，另外一种比较常见的评估方法是交叉验证法（ Cross Validation）。k 折交叉验证对 k 个不同分组训练的结果进行平均来减少方差，因此模型的性能对数据的划分就不那么敏感，对数据的使用也会更充分，模型评估结果更加稳定。\n自助法（Bootstrap）是一种用小样本估计总体值的一种非参数方法，在进化和生态学研究中应用十分广泛。\nBootstrap通过有放回抽样生成大量的伪样本，通过对伪样本进行计算，获得统计量的分布，从而估计数据的整体分布。\n模型调优与选择准则\n我们希望找到对当前问题表达能力好，且模型复杂度较低的模型：\n表达力好的模型，可以较好地对训练数据中的规律和模式进行学习； 复杂度低的模型，方差较小，不容易过拟合，有较好的泛化表达。 如何选择最优的模型\n验证集评估选择 切分数据为训练集和验证集。 对于准备好的候选超参数，在训练集上进行模型，在验证集上评估。 网格搜索/随机搜索交叉验证 通过网格搜索/随机搜索产出候选的超参数组。 对参数组的每一组超参数，使用交叉验证评估效果。 选出效果最好的超参数。 贝叶斯优化 基于贝叶斯优化的超参数调优。 ","date":"13 April 2024","permalink":"/posts/ai/%E5%9B%BE%E8%A7%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","section":"博客","summary":"\u003cstrong\u003e机器学习\u003c/strong\u003e（Machine learning）是人工智能的子集，是实现人工智能的一种途径，但并不是唯一的途径。它是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能的学科。大概在上世纪80年代开始蓬勃发展，诞生了一大批数学统计相关的机器学习模型。","title":"图解机器学习 | 机器学习基础知识"},{"content":"","date":"12 April 2024","permalink":"/posts/architecture/backend/","section":"博客","summary":"后端开发是指构建和维护网站或应用程序的服务器端逻辑和数据库管理。它涉及处理服务器、数据库、API，以及确保数据的安全性和高效传输，以支持前端用户体验。","title":"后端开发"},{"content":"Resources # 蓄水池抽样算法（Reservoir Sampling） 蓄水池算法原理 ","date":"12 April 2024","permalink":"/posts/architecture/backend/%E8%93%84%E6%B0%B4%E6%B1%A0%E6%8A%BD%E6%A0%B7%E7%AE%97%E6%B3%95/","section":"博客","summary":"我是在一次失败的面试经历中听说蓄水池算法的。之后上网搜了搜，知道是一个数据抽样算法，寥寥几行，却暗藏玄机。主要用来解决如下问题。","title":"蓄水池抽样算法（Reservoir Sampling）"},{"content":"","date":"11 April 2024","permalink":"/tags/review/","section":"Tags","summary":"","title":"Review"},{"content":" import java.util.*; /* * public class ListNode { * int val; * ListNode next = null; * public ListNode(int val) { * this.val = val; * } * } */ public class Solution { /** * 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可 * * * @param head ListNode类 * @return ListNode类 */ public ListNode insert0 (ListNode head) { // write code here ListNode temp = head; while(temp!=null \u0026amp;\u0026amp; temp.next!=null){ ListNode newNode = new ListNode(0); newNode.next = temp.next; temp.next = newNode; temp = newNode.next; } return head; } } import java.util.*; public class Solution { /** * 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可 * * * @param a int整型一维数组 * @return int整型 */ public int getMethods (int[] a) { // int n = a.length; // int[][] dp = new int[n][n]; // for // write code here int n = a.length; boolean[] used = new boolean[n]; Arrays.sort(a); Set\u0026lt;String\u0026gt; ans = new HashSet\u0026lt;\u0026gt;(); dfs(a, 0, used, n, new ArrayList\u0026lt;Integer\u0026gt;(), ans); return ans.size(); } public static void dfs( int[] a, int index, boolean[] used, int maxx, ArrayList\u0026lt;Integer\u0026gt; temp, Set\u0026lt;String\u0026gt; ans) { if (index == maxx) { //System.out.println(1); if (isRight(temp)) { ans.add(temp.toString()); } return; } // System.out.println(maxx); for (int i = 0; i \u0026lt; maxx; i++) { if (used[i] == false) { temp.add(a[i]); used[i] = true; dfs(a, index + 1, used, maxx, temp, ans); used[i] = false; temp.remove(index); } } } public static boolean isRight(ArrayList\u0026lt;Integer\u0026gt; temp) { for (int i = 1; i \u0026lt; temp.size(); i++) { // System.out.println(i); if (isSu(temp.get(i - 1) + temp.get(i)) == false) { return false; } } return true; } public static boolean isSu(int k) { for (int i = 2; i * i \u0026lt; k; i++) { if (k % i == 0) return false; } return true; } } 过了 8/30，爆时间超时\nimport java.util.*; public class Solution { private Set\u0026lt;String\u0026gt; ans; private int[] a; private boolean[] used; private int n; private List\u0026lt;Integer\u0026gt; primes; // Store primes up to a certain number public int getMethods(int[] a) { this.a = a; this.n = a.length; this.used = new boolean[n]; Arrays.sort(a); this.ans = new HashSet\u0026lt;\u0026gt;(); int maxPossibleSum = 2 * a[n-1]; // Maximum sum of two elements generatePrimes(maxPossibleSum); dfs(0, new ArrayList\u0026lt;\u0026gt;()); return ans.size(); } private void dfs(int index, ArrayList\u0026lt;Integer\u0026gt; temp) { if (index == n) { if (isRight(temp)) { ans.add(temp.toString()); } return; } for (int i = 0; i \u0026lt; n; i++) { if (!used[i]) { if (temp.size() \u0026gt; 0 \u0026amp;\u0026amp; !primes.contains(temp.get(temp.size() - 1) + a[i])) { continue; // Early pruning if the sum is not prime } temp.add(a[i]); used[i] = true; dfs(index + 1, temp); used[i] = false; temp.remove(temp.size() - 1); // Remove the last element } } } private boolean isRight(ArrayList\u0026lt;Integer\u0026gt; temp) { for (int i = 1; i \u0026lt; temp.size(); i++) { if (!primes.contains(temp.get(i - 1) + temp.get(i))) { return false; } } return true; } private void generatePrimes(int max) { primes = new ArrayList\u0026lt;\u0026gt;(); boolean[] isPrime = new boolean[max + 1]; Arrays.fill(isPrime, true); for (int i = 2; i \u0026lt;= max; i++) { if (isPrime[i]) { primes.add(i); for (int j = 2 * i; j \u0026lt;= max; j += i) { isPrime[j] = false; } } } } } 过了 23/30，爆时间超时\nimport java.util.*; public class Solution { private Set\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; uniquePerms = new HashSet\u0026lt;\u0026gt;(); private List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; validPairs = new ArrayList\u0026lt;\u0026gt;(); private boolean[] visited; private int[] nums; public int getMethods(int[] nums) { this.nums = nums; Arrays.sort(nums); // 排序以处理重复元素 this.visited = new boolean[nums.length]; prepareValidPairs(nums); // 尝试每个数字作为起始点 for (int i = 0; i \u0026lt; nums.length; i++) { if (i == 0 || nums[i] != nums[i - 1]) { // 跳过重复的起始数字 visited[i] = true; dfs(i, new ArrayList\u0026lt;\u0026gt;(Arrays.asList(nums[i]))); visited[i] = false; } } return uniquePerms.size(); } private void dfs(int currentIndex, List\u0026lt;Integer\u0026gt; currentPath) { if (currentPath.size() == nums.length) { uniquePerms.add(new ArrayList\u0026lt;\u0026gt;(currentPath)); return; } for (int nextIndex : validPairs.get(currentIndex)) { if (!visited[nextIndex]) { visited[nextIndex] = true; currentPath.add(nums[nextIndex]); dfs(nextIndex, currentPath); currentPath.remove(currentPath.size() - 1); visited[nextIndex] = false; } } } private void prepareValidPairs(int[] nums) { for (int i = 0; i \u0026lt; nums.length; i++) { validPairs.add(new ArrayList\u0026lt;\u0026gt;()); for (int j = 0; j \u0026lt; nums.length; j++) { if (i != j \u0026amp;\u0026amp; isPrime(nums[i] + nums[j])) { validPairs.get(i).add(j); } } } } private boolean isPrime(int num) { if (num \u0026lt;= 1) return false; if (num \u0026lt;= 3) return true; if (num % 2 == 0 || num % 3 == 0) return false; for (int i = 5; i * i \u0026lt;= num; i += 6) { if (num % i == 0 || num % (i + 2) == 0) return false; } return true; } public static void main(String[] args) { Solution solution = new Solution(); int[] input = {1, 1, 2}; System.out.println(\u0026#34;Total permutations: \u0026#34; + solution.getMethods(input)); } } 过了 29/30，爆时间超时\nimport java.util.*; public class Solution { private Set\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; uniquePerms = new HashSet\u0026lt;\u0026gt;(); private List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; validPairs = new ArrayList\u0026lt;\u0026gt;(); private boolean[] visited; private int[] nums; public int getMethods(int[] nums) { this.nums = nums; Arrays.sort(nums); // 排序以处理重复元素 if (nums.length \u0026gt; 0 \u0026amp;\u0026amp; allSame(nums)) { return 1; // All elements are the same, only one valid permutation } this.visited = new boolean[nums.length]; prepareValidPairs(nums); // 尝试每个数字作为起始点 for (int i = 0; i \u0026lt; nums.length; i++) { if (i == 0 || nums[i] != nums[i - 1]) { // 跳过重复的起始数字 visited[i] = true; dfs(i, new ArrayList\u0026lt;\u0026gt;(Arrays.asList(nums[i]))); visited[i] = false; } } return uniquePerms.size(); } private void dfs(int currentIndex, List\u0026lt;Integer\u0026gt; currentPath) { if (currentPath.size() == nums.length) { uniquePerms.add(new ArrayList\u0026lt;\u0026gt;(currentPath)); return; } for (int nextIndex : validPairs.get(currentIndex)) { if (!visited[nextIndex]) { visited[nextIndex] = true; currentPath.add(nums[nextIndex]); dfs(nextIndex, currentPath); currentPath.remove(currentPath.size() - 1); visited[nextIndex] = false; } } } private void prepareValidPairs(int[] nums) { for (int i = 0; i \u0026lt; nums.length; i++) { validPairs.add(new ArrayList\u0026lt;\u0026gt;()); for (int j = 0; j \u0026lt; nums.length; j++) { if (i != j \u0026amp;\u0026amp; isPrime(nums[i] + nums[j])) { validPairs.get(i).add(j); } } } } private boolean isPrime(int num) { if (num \u0026lt;= 1) return false; if (num \u0026lt;= 3) return true; if (num % 2 == 0 || num % 3 == 0) return false; for (int i = 5; i * i \u0026lt;= num; i += 6) { if (num % i == 0 || num % (i + 2) == 0) return false; } return true; } private boolean allSame(int[] array) { int first = array[0]; for (int num : array) { if (num != first) { return false; } } return true; } public static void main(String[] args) { Solution solution = new Solution(); int[] input = {1, 1, 2}; System.out.println(\u0026#34;Total permutations: \u0026#34; + solution.getMethods(input)); } } 问题：\n质疑研究生的项目？问核心的技术点在哪里？ 问 concurrentHashMap？ 问 write 1024Byte 10次，另一个read，要读几次？UDP 问知道 sky compute 吗？ 问你知道 Ray 吗 k8s 中的一些概念 下午跟导师聊完毕设，心力憔悴，第一次在面试的时候感觉到不想说话，状态极差！\n","date":"11 April 2024","permalink":"/posts/reviews/24%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0/%E8%85%BE%E8%AE%AF/%E5%85%A8%E6%B0%91k%E6%AD%8C%E4%BA%8C%E9%9D%A24.20/","section":"博客","summary":"全民k歌的面试官很尖锐","title":"全民k歌二面4.20"},{"content":" https://dolphinscheduler.apache.org/zh-cn/docs/3.2.1/guide/start/quick-start https://github.com/idcos/cloudiac 资源漂移是什么 ","date":"11 April 2024","permalink":"/posts/reviews/knowledge/%E9%83%A8%E7%BD%B2%E5%B7%A5%E5%85%B7/","section":"博客","summary":"电网项目调研部署工具","title":"电网项目调研部署工具"},{"content":"","date":"11 April 2024","permalink":"/posts/reviews/knowledge/","section":"博客","summary":"面试中细碎的知识点","title":"面试小知识点"},{"content":"昨天，跟着导师去对接的公司开了项目会，导师跟对面公司的对接人说，你还需要学，你最好去查查靠谱三要素是啥？\n说者无心，听者有意。靠谱三要素是啥呢？\nhttps://36kr.com/p/2406339371311879 靠谱，才是最顶级的聪明\n你可能也发现了这个现象：无论是在哪个团队里，我们都喜欢靠谱的人，对别人也往往会用“靠不靠谱”来评价。\n为什么靠谱这么重要？\n因为靠谱是一种稀缺品质，自带安全属性。与靠谱的人相处，不用猜疑、顾虑，不用提心吊胆。\n李嘉诚曾说：做事要找靠谱的人，聪明的人只能聊聊天。\n在这个聪明过剩的年代，靠谱才是最顶级的聪明。靠谱的人，就该被世界狠狠地奖励。\n那么，什么是靠谱呢？\n评价一个人是否靠谱，我们往往不会把这些当做标准：学历高不高，能力强不强，聪不聪明，等等。\n只有办事利索、说话得当、人品好，才能算是靠谱。\n也就是说，真正靠谱的人，事不拖，话不多，人不作。\n一、事不拖：做人有责任心，做事有责任感 # “不靠谱的人，都有什么特征？”\n有人这样回答：“一边拖延和等待，一边又天真的想要一个好结果。”\n事不拖，说起来好像很简单，但能做到的人并不多。\n生活中我们可能都有过这种经历：\n有的人对某件事满口答应，却总是无法按时交付，还找了很多的借口和理由；\n有的人做事喜欢靠时间，时间充足的时候不干事，临到时间点了，就想方设法凑数应付。\n爱拖延，就是不靠谱的特征。\n为什么有的人爱拖延？\n说白了，就是懒。懒得想，懒得干，懒得尽快出成果。总想往后靠一会儿，等等看。\n等到限定的时间到了，就开始着急了，赶紧先准备一堆理由，安慰自己，抚慰别人。\n你肯定听说过“延迟满足感”这个词语，而爱拖延的人都是追求当下的满足感。反正明天才给结果，今天就先歇一歇玩一玩，到时候想办法应付一下就行了。\n持续发作的拖延症会形成恶性循环，越拖延越没时间，越没时间越焦虑，越焦虑越干不好……循环往复。\n靠谱的人，事不拖。\n说干就干，答应了就要干好。不仅要完成，而且要尽量做得更好。能拿出80分的成果，就绝不用60分来凑数。\n但是，拖延症是人性的舒适区，“延迟难受感”。如何克服它呢？\n克服拖延症，就是要养成迅速开始的习惯。不是马上干，而是现在就开始。有几种方法可以参考。\n第一种，从最重要的任务开始。\n先做那个容易被你拖延，且完成之后对你影响最大的任务。\n那是什么任务呢？通常来说是巨大的任务、麻烦的任务、困难的任务。\n比如在美国，大约有65%的成年人梦想有一天能创业，然而真正走上创业之路的只有1%。大部分人没有行动的原因，是他们不知道如何创业。然而，美国现在还是有3000万个企业，大量的企业是由毫无商业经验的人开办的。\n确立最重要的任务后，就把它分解为一个一个具体的步骤，一次只完成一个步骤就好。\n第二种，先考虑代价。\n我们总是非常忙，从而难以集中精力做好该做的事。所以，你可以先问自己：“如果我不完成这项工作，将会发生什么？”\n比如有的大学挂科太多，会导致无法毕业；老师点名总不在，就无法得到“平时分数”。\n对职场人来说，某些重要工作没有高质量完成，可能影响上级对你的绩效评定，从而影响绩效奖金、年终奖，以及升职加薪。\n有句话叫“年头出事白干一年，年底出事一年白干。”虽然这句话说得很偏激，做法本身也很不好，但不得不承认，在某些情况下这是事实。\n所以，你需要“恐吓”自己，先想清楚坏的结果，激励自己立即开始。\n第三种，是考虑收益。\n从积极的角度来考虑：“如果我成功完成工作，把这个项目干成了，那么我的职业生涯将会如何？我将获得的好处是什么？”\n说简单点，增长了经验，交付了成果。说大一点，你做过的每一项工作，都会成为你的“资产”，铺垫在你的脚下，让你站得更高、走得更远。\n第四种方法，是预留出固定的15分钟时间，来完成一项工作。\n为什么要选择15分钟？因为时间够短，你总能找到15分钟，很容易就开始工作。\n一旦开始工作，就一直工作15分钟。你可以放一个计时器，在15分钟结束时响起。你就能体会到“与时间赛跑”的感觉。\n但你可能会说：“15分钟能完成多少工作呀？我只有15分钟，然后我就得回去做那些不太要紧的事情。”\n通常，在完成了15分钟的工作后，你会想：“也许我需要再花15分钟。”你会得到鼓励，感到兴奋，并且干劲十足。\n一旦有了这种自信的感觉，你就不会再拖延。当你开始并完成任务的一部分时，你会感到快乐，甚至兴高采烈。\n如果你继续这样做，很快你就会拥有自驱力。\n第五种方法，拒绝完美主义。\n每个人都有完美主义的倾向。有的人会说：“既然没有时间把它做到最好，那就不要做好了。等到我有足够的时间再说吧。”\n这无异于，为了追求100分的成绩，就不参加考试。“啥时候能考到100分，我再上考场”。\n如果说做事有秘诀，那很可能只有一个——先完成，再完美。先把事闭环，再来考虑是否要提升标准。不要把追求完美作为拖延的借口。\n第六种方法，时刻提醒自己。\n你可以默念：“我现在就完成它，必须在截止时间之前完成。而完成之前，我不会做任何其他事情。”\n告诉自己，“不是一会儿开始，就是现在，立刻、马上！”\n第七种方法，保持快节奏。\n慢节奏是一种比较舒服、慵懒的状态，如果是在度假，当然可以。但这不是做事的状态。\n快节奏，就是要提高效率，能用10分钟做完的事，绝不多花1分钟。\n当你能保持“快节奏”的状态，拖延症自然就消失了。\n拖延症的问题，本质上是责任心和责任感的问题。能清晰地感受到自己的责任，就很容易行动起来，拒绝拖延。\n归根结底，所谓的“事不拖”，其实就是做人有责任心、做事有责任感。\n二、话不多：说好该说的话 # **靠谱的人，话不多。**当然，这不是鼓励沉默、不要说话或少说话，而是说好该说的话。\n1. 不要一味解释\n首先，不要一味解释。\n在知乎上曾有一个问题：我们为什么讨厌爱解释的人？\n有个高赞的回答是：错了就是错了，但解释却是，不仅不承认自己有错，还要试图去掩饰。\n比如在上班时，总有人解释，迟到是因为堵车，但他们就是不承认自己晚起的事实。\n比如在开会时，总有人解释，手机响是忘了调震动，但他们就是不承认自己不守规矩。\n比如在发通知时，总有人解释，没有回复是因为没看到信息，但他们就是不承认自己不上心。\n靠谱的人，不是不犯错，而是敢承认、敢担当。\n在军队里，新兵们初到军营，学到的最重要的事情之一就是“不要解释”。结果就是这样，班长已经看到了，有什么好解释的？\n当然，军队有它的特殊性。但就一家公司来说，有“解释氛围”的公司恐怕很难在逆境下打硬仗。满脑子想着如何把结果解释好，哪有精力去想怎么把事干得更好呢？\n换个角度来看，如果你是某公司的老板，一名下属对于任何结果都总喜欢解释，你会认为他靠谱吗？\n靠谱的人，很少会把注意力都放在“如何解释”上。\n2. 直击重点不啰嗦\n其次，是直击重点不啰嗦。\n三言两语，往往抵得过长话连篇。比如，我们有时候会夸赞某个人说话一针见血。\n什么是一针见血？简短，但直击要害。\n举个例子。每家公司年终都要开工作汇报会，作为上级，看到那种滔滔不绝但听下来没什么重点的汇报，你会怎么评价？\n大概率不会是好的评价吧。甚至，着急的上级可能会直接打断。\n**啰嗦的人，带给别人的感觉，往往是没有想清楚。**如果是跟朋友聊天，包容性会很强。但如果是跟上级沟通，这会直接降低上级对你的评分。\n**大多数的表达问题，归根结底都是思考能力的问题。**没想明白，当然说不明白。每个人都需要锻炼自己的结构化思考能力，四个字——“论证类比”。\n论，就是结论先行。\n听你说话的人，是想听结论的。不要上来直接一大堆铺垫，实在想铺垫，就“倒叙”。\n所以，当你在表达一件事情的时候，一定要先说观点——有中心思想的主题句。\n有三个领域，结论先行做得特别好，我们可以跨界学习一下：\n第一个，媒体报纸的标题。正规媒体的标题一定是结论先行，人物、时间、地点、人物、事件全告诉你。\n第二个，政府工作报告。每段的开头第一句话一定是一个有中心思想的主题句，一定是一个结论。\n第三个，咨询行业。咨询公司的PPT标题有什么特点？所有标题都是结论先行。结构化思考的概念最早是由麦肯锡提出来的，它的 PPT 每一页的标题都是一句话，概括了这一页内容的中心思想。\n那如何找结论呢？\n**可以基于共性找结论。**比如这3句话：欢迎老客户来体验我们的新产品；引导客户购买相关互补产品；鼓励客户向朋友分享使用感受。共同点是什么？\n客户、产品。所以，结论就是“为了提高产品销量，继续做好对客户的二次营销”。\n**也可以基于目标去找结论。**目标是什么，就围绕什么来表达，来做结论。\n比如今年的目标是提升老顾客复购率，结论就要把复购率作为重点，而不是去谈又增长了多少。\n证，就是以上统下。\n这些都是你的理由，也可以支撑你的结论，但是说服不了老板。\n老板关心什么？比如，你是不是创造了超出当前薪酬的价值？你的岗位是不是核心岗位？你的能力符不符合市场上的薪酬水平？等等。\n所以，要从对方角度看。对方关心什么问题，基于这个问题的答案才能支撑结论。\n类，就是归类分组。\n我们都知道，相近的事分类就容易记。\n如果你跟老板汇报工作：“老板，跟您说件事，这件事包含以下几点，一二三四五六七八九十……”你觉得他能记住吗？\n人类大脑一次性接受信息的量是有限的。有两种说法，一种是7±2，一种是5±2。对于这两种，7是一个临界值，突破7以后对记忆开始造成负担。\n因此，不论是写报告还是口头表达，千万不要超过7条。\n那么，如何保证分类是清晰的，对方能认同呢？用MECE原则：相互独立，完全穷尽。\n比，就是逻辑递进。\n在思考和表达一件事情的时候，一定要有先后的逻辑顺序。而逻辑顺序是比较出来的。\n举个例子。一个小和尚跟他师傅说：“师傅，我念经的时候困了，可以抽根烟吗？”师傅准备揍他一顿。\n另外一个小和尚说：“师傅，我抽烟的时候可以念经吗？”师傅说：“好孩子，就应该这样。”\n这两个徒弟做了同样的事情，但是结果是相反的。为什么呢？\n顺序不一样。\n顺序不一样，结构就不一样；结构不一样，事物的性质就不一样。好比石墨跟钻石，这俩东西元素是一样的，但是价格差得特别多。\n一般情况下，我们有三种顺序。\n① 时间顺序\n彼此可能存在因果，可能不存在因果。比如常用的昨天、今天、明天。\n② 结构顺序\n从概念或者空间维度，将整体分为部分。如概念的不同部分、按照顺时针、按从上到下的顺序等。\n结构顺序也就是我们日常理解的要素。当我们看一个专家总结他的方法论的时候，最简单的是按照步骤去总结，比如这件事分成三个步骤、四个步骤。最难的是按照要素去总结，比如做战略的3C。\n③ 重要性顺序\n将事物按重要性或非重要分组，如首先、其次、再次。\n有结构化思考能力的人，能想清楚也能说明白，更容易成为靠谱的人。\n3. 少说多做不自夸\n然后，话不多还有一层意思，是少说多做，不夸夸其谈。\n比如，你可能也遇到过这样的情况：\n开会时，总有人喜欢在老板面前插话，见缝插针地吹嘘自己，好像公司的进步都是自己的功劳似的。\n报告时，总有人爱对你的工作指指点点，有时还要拿自己的业绩和你对比一下，好像他永远都比别人强一样。\n他们的种种表现无不给他人传达一种信息：我很厉害，我很靠谱。\n但是，真正的靠谱是做出来的，而非说出来的。\n把自己夸得太厉害，会提高别人对你的预期。如果你无法做到，那印象会大打折扣。\n真正靠谱的人都知道，行动永远比语言更有说服力。\n三、人不作：讲信用，讲底线，不敷衍，不自私 # 靠谱的人，不会作。\n我们常说，先做人再做事。人品是靠谱的关键一环。在工作上，一个人再有能力，但若人品有问题，也很难让人真正放心。\n人品好的人，讲信用。\n早年罗永浩创办英语培训学校时，极度缺钱。有位多年不联系的发小知道后，二话不说转给他300万。\n罗永浩问：“这笔钱不是小数目，况且我们这么多年不联系了，你凭什么相信我？”\n发小回答：“你还记不记得，小时候我们都喜欢翻录录像带，互相交换着看。那时候很多人耍滑头，把一部故事片分录成五盘，然后去跟别人换五盘录像带。渐渐地，大家都这么干了，唯独你没有。”\n发小说，细节见人品，我当时就觉得你这人靠谱，值得深交。\n讲信用的人，每个人都喜欢跟他相处。\n人品好的人，讲底线。\n雷军曾在一次内部会议上，跟员工们分享了一个关于面试的故事。\n当时，雷军面试了一个人，不管是从哪个方面来看，这个人都是接近完美的。\n他之前在一家供应商任职，只花了4年时间就把900万美金的生意做到了2亿美金。这个人甚至很自豪地表示，自己有能力将稻草卖出黄金价。\n但是雷军拒绝了这个人，理由是价值观不符。\n觉得能力强的人就一定靠谱，是人最大的错觉。一个人的底线，就是他人品的底色。人品坏了，能力再强成绩再好也没有保质期。\n底线，就是我们的生命线。守住底线，也就守住了人生。\n人品好的人，不敷衍。\n与靠谱的人相处，你会感到特别心安。因为你交代或请求他做一件事时，他如果答应下来，就会真正把这件事当成自己的事情看待。\n从目标的制定，过程中遇到的问题，甚至完成时间，他都会及时跟你反馈。\n如果没有完成，他甚至会感到不好意思，而不是像大多数人那样“嘴上说着已经尽力了，实际没出多少力，敷衍了事”。\n人品好的人，不自私。\n不自私，不是不考虑自己，而是把别人也放在心上。不要“单赢”，只要“双赢”和“多赢”。\n没有人是傻子，你有没有为他人考虑，他人一定可以看出来。\n那些自私自利，经常给别人添麻烦的人，能力再强，也不会给人留下靠谱的印象。\n总之，所谓的“人不作”，就是讲信用、讲底线、不敷衍、不自私。\n结语 # 靠谱，不容易做到。\n它意味着：\n事不拖，做人有责任心，做事有责任感； 话不多，不要一味解释，直击重点不啰嗦，少说多做不自夸； 人不作，讲信用，讲底线，不敷衍，不自私。 靠谱的人，总是很难得。但只有靠谱的人，无论遇到任何事，才都能靠得住。\n祝愿你身边的每个人，都是靠谱的人。\n","date":"11 April 2024","permalink":"/read/%E9%9D%A0%E8%B0%B1%E4%B8%89%E8%A6%81%E7%B4%A0/","section":"阅读","summary":"昨天，跟着导师去对接的公司开了项目会，导师跟对面公司的对接人说，你还需要学，你最好去查查靠谱三要素是啥？说者无心，听者有意。靠谱三要素是啥呢？真正靠谱的人，事不拖、话不多、人不作。","title":"真正靠谱的人，事不拖、话不多、人不作"},{"content":" ","date":"11 April 2024","permalink":"/read/","section":"阅读","summary":"一个人但凡有了读书的癖好，也就有了看世界的一种特别的眼光，甚至有了一个属于他（她）的丰富多彩的世界。\u0026ndash;周国平","title":"阅读"},{"content":"虽然很支持遥遥领先，甚至手机都已经换成了遥遥领先，但还是要吐槽下华为的笔试。\n他的笔试页面跟牛客很像，但是没有代码补全，做过一段时间IDE Editor的我可以理解，他只开了个monarch syntax来做语法高亮，剩下的language server都没跑起来，代码补全、光标事件都没有，就更不要提什么语法错误提示了。这是去华子实习，不给用代码补全的IDE吗？笔试的难度怎么体现在记忆类库方法上，而不是算法了呢？\n而且他的输出异常严格，属于是PAT风格，多一个空格就会报错，JAVA选手表示好难控制输出。\n总的来说，还是自己菜了。\n1.云服务计费 # 编写一个程序为某云服务计算客户话单，输入为某云服务的计费日志和各种计费因子的计费单价的列表，计费日志内容包含时间戳、客户标识、计费因子、计费时长4个字段。日志中如果同一客户同一计费因子在相同时间戳上报多次话单只能计费一次，选先上报的日志计费。计算每个客户的话单总费用。\n解答要求\n时间限制: C/C++ 1000ms,其他语言: 2000ms内存限制: C/C++ 256MB,其他语言: 512MB\n输入\n第1行表示计费日志的条数n，是一个正整数，范围是1\u0026lt;=n\u0026lt;=1000\n第2到n+1行表示云服务的计费日志，共4列，第1列表示时间戳(是一个数字字符串，长度为10) 、第2列表示客户标识(是一个字符串，长度为1-16)，第3列表示计费因子 (是一个字符串，长度为1-16，计费因子查不到时认为计费因子单价是0），第四列表示计费时长时长（范围为0-100，当计费时长不在范围内要认为是计费日志有问题，当成计费为0处理)，这4个字段使用迈号分隔\n第n+2行表示计费因子的数量m，m是一个正整数，范围是1\u0026lt;=m\u0026lt;=100\n第n+3到n+3+m行表示各种计费因子的计费单价的列表，该表有2列,第1列表示计费因子 (是一个字符串，长度为1-16)，第2列表示单价(是一个正整数，范围为1~100)，这2个字段使用逗号分\n输出\n每个客户的话单总费用,共2列，第1列表示客户名，第2列表示话单费用，2列用逗号分割，输出按客户标识字典序升序排序\n样例\n输入\n5 1627845600,client1,factorA,10 1627845605,client2,factorB,15 1627845610,ciient1,factorA,5 1627845610,client1,factorB,8 1627845620.client2,factorB,20 2 factorA,5 factorB,7 输出\nclient1,131 client2,245 思路与代码\n本题题目量特别多，属于是阅读理解了，理解题目就简单。\n通过stringstream对输入的字符串进行拆分；\n通过set进行判断是否存在相同的时间戳客户和计费因子；\n通过map进行存储不同的客户和计费因子；\n最后合并统计出各个用户的消费。\nimport java.util.*; public class Main { public static void main(String[] args) { Scanner sc = new Scanner(System.in); int n = Integer.parseInt(sc.nextLine()); String[] a = new String[5]; Map\u0026lt;String, Map\u0026lt;String, Integer\u0026gt;\u0026gt; h1 = new HashMap\u0026lt;\u0026gt;(); Set\u0026lt;String\u0026gt; ses = new HashSet\u0026lt;\u0026gt;(); while (n-- \u0026gt; 0) { String s = sc.nextLine(); String[] parts = s.split(\u0026#34;,\u0026#34;); if (parts.length \u0026lt; 4) continue; String ts = parts[0] + parts[1] + parts[2]; if (ses.contains(ts)) continue; ses.add(ts); int tt = Integer.parseInt(parts[3]); if (tt \u0026lt; 0 || tt \u0026gt; 100) continue; h1.putIfAbsent(parts[1], new HashMap\u0026lt;\u0026gt;()); h1.get(parts[1]).merge(parts[2], tt, Integer::sum); } int m = Integer.parseInt(sc.nextLine()); Map\u0026lt;String, Integer\u0026gt; h2 = new HashMap\u0026lt;\u0026gt;(); while (m-- \u0026gt; 0) { String s = sc.nextLine(); String[] parts = s.split(\u0026#34;,\u0026#34;); if (parts.length \u0026lt; 2) continue; h2.put(parts[0], Integer.parseInt(parts[1])); } Map\u0026lt;String, Integer\u0026gt; h3 = new HashMap\u0026lt;\u0026gt;(); for (Map.Entry\u0026lt;String, Map\u0026lt;String, Integer\u0026gt;\u0026gt; entry : h1.entrySet()) { String client = entry.getKey(); for (Map.Entry\u0026lt;String, Integer\u0026gt; subentry : entry.getValue().entrySet()) { String factor = subentry.getKey(); int value = subentry.getValue(); h3.merge(client, h2.getOrDefault(factor, 0) * value, Integer::sum); } } List\u0026lt;Map.Entry\u0026lt;String, Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(h3.entrySet()); res.sort(Map.Entry.comparingByKey()); for (Map.Entry\u0026lt;String, Integer\u0026gt; entry : res) { System.out.println(entry.getKey() + \u0026#34;,\u0026#34; + entry.getValue()); } sc.close(); } } 2.相似图片分类 # 小明想要处理一批图片，将相似的图片分类。他首先对图片的特征采样，得到图片之间的相似度，然后按照以下规则判断图片是否可以归为一类:\n相似度$\u0026gt;0$表示两张图片相似， 如果$A$和$B$相似，$B$和$C$相似，但$A$和$C$不相似。那么认为$A$和$C$间接相似，可以把$ABC$归为一类，但不计算$AC$的相似度 如果$A$和所有其他图片都不相似，则$A$自己归为一类，相似度为0.给定一个大小为$N\\times N$的矩阵M存储任意两张图片的相似度，$M[i][j]$ 即为第$i$个图片和第$j$个图片的相似度，请按照\u0026quot;从大到小”的顺序返回每个相似类中所有图片的相似度之和。 解答要求\n时间限制: C/C++ 1000ms,其他语言: 2000ms内存限制: C/C++ 256MB,其他语言: 512MB\n输入\n第一行一个数N，代表矩阵M中有N个图片，下面跟着N行，每行有N列数据，空格分隔(为了显示整弃，空格可能为多个) 代表N个图片之间的相似度。\n约束： 1.0\u0026lt;N\u0026lt;=900 2.0\u0026lt;=M[i][j]\u0026lt;=100,输入保证M[i][i] =0,M[i][j]=M[j][i] 输出\n每个相似类的相似度之和。格式为:一行数字，分隔符为1个空格。\n样例\n输入\n5 0 0 50 0 0 0 0 0 25 0 50 0 0 0 15 0 25 0 0 0 0 0 15 0 0 输出\n65 25 思路与代码\n考察并查集，将相似度大于0的节点合并，用num数组储存分数，每次合并的时候累加分数，最后对不同集合类的分数排序输出就行。\nimport java.util.*; public class Main { private static class UnionFind { private int[] parent; private int[] num; // To keep track of the sum of connected components public UnionFind(int N) { parent = new int[N]; num = new int[N]; for (int i = 0; i \u0026lt; N; i++) { parent[i] = i; num[i] = 0; // Initialize sum as 0 for each component } } private int find(int v) { if (v != parent[v]) { parent[v] = find(parent[v]); } return parent[v]; } public void union(int v1, int v2, int value) { int f1 = find(v1); int f2 = find(v2); if (f1 != f2) { parent[f1] = f2; num[f2] += value + num[f1]; } else { num[f2] += value; } } public int[] getSums() { HashSet\u0026lt;Integer\u0026gt; uniqueRoots = new HashSet\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; parent.length; i++) { int root = find(i); if (root == i) { uniqueRoots.add(root); } } int[] sums = new int[uniqueRoots.size()]; int idx = 0; for (int root : uniqueRoots) { sums[idx++] = num[root]; } return sums; } } public static void main(String[] args) { Scanner sc = new Scanner(System.in); int n = sc.nextInt(); UnionFind uf = new UnionFind(n); int[][] a = new int[n][n]; for (int i = 0; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; n; j++) { a[i][j] = sc.nextInt(); } } for (int i = 0; i \u0026lt; n; i++) { for (int j = i; j \u0026lt; n; j++) { if (a[i][j] \u0026gt; 0) { uf.union(i, j, a[i][j]); } } } int[] res = uf.getSums(); Arrays.sort(res); for (int i = res.length - 1; i \u0026gt;= 0; i--) { if (i \u0026lt; res.length - 1) { System.out.print(\u0026#34; \u0026#34;); } System.out.print(res[i]); } } } 3.网络保卫战 # 公有云的某个region内，N个网络节点组网情况可以使用一个n* n的矩阵matrix表示，在这个组网图中，matrix[i][j] = p 时，表示用户在编号为 i的节点访问编号为j的节点时，必须在 i节点上具有\u0026gt;=p 的权限等级(p=0时表示无法通过第i节点访问j节点)，如果用户成功访问了j节点，那么它在j节点上的权限等级调整为 P。\nexposed 为一个整数数组，表示暴露在公网上的网络节点的编号列表。某天扫描发现这批暴需在公网的节点存在被外部恶意攻击风险且该攻击会影响到可访问的其他节点，并可以持续传递进行攻击，被恶意攻击的节点从公网访问时，攻击者获得了ROOT 权限(权限等级为10，即最大值)。\n小李是一名网络安全工程师，为了在有限的时间内尽可能的减少故障带来的损失，需要立即将某个节点从公网\u0026quot;下线\u0026quot;。\n假设攻击结束时，被攻击过的节点数量为R，请帮小李计算出将哪个节点下线能使R尽可能小，如果答案有多个节点，返回索引最小的那个节点。\n请注意：从公网“下线”的节点，不会受到来自公网的攻击，但仍然可能被“可访问”的其他节点传递攻击。\n解答要求\n时间限制: C/C++ 5000ms,其他语言: 10000ms内存限制: C/C++ 128MB，其他语言: 256MB\n输入\n输入的第一行是网络节点数量N 后续的N行，每行N个数字v，以空格分割，形成一个N*N的矩阵，表示网络节点组网的矩阵。 最后一行，输入 exposed 数组，表示暴露在公网上的网络节点的编号列表，数组元素不会重复。 输入范围说明： 2\u0026lt;=n\u0026lt;=24 0\u0026lt;=v\u0026lt;=10 0\u0026lt;=exposed[i]\u0026lt;=n-1 输出\n输出在 exposed 数组中，计划\u0026quot;下线”的那个节点的编号。\n样例1\n输入\n4 1 0 0 0 0 1 2 0 0 1 1 4 0 0 3 1 1 3 输出\n3 思路与代码\n本题数据范围小， 直接模拟暴力枚举就行。遍历每个暴露的节点，dfs搜索遍历能走到多少节点， 选择能到达节点数最多的点就是要下线的答案。\n// We have imported the necessary tool classes. // If you need to import additional packages or classes, please import here. public class Main { public static void main(String[] args) { // please define the JAVA input here. For example: Scanner s = new Scanner(System.in); // please finish the function body here. // please define the JAVA output here. For example: System.out.println(s.nextInt()); Scanner in = new Scanner(System.in); int n = in.nextInt(); int[][] m = new int[n][n]; for(int i = 0; i \u0026lt; n; i++) { for(int j = 0; j \u0026lt; n; j++) { m[i][j] = in.nextInt(); } } in.nextLine(); String l = in.nextLine(); String[] tt = l.split(\u0026#34; \u0026#34;); //HashMap\u0026lt;Integer, Integer\u0026gt; aa = new HashMap\u0026lt;\u0026gt;(); int minn = 0; int ans = -1; for(String t : tt) { int k = Integer.valueOf(t); HashSet\u0026lt;Integer\u0026gt; s = new HashSet\u0026lt;\u0026gt;(); Queue\u0026lt;int[]\u0026gt; q = new LinkedList\u0026lt;\u0026gt;(); q.offer(new int[]{k, 10}); while(q.size()\u0026gt;0) { int[] node = q.poll(); //System.out.println(q.size()); for(int i=0;i\u0026lt;n;i++) { if(i==k) continue; if(m[node[0]][i]\u0026lt;=node[1] \u0026amp;\u0026amp; m[node[0]][i] \u0026gt;0 \u0026amp;\u0026amp; !s.contains(node[0])) { //s.add(k); //System.out.println(node[0] + \u0026#34; \u0026#34; +m[node[0]][i]); s.add(node[0]); q.offer(new int[]{node[0], m[node[0]][i]}); } } } //aa.put(k, s.size()); System.out.println(k+\u0026#34; \u0026#34;+s.size()); if(s.size() \u0026gt; minn) { minn = s.size(); ans = k; } } System.out.println(ans); } } 答案\nimport java.util.Scanner; public class Main { static final int N = 30; static int[][] a = new int[N][N]; static boolean[] st = new boolean[N]; static int n; public static int dfs(int id, int ra) { int num = 0; for (int i = 0; i \u0026lt; n; i++) { if (id == i || a[id][i] == 0) continue; if (ra \u0026gt;= a[id][i]) { if (st[i]) continue; st[i] = true; num += dfs(i, a[id][i]); st[i] = false; } } return num + 1; } public static void main(String[] args) { Scanner sc = new Scanner(System.in); n = sc.nextInt(); for (int i = 0; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; n; j++) { a[i][j] = sc.nextInt(); } } int res = -1, ans = 0; while (sc.hasNextInt()) { int x = sc.nextInt(); for (int i = 0; i \u0026lt; n; i++) { if (x == i) continue; st[x] = true; int t = dfs(x, 10); st[x] = false; if (t \u0026gt; ans) { ans = t; res = x; } } } System.out.println(res); sc.close(); } } ","date":"10 April 2024","permalink":"/posts/reviews/24%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0/%E5%8D%8E%E4%B8%BA/%E5%8D%8E%E4%B8%BA4.10%E7%AC%94%E8%AF%95/","section":"博客","summary":"华为笔试4.10","title":"华为笔试4.10"},{"content":"高并发指的是系统同时处理很多请求。\n高并发是一个结果导向的东西，例如，常见的高并发场景有：淘宝的双11、春运时的抢票、微博大V的热点新闻等，这些典型场景并不是陡然出世，而是随着业务发展的发展而逐渐出现。像2020年淘宝双11全球狂欢季，订单创建峰值达到了惊人的58.3万笔/秒，4年前的2016年，这个数字大概是四分之一。\n高并发的业务场景出现了，随之而来的就是要支持这个高并发业务场景的架构——技术要为业务服务，业务倒逼技术发展。高并发的架构也不是某个天才冥思苦想或者灵机一动，这个过程是随着业务的发展而演进。用一个比喻，先有了秋名山，才到了老司机。\n那到底多大并发才算高并发呢？\n这个本身是没有具体标准的事情，只看数据是不行的，要结合具体的场景。不能说10W QPS的秒杀是高并发，而1千 QPS的信息流就不是高并发。信息流场景涉及复杂的推荐模型和各种人工策略，它的业务逻辑可能比秒杀场景复杂10倍不止。业务场景不一样，执行复杂度不一样，单看并发量也没有意义。\n高并发目标 # 宏观目标\n高并发绝不意味着只追求高性能。从宏观角度看，高并发系统设计的目标有三个：高性能、高可用，以及高可扩展。就是所谓的“三高”，三高不是孤立的，而是相互支撑的。\n高性能：性能体现了系统的并行处理能力，在有限的硬件投入下，提高性能意味着节省成本。同时，性能也反映了用户体验，响应时间分别是100毫秒和1秒，给用户的感受是完全不同的。 高可用：表示系统可以正常服务的时间。一个全年不停机、无故障；另一个隔三差五出线上事故、宕机，用户肯定选择前者。另外，如果系统只能做到90%可用，也会大大拖累业务。 高扩展：表示系统的扩展能力，流量高峰时能否在短时间内完成扩容，更平稳地承接峰值流量，比如双11活动、明星离婚等热点事件。 这3个目标是需要通盘考虑的，因为它们互相关联、甚至也会相互影响。\n比如说：考虑系统的扩展能力，你需要将服务设计成无状态的，这种集群设计保证了高扩展性，其实也间接提升了系统的性能和可用性。\n再比如说：为了保证可用性，通常会对服务接口进行超时设置，以防大量线程阻塞在慢请求上造成系统雪崩，那超时时间设置成多少合理呢？一般，我们会参考依赖服务的性能表现进行设置。\n性能指标\n性能指标通过性能指标可以度量目前存在的性能问题，也是高并发主要关注的指标，性能和流量方面常用的一些指标有\nQPS/TPS/HPS：QPS是每秒查询数，TPS是每秒事务数，HPS是每秒HTTP请求数。最常用的指标是QPS。需要注意的是，并发数和QPS是不同的概念，并发数是指系统同时能处理的请求数量，反应了系统的负载能力。$并发数 = QPS \\times 平均响应时间$ 响应时间：从请求发出到收到响应花费的时间，例如一个系统处理一个HTTP请求需要100ms，这个100ms就是系统的响应时间。 平均响应时间：最常用，但是缺陷很明显，对于慢请求不敏感。比如 1 万次请求，其中 9900 次是 1ms，100 次是 100ms，则平均响应时间为 1.99ms，虽然平均耗时仅增加了 0.99ms，但是 1%请求的响应时间已经增加了 100 倍。 TP90、TP99 等分位值：将响应时间按照从小到大排序，TP90 表示排在第 90 分位的响应时间， 分位值越大，对慢请求越敏感。 RPS（吞吐量）：单位时间内处理的请求量，通常由QPS和并发数决定。 通常，设定性能目标时会兼顾吞吐量和响应时间，比如这样表述：在每秒 1 万次请求下，AVG 控制在 50ms 以下，TP99 控制在 100ms 以下。对于高并发系统，AVG 和 TP 分位值必须同时要考虑。\n另外，从用户体验角度来看，200 毫秒被认为是第一个分界点，用户感觉不到延迟，1 秒是第二个分界点，用户能感受到延迟，但是可以接受。\n因此，对于一个健康的高并发系统，TP99 应该控制在 200 毫秒以内，TP999 或者 TP9999 应该控制在 1 秒以内。\nPV：综合浏览量，即页面浏览量或者点击量，一个访客在24小时内访问的页面数量。\nUV：独立访客 ，即一定时间范围内相同访客多次访问网站，只计算为一个独立的访客。\n带宽： 计算带宽大小需要关注两个指标，峰值流量和页面的平均大小。\n日网站带宽可以使用下面的公式来粗略计算：$日网站带宽 = pv \\div 统计时间(换算到秒) \\times 平均页面大小(单位kB) \\times 8$\n峰值一般是平均值的倍数\nQPS不等于并发连接数，QPS是每秒HTTP请求数量，并发连接数是系统同时处理的请求数量: $峰值每秒请求数(QPS) = (总PV数 \\times 80%) \\div (6小时秒数 \\times 20%)$\n可用性指标\n高可用性是指系统具有较高的无故障运行能力，$可用性 = 平均故障时间 \\div 系统总运行时间$，一般使用几个 9 来描述系统的可用性。\n对于大多数系统。2个9是基本可用（如果达不到开发和运维可能就要被祭天了），3个9是较高可用，4个9是具有自动恢复能力的高可用。要想达到3个9和4个9很困难，可用性影响因素非常多，很难控制，需要过硬的技术、大量的设备资金投入，工程师要具备责任心，甚至还要点运气。\n可扩展性指标\n面对突发流量，不可能临时改造架构，最快的方式就是增加机器来线性提高系统的处理能力。\n对于业务集群或者基础组件来说，$扩展性 = 性能提升比例 \\div 机器增加比例$，理想的扩展能力是：资源增加几倍，性能提升几倍。通常来说，扩展能力要维持在 70% 以上。\n但是从高并发系统的整体架构角度来看，扩展的目标不仅仅是把服务设计成无状态就行了，因为当流量增加 10 倍，业务服务可以快速扩容 10 倍，但是数据库可能就成为了新的瓶颈。\n像 MySQL 这种有状态的存储服务通常是扩展的技术难点，如果架构上没提前做好规划（垂直和水平拆分），就会涉及到大量数据的迁移。\n我们需要站在整体架构的角度，而不仅仅是业务服务器的角度来考虑系统的扩展性 。所以说，数据库、缓存、依赖的第三方、负载均衡、交换机带宽等等都是系统扩展时需要考虑的因素。我们要知 道系统并发到了某一个量级之后，哪一个因素会成为我们的瓶颈点，从而针对性地进行扩展。\n高并发架构演进 # 谁不是生下来就是老司机，架构也不是架起来就支持高并发。我们来看一个经典的架构演进的例子——淘宝，真实诠释了“好的架构是进化来的，不是设计来的”。\n以下是来自《淘宝技术这十年》描述的淘宝2003—2012年的架构演进。\n个人网站\n初代淘宝的团队人员只有十来个，而且面临千载难逢的商业机会，所以要求上线的时间越快越好（实际用了不到一个月），那么淘宝的这些牛人是怎么做到的呢？\n——买一个。\n初代淘宝买了这样一个架构的网站： LAMP（Linux+Apache+MySQL+PHP）。整个系统的架构如下：\n最后开发的网站是这样的：\n由于商品搜索比较占用数据库资源，后来还引入了阿里巴巴的搜索引擎iSearch。\nOracle/支付宝/旺旺\n淘宝飞速发展，流量和交易量迅速提升，给技术带来了新的问题——MySQL抗不住了。怎么办？要搞点事情吗？没有，淘宝买了Oracle数据库，当然这个也考虑到团队里有Oracle大牛的原因。\n替换了数据库之后的架构：\n比较有意思的，当时由于买不起商用的连接池，所以用了一个开源的连接池代理服务SQLRelay，这个代理服务经常会死锁，怎么解决呢？人肉运维，工程师24小时待命，出现问题赶紧重启SQL Relay服务😂😂\n后来为了优化存储，又买了NAS（Network Attached Storage，网络附属存储），NetApp 的 NAS 存储作为了数据库的存储设备，加上 Oracle RAC（Real Application Clusters，实时应用集群）来实现负载均衡。\nJava 时代 1.0\n2004年，淘宝已经运行了一年的时间，上面提到的SQLRelay的问题解决不了，数据库必须要用Oracle，所以决定更换开发语言。\n在不拖慢现有业务发展的情况下，平滑更换整体的架构，对当时的淘宝仍然是个有挑战性的事情。所以怎么办？淘宝的解决方案是请了Sun公司的大佬。\n当时，由于struts1.x存在很多问题，所以淘宝自研了一套MVC框架。Sun当时在推EJB，所以这套架构里也引入了EJB。\nJava 时代 2.0\n在之前，淘宝的架构的架构主要思路还是“买”，随着业务的发展，到了2005 年，“买”已经很难解决问题了，需要对整个架构进行调整和优化，需要综合考虑容量、性能、成本的问题。\n在Java时代2.0，主要做了对数据分库、放弃EJB、引入Spring、加入缓存、加入CDN等。\nJava 时代 3.0\nJava时代3.0的最大特点就是淘宝开始从商用转为“自研”，开始真正创造自己的核心技术，例如缓存存储引擎Tair，分布式存储系统TFS。搜索引擎iSearch也进行了升级。引入了自研技术的淘宝架构：\n分布式时代 1.0\n到了2008年的时候，淘宝的业务进一步发展。\n整个主站系统的容量已经到了瓶颈，商品数在1亿个以上，PV在2.5亿个以上，会员数超过了 5000万个。这时Oracle的连接池数量都不够用了，数据库的容量到了极限，即使上层系统加机器也无法继续扩容，我们只有把底层的基础服务继续拆分，从底层开始扩容，上层才能扩展，这才能容纳以后三五年的增长。\n淘宝开始对业务模块逐步拆分和服务化改造。例如拆分出了商品中心、商品中心等等。同时引入了一些自研的中间件，如分布式数据库中间件，分布式消息中间件等等。\n对淘宝的技术感兴趣的具体可以见《淘宝技术这十年》，上图是根据 七年磨一剑，独家揭秘淘宝技术发展历程和架构经验 绘制的。\n转眼离2012又过了十二年，这十二年，阿里巴巴进入极盛时代，又经历拆分重组，但是阿里的技术可谓是风起云涌，才人辈出。粒度更细的微服务、隔离差距的容器化技术、快速伸缩的云平台技术…… 如果《淘宝技术这十年》的作者能再写一个十年，一定也是非常精彩。\n根据 阿里技术专家：日活5亿的淘宝技术发展历程和架构经验分享！18页ppt详解 接下来的淘宝服务化开始逐渐演进到云平台架构，由于资料实在难找，而且这时候以淘宝的体量，内部的架构复杂度足以写一本书了。所以接下来的架构演进参考 服务端高并发分布式架构演进之路，是一个牛人以淘宝为模拟对象进行的架构演进，虽然不是淘宝真正的架构技术演进，但也很值得借鉴。\n在这里我们略过了微服务架构——分布式时代2.0，微服务本身是更细粒度、更轻量级的服务化，这里插入一个关于微服务很有意思的说法——马丁老哥老被人说设计的东西不符合面向服务的概念，于是他就自己发明创造了一个灵活的微服务理论，以后再有人说：马老师，你又不遵循微服务架构设计的原则了。嗯，你说哪一点不符合，我立马去改微服务的理论。\n容器化时代\n前最流行的容器化技术是Docker，最流行的容器管理服务是Kubernetes(K8S)，应用/服务可以打包为Docker镜像，通过K8S来动态分发和部署镜像。Docker镜像可理解为一个能运行你的应用/服务的最小的操作系统，里面放着应用/服务的运行代码，运行环境根据实际的需要设置好。把整个“操作系统”打包为一个镜像后，就可以分发到需要部署相关服务的机器上，直接启动Docker镜像就可以把服务起起来，使服务的部署和运维变得简单。\n云平台时代\n在服务化的时候，淘宝已经演进到了云平台架构。\n所谓的云平台，就是把海量机器资源，通过统一的资源管理，抽象为一个资源整体，在之上可按需动态申请硬件资源（如CPU、内存、网络等），并且之上提供通用的操作系统，提供常用的技术组件（如Hadoop技术栈，MPP数据库等）供用户使用，甚至提供开发好的应用，用户不需要关系应用内部使用了什么技术，就能够解决需求（如音视频转码服务、邮件服务、个人博客等）。\n简单总结一下：高并发的架构某种程度上是逼出来的，一般人谁能想到淘宝当年抛弃php是因为解决不了数据库连接池的问题。架构演进就像是西湖的水——西湖的水，工程师的泪，说起来容易，里面究竟灭了多少火，填了多少坑。我们外人看到的平湖秋波，里面水很深🐶。\n高并发架构实现 # 想让系统抗住更多的并发，主要就是两个方向：\n纵向扩展： 提升单机的硬件性能：通过增加内存、 CPU核数、存储容量、或者将磁盘 升级成SSD等堆硬件的方式来提升 提升单机的软件性能：使用缓存减少IO次数，使用并发或者异步的方式增加吞吐量。 横向扩展：单机性能总会存在极限，所以最终还需要引入横向扩展，通过集群部署以进一步提高并发处理能力。 做好分层架构：这是横向扩展的前提，因为高并发系统往往业务复杂，通过分层处理可以简化复杂问题，更容易做到横向扩展。 各层进行水平扩展：无状态水平扩容，有状态做分片路由。业务集群通常能设计成无状态的，而数据库和缓存往往是有状态的，因此需要设计分区键做好存储分片，当然也可以通过主从同步、读写分离的方案提升读性能。 用一个比喻，你要去打十个大汉，你大概是打不过的，最好的结果就是他们打不倒你——吊起来打。所以这时候就得想办法了。第一个办法就是努力锻炼，然后全副武装，也许还有点希望，这就是纵向扩展；第二个办法，不行，你一看对面人多，你就叫了十九个兄弟，然后你们二十个打他们十个，唉，这下看上去能打的过了，这就是横向扩展；还有第三个不常用的办法，你找个门把住，每次就放一个大汉进来，打倒一个再放下一个，这个就是削峰限流的做法。\n我们看一下一个大概的支持三高的典型架构：\n接下来，我们从上往下，看一下，各层的一些关键技术。\n网络层 # 多机器 # 堆机器不是万能的，不堆机器是万万不能的。\n我们努力地升级改造架构，最后让我们提供的服务能够快速横向扩展。横向扩展的基础同样是要有一定数量的、一定性能的机器。\n还是上面哪个比喻，你要打十个大汉，等你努力练成了叶师傅，你突然发现对面的孩子都长大了，人数×2，这时候你还是得叫兄弟。\n一般大厂在全国各地都有机房，可能光北京就有两个，把不同地方的请求分到不同的机房，再分到不同的集群，再分到不同的机器，这么一匀，就在服务能扛的范畴之内了。我们大概来看一下，怎么估算所需机器的数量。\n通过QPS和PV计算部署服务器的台数\n单台服务器每天PV计算：\n$$ 公式1：每天总PV = QPS \\times 3600 \\times 6 公式2：每天总PV = QPS \\times 3600 \\times 8 $$\n服务器计算：\n$$ 服务器数量 = ceil( 每天总PV \\div 单台服务器每天总PV ) $$\n峰值QPS和机器计算公式\n原理：每天80%的访问集中在20%的时间里，这20%时间叫做峰值时间 公式：$( 总PV数 \\times 80% ) \\div ( 每天秒数 \\times 20% ) = 峰值时间每秒请求数(QPS)$ 机器：$峰值时间每秒QPS \\div 单台机器的QPS = 需要的机器$。 一般有大流量业务的公司都实现了多机房，包括同城多机房、跨城多机房、跨国多机房等。为了保证可用性，财大气粗的公司会预备大量的冗余，一般会保证机器数是计算峰值所需机器数的两倍。需要节约成本的，也可以考虑当前流行的云平台，之前热点事件的时候，微博就从阿里云租了不少云服务器。\nDNS # DNS是请求分发的第一个关口，实现的是地理级别的均衡。dns-server对一个域名配置了多个解析ip，每次DNS解析请求来访问dns-server。通常会返回离用户距离比较近的ip，用户再去访问ip。例如，北京的用户访问北京的机房，南京的用户访问南京的资源。\n一般不会使用DNS来做机器级别的负载均衡，因为造不起，IP资源实在太宝贵了，例如百度搜索可能需要数万台机器，不可能给每个机器都配置公网IP。一般只会有有限的公网IP的节点，然后再在这些节点上做机器级别的负载均衡，这样各个机房的机器只需要配置局域网IP就行了。\nDNS负载均衡的优点是通用（全球通用）、成本低（申请域名，注册DNS即可）。\n缺点也比较明显，主要体现在：\nDNS 缓存的时间比较长，即使将某台业务机器从 DNS 服务器上删除，由于缓存的原因，还是有很多用户会继续访问已经被删除的机器。 DNS 不够灵活。DNS 不能感知后端服务器的状态，只能根据配置策略进行负载均衡，无法做到更加灵活的负载均衡策略。比如说某台机器的配置比其他机器要好很多，理论上来说应该多分配一些请求给它，但 DNS 无法做到这一点。 所以对于时延和故障敏感的业务，有实力的公司可能会尝试实现HTTP-DNS的功能，即使用HTTP 协议实现一个私有的 DNS 系统。HTTP-DNS 主要应用在通过 App 提供服务的业务上，因为在 App 端可以实现灵活的服务器访问策略，如果是 Web 业务，实现起来就比较麻烦一些，因为 URL 的解析是由浏览器来完成的，只有 Javascript 的访问可以像 App 那样实现比较灵活的控制。\nCDN # CDN是为了解决用户网络访问时的“最后一公里”效应，本质是一种“以空间换时间”的加速策略，即将内容缓存在离用户最近的地方，用户访问的是缓存的内容，而不是站点实时访问的内容。\n由于CDN部署在网络运营商的机房，这些运营商又是终端用户的网络提供商，因此用户请求路由的第一跳就到达了CDN服务器，当CDN中存在浏览器请求的资源时，从CDN直接返回给浏览器，最短路径返回响应，加快用户访问速度。\n下面是简单的CDN请求流程示意图：\nCDN能够缓存的一般是静态资源，如图片、文件、CSS、Script脚本、静态网页等，但是这些文件访问频度很高，将其缓存在CDN可极大改善网页的打开速度。\n反向代理层 # 我们把这一层叫反向代理层，也可以叫接入层、或者负载层。这一层是流量的入口，是系统抗并发很关键的一层。\n还是那个比喻，还是你打十个大汉，这次你叫了十九个兄弟，理想的情况是你们两个打对面一个，但是你由于太激动，冲在了最前面，结果瞬间被十个大汉暴打……\n反向代理会对流量进行分发，保证最终落到每个服务上的流量是服务能扛的范围之内。\nNginx、LVS、F5\nDNS 用于实现地理级别的负载均衡，而 Nginx、 LVS、 F5 用于同一地点内机器级别的负载均衡。其中 Nginx 是软件的 7 层负载均衡，LVS 是内核的 4 层负载均衡，F5 是硬件的 4 层负载均衡。\n软件和硬件的区别就在于性能，硬件远远高于软件，Ngxin 的性能是万级，一般的 Linux 服务器上装个 Nginx 大概能到 5 万 / 秒；LVS 的性能是十万级，据说可达到 80万 / 秒；F5 性能是百万级，从 200 万 / 秒到 800 万 / 秒都有。\n硬件虽然性能高，但是单台硬件的成本也很高，一台最便宜的 F5 都是几十万，但是如果按照同等请求量级来计算成本的话，实际上硬件负载均衡设备可能会更便宜，例如假设每秒处理 100 万请求，用一台 F5 就够了，但用 Nginx, 可能要 20 台，这样折算下来用 F5 的成本反而低。因此通常情况下，如果性能要求不高，可以用软件负载均衡；如果性能要求很髙，推荐用硬件负载均衡。\n4 层和 7 层的区别就在于协议和灵活性。Nginx 支持 HTTP、 E-mail 协议，而 LVS 和 F5 是 4层负载均衡，和协议无关，几乎所有应用都可以做，例如聊天、数据库等。目前很多云服务商都已经提供了负载均衡的产品，例如阿里云的 SLB、UCIoud 的 ULB 等，中小公司直接购买即可。\n对于开发而言，一般只需要关注到Nginx这一层面就行了。\n负载均衡典型架构 # 像上面提到的负载均衡机制，在使用中，可以组合使用。\nDNS负载均衡用于实现地理级别的负载均衡， 硬件件负载均衡用于实现集群级别的负载均衡； 软件负载均衡用于实现机器级别的负载均衡。 整个系统的负载均衡分为三层。\n地理级别负载均衡：www.xxx.com 部署在北京、广州、上海三个机房，当用户访问时，DNS 会根据用户的地理位置来决定返回哪个机房的 IP，图中返回了广州机房的 IP 地址，这样用户就访问到广州机房了。 集群级别负载均衡：广州机房的负载均衡用的是 F5 设备，F5 收到用户请求后，进行集群级别的负载均衡，将用户请求发给 3 个本地集群中的一个，我们假设 F5 将用户请求发给了 “广州集群 2” 。 机器级别的负载均衡：广州集群 2 的负载均衡用的是 Nginx, Nginx 收到用户请求后，将用户请求发送给集群里面的某台服务器，服务器处理用户的业务请求并返回业务响应。 Nginx负载均衡 # 我们主要关心是Nginx这一层的负载，通常LVS 和 F5这两层都是由网络运维工程师管控。\n对于负载均衡我们主要关心的几个方面如下：\n上游服务器配置：使用 upstream server配置上游服务器 负载均衡算法：配置多个上游服务器时的负载均衡机制。 失败重试机制：配置当超时或上游服务器不存活时，是否需要重试其他上游服务器。 服务器心跳检查：上游服务器的健康检查/心跳检查。 upstream server中文直接翻译是上游服务器，意思就是负载均衡服务器设置，就是被nginx代理最后真实访问的服务器。\n负载均衡算法\n负载均衡算法数量较多，Nginx主要支持以下几种负载均衡算法：\n轮询（默认）：每个请求按时间顺序逐一分配到不同的后端服务，如果后端某台服务器死机，自动剔除故障系统，使用户访问不受影响。 weight（轮询权值）：weight的值越大分配到的访问概率越高，主要用于后端每台服务器性能不均衡的情况下。或者仅仅为在主从的情况下设置不同的权值，达到合理有效的地利用主机资源。 ip_hash：每个请求按访问IP的哈希结果分配，使来自同一个IP的访客固定访问一台后端服务器，并且可以有效解决动态网页存在的session共享问题。 fair：比 weight、ip_hash更加智能的负载均衡算法，fair算法可以根据页面大小和加载时间长短智能地进行负载均衡，也就是根据后端服务器的响应时间 来分配请求，响应时间短的优先分配。Nginx本身不支持fair，如果需要这种调度算法，则必须安装upstream_fair模块。 url_hash：按访问的URL的哈希结果来分配请求，使每个URL定向到一台后端服务器，可以进一步提高后端缓存服务器的效率。Nginx本身不支持url_hash，如果需要这种调度算法，则必须安装Nginx的hash软件包。 失败重试\nNginx关于失败重试主要有两部分配置，upstream server 和 proxy_pass。\n通过配置上游服务器的 max_fails和 fail_timeout，来指定每个上游服务器，当fail_timeout时间内失败了max_fail次请求，则认为该上游服务器不可用/不存活，然后将会摘掉该上游服务器，fail_timeout时间后会再次将该服务器加入到存活上游服务器列表进行重试。\n健康检查\nNginx 对上游服务器的健康检查默认采用的是惰性策略，Nginx 商业版提供了healthcheck 进行主动健康检查。当然也可以集成 nginx_upstream_check_module 模块来进行主动健康检查。\nnginx_upstream_check_module 支持 TCP 心跳和 HTTP 心跳来实现健康检查。\n流量控制\n流量分发\n流量分发就不多说了，上面已经讲了，是接入层的基本功能。\n流量切换\n我听朋友说过一个有意思的事情，他们公司将流量从一个机房切到另一个机房，结果翻车，所有工程师运维平台一片飘红，全公司集体围观，运维团队就很丢面子。\n流量切换就是在某些情况下，比如机房故障、光纤被挖断、服务器故障故障情况，或者灰度发布、A/B等运维测试场景，需要将流量切到不同的机房、服务器等等。\n就像我们上面提到的负载均衡典型架构，不同层级的负载负责切换不同层级的流量。\nDNS：切换机房入口。 HttpDNS：主要 APP 场景下，在客户端分配好流量入口，绕过运营商 LocalDNS并实现更精准流量调度。 LVS/HaProxy：切换故障的 Nginx 接入层。 Nginx：切换故障的应用层。 另外，有些应用为了更方便切换，还可以在 Nginx 接入层做切换，通过 Nginx 进行一些流量切换，而没有通过如 LVS/HaProxy 做切换。\n限流\n限流是保证系统可用的一个重要手段，防止超负荷的流量直接打在服务上，限流算法主要有令牌桶、漏桶。\n可以在很多层面做限流，例如服务层网关限流、消息队列限流、Redis限流，这些主要是业务上的限流。\n这里我们主要讨论的是接入层的限流，直接在流量入口上限流。\n对于 Nginx接入层限流可以使用 Nginx自带的两个模块：连接数限流模块 ngx_http_limit_conn_module和漏桶算法实现的请求限流模块 ngx_http_limit_req_module\n还可以使用 OpenResty 提供的 Lua限流模块 ua-resty-limit-traffic应对更复杂的限流场景。\nlimmit_conn用来对某个 key 对应的总的网络连接数进行限流，可以按照如 IP、域名维度进行限流。limit_req用来对某个 key对应的请求的平均速率进行限流,有两种用法：平滑模式(delay) 和允许突发模式(nodelay)。\n流量过滤\n很多时候，一个网站有很多流量是爬虫流量，或者直接是恶意的流量。\n可以在接入层，对请求的参数进行校验，如果参数校验不合法，则直接拒绝请求，或者把请求打到专门用来处理非法请求的服务。\n最简单的是使用Nginx，实际场景可能会使用OpenResty，对爬虫 user-agent 过滤和一些恶意IP (通过统计 IP 访问量来配置阈值)，将它们分流到固定分组，这种情况会存在一定程度的误杀，因为公司的公网 IP 一般情况下是同一个，大家使用同一个公网出口 IP 访问网站，因此，可以考虑 IP+Cookie 的方式，在用户浏览器种植标识用户身份的唯一 Cookie。访问服务前先种植 Cookie, 访问服务时验证该 Cookie, 如果没有或者不正确，则可以考虑分流到固定分组，或者提示输入验证码后访问。\n降级\n降级也是保证高可用的一把利剑，降级的思路是“弃车保帅”，在眼看着不能保证全局可用的情况下，抛弃或者限制一些不重要的服务。\n降级一般分为多个层级，例如在应用层进行降级，通过配置中心设置降级的阈值，一旦达到阈值，根据不同的降级策略进行降级。\n也可以把降级开关前置到接入层，在接入层配置功能降级开发，然后根据情况行自动/人工降级。后端应用服务出问题时，通过接入层降级，可以避免无谓的流量再打到后端服务，从而给应用服务有足够的时间恢复服务。\nWeb层 # 经过一系列的负载均衡，用户终于请求到了web层的服务。web服务开发完成，经过部署，运行在web服务器中给用户提供服务。\n集群 # 一般会根据业务模块，来划分不同的服务，一个服务部署多个实例组成集群。\n为了隔离故障，可以再将集群进行分组，这样一个分组出现问题，也不会影响其它分组。像比较常问的秒杀，通常会将秒杀的服务集群和普通的服务集群进行隔离。\n能做到集群化部署的三个要点是无状态、拆分、服务化。\n无状态：设计的应用是无状态的，那么应用比较容易进行水平扩展。 拆分：设计初期可以不用拆分，但是后期访问量大的时候，就可以考虑按功能拆分系统。拆分的维度也比较灵活，根据实际情况来选择，例如根据系统维度、功能维度、读写维度、AOP 维度、模块维度等等。 服务化：拆分更多的是设计，服务化是落地，服务化一般都得服务治理的问题。除了最基本的远程调用，还得考虑负载均衡、服务发现、服务隔离、服务限流、服务访问黑白名单等。甚至还有细节需要考虑，如超时时间、重试机制、服务路由、故障补偿等。 Web服务器 # 独立开发一个成熟的 Web 服务器，成本非常高，况且业界又有那么多成熟的开源 Web 服务器，所以互联网行业基本上都是 \u0026ldquo;拿来主义\u0026rdquo; ，挑选一个流行的开源服务器即可。大一点的公司，可能会在开源服务器的基础上，结合自己的业务特点做二次开发，例如淘宝的 Tengine,但一般公司基本上只需要将开源服务器摸透，优化一下参数，调整一下配置就差不多了。\n服务器的选择主要和开发语言相关，例如，Java 的有 Tomcat、JBoss、Resin 等,PHP/Python 的用 Nginx。\nWeb服务器的性能之类的一般不会成为瓶颈，例如Java最流行的Web服务器Tomcat默认配置的最大请求数是 150，但是没有关系，集群部署就行了。\n容器 # 容器是最近几年才开始火起来的，其中以 Docker 为代表，在 BAT 级别的公司已经有较多的应用。\n容器化可以说给运维带来了革命性的变化。Docker 启动快，几乎不占资源，随时启动和停止，基于Docker 打造自动化运维、智能化运维逐渐成为主流方式。\n容器化技术也天生适合当前流行的微服务，容器将微服务进程和应用程序隔离到更小的实例里，使用更少的资源，更快捷地部署。结合容器编排技术，可以更方便快速地搭建服务高可用集群。\n服务层 # 开发框架 # 一般，互联网公司都会指定一个大的技术方向，然后使用统一的开发框架。例如，Java 相关的开发框架 SSH、SpringBoot, Ruby 的 Ruby on Rails, PHP 的 ThinkPHP, Python 的Django 等。\n框架的选择，有一个总的原则：优选成熟的框架，避免盲目追逐新技术!\n对于一般的螺丝工而言，所做的主要工作都是在这个开发框架之下。对于开发语言和框架的使用，一定要充分了解和利用语言和框架的特性。\n以Java为例，在作者的开发中，涉及到一个加密解密的服务调用，服务提供方利用了JNI的技术——简单说就是C语言编写代码，提供api供Java调用，弥补了Java相对没那么底层的劣势，大大提高了运算的速度。\n在服务开发这个日常工作的层面，可以做到这些事情来提高性能：\n并发处理，通过多线程将串行逻辑并行化。 减少IO次数，比如数据库和缓存的批量读写、RPC的批量接口支持、或者通过冗余数据的方式干掉RPC调用。 减少IO时的数据包大小，包括采用轻量级的通信协议、合适的数据结构、去掉接口中的多余字段、减少缓存key的大小、压缩缓存value等。 程序逻辑优化，比如将大概率阻断执行流程的判断逻辑前置、For循环的计算逻辑优化，或者采用更高效的算法 各种池化技术的使用和池大小的设置，包括HTTP请求池、线程池（考虑CPU密集型还是IO密集型设置核心参数）、数据库和Redis连接池等。 JVM优化，包括新生代和老年代的大小、GC算法的选择等，尽可能减少GC频率和耗时。 锁选择，读多写少的场景用乐观锁，或者考虑通过分段锁的方式减少锁冲突。 可以通过这些事情来提高可用性：设置合适的超时时间、重试次数及机制，必要时要及时降级，返回兜底数据等，防止把服务提方供打崩 防重设计：通过防重key、防重表等方式实现防重 幂等设计：在接口层面实现幂等设计 服务中心 # 当系统数量不多的时候，系统间的调用一般都是直接通过配置文件记录在各系统内部的，但当系统数量多了以后，这种方式就存在问题了。\n比如说总共有 10 个系统依赖 A 系统的 X 接口，A 系统实现了一个新接口 Y, 能够更好地提供原有 X 接口的功能，如果要让已有的 10 个系统都切换到 Y 接口，则这 10 个系统的几十上百台器的配置都要修改，然后重启，可想而知这个效率是很低的。\n服务中心的实现主要采用服务名字系统。\n服务务名字系统 (Service Name System) 看到这个翻译，相信你会立刻联想到 DNS, 即 Domain Name System。没错，两者的性质是基本类似的。\nDNS 的作用将域名解析为 IP 地址，主要原因是我们记不住太多的数字 IP, 域名就容易记住。服务名字系统是为了将 Service 名称解析为 \u0026ldquo;host + port + 接口名称\u0026rdquo; ，但是和 DNS一样，真正发起请求的还是请求方。\n在微服务的架构下，实现这个功能的称之为注册中心，例如在Java语言体系下，开源的注册中心有Nacos、Ecuraka等。\n配置中心 # 配置中心就是集中管理各个服务的配置。\n在服务不多的时候，各个服务各自管理自己的配置，没有问题，但是当服务成百上千，再各行其政，就是一个比较头疼的事。\n所以将配置中心抽象成公共的组件，集中配置多个系统，操作效率高。\n在微服务架构体系下，配置中心的开源方案有SpringCloud的SpringCloud Config、阿里的Nacos等。\n服务框架 # 服务拆分最直接的影响就是本地调用的服务变成了远程调用，服务消费者A需要通过注册中心去查询服务提供者B的地址，然后发起调用，这个看似简单的过程就可能会遇到下面几种情况，比如：\n注册中心宕机； 服务提供者B有节点宕机； 服务消费者A和注册中心之间的网络不通； 服务提供者B和注册中心之间的网络不通； 服务消费者A和服务提供者B之间的网络不通； 服务提供者B有些节点性能变慢； 服务提供者B短时间内出现问题。 怎么去保证服务消费者成功调用服务生产者？这就是服务治理框架要解决的问题。\n在Java语言体系下，目前流行的服务治理框架有SpringCloud和Dubbo。\n以SpringCloud为例：\nFeign封装RestTemplate实现http请求方式的远程调用 Feign封装Ribbon实现客户端负载均衡 Euraka集群部署实现注册中心高可用 注册中心心跳监测，更新服务可用状态 集成Hystrix实现熔断机制 Zuul作为API 网关 ，提供路由转发、请求过滤等功能 Config实现分布式配置管理 Sluth实现调用链路跟踪 集成ELK，通过Kafka队列将日志异步写入Elasticsearch，通过Kibana可视化查看 SpringCloud是一整套完整微服务解决方案，被称为“SpringCloud 全家桶”。这里只是简单地介绍一下。\nDubbo主要提供了最基础的RPC功能。\n不过SpringCloud的RPC采用了HTTP协议，可能性能会差一些。\n利好的是，“SpringCloud2.0”——SpringCloud Alibaba流行了起来，Dubbo也可以完美地融入SpringCloud的生态。\n消息队列 # 消息队列在高性能、高扩展、高可用的架构中扮演着很重要的角色。\n消息队列是用来解耦一些不需要同步调用的服务或者订阅一些自己系统关心的变化。使用消息队列可以实现服务解耦（一对多消费）、异步处理、流量削峰/缓冲等。\n服务解耦\n服务解耦可以降低服务间耦合，提高系统系统的扩展性。\n例如一个订单服务，有多个下游，如果不用消息队列，那么订单服务就要调用多个下游。如果需求要再加下游，那么订单服务就得添加调用新下流的功能，这就比较烦。\n引入消息队列之后，订单服务就可以直接把订单相关消息塞到消息队列中，下游系统只管订阅就行了。\n异步处理\n异步处理可以降低响应时间，提高系统性能。\n随着业务的发展项目的请求链路越来越长，这样一来导致的后果就是响应时间变长，有些操作其实不用同步处理，这时候就可以考虑采用异步的方式了。\n流量削峰/缓冲\n流量削峰/缓冲可以提高系统的可用性。\n我们前面提到了接入层的限流，在服务层的限流可以通过消息队列来实现。网关的请求先放入消息队列中，后端服务尽可能去消息队列中消费请求。超时的请求可以直接返回错误，也可以在消息队列中等待。\n消息队列系统基本功能的实现比较简单，但要做到高性能、高可用、消息时序性、消息事务性则比较难。业界已经有很多成熟的开源实现方案，如果要求不高，基本上拿来用即可，例如，RocketMQ、Kafka、ActiveMQ 等。\n但如果业务对消息的可靠性、时序、事务性要求较高时，则要深入研究这些开源方案，提前考虑可能会遇到的问题，例如消息重复消费、消息丢失、消息堆积等等。\n平台层 # 当业务规模比较小、系统复杂度不高时，运维、测试、数据分析、管理等支撑功能主要由各系统或者团队独立完成。随着业务规模越来越大，系统复杂度越来越高，子系统数量越来越多，如果继续采取各自为政的方式来实现这些支撑功能，会发现重复工作非常多。所以就会自然地把相关功能抽离出来，作为公共的服务，避免重复造轮子，减少不规范带来的沟通和协作成本。\n平台层是服务化思维下的产物。将公共的一些功能拆分出来，让相关的业务服务只专注于自己的业务，这样有利于明确服务的职责，方便服务扩展。\n同时一些公共的平台，也有利于各个服务之间的统筹，例如数据平台，可以对数据进行聚合，某个服务以前需要一些整合一些数据可能要调用多个上游服务，但是引入数据平台以后，只需要从数据平台取数据就可以了，可以降低服务的响应时间。\n运维平台 # 运维平台核心的职责分为四大块：配置、部署、监控、应急，每个职责对应系统生命周期的一个阶段，如下图所示：\n部署：主要负责将系统发布到线上。例如，包管理、灰度发布管理、回滚等。 监控：主要负责收集系统上线运行后的相关数据并进行监控，以便及时发现问题。 应急：主要负责系统出故障后的处理。例如，停止程序、下线故障机器、切换 IP 等。 运维平台的核心设计要素是“四化\u0026quot;——标准化、平台化、自动化、可视化。\n标准化：要制定运维标准，规范配置管理、部署流程、监控指标、应急能力等，各系统按照运维标准来实现，避免不同的系统不同的处理方式。 平台化：传统的手工运维方式需要投入大量人力，效率低，容易出错，因此需要在运维标准化的基础上，将运维的相关操作都集成到运维平台中，通过运维平台来完成运维工作。 自动化：传统手工运维方式效率低下的一个主要原因就是要执行大量重复的操作，运维平台可以将这些重复操作固化下来，由系统自动完成。 可视化：运维平台有非常多的数据，如果全部通过人工去查询数据再来判断，则效率很低，可视化的主要目的就是为了提升数据查看效率。 测试平台 # 测试平台核心的职责当然就是测试了，包括单元测试、集成测试、接口测试、性能测试等，都可以在测试平台来完成。\n测试平台的核心目的是提升测试效率，从而提升产品质量，其设计关键就是自动化。\n数据平台 # 数据平台的核心职责主要包括三部分：数据管理、数据分析和数据应用。每一部分又包含更多的细分领域，详细的数据平台架构如下图所示：\n数据管理：数据管理包含数据采集、数据存储、数据访问和数据安全四个核心职责，是数据平台的基础功能。\n数据采集：从业务系统搜集各类数据。例如，日志、用户行为、业务数据等，将这些数据传送到数据平台。 数据存储：将从业务系统采集的数据存储到数据平台，用于后续数据分析。 数据访问：负责对外提供各种协议用于读写数据。例如，SQL、 Hive、 Key-Value 等读写协议。 数据安全：通常情况下数据平台都是多个业务共享的，部分业务敏感数据需要加以保护，防止被其他业务读取甚至修改，因此需要设计数据安全策略来保护数据。 数据分析：数据分析包括数据统计、数据挖掘、机器学习、深度学习等几个细分领域。\n数据挖掘：数据挖掘这个概念本身含义可以很广，为了与机器学习和深度学习区分开，这里的数据挖掘主要是指传统的数据挖掘方式。例如，有经验的数据分析人员基于数据仓库构建一系列规则来对数据进行分析从而发现一些隐含的规律、现象、问题等，经典的数据挖掘案例就是沃尔玛的啤酒与尿布的关联关系的发现。 机器学习、深度学习：机器学习和深度学习属于数据挖掘的一种具体实现方式，由于其实现方式与传统的数据挖掘方式差异较大，因此数据平台在实现机器学习和深度学习时，需要针对机器学习和深度学习独立进行设计。 数据应用：数据应用很广泛，既包括在线业务，也包括离线业务。例如，推荐、广告等属于在线应用，报表、欺诈检测、异常检测等属于离线应用。数据应用能够发挥价值的前提是需要有 \u0026ldquo;大数据\u0026rdquo; ，只有当数据的规模达到一定程度，基于数据的分析、挖掘才能发现有价值的规律、现象、问题等。如果数据没有达到一定规模，通常情况下做好数据统计就足够了，尤其是很多初创企业，无须一开始就参考 BAT 来构建自己的数据平台。\n管理平台 # 管理平台的核心职责就是权限管理，无论是业务系统（例如，淘宝网） 、中间件系统（例如，消息队列 Kafka）, 还是平台系统（例如，运维平台） ，都需要进行管理。如果每个系统都自己来实现权限管理，效率太低，重复工作很多，因此需要统一的管理平台来管理所有的系统的权限。\n说到“平台”，不由地想起这几年一会儿被人猛吹，一会儿被人唱衰的“中台”。在平台里的数据平台，其实已经和所谓的“数据中台”类似了。“中台”是个概念性的东西，具体怎么实现，没有统一的标准方案。作者所在的公司，也跟风建了中台，以“数据中台”为例，我们数据中台的建设主要为了数据共享和数据可视化，简单说就是把各个业务模块的一些数据汇聚起来。说起来简单，落地很难，数据汇聚的及时性、数据共享的快速响应……最终的解决方案是采购了阿里的一些商业化组件，花了老鼻子钱，但是效果，不能说一地鸡毛，也差不多吧。\n缓存层 # 虽然我们可以通过各种手段来提升存储系统的性能，但在某些复杂的业务场景下，单纯依靠存储系统的性能提升不够的。\n绝大部分在线业务都是读多写少。例如，微博、淘宝、微信这类互联网业务，读业务占了整体业务量的 90%以上。以微博为例：一个明星发一条微博，可能几千万人来浏览。\n如果直接从DB中取数据，有两个问题，一个是DB查询的速度有瓶颈，会增加系统的响应时间，一个是数据库本身的并发瓶颈。缓存就是为了弥补读多写少场景下存储系统的不足。\n在前面我们提到的CDN可以说是缓存的一种，它缓存的是静态资源。\n从整个架构来看，一般采用多级缓存的架构，在不同层级对数据进行缓存，来提升访问效率。\n简单说一下整体架构和流程，缓存一级一级地去读取，没有命中再去读取下一级，先读取本地缓存，再去读取分布式缓存，分布式缓存也没有命中，最后就得去读取DB。\n分布式缓存 # 为了提高缓存的可用性，一般采用分布式缓存。分布式缓存一般采用分片实现，即将数据分散到多个实例或多台服务器。算法一般釆用取模和一致性哈希。\n要采用不过期缓存机制，可以考虑取模机制，扩容时一般是新建一个集群。\n而对于可以丢失的缓存数据，可以考虑一致性哈希，即使其中一个实例出问题只是丢一小部分。\n对于分片实现可以考虑客户端实现，或者使用如Twemproxy 中间件进行代理（分片对客户端是透明的）。\n如果使用 Redis, 则 可 以考虑使用 redis-cluster 分布式集群方案。\n热点本地缓存 # 对于那些访问非常频繁的热点缓存，如果每次都去远程缓存系统中获取，可能会因为访问量太大导致远程缓存系统请求过多、负载过高或者带宽过高等问题，最终可能导致缓存响应慢，使客户端请求超时。\n一种解决方案是通过挂更多的从缓存，客户端通过负载均衡机制读取从缓存系统数据。不过也可以在客户端所在的应用/代理层本地存储一份，从而避免访问远程缓存，即使像库存这种数据，在有些应用系统中也可以进行几秒钟的本地缓存，从而降低远程系统的压力。\n缓存穿透 # 缓存穿透是指缓存没有发挥作用，业务系统虽然去缓存查询数据，但缓存中没有数据，业务系统需要再次去存储系统查询数据，结果存储系统也没有数据。\n缓存穿透的示意图：\n一般情况下，如果存储系统中没有某个数据，则不会在缓存中存储相应的数据，这样就导致用户查询的时候，在缓存中找不到对应的数据，每次都要去存储系统中再查询一遍，然后返回数据不存在。缓存在这个场景中并没有起到分担存储系统访问压力的作用。\n通常情况下，业务上读取不存在的数据的请求量并不会太大，但如果出现一些异常情况，例如被黑客攻击，故意大量访问某些读取不存在数据的业务，有可能会将存储系统拖垮。\n这种情况的解决办法有两种：\n一种比较简单，如果查询存储系统的数据没有找到，则直接设置一个默认值（可以是空值，也可以是具体的值） 存到缓存中，这样第二次读取缓存时就会获取到默认值，而不会继续访问存储系统。\n一种需要引入布隆过滤器，它的原理也很简单就是利用高效的数据结构和算法，快速判断出查询的Key是否在数据库中存在，不存在直接返回空，存在就去查了DB，刷新KV再返回值。\n缓存击穿 # 缓存击穿和缓存穿透也有点难以区分，缓存穿透表示的是缓存和数据库中都没有数据，缓存击穿表示缓存中没有数据而数据库中有数据。缓存击穿是某个热点的key失效，大并发集中对其进行请求，就会造成大量请求读缓存没读到数据，从而导致高并发访问数据库，引起数据库压力剧增。这种现象就叫做缓存击穿。\n缓存击穿示意图：\n关键在于某个热点的key失效了，导致大并发集中打在数据库上。所以要从两个方面解决，第一是否可以考虑热点key不设置过期时间，第二是否可以考虑降低打在数据库上的请求数量。\n主要有两个解决办法：\n利用互斥锁保证同一时刻只有一个客户端可以查询底层数据库的这个数据，一旦查到数据就缓存至Redis内，避免其他大量请求同时穿过Redis访问底层数据库。这种方式会阻塞其他的线程，此时系统的吞吐量会下降 热点数据缓存永远不过期。 永不过期有两种方式：\n物理不过期，针对热点key不设置过期时间 逻辑过期，把过期时间存在key对应的value里，如果发现要过期了，通过一个后台的异步线程进行缓存的构建 缓存雪崩 # 缓存雪崩，指的是是缓存不可用，或者同一时刻是大量热点key失效。\n两种情况导致的同样的后果就是大量的请求直接落在数据库上，对于一个高并发的业务系统来说，几百毫秒内可能会接到几百上千个请求，最严重的后果就是直接导致数据库宕机，可能会引起连锁反应，导致系统崩溃。\n缓存雪崩的解决方案可以分为三个维度：\n事前：\n均匀过期：设置不同的过期时间，让缓存失效的时间尽量均匀，避免相同的过期时间导致缓存雪崩，造成大量数据库的访问。 分级缓存：第一级缓存失效的基础上，访问二级缓存，每一级缓存的失效时间都不同。 热点数据缓存永远不过期。 保证Redis缓存的高可用，防止Redis宕机导致缓存雪崩的问题。可以使用 Redis集群等方式来避免 Redis 全盘崩溃的情况。 事中：\n互斥锁：在缓存失效后，通过互斥锁或者队列来控制读数据写缓存的线程数量，比如某个key只允许一个线程查询数据和写缓存，其他线程等待。这种方式会阻塞其他的线程，此时系统的吞吐量会下降 使用熔断机制，限流降级。当流量达到一定的阈值，直接返回“系统拥挤”之类的提示，防止过多的请求打在数据库上将数据库击垮，至少能保证一部分用户是可以正常使用，其他用户多刷新几次也能得到结果。 事后：\n开启Redis持久化机制，尽快恢复缓存数据，一旦重启，就能从磁盘上自动加载数据恢复内存中的数据。 存储层 # 不管是为了满足业务发展的需要，还是为了提升自己的竞争力，关系数据库厂商（Oracle、DB2、MySQL 等）在优化和提升单个数据库服务器的性能方面也做了非常多的技术优化和改进。但业务发展速度和数据增长速度，远远超出数据库厂商的优化速度，尤其是互联网业务兴起之后，海量用户加上海量数据的特点，单个数据库服务器已经难以满足业务需要，必须考虑数据库集群的方式来提升性能。\n读写分离 # 读写分离的基本原理是将数据库读写操作分散到不同的节点上，下面是其基本架构图：\n读写分离的基本实现是:\n数据库服务器搭建主从集群，一主一从、一主多从都可以。 数据库主机负责读写操作，从机只负责读操作。 数据库主机通过复制将数据同步到从机，每台数据库服务器都存储了所有的业务数据。 业务服务器将写操作发给数据库主机，将读操作发给数据库从机。 读写分离的实现逻辑并不复杂，但有两个细节点将引入设计复杂度：主从复制延迟和分配机制。\n复制延迟 # 以 MySQL 为例，主从复制延迟可能达到 1 秒，如果有大量数据同步，延迟 1 分钟也是有可能的。\n主从复制延迟会带来一个问题：如果业务服务器将数据写入到数据库主服务器后立刻 （1 秒 内）进行读取，此时读操作访问的是从机，主机还没有将数据复制过来，到从机读取数据是读不到最新数据的，业务上就可能出现问题。\n比如说将微博的信息同步给审核系统，所以我们在更新完主库之后，会将微博的 ID 写入消息队列，再由队列处理机依据 ID 在从库中 获取微博信息再发送给审核系统。此时如果主从数据库存在延迟，会导致在从库中获取不到微博信息，整个流程会出现异常。\n解决主从复制延迟的常见方法：\n数据的冗余：我们可以在发送消息队列时不仅仅发送微博 ID，而是发送队列处理机需要的所有微博信息，借此避免从数据库中重新查询数据。 使用缓存：我们可以在同步写数据库的同时，也把微博的数据写入到缓存里面，队列处理机在获取微博信息的时候会优先查询缓存，这样也可以保证数据的一致性。 二次读取：我们可以对底层数据库访问的API进行封装，一次读取从库发现不实时之后再读取一次，例如我们通过微博ID没有在从库里读到微博，那么第二次就直接去主库读取。 查询主库：我们可以把关键业务，或者对实时性有要求的业务读写操作全部指向主机，非关键业务或者实时性要求不高的业务采用读写分离。 分配机制 # 将读写操作区分开来，然后访问不同的数据库服务器，一般有两种方式：程序代码封装和中间件封装。\n程序代码封装：程序代码封装指在代码中抽象一个数据访问层（所以有的文章也称这种方式为 \u0026ldquo;中间层封装\u0026rdquo; ） ，实现读写操作分离和数据库服务器连接的管理。例如，基于 Hibernate 进行简单封装，就可以实现读写分离，基本架构是： 程序代码封装的方式具备几个特点:\n实现简单，而且可以根据业务做较多定制化的功能。 每个编程语言都需要自己实现一次，无法通用，如果一个业务包含多个编程语言写的多个子系统，则重复开发的工作量比较大。 故障情况下，如果主从发生切换，则可能需要所有系统都修改配置并重启。 如果不想自己造轮子，也可以用开源的方案，淘宝的TDDL是比较出名的一个。\n中间件封装：中间件封装指的是独立一套系统出来，实现读写操作分离和数据库服务器连接的管理。中间件对业务服务器提供 SQL 兼容的协议，业务服务器无须自己进行读写分离。对于业务服务器来说，访问中间件和访问数据库没有区别，事实上在业务服务器看来，中间件就是一个数据库服务器。 其基本架构是：\n数据库中间件的方式具备的特点是:\n能够支持多种编程语言，因为数据库中间件对业务服务器提供的是标准 SQL 接口。 数据库中间件要支持完整的 SQL 语法和数据库服务器的协议（例如，MySQL 客户端和服务器的连接协议） ，实现比较复杂，细节特别多，很容易出现 bug, 需要较长的时间才能稳定。 数据库中间件自己不执行真正的读写操作，但所有的数据库操作请求都要经过中间件，中间件的性能要求也很高。 数据库主从切换对业务服务器无感知，数据库中间件可以探测数据库服务器的主从状态。例如，向某个测试表写入一条数据，成功的就是主机，失败的就是从机。 目前开源的数据库中间件有基于 MySQL Proxy 开发的奇虎 360 的 Atlas 、阿里的 Cobar、基于 Cobar 开发的 Mycat 等。\n分库分表 # 读写分离分散了数据库读写操作的压力，但没有分散存储压力，当数据量达到干万甚至上亿条的时候，单台数据库服务器的存储能力会成为系统的瓶颈，主要体现在这几个方面：\n数据量太大，读写的性能会下降，即使有索引，索引也会变得很大，性能同样会下降。 数据文件会变得很大，数据库备份和恢复需要耗费很长时间。 数据文件越大，极端情况下丟失数据的风险越高（例如，机房火灾导致数据库主备机都发生故障）。 基于上述原因，单个数据库服务器存储的数据量不能太大，需要控制在一定的范围内。为了满足业务数据存储的需求，就需要将存储分散到多台数据库服务器上。\n业务分库\n业务分库指的是按照业务模块将数据分散到不同的数据库服务器。例如，一个简单的电商网站，包括用户、商品、订单三个业务模块，我们可以将用户数据、商品数据、订单数据分开放到三台不同的数据库服务器上，而不是将所有数据都放在一台数据库服务器上。\n虽然业务分库能够分散存储和访问压力，但同时也带来了新的问题，接下来我们详细分析一下。\njoin 操作问题：业务分库后，原本在同一个数据库中的表分散到不同数据库中，导致无法使用 SQL 的 join 查询。 例如： \u0026ldquo;查询购买了化妆品的用户中女性用户的列表〃 这个功能，虽然订单数据中有用户的 ID信息，但是用户的性别数据在用户数据库中，如果在同一个库中，简单的 join 查询就能完成；但现在数据分散在两个不同的数据库中，无法做 join 查询，只能采取先从订单数据库中查询购买了化妆品的用户 ID 列表，然后再到用户数据库中查询这批用户 ID 中的女性用户列表，这样实现就比简单的 join 查询要复杂一些。\n事务问题：原本在同一个数据库中不同的表可以在同一个事务中修改，业务分库后，表分散到不同的数据库中，无法通过事务统一修改。虽然数据库厂商提供了一些分布式事务的解决方案（例如，MySQL 的 XA）, 但性能实在太低，与高性能存储的目标是相违背的。 例如，用户下订单的时候需要扣商品库存，如果订单数据和商品数据在同一个数据库中，我们可订单，如果因为订单数据库异常导致生成订单失败，业务程序又需要将商品库存加上；而如果因为业务程序自己异常导致生成订单失败，则商品库存就无法恢复了，需要人工通过曰志等方式来手工修复库存异常。\n成本问题：业务分库同时也带来了成本的代价，本来 1 台服务器搞定的事情，现在要 3 台，如果考虑备份，那就是 2 台变成了 6 台。 基于上述原因，对于小公司初创业务，并不建议一开始就这样拆分，主要有几个原因:初创业务存在很大的不确定性，业务不一定能发展起来，业务开始的时候并没有真正的存储和访问压力，业务分库并不能为业务带来价值。业务分库后，表之间的 join 查询、数据库事务无法简单实现了。\n业务分库后，因为不同的数据要读写不同的数据库，代码中需要增加根据数据类型映射到不同数据库的逻辑，增加了工作量。而业务初创期间最重要的是快速实现、快速验证，业务分库会拖慢业务节奏。\n单表拆分\n将不同业务数据分散存储到不同的数据库服务器，能够支撑百万甚至千万用户规模的业务，但如果业务继续发展，同一业务的单表数据也会达到单台数据库服务器的处理瓶颈。例如，淘宝的几亿用户数据，如果全部存放在一台数据库服务器的一张表中，肯定是无法满足性能要求的，此时就需要对单表数据进行拆分。\n单表数据拆分有两种方式：垂直分表和水平分表。示意图如下：\n分表能够有效地分散存储压力和带来性能提升，但和分库一样，也会引入各种复杂性。\n两种分表方式可以用一个例子比喻，我们很多人可能都看过这么一篇文章，怎么把苹果切出星星来，答案是横着切。\n垂直分表：垂直分表适合将表中某些不常用且占了大量空间的列拆分出去。 例如，前面示意图中的nickname 和 desc 字段，假设我们是一个婚恋网站，用户在筛选其他用户的时候，主要是用 age 和 sex 两个字段进行查询，而 nickname 和 description 两个字段主要用于展示，一般不会在业务查询中用到。description 本身又比较长，因此我们可以将这两个字段独立到另外—张表中，这样在查询 age 和 sex 时，就能带来一定的性能提升。垂直分表引入的复杂性主要体现在表操作的数量要增加。例如，原来只要一次查询就可以获取name、age、sex、nickname、description, 现在需要两次查询，—次查询获取 name、age、 sex, 另一次查询获取 nickname、desc。\n不过相比接下来要讲的水平分表，这个复杂性就是小巫见大巫了。\n水平分表 水平分表适合表行数特别大的表，有的公司要求单表行数超过 5000 万就必须进行分表，这个数字可以作为参考，但并不是绝对标准，关键还是要看表的访问性能。对于一些比较复杂的表，可能超过 1000 万就要分表了；而对于一些简单的表，即使存储数据超过 1 亿行，也可以不分表。但不管怎样，当看到表的数据量达到干万级别时，这很可能是架构的性能瓶颈或者隐患。\n水平分表相比垂直分表，会引入更多的复杂性，主要表现在下面几个方面:\n路由：水平分表后，某条数据具体属于哪个切分后的子表，需要增加路由算法进行计算，这个算法会引入一定的复杂性。 常见的路由算法有：\n范围路由：选取有序的数据列 （例如，整形、时间戳等） 作为路由的条件，不同分段分散到不同的数据库表中。以订单 Id 为例，路由算法可以按照 1000万 的范围大小进行分段。范围路由设计的复杂点主要体现在分段大小的选取上，分段太小会导致切分后子表数量过多，增加维护复杂度；分段太大可能会导致单表依然存在性能问题，一般建议分段大小在 100 万至2000 万之间，具体需要根据业务选取合适的分段大小。 范围路由的优点是可以随着数据的增加平滑地扩充新的表。例如，现在的用户是 100 万，如果增加到 1000 万，只需要增加新的表就可以了，原有的数据不需要动。范围路由的一个比较隐含的缺点是分布不均匀，假如按照 1000 万来进行分表，有可能某个分段实际存储的数据量只有 1000 条，而另外一个分段实际存储的数据量有 900 万条。\nHash 路由：选取某个列 （或者某几个列组合也可以） 的值进行 Hash 运算，然后根据 Hash 结果分散到不同的数据库表中。同样以订单 id 为例，假如我们一开始就规划了 4个数据库表，路由算法可以简单地用 id % 4 的值来表示数据所属的数据库表编号，id 为 12的订单放到编号为 50的子表中，id为 13的订单放到编号为 61的字表中。 Hash 路由设计的复杂点主要体现在初始表数量的选取上，表数量太多维护比较麻烦，表数量太少又可能导致单表性能存在问题。而用了 Hash 路由后，增加字表数量是非常麻烦的，所有数据都要重分布。\nHash 路由的优缺点和范围路由基本相反，Hash 路由的优点是表分布比较均匀，缺点是扩充新的表很麻烦，所有数据都要重分布。\n配置路由：配置路由就是路由表，用一张独立的表来记录路由信息。 同样以订单id 为例，我们新增一张 order_router 表，这个表包含 orderjd 和 tablejd 两列 , 根据 orderjd 就可以查询对应的 table_id。\n配置路由设计简单，使用起来非常灵活，尤其是在扩充表的时候，只需要迁移指定的数据，然后修改路由表就可以了。\n配置路由的缺点就是必须多查询一次，会影响整体性能；而且路由表本身如果太大（例如，几亿条数据），性能同样可能成为瓶颈，如果我们再次将路由表分库分表，则又面临一个死循环式的路由算法选择问题。\njoin 操作：水平分表后，数据分散在多个表中，如果需要与其他表进行 join 查询，需要在业务代码或者数据库中间件中进行多次 join 查询，然后将结果合并。 count()操作：分表后就没那么简单了。常见的处理方式有下面两种: count() 相加：具体做法是在业务代码或者数据库中间件中对每个表进行 count操作，然后将结果相加。这种方式实现简单，缺点就是性能比较低。例如，水平分表后切分为 20 张表，则要进行 20 次 count()操作，如果串行的话，可能需要几秒钟才能得到结果。 记录数表：具体做法是新建一张表，假如表名为 \u0026ldquo;记录数表” ，包含 table_name、 row_count两个字段，每次插入或者删除子表数据成功后，都更新 \u0026ldquo;记录数表“。这种方式获取表记录数的性能要大大优于 count()相加的方式，因为只需要一次简单查询就可以获取数据。缺点是复杂度增加不少，对子表的操作要同步操作 \u0026ldquo;记录数表\u0026rdquo; ，如果有一个业务逻辑遗漏了，数据就会不一致；且针对 \u0026ldquo;记录数表\u0026rdquo; 的操作和针对子表的操作无法放在同一事务中进行处理，异常的情况下会出现操作子表成功了而操作记录数表失败，同样会导致数据不一致。此外，记录数表的方式也增加了数据库的写压力，因为每次针对子表的 insert 和 delete 操作都要 update 记录数表，所以对于一些不要求记录数实时保持精确的业务，也可以通过后台定时更新记录数表。定时更新实际上就是 \u0026ldquo;count()相加\u0026rdquo; 和 \u0026ldquo;记录数表\u0026rdquo; 的结合，即定时通过count()相加计算表的记录数，然后更新记录数表中的数据。 order by 操作：水平分表后，数据分散到多个子表中，排序操作无法在数据库中完成，只能由业务代码或者数据库中间件分别查询每个子表中的数据，然后汇总进行排序。 实现方法\n和数据库读写分离类似，分库分表具体的实现方式也是 \u0026ldquo;程序代码封装\u0026rdquo; 和 \u0026ldquo;中间件封装\u0026rdquo; ，但实现会更复杂。读写分离实现时只要识别 SQL 操作是读操作还是写操作，通过简单的判断SELECT、UPDATE、 INSERT、DELETE 几个关键字就可以做到，而分库分表的实现除了要判断操作类型外，还要判断 SQL 中具体需要操作的表、操作函数（例如 count 函数）、order by、group by 操作等，然后再根据不同的操作进行不同的处理。例如 order by 操作，需要先从多个库查询到各个库的数据，然后再重新 order by 才能得到最终的结果。\n数据异构 # 完成分库分表以后，我们看到存在一些问题，除了\u0026quot;程序代码封装\u0026rdquo; 和 \u0026ldquo;中间件封装\u0026quot;之外，我们还有一种办法，就是数据异构。数据异构就是将数据进行异地存储，比如业务上将MySQL的数据，写一份到Redis中，这就是实现了数据在集群中的异地存储，也就是数据异构。\n在数据量和访问量双高时使用数据异构是非常有效的，但增加了架构的复杂度。异构时可以通过双写、订阅 MQ 或者 binlog 并解析实现。\n双写：在写入数据的时候，同时将数据写入MySQL和异构存储系统； MQ：写入MySQL成功后，发一个mq消息，缓存读取mq消息并将消息写入异构存储系统； binlog：写入MySQL后，缓存系统x消费binlog，将变动写入异构存储系统。 这是一个异构的数据架构示意图：\n在图中用到了ES搜索集群来处理搜索业务，同样也可以我们前面提到的跨库join的问题。\n在设计异构的时候，我们可以充分利用一些流行的NoSQL数据库。NoSQL尽管已经被证明不能取代关系型数据库，但是在很多场景下是关系型数据库的有力补充。\n举几个例子，像我们熟悉的Redis这样的KV存储，有极高的读写性能，在读写性能有要求的场景可以使用；\nHbase、Cassandra 这样的列式存储数据库。这种数据库的特点是数据不像传统数据库以行为单位来存储，而是以列来存储，适用于一些离线数据统计的场景；\nMongoDB、CouchDB 这样的文档型数据库，具备 Schema Free（模式自由）的特点，数据表中的字段可以任意扩展，可以用于数据字段不固定的场景。\n查询维度异构\n比如对于订单库，当对其分库分表后，如果想按照商家维度或者按照用户维度进行查询，那么是非常困难的，因此可以通过异构数据库来解决这个问题。可以采用下图的架构。\n或者采用下图的ES异构：\n异构数据主要存储数据之间的关系，然后通过查询源库查询实际数据。不过，有时可以通过数据冗余存储来减少源库查询量或者提升查询性能。\n聚合数据异构\n商品详情页中一般包括商品基本信息、商品属性、商品图片，在前端展示商品详情页时，是按照商品 ID 维度进行查询，并且需要查询 3 个甚至更多的库才能查到所有展示数据。此时，如果其中一个库不稳定，就会导致商品详情页出现问题，因此，我们把数据聚合后异构存储到 KV 存储集群（如存储 JSON）, 这样只需要一次查询就能得到所有的展示数据。这种方式也需要系统有了一定的数据量和访问量时再考虑。\n高并发架构总结 # 通过前面的内容，已经差不多了解高并发的架构是一个什么样，接下来做一些总结和补充。\n高性能要点 # 高可用要点 # 除了从技术的角度来考虑，保证高可用同样需要良好的组织制度，来保证服务出现问题的快速恢复。\n高扩展要点 # 合理的分层架构：比如上面谈到的互联网最常见的分层架构，另外还能进一步按照数据访问层、业务逻辑层对微服务做更细粒度的分层（但是需要评估性能，会存在网络多一跳的情况）。 存储层的拆分：按照业务维度做垂直拆分、按照数据特征维度进一步做水平拆分（分库分表）。 业务层的拆分：最常见的是按照业务维度拆（比如电商场景的商品服务、订单服务等），也可以按照核心请求和非核心请求拆分，还可以按照请求源拆（比如To C和To B，APP和H5 ）。 Resources # 极客时间 《从零开始学架构》 知乎问答：我没有高并发项目经验，但是面试的时候经常被问到高并发、性能调优方面的问题，有什么办法可以解决吗？ 什么是高并发 ，详细讲解 【高并发】如何设计一个支撑高并发大流量的系统？这次我将设计思路分享给大家！ 《淘宝技术这十年》 知乎问答：如何获得高并发的经验？ 服务端高并发分布式架构演进之路 七年磨一剑，独家揭秘淘宝技术发展历程和架构经验 《大型网站技术架构核心原理与案例分析》 阿里技术专家：日活5亿的淘宝技术发展历程和架构经验分享！18页ppt详解 《亿级流量网站架构技术》 极客时间《从零开始学微服务》 面试题：如何保证消息不丢失？处理重复消息？消息有序性？消息堆积处理？ 极客时间 高并发系统设计40问 《Redis深度历险：核心原理和应用实践》 Redis的缓存雪崩、缓存击穿、缓存穿透与缓存预热、缓存降级 如何优雅的设计和使用缓存？ 缓存穿透、缓存击穿、缓存雪崩，看这篇就够了 数据异构 分库分表？如何做到永不迁移数据和避免热点？ ","date":"10 April 2024","permalink":"/posts/architecture/high-concurrency/tutorial/","section":"博客","summary":"慎入，高并发的水太深，什么高并发，大流量的东西都是虚拟的，笔者还太年轻，没有那个经历，把握不住。系统最大只搞过几千QPS，开心快乐就行，不PK，文明PK。","title":"高并发架构学习笔记"},{"content":"数学 # 取整\n// 向下取整 Math.floor() // 向上取整 Math.ceil() // 返回最接近参数的整数，如果有2个数同样接近，则会返回偶数的那个 Math.rint() // 四舍五入 Math.round() 算法 # 链表 # /** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode() {} * ListNode(int val) { this.val = val; } * ListNode(int val, ListNode next) { this.val = val; this.next = next; } * } */ public ListNode middleNode(ListNode head) { ListNode slow = head; ListNode fast = head; while (fast.next != null \u0026amp;\u0026amp; fast.next.next != null) { slow = slow.next; fast = fast.next.next; } return slow; } public ListNode reverseList(ListNode head) { ListNode prev = null; ListNode curr = head; while (curr != null) { ListNode nextTemp = curr.next; curr.next = prev; prev = curr; curr = nextTemp; } return prev; } // 依次合并两个链表 public void mergeList(ListNode l1, ListNode l2) { ListNode l1_tmp; ListNode l2_tmp; while (l1 != null \u0026amp;\u0026amp; l2 != null) { l1_tmp = l1.next; l2_tmp = l2.next; l1.next = l2; l1 = l1_tmp; l2.next = l1; l2 = l2_tmp; } } // 合并k个已排序的链表 public ListNode mergeKLists(ArrayList\u0026lt;ListNode\u0026gt; lists) { //小顶堆 Queue\u0026lt;ListNode\u0026gt; pq = new PriorityQueue\u0026lt;\u0026gt;((v1, v2) -\u0026gt; v1.val - v2.val); //遍历所有链表第一个元素 for(int i = 0; i \u0026lt; lists.size(); i++){ //不为空则加入小顶堆 if(lists.get(i) != null) pq.add(lists.get(i)); } //加一个表头 ListNode res = new ListNode(-1); ListNode head = res; //直到小顶堆为空 while(!pq.isEmpty()){ //取出最小的元素 ListNode temp = pq.poll(); //连接 head.next = temp; head = head.next; //每次取出链表的后一个元素加入小顶堆 if(temp.next != null) pq.add(temp.next); } //去掉表头 return res.next; } // 单链表排序 public ListNode sortInList (ListNode head) { // write code here if (head == null || head.next == null) return head; // 使用快慢指针寻找链表的中点 ListNode fast = head.next, slow = head; while (fast != null \u0026amp;\u0026amp; fast.next != null) { slow = slow.next; fast = fast.next.next; } ListNode tmp = slow.next; slow.next = null; // 递归左右两边进行排序 ListNode left = sortInList(head); ListNode right = sortInList(tmp); // 创建新的链表 ListNode h = new ListNode(0); ListNode res = h; // 合并 left right两个链表 while (left != null \u0026amp;\u0026amp; right != null) { // left right链表循环对比 if (left.val \u0026lt; right.val) { h.next = left; left = left.next; } else { h.next = right; right = right.next; } h = h.next; } // 最后添加未对比的链表部分判断左链表是否为空 h.next = left != null ? left : right; return res.next; } // 删除有序链表中重复的元素 public ListNode deleteDuplicates (ListNode head) { //空链表 if(head == null) return null; ListNode res = new ListNode(0); //在链表前加一个表头 res.next = head; ListNode cur = res; while(cur.next != null \u0026amp;\u0026amp; cur.next.next != null){ //遇到相邻两个节点值相同 if(cur.next.val == cur.next.next.val){ int temp = cur.next.val; //将所有相同的都跳过 while (cur.next != null \u0026amp;\u0026amp; cur.next.val == temp) cur.next = cur.next.next; } else cur = cur.next; } //返回时去掉表头 return res.next; } 回溯 # // 全排列 // 给定一个不含重复数字的数组 nums ，返回其 所有可能的全排列 。你可以 按任意顺序 返回答案。 class Solution { public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; permute(int[] nums) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; ans = new ArrayList\u0026lt;\u0026gt;(); int n = nums.length; boolean[] used = new boolean[n]; dfs(ans, nums, 0, n, used, new ArrayList\u0026lt;Integer\u0026gt;()); return ans; } public void dfs(List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; ans, int[] nums, int index, int maxx, boolean[] used, ArrayList\u0026lt;Integer\u0026gt; temp) { if (index == maxx) { ans.add(new ArrayList\u0026lt;\u0026gt;(temp)); return; } for (int i = 0; i \u0026lt; maxx; i++) { if (used[i] == false) { temp.add(nums[i]); used[i] = true; dfs(ans, nums, index + 1, maxx, used, temp); used[i] = false; temp.remove(index); } } } } 并查集 # public static class UnionFind { private int[] parent; private int[] sizeMap; private int size; public UnionFind(int N) { size = N; parent = new int[N]; sizeMap = new int[N]; for (int i = 0; i \u0026lt; N; i++) { parent[i] = i; sizeMap[i] = 1; } } public int size() { return size; } private int find(int v) { if (v != parent[v]) { parent[v] = find(parent[v]); } return parent[v]; } public void union(int v1, int v2) { int f1 = find(v1); int f2 = find(v2); if (f1 != f2) { size--; int s1 = sizeMap[f1]; int s2 = sizeMap[f2]; if (s1 \u0026gt; s2) { parent[f2] = f1; sizeMap[f1] += s2; } else { parent[f1] = f2; sizeMap[f2] += s1; } } } } 树 # 二叉树\nimport java.util.List; import java.util.ArrayList; public class TreeNode { int val; TreeNode left; TreeNode right; TreeNode(int x) { val = x; left = null; right = null; } } public class BinaryTree { List\u0026lt;Integer\u0026gt; ans = new ArrayList\u0026lt;\u0026gt;(); // 前序遍历 public void preorder(TreeNode root) { if (root == null) { return; } ans.add(root.val); // 访问根节点 preorder(root.left); // 遍历左子树 preorder(root.right); // 遍历右子树 } // 中序遍历 public void inorder(TreeNode root) { if (root == null) { return; } inorder(root.left); // 遍历左子树 ans.add(root.val); // 访问根节点 inorder(root.right); // 遍历右子树 } // 后序遍历 public void postorder(TreeNode root) { if (root == null) { return; } postorder(root.left); // 遍历左子树 postorder(root.right); // 遍历右子树 ans.add(root.val); // 访问根节点 } } 字典树\nclass Trie { private Trie[] children; private boolean isEnd; public Trie() { children = new Trie[26]; isEnd = false; } public void insert(String word) { Trie node = this; for (int i = 0; i \u0026lt; word.length(); i++) { char ch = word.charAt(i); int index = ch - \u0026#39;a\u0026#39;; if (node.children[index] == null) { node.children[index] = new Trie(); } node = node.children[index]; } node.isEnd = true; } public boolean search(String word) { Trie node = searchPrefix(word); return node != null \u0026amp;\u0026amp; node.isEnd; } public boolean startsWith(String prefix) { return searchPrefix(prefix) != null; } private Trie searchPrefix(String prefix) { Trie node = this; for (int i = 0; i \u0026lt; prefix.length(); i++) { char ch = prefix.charAt(i); int index = ch - \u0026#39;a\u0026#39;; if (node.children[index] == null) { return null; } node = node.children[index]; } return node; } } 从前序与中序遍历序列构造二叉树\nclass Solution { private Map\u0026lt;Integer, Integer\u0026gt; indexMap; public TreeNode myBuildTree(int[] preorder, int[] inorder, int preorder_left, int preorder_right, int inorder_left, int inorder_right) { if (preorder_left \u0026gt; preorder_right) { return null; } // 前序遍历中的第一个节点就是根节点 int preorder_root = preorder_left; // 在中序遍历中定位根节点 int inorder_root = indexMap.get(preorder[preorder_root]); // 先把根节点建立出来 TreeNode root = new TreeNode(preorder[preorder_root]); // 得到左子树中的节点数目 int size_left_subtree = inorder_root - inorder_left; // 递归地构造左子树，并连接到根节点 // 先序遍历中「从 左边界+1 开始的 size_left_subtree」个元素就对应了中序遍历中「从 左边界 开始到 根节点定位-1」的元素 root.left = myBuildTree(preorder, inorder, preorder_left + 1, preorder_left + size_left_subtree, inorder_left, inorder_root - 1); // 递归地构造右子树，并连接到根节点 // 先序遍历中「从 左边界+1+左子树节点数目 开始到 右边界」的元素就对应了中序遍历中「从 根节点定位+1 到 右边界」的元素 root.right = myBuildTree(preorder, inorder, preorder_left + size_left_subtree + 1, preorder_right, inorder_root + 1, inorder_right); return root; } public TreeNode buildTree(int[] preorder, int[] inorder) { int n = preorder.length; // 构造哈希映射，帮助我们快速定位根节点 indexMap = new HashMap\u0026lt;Integer, Integer\u0026gt;(); for (int i = 0; i \u0026lt; n; i++) { indexMap.put(inorder[i], i); } return myBuildTree(preorder, inorder, 0, n - 1, 0, n - 1); } } 从中序与后序遍历序列构造二叉树\nclass Solution { int post_idx; int[] postorder; int[] inorder; Map\u0026lt;Integer, Integer\u0026gt; idx_map = new HashMap\u0026lt;Integer, Integer\u0026gt;(); public TreeNode helper(int in_left, int in_right) { // 如果这里没有节点构造二叉树了，就结束 if (in_left \u0026gt; in_right) { return null; } // 选择 post_idx 位置的元素作为当前子树根节点 int root_val = postorder[post_idx]; TreeNode root = new TreeNode(root_val); // 根据 root 所在位置分成左右两棵子树 int index = idx_map.get(root_val); // 下标减一 post_idx--; // 构造右子树 root.right = helper(index + 1, in_right); // 构造左子树 root.left = helper(in_left, index - 1); return root; } public TreeNode buildTree(int[] inorder, int[] postorder) { this.postorder = postorder; this.inorder = inorder; // 从后序遍历的最后一个元素开始 post_idx = postorder.length - 1; // 建立（元素，下标）键值对的哈希表 int idx = 0; for (Integer val : inorder) { idx_map.put(val, idx++); } return helper(0, inorder.length - 1); } } 图 # 拓扑排序\n拓扑排序是针对有向无环图（DAG，Directed Acyclic Graph）的一种排序方式，它将图中的所有顶点排列成线性序列，使得对于任何从顶点 $u$ 到顶点 $v$ 的有向边 $u \\rightarrow v$，顶点 $u$ 都出现在顶点 $v$ 的前面。拓扑排序不是唯一的，一个图可以有多个拓扑排序的序列。\n下面是使用 Kahn 算法 实现拓扑排序的 Java 模板代码：\nimport java.util.*; public class TopologicalSort { public static List\u0026lt;Integer\u0026gt; topologicalSort(int numCourses, int[][] prerequisites) { // 创建一个数组来存储每个顶点的入度 int[] inDegree = new int[numCourses]; // 创建一个邻接表 List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; adjList = new ArrayList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; numCourses; i++) { adjList.add(new ArrayList\u0026lt;\u0026gt;()); } // 填充邻接表并记录每个顶点的入度 for (int[] prerequisite : prerequisites) { int dest = prerequisite[0]; int src = prerequisite[1]; adjList.get(src).add(dest); inDegree[dest]++; } // 使用队列找到所有入度为0的顶点 Queue\u0026lt;Integer\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; numCourses; i++) { if (inDegree[i] == 0) { queue.add(i); } } // 进行拓扑排序 List\u0026lt;Integer\u0026gt; topOrder = new ArrayList\u0026lt;\u0026gt;(); while (!queue.isEmpty()) { int vertex = queue.poll(); topOrder.add(vertex); // 遍历每一个顶点的邻居 for (int neighbor : adjList.get(vertex)) { inDegree[neighbor]--; if (inDegree[neighbor] == 0) { queue.add(neighbor); } } } // 检查是否有环 if (topOrder.size() != numCourses) { return new ArrayList\u0026lt;\u0026gt;(); // 如果排序后的顶点数不等于课程数，说明存在环 } return topOrder; } public static void main(String[] args) { int numCourses = 4; int[][] prerequisites = {{1, 0}, {2, 0}, {3, 1}, {3, 2}}; List\u0026lt;Integer\u0026gt; order = topologicalSort(numCourses, prerequisites); System.out.println(\u0026#34;Topological Order: \u0026#34; + order); } } Dijkstra 算法\nDijkstra 算法适用于求解带权重的有向图或无向图中单源最短路径问题。它不能处理带有负权重的边。\nimport java.util.*; public class DijkstraAlgorithm { private static final int NO_PARENT = -1; public static void dijkstra(int[][] adjacencyMatrix, int startVertex) { int nVertices = adjacencyMatrix.length; int[] shortestDistances = new int[nVertices]; boolean[] added = new boolean[nVertices]; Arrays.fill(shortestDistances, Integer.MAX_VALUE); Arrays.fill(added, false); shortestDistances[startVertex] = 0; int[] parents = new int[nVertices]; Arrays.fill(parents, NO_PARENT); for (int i = 1; i \u0026lt; nVertices; i++) { int nearestVertex = -1; int shortestDistance = Integer.MAX_VALUE; for (int vertexIndex = 0; vertexIndex \u0026lt; nVertices; vertexIndex++) { if (!added[vertexIndex] \u0026amp;\u0026amp; shortestDistances[vertexIndex] \u0026lt; shortestDistance) { nearestVertex = vertexIndex; shortestDistance = shortestDistances[vertexIndex]; } } added[nearestVertex] = true; for (int vertexIndex = 0; vertexIndex \u0026lt; nVertices; vertexIndex++) { int edgeDistance = adjacencyMatrix[nearestVertex][vertexIndex]; if (edgeDistance \u0026gt; 0 \u0026amp;\u0026amp; ((shortestDistance + edgeDistance) \u0026lt; shortestDistances[vertexIndex])) { parents[vertexIndex] = nearestVertex; shortestDistances[vertexIndex] = shortestDistance + edgeDistance; } } } printSolution(startVertex, shortestDistances, parents); } private static void printSolution(int startVertex, int[] distances, int[] parents) { System.out.println(\u0026#34;Vertex\\t Distance\\tPath\u0026#34;); for (int vertexIndex = 0; vertexIndex \u0026lt; distances.length; vertexIndex++) { if (vertexIndex != startVertex) { System.out.print(\u0026#34;\\n\u0026#34; + startVertex + \u0026#34; -\u0026gt; \u0026#34;); System.out.print(vertexIndex + \u0026#34; \\t\\t \u0026#34;); System.out.print(distances[vertexIndex] + \u0026#34;\\t\\t\u0026#34;); printPath(vertexIndex, parents); } } } private static void printPath(int currentVertex, int[] parents) { if (currentVertex == NO_PARENT) { return; } printPath(parents[currentVertex], parents); System.out.print(currentVertex + \u0026#34; \u0026#34;); } public static void main(String[] args) { int[][] adjacencyMatrix = { { 0, 4, 0, 0, 0, 0, 0, 8, 0 }, { 4, 0, 8, 0, 0, 0, 0, 11, 0 }, { 0, 8, 0, 7, 0, 4, 0, 0, 2 }, { 0, 0, 7, 0, 9, 14, 0, 0, 0 }, { 0, 0, 0, 9, 0, 10, 0, 0, 0 }, { 0, 0, 4, 14, 10, 0, 2, 0, 0 }, { 0, 0, 0, 0, 0, 2, 0, 1, 6 }, { 8, 11, 0, 0, 0, 0, 1, 0, 7 }, { 0, 0, 2, 0, 0, 0, 6, 7, 0 } }; dijkstra(adjacencyMatrix, 0); } } Floyd-Warshall 算法\nFloyd-Warshall 算法适用于求解所有顶点对之间的最短路径问题，适用于有向图和无向图，并且可以处理带有正权重或负权重的边（但不能有负权重循环）。\npublic class FloydWarshallAlgorithm { final static int INF = 99999, V = 4; public static void floydWarshall(int graph[][]) { int dist[][] = new int[V][V]; int i, j, k; for (i = 0; i \u0026lt; V; i++) for (j = 0; j \u0026lt; V; j++) dist[i][j] = graph[i][j]; for (k = 0; k \u0026lt; V; k++) { for (i = 0; i \u0026lt; V; i++) { for (j = 0; j \u0026lt; V; j++) { if (dist[i][k] + dist[k][j] \u0026lt; dist[i][j]) dist[i][j] = dist[i][k] + dist[k][j]; } } } printSolution(dist); } public static void printSolution(int dist[][]) { System.out.println(\u0026#34;The following matrix shows the shortest distances between every pair of vertices\u0026#34;); for (int i = 0; i \u0026lt; V; ++i) { for (int j = 0; j \u0026lt; V; ++j) { if (dist[i][j] == INF) System.out.print(\u0026#34;INF \u0026#34;); else System.out.print(dist[i][j] + \u0026#34; \u0026#34;); } System.out.println(); } } public static void main(String[] args) { int graph[][] = { {0, 5, INF, 10}, {INF, 0, 3, INF}, {INF, INF, 0, 1}, {INF, INF, INF, 0} }; floydWarshall(graph); } } 二分 # public class BinarySearch { public static int binarySearch(int[] array, int target) { int left = 0; int right = array.length - 1; while (left \u0026lt;= right) { int mid = left + (right - left) / 2; if (array[mid] == target) { return mid; // 目标值在数组中的索引 } else if (array[mid] \u0026lt; target) { left = mid + 1; // 搜索右半区间 } else { right = mid - 1; // 搜索左半区间 } } return -1; // 如果未找到目标值，返回 -1 } public static void main(String[] args) { int[] numbers = {1, 3, 5, 7, 9, 11, 13, 15}; int target = 9; int result = binarySearch(numbers, target); System.out.println(\u0026#34;Index of target is: \u0026#34; + result); } } 高精度 # 高精度乘法\nimport java.util.*; public class Main { public static String mulNums(String a, String b) { // Create an array to store multiplication results int[] res = new int[a.length() + b.length() - 1]; // Populate the result array with products of digits for (int i = 0; i \u0026lt; a.length(); i++) { for (int j = 0; j \u0026lt; b.length(); j++) { res[i + j] += (a.charAt(i) - \u0026#39;0\u0026#39;) * (b.charAt(j) - \u0026#39;0\u0026#39;); } } // Convert the result array into the final result string StringBuilder ret = new StringBuilder(); int carry = 0; for (int i = res.length - 1; i \u0026gt;= 0; i--) { int num = res[i] + carry; ret.insert(0, (char) (num % 10 + \u0026#39;0\u0026#39;)); carry = num / 10; } // Add remaining carry if present if (carry \u0026gt; 0) ret.insert(0, (char) (carry + \u0026#39;0\u0026#39;)); return ret.toString(); } public static void main(String[] args) { System.out.println(mulNums(\u0026#34;12345\u0026#34;, \u0026#34;67890111\u0026#34;)); } } 高精度除法\nimport java.math.BigInteger; public class HighPrecisionDivision { public static String divideNumbers(String dividend, String divisor) { // Convert strings to BigInteger for handling large numbers BigInteger bigDividend = new BigInteger(dividend); BigInteger bigDivisor = new BigInteger(divisor); // Perform division and get the quotient BigInteger quotient = bigDividend.divide(bigDivisor); // Return the quotient as a string return quotient.toString(); } public static void main(String[] args) { String dividend = \u0026#34;12345678901234567890\u0026#34;; String divisor = \u0026#34;12345\u0026#34;; System.out.println(\u0026#34;Quotient: \u0026#34; + divideNumbers(dividend, divisor)); } } 滑动窗口 # public class SlidingWindowTemplate { /** * Finds the minimum length of a subarray with a sum at least equal to the given target. * @param nums Array of integers. * @param target Sum target for the subarray. * @return Minimum length of the subarray with sum at least target, or 0 if no such subarray exists. */ public static int minSubArrayLen(int target, int[] nums) { int n = nums.length; int minLength = Integer.MAX_VALUE; // Set to a large number initially. int start = 0; // Start index of the sliding window. int sum = 0; // Current sum of the sliding window. for (int end = 0; end \u0026lt; n; end++) { sum += nums[end]; // Expand the window by including the current element. // Once we have a window with sum \u0026gt;= target, we try to shrink it from the left. while (sum \u0026gt;= target) { minLength = Math.min(minLength, end - start + 1); sum -= nums[start]; // Shrink the window from the start. start++; // Move the start index to the right. } } // If minLength was updated, return its value, otherwise return 0 for no valid subarray. return minLength == Integer.MAX_VALUE ? 0 : minLength; } public static void main(String[] args) { int[] nums = {2, 3, 1, 2, 4, 3}; int target = 7; int result = minSubArrayLen(target, nums); System.out.println(\u0026#34;Minimum length of subarray: \u0026#34; + result); // Output should be 2, for the subarray [4, 3] } } 数据结构 # Scanner # Scanner in = new Scanner(System.in); HashSet # HashSet\u0026lt;Type\u0026gt; set = new HashSet\u0026lt;\u0026gt;(); // 添加元素 set.add(element); // 删除元素 set.remove(element); // 清空集合 set.clear(); // 检查集合是否为空 boolean isEmpty = set.isEmpty(); // 获取集合大小 int size = set.size(); // 检查集合是否包含某元素 boolean containsElement = set.contains(element); // 遍历集合 // 使用迭代器 Iterator\u0026lt;Type\u0026gt; iterator = set.iterator(); while(iterator.hasNext()) { Type element = iterator.next(); // 执行操作 } // 使用增强型 for 循环（foreach） for (Type element : set) { // 执行操作 } // 将 HashSet 转换为数组 Type[] array = set.toArray(new Type[set.size()]); // 将另一个集合的元素添加到当前集合中 set.addAll(anotherSet); // 从当前集合中移除另一个集合的元素 set.removeAll(anotherSet); // 留两个集合的交集 set.retainAll(anotherSet); // 检查两个集合是否相等 boolean isEqual = set.equals(anotherSet); // 获取哈希码 int hashCode = set.hashCode(); Character # // 判断字符是否是字母 boolean isLetter = Character.isLetter(char ch); // 判断字符是否是数字 boolean isDigit = Character.isDigit(char ch); // 判断字符是否是字母或数字 boolean isLetterOrDigit = Character.isLetterOrDigit(char ch); // 判断字符是否是小写字母 boolean isLowerCase = Character.isLowerCase(char ch); // 判断字符是否是大写字母 boolean isUpperCase = Character.isUpperCase(char ch); // 将字符转换为小写 char lowerCaseChar = Character.toLowerCase(char ch); // 将字符转换为大写 char upperCaseChar = Character.toUpperCase(char ch); // 判断字符是否是空白字符 boolean isWhitespace = Character.isWhitespace(char ch); PriorityQueue # PriorityQueue\u0026lt;Integer\u0026gt; pq = new PriorityQueue\u0026lt;\u0026gt;(); PriorityQueue\u0026lt;Integer\u0026gt; pq = new PriorityQueue\u0026lt;\u0026gt;(new Comparator\u0026lt;Integer\u0026gt;() { public int compare(Integer a, Integer b) { return a - b; } }); PriorityQueue\u0026lt;Integer\u0026gt; pq = new PriorityQueue\u0026lt;\u0026gt;((a, b) -\u0026gt; b.compareTo(a)); // 添加元素 pq.add(5); pq.offer(3); // 获取队首元素 int first = pq.peek(); // 移除队首元素 int removedElement = pq.poll(); // 检查队列是否为空 boolean isEmpty = pq.isEmpty(); // 获取队列大小 int size = pq.size(); // 遍历队列 Iterator\u0026lt;Integer\u0026gt; iterator = pq.iterator(); while (iterator.hasNext()) { System.out.println(iterator.next()); } ArrayList # // 创建一个空的 ArrayList ArrayList\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); // 添加元素到列表末尾 list.add(\u0026#34;Apple\u0026#34;); list.add(\u0026#34;Banana\u0026#34;); // 在指定位置插入元素 list.add(1, \u0026#34;Avocado\u0026#34;); // 访问指定位置的元素 String item = list.get(1); // 返回 \u0026#34;Avocado\u0026#34; // 修改指定位置的元素 list.set(1, \u0026#34;Blueberry\u0026#34;); // 删除指定位置的元素 list.remove(1); // 移除 \u0026#34;Blueberry\u0026#34; // 删除指定的元素 list.remove(\u0026#34;Banana\u0026#34;); // 获取列表的大小 int size = list.size(); // 返回当前列表的元素数量 // 检查列表是否为空 boolean isEmpty = list.isEmpty(); // 返回 true 如果列表为空 // 清空列表 list.clear(); // 检查列表是否包含某个元素 boolean contains = list.contains(\u0026#34;Apple\u0026#34;); // 返回 true 如果列表包含 \u0026#34;Apple\u0026#34; // 遍历列表 for (String element : list) { System.out.println(element); } // 添加一些元素以便排序 list.add(\u0026#34;Mango\u0026#34;); list.add(\u0026#34;Strawberry\u0026#34;); list.add(\u0026#34;Raspberry\u0026#34;); // 排序列表 Collections.sort(list); // 使用 Lambda 表达式按字符串长度排序 Collections.sort(list, (a, b) -\u0026gt; a.length() - b.length()); // 转换为数组 Object[] array = list.toArray(); // 直接在 ArrayList 上进行排序（Java 8+） list.sort(String::compareTo); LinkedList # // 创建一个空的 LinkedList LinkedList\u0026lt;String\u0026gt; list = new LinkedList\u0026lt;\u0026gt;(); // 在链表末尾添加元素 list.add(\u0026#34;Apple\u0026#34;); list.add(\u0026#34;Banana\u0026#34;); // 在链表的开始处添加元素 list.addFirst(\u0026#34;Almond\u0026#34;); // 在链表的末尾添加元素 list.addLast(\u0026#34;Date\u0026#34;); // 在指定位置插入元素 list.add(2, \u0026#34;Cherry\u0026#34;); // 获取链表的第一个元素 String first = list.getFirst(); // 获取链表的最后一个元素 String last = list.getLast(); // 访问指定位置的元素 String item = list.get(2); // 返回 \u0026#34;Cherry\u0026#34; // 修改指定位置的元素 list.set(2, \u0026#34;Blueberry\u0026#34;); // 删除指定位置的元素 list.remove(2); // 移除 \u0026#34;Blueberry\u0026#34; // 删除首次出现的指定元素 list.remove(\u0026#34;Banana\u0026#34;); // 删除链表的第一个元素 list.removeFirst(); // 删除链表的最后一个元素 list.removeLast(); // 获取链表的大小 int size = list.size(); // 返回当前链表的元素数量 // 检查链表是否为空 boolean isEmpty = list.isEmpty(); // 返回 true 如果链表为空 // 清空链表 list.clear(); // 检查链表是否包含某个元素 boolean contains = list.contains(\u0026#34;Apple\u0026#34;); // 返回 false 因为已经 clear // 遍历链表 for (String element : list) { System.out.println(element); } // 转换为数组 Object[] array = list.toArray(); Queue # // 创建一个 Queue 实例，这里使用 LinkedList 作为 Queue 的实现 Queue\u0026lt;String\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); // 向队列添加元素（offer 和 add 方法都可以） queue.offer(\u0026#34;Apple\u0026#34;); // 添加一个元素到队尾 queue.add(\u0026#34;Banana\u0026#34;); // add 也是添加元素到队尾，与 offer 的区别在于 add 在失败时抛出异常 // 查看队列的头部元素但不移除（peek 和 element 方法都可以） String head = queue.peek(); // 返回队头元素，队列为空时返回 null String first = queue.element(); // 返回队头元素，队列为空时抛出异常 NoSuchElementException // 从队列中移除并返回头部元素（poll 和 remove 方法都可以） String removedItem = queue.poll(); // 移除并返回队头元素，队列为空时返回 null String removedFirst = queue.remove(); // 移除并返回队头元素，队列为空时抛出异常 NoSuchElementException // 检查队列是否为空 boolean isEmpty = queue.isEmpty(); // 返回 true 如果队列为空 // 获取队列的大小 int size = queue.size(); // 返回队列中的元素数量 // 遍历队列 for (String item : queue) { System.out.println(item); } // 清空队列 queue.clear(); Stack # // 创建一个 Stack 实例 Stack\u0026lt;String\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); // 向栈中添加元素（push 方法） stack.push(\u0026#34;Apple\u0026#34;); // 将 \u0026#34;Apple\u0026#34; 压入栈顶 stack.push(\u0026#34;Banana\u0026#34;); // 将 \u0026#34;Banana\u0026#34; 压入栈顶 // 查看栈顶元素但不移除（peek 方法） String top = stack.peek(); // 返回栈顶元素 \u0026#34;Banana\u0026#34;，但不从栈中移除 // 从栈中移除并返回栈顶元素（pop 方法） String popped = stack.pop(); // 移除并返回栈顶元素 \u0026#34;Banana\u0026#34; // 检查栈是否为空 boolean isEmpty = stack.isEmpty(); // 返回 true 如果栈为空 // 获取栈的大小 int size = stack.size(); // 返回栈中的元素数量 // 遍历栈（栈遍历是从底部到顶部，不是典型的 LIFO 遍历方式） for (String item : stack) { System.out.println(item); // 输出每个元素，注意这种遍历方式不影响栈的内容 } // 清空栈 stack.clear(); // 清除栈中所有元素 BigDecimal 和 BigInteger # import java.math.BigInteger; import java.math.BigDecimal; public class BigNumbersExample { public static void main(String[] args) { // 创建 BigInteger 实例 BigInteger bigInteger1 = new BigInteger(\u0026#34;123456789012345678901234567890\u0026#34;); BigInteger bigInteger2 = new BigInteger(\u0026#34;987654321098765432109876543210\u0026#34;); // BigInteger 加法 BigInteger bigIntSum = bigInteger1.add(bigInteger2); // BigInteger 减法 BigInteger bigIntDifference = bigInteger1.subtract(bigInteger2); // BigInteger 乘法 BigInteger bigIntProduct = bigInteger1.multiply(bigInteger2); // BigInteger 除法 BigInteger bigIntQuotient = bigInteger1.divide(bigInteger2); // BigInteger 求余 BigInteger bigIntRemainder = bigInteger1.mod(bigInteger2); System.out.println(\u0026#34;BigInteger Sum: \u0026#34; + bigIntSum); System.out.println(\u0026#34;BigInteger Difference: \u0026#34; + bigIntDifference); System.out.println(\u0026#34;BigInteger Product: \u0026#34; + bigIntProduct); System.out.println(\u0026#34;BigInteger Quotient: \u0026#34; + bigIntQuotient); System.out.println(\u0026#34;BigInteger Remainder: \u0026#34; + bigIntRemainder); // 创建 BigDecimal 实例 BigDecimal bigDecimal1 = new BigDecimal(\u0026#34;123456.789012345678901234567890\u0026#34;); BigDecimal bigDecimal2 = new BigDecimal(\u0026#34;98765.4321098765432109876543210\u0026#34;); // BigDecimal 加法 BigDecimal bigDecSum = bigDecimal1.add(bigDecimal2); // BigDecimal 减法 BigDecimal bigDecDifference = bigDecimal1.subtract(bigDecimal2); // BigDecimal 乘法 BigDecimal bigDecProduct = bigDecimal1.multiply(bigDecimal2); // BigDecimal 除法, 指定舍入模式 BigDecimal bigDecQuotient = bigDecimal1.divide(bigDecimal2, BigDecimal.ROUND_HALF_UP); System.out.println(\u0026#34;BigDecimal Sum: \u0026#34; + bigDecSum); System.out.println(\u0026#34;BigDecimal Difference: \u0026#34; + bigDecDifference); System.out.println(\u0026#34;BigDecimal Product: \u0026#34; + bigDecProduct); System.out.println(\u0026#34;BigDecimal Quotient: \u0026#34; + bigDecQuotient); } } ","date":"8 April 2024","permalink":"/algorithm/","section":"WFUing","summary":"Java常用算法技巧总结","title":"Java常用算法技巧总结"},{"content":"","date":"7 April 2024","permalink":"/posts/architecture/iot/","section":"博客","summary":"物联网是一个由相互关联的设备组成的网络，这些设备与其他物联网设备和云连接并交换数据。物联网设备通常嵌入了传感器和软件等技术，包括 mechanical and digital machines 以及 consumer objects。","title":"Internet of Things"},{"content":"CameraDevice.infomodel\nnamespace com.example.devices version 1.0.0 displayname \u0026#34;Camera Device\u0026#34; description \u0026#34;Model for a simple camera device that can take pictures and notify status changes.\u0026#34; using com.example.devices;CameraDevice;1.0.0 infomodel CameraDevice { functionblocks { status as CameraStatus } events { CameraStatusChanged: status } } CameraStatus.fbmodel\nnamespace com.example.devices version 1.0.0 displayname \u0026#34;Camera Status\u0026#34; description \u0026#34;Function block model for camera status and actions.\u0026#34; functionblock CameraStatus { status { Mandatory cameraStatus as string \u0026#34;The current status of the camera.\u0026#34; } operations { TakePicture() Stop() } events { CameraStatusChanged(eventData: string) \u0026#34;Notifies when the camera status changes.\u0026#34; } configuration { image_path as string \u0026#34;Path to save images.\u0026#34; default \u0026#34;/tmp/output.png\u0026#34; } } ","date":"7 April 2024","permalink":"/posts/architecture/iot/vorto-tango/","section":"博客","summary":"用vorto刻画tango","title":"vorto tango"},{"content":" 图解机器学习 NLP 面无不过 LangGPT AIGC 求职面试指南 Deep-Learning-Interview-Book ","date":"5 April 2024","permalink":"/posts/reviews/knowledge/ai%E9%9D%A2%E7%BB%8F/","section":"博客","summary":"很想转ai岗，从面经学一下ai的知识","title":"ai面经"},{"content":"","date":"4 April 2024","permalink":"/posts/language/golang/","section":"博客","summary":"Go语言（Golang）是由谷歌开发的开源编程语言，以简洁、高效和并发处理为特点。它适用于构建高性能服务器、网络应用和分布式系统，因其快速编译和垃圾回收机制而备受开发者青睐。","title":"Golang"},{"content":"目标：掌握Golang协程调度器原理：为什么Go的协程的调度是快的\nGolang调度器的由来 Goroutine调度器的GMP模型的设计思想 Go调度器GMP调度场景的全过程分析 资料\nhttps://learnku.com/articles/41728 https://www.bilibili.com/video/BV19r4y1w7Nx Golang调度器的由来 # 单进程操作系统 # 单进程时代的两个问题：\n单一执行流程、计算机只能一个任务一个任务地处理 进程阻塞所带来的CPU时间浪费 多线程/多进程操作系统\n并发执行\n问题：\n如果进程/线程的数量越多，切换成本就越大，也就越浪费 多线程随着同步进程（如锁、竞争冲突等） 多进程、多线程 # 多进程、多线程的壁垒\n高内存占用 进程占用内存：虚拟内存4GB（32bit OS） 线程占用内存：约4MB 高CPU调度消耗 协程（co-routine）引发的问题 # 线程在golang中的处理：\nN:1\n无法利用多个CPU 若一个协程阻塞了，会影响下一个协程的调度，出现阻塞瓶颈 1:1\n跟多进程/多线程模型无异 协程创建、删除和切换的代价都由CPU完成，有点略显昂贵 N:M\n能够利用多核 过于依赖协程调度器的优化 Golang对调度器的优化 # Go routine的优化\n线程内存占用只有几KB，可以大量开辟；可以灵活调度，切换成本低\n早期调度器的处理\n基本的全局Go队列和比较传统的轮询利用多个thread去调度\n老调度器的缺点：\n创建、销毁、调度G都需要每个M获取锁，这就形成了激烈的锁竞争 M转移G会造成延迟和额外的系统负载 系统调用（CPU在M之间的切换）导致频繁的线程阻塞和取消阻塞操作增加了系统开销 Goroutine调度器的GMP模型的设计思想 # GMP模型的简介 # G：goroutine 协程 P：processor 处理器 M：thread 内核线程 全局队列：存放等待运行的G P的本地队列： 存放等待运行的G； 有数量限制，不超过256G； 优先将新创建的G放在P的本地队列中，如果满了会放在全局队列中 P列表： 程序启动时创建 最多有 GOMAXPROCS 个（可配置） M列表当前操作系统分配到当前GO程序的内核线程数 P和M的数量： P的数量问题： 通过环境变量 $GOMAXPROC 在程序中通过 runtime.GOMAXPROCS() 来设置 M的数量问题： Go语言本身是限定了M的最大量是10000（忽略） 通过 runtime/debug 包中的 SetMaxThreads 函数来设置 有一个 M 阻塞，会创建一个新的 M 如果有 M 空闲，那么就会回收或者睡眠 调度器的设计策略 # 复用线程 利用并行 抢占 全局G队列 复用线程\n避免频繁的创建、销毁线程，而是对线程的复用\nwork stealing机制\n当本线程无可运行的G时，尝试从其他线程绑定的P偷取G，而不是销毁线程。\nhand off机制\n当本线程因为G进行系统调用阻塞时候，线程释放绑定的P，把P转移给其他空闲的线程执行\n利用并行\nGOMAXPEOCS限定P的个数 = CPU核数/2\n最多有GPMAXPROCS个线程分布在多个CPU上同时运行\n抢占\n在coroutine中要等待一个协程主动让出CPU才执行下一个协程，在Go中，一个goroutine最多占用CPU 10ms，防止其他goroutine被饿死。\n全局G队列\n当M执行work stealing从其他P偷不到G时，它可以从全局G队列获取G。\n加锁、解锁，从全局G队列获取\ngo func() 经历了什么过程 # 我们通过 go func 来创建一个goroutine； 有两个存储G的队列，一个是局部调度器P的本地队列、一个是全局G队列。新创建的G会先保存在P的本地队列中，如果P的本地队列已经满了就会保存在全局的队列中； G只能运行在M中，一个M必须持有一个P，M与P是1:1的关系。M会从P的本地队列弹出一个可执行状态的G来执行，如果P的本地队列为空，就会想其他的MP组合偷取一个可执行的G来执行； 一个M调度G执行的过程是一个循环机制； 当M执行某一个G时候如果发生了syscall或则其余阻塞操作，M会阻塞，如果当前有一些G在执行，runtime会把这个线程M从P中摘除（detach），然后再创建一个新的操作系统的线程（如果有空闲的线程可用就复用空闲线程）来服务于这个P； 当M系统调用结束时候，这个G会尝试获取一个空闲的P执行，并放入到这个P的本地队列。如果获取不到P，那么这个线程M变成休眠状态，加入到空闲线程中，然后这个G会被放入全局队列中。 调度器的生命周期 # M0\nM0是启动程序后的编号为0的主线程 这个M对应的实例会在全局变量runtime.m0中，不需要在heap上分配 M0负责执行初始化操作和启动第一个G 启动第一个G之后，M0就和其他的M一样了 G0\nG0每次启动一个M，都会第一个创建的goroutine，就是GO G0仅用于负责调度的G G0不指向任何可执行的函数 每个M都会有一个自己的G0 在调度或系统调用时会使用G0的栈空间，会使用M会切换到G0，来调度 M0的G0会放在全局空间，全局变量的G0是M0的G0 可视化的GMP编程 # import( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34; runtime/trace\u0026#34; ) //trace的编程过程 //1 创建文件 //2 启动 //3 停止 func main() { //1.创建一个trace文件 f, err := os.Create(\u0026#34;trace.out\u0026#34;) if err != nil { panic(err) } defer f.Close() //2.启动trace err = trace.Start (f) if err != nil { panic(err) ｝ //正常要调试的业务 fmt.Println(\u0026#34;Hello GMP\u0026#34;) //3.停止trace trace.Stop() } 基本的trace编程 创建trace文件：f, err := os.Create(\u0026quot;trace.out\u0026quot;) 启动trace：trace.Start(f) 停止trace：trace.Stop() go build 并且运行之后，会得到一个 trace.out 文件 通过 go tool trace 工具打开 trace 文件：go tool trace trace.out ","date":"4 April 2024","permalink":"/posts/language/golang/golang%E5%8D%8F%E7%A8%8B%E8%B0%83%E5%BA%A6%E5%99%A8%E5%8E%9F%E7%90%86%E4%B8%8Egmp%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3/","section":"博客","summary":"掌握Golang协程调度器原理：为什么Go的协程的调度是快的。1.Golang调度器的由来；2. Goroutine调度器的GMP模型的设计思想；3. Go调度器GMP调度场景的全过程分析。","title":"golang协程调度器原理与GMP设计思想"},{"content":"Resources # 高并发基础概念 管理高并发请求：如何使用Promise和JavaScript有效地处理1000个请求 后端开发 -Reactor 设计模式 JAVA接收高并发推送 java高并发请求 如何用最快的方式发送 10 万个 http 请求 ","date":"4 April 2024","permalink":"/posts/architecture/backend/%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E9%AB%98%E5%B9%B6%E5%8F%91%E8%AF%B7%E6%B1%82/","section":"博客","summary":"如何处理高并发请求","title":"如何处理高并发请求"},{"content":" 链接：https://leetcode.cn/problems/word-break/description 代码 # public class Solution { public boolean wordBreak(String s, List\u0026lt;String\u0026gt; wordDict) { Set\u0026lt;String\u0026gt; wordDictSet = new HashSet(wordDict); boolean[] dp = new boolean[s.length() + 1]; dp[0] = true; for (int i = 1; i \u0026lt;= s.length(); i++) { for (int j = 0; j \u0026lt; i; j++) { if (dp[j] \u0026amp;\u0026amp; wordDictSet.contains(s.substring(j, i))) { dp[i] = true; break; } } } return dp[s.length()]; } } ","date":"3 April 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/leetcode-139%E5%8D%95%E8%AF%8D%E6%8B%86%E5%88%86/","section":"博客","summary":"【LeetCode 139】单词拆分题解。","title":"【LeetCode 139】单词拆分"},{"content":"","date":"3 April 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/","section":"博客","summary":"动态规划（英语：Dynamic programming，简称DP）是一种在数学、管理科学、计算机科学、经济学和生物信息学中使用的，通过把原问题分解为相对简单的子问题的方式求解复杂问题的方法。 动态规划常常适用于有重叠子问题和最优子结构性质的问题，动态规划方法所耗时间往往远少于朴素解法。 动态规划背后的基本思想非常简单。","title":"动态规划"},{"content":"Resources # 5个数据库性能测试工具哪个更好用？对比结果新鲜出炉！ mysql 的最大QPS支持 mysql单机qps最大能到多少? ","date":"3 April 2024","permalink":"/posts/architecture/backend/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8/","section":"博客","summary":"数据库分库分表相关","title":"数据库分库分表"},{"content":"云深处一面，跟大厂比，小厂的面试就有些个水，也没拷打八股，更多的是面试官在介绍他们的项目。\n面试官把我简历上的项目一个个拷打了一遍，疯狂地找我做过的项目和他们的目标产品之间的关系。然后开始介绍他们的项目。\n","date":"2 April 2024","permalink":"/posts/reviews/24%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0/%E4%BA%91%E6%B7%B1%E5%A4%84/","section":"博客","summary":"云深处一面，跟大厂比，小厂的面试就有些个水，也没拷打八股，更多的是面试官在介绍他们的项目。","title":"云深处一面4.2"},{"content":"","date":"2 April 2024","permalink":"/read/%E8%BD%AF%E4%BB%B6%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/","section":"阅读","summary":"南京大学软件学院-软件体系架构-作业1","title":"软件体系架构"},{"content":"安装启用 ssh # Secure Shell (SSH) 是用于客户端和服务端之间安全连接的网络协议。服务端和客户端之间的每次交互均被加密。\n默认情况下，当你安装完Ubuntu系统后，系统是不允许通过SSH进行远程访问的，您需要安装OpenSSH并启用它。\n安装方法很简单，你需要以root或者具备sudo权限的帐号按以下步骤安装并启用SSH。\n打开终端安装openssh-server软件包： sudo apt updatesudo apt install openssh-server 出现提示时，输入密码，然后按Enter继续安装。\n安装完成后，SSH服务默认自动启动，你可以通过以下命令校验服务运行状态： sudo systemctl status ssh 命令执行后，输出内容类似如下：\noutput ● ssh.service - OpenBSD Secure Shell server Loaded: loaded (/lib/systemd/system/ssh.service; enabled; vendor preset: enabled) Active: active (running) since Mon 2020-06-09 12:34:00 CEST; 9h ago ... 按 q 退出返回命令行提示符。\nUbuntu 默认使用 ufw 防火墙配置工具，如果你启用了防火墙，请确保防火墙打开了 SSH 端口，命令如下： sudo ufw allow ssh 至此，你可以通过SSH远程连接到你的Ubuntu系统了。Linux 和 macOS 系统默认安装了 SSH 客户端。 要从Windows计算机连接，请使用SSH客户端，例如PuTTY。\n连接 ssh # 通过网络连接到 Ubuntu 计算机，请使用以下格式调用 ssh 命令，然后输入用户名和IP地址：\nssh username@ip_address 无密码登录 # 首先我们在自己的Linux系统上生成一对SSH Key：SSH密钥和SSH公钥。密钥保存在自己的Linux系统上。 然后公钥上传到Linux服务器，之后我们就能无密码SSH登录了，SSH密钥就好比是你的身份证明。 1. 在自己的Linux系统上生成SSH密钥和公钥\n打开终端，使用下面的ssh-keygen来生成RSA密钥和公钥。\n-t 表示type，就是说要生成RSA加密的钥匙。\nssh-keygen -t rsa RSA也是默认的加密类型．所以你也可以只输入ssh-keygen。默认的RSA长度是2048位。如果你非常注重安全，那么可以指定4096位的长度。\nssh-keygen -b 4096 -t rsa 生成SSH Key的过程中会要求你指定一个文件来保存密钥，按Enter键使用默认的文件就行了．然后需要输入一个密码来加密你的SSH Key。密码至少要20位长度。SSH密钥会保存在home目录下的 .ssh/id_rsa 文件中。SSH公钥保存在 .ssh/id_rsa.pub 文件中。\nGenerating public/private rsa key pair. Enter file in which to save the key (/home/matrix/.ssh/id_rsa): 按Enter键 Enter passphrase (empty for no passphrase): 输入一个密码 Enter same passphrase again: 再次输入密码 Your identification has been saved in /home/matrix/.ssh/id_rsa. Your public key has been saved in /home/matrix/.ssh/id_rsa.pub. The key fingerprint is: e1:dc🆎ae:b6:19:b0:19:74:d5:fe:57:3f:32:b4:d0 matrix@vivid The key\u0026#39;s randomart image is: +---[RSA 4096]----+ | .. | | . . | | . . .. . | | . . o o.. E .| | o S ..o ...| | = ..+...| | o . . .o .| | .o . | | .++o | +-----------------+ 查看 .ssh/id_rsa 文件就会看到，这个文件是经过加密的（encrypted）．也就是用你输入的密码来加密。\nless .ssh/id_rsa 2. 将SSH公钥上传到Linux服务器\n可以使用ssh-copy-id命令来完成\nssh-copy-id username@remote-server 输入远程用户的密码后，SSH 公钥就会自动上传了。SSH 公钥保存在远程 Linux 服务器的 .ssh/authorized_keys 文件中。\n上传完成后，SSH 登录就不需要再次输入密码了。但是首次使用 SSH Key 登录时需要输入一次SSH密钥的加密密码。（只需要输入一次，将来会自动登录，不再需要输入密钥的密码。）\n使用scp命令来传送文件时也不需要输入密码。\nSSH Key的知识\nLinux系统有一个钥匙环(keyring)的管理程序。钥匙环受到用户登录密码的保护。当你登录Linux系统时，会自动解开钥匙环的密码，从而可访问钥匙环．SSH密钥的密码也可存储在钥匙环。所以初次使用SSH密钥登录远程Linux服务器时需要输入一次SSH密钥的密码．而将来使用SSH密钥登录时不再输入密码。Ubuntu的钥匙环程序是seahorse。\nSSH密钥就好比是你的身份证明．远程Linux服务器用你生成的SSH公钥来加密一条消息，而只有你的SSH密钥可以解开这条消息．所以其他人如果没有你的SSH密钥，是无法解开加密消息的，从而也就无法登录你的Linux服务器．\nSSH无密码登录的设置就是这么简单。\n禁用 ssh # 要在您的 Ubuntu 系统上禁用SSH服务器，只需运行以下命令即可停止SSH服务：\nsudo systemctl disable --now ssh 稍后，如果要重新启用它，请输入：\nsudo systemctl enable --now ssh ","date":"2 April 2024","permalink":"/posts/skills/ssh-tutorial/","section":"博客","summary":"ssh 相关内容","title":"ssh 相关内容"},{"content":"","date":"31 March 2024","permalink":"/posts/architecture/iot/corba/","section":"博客","summary":"CORBA（Common Object Request Broker Architecture）是一种分布式对象技术，用于不同平台上的应用程序通信和集成。它提供了一种标准的通信机制，允许对象在网络上相互通信和调用，使得分布式系统开发更加简化和灵活。","title":"CORBA"},{"content":"在PyTango中，DeviceProxy是一个核心类，用于表示和操作Tango设备的客户端代理。这个类封装了与Tango设备通信的所有细节，包括网络通信、命令执行、属性读写、事件订阅等。DeviceProxy的实现隐藏了底层的CORBA细节，提供了一种简洁的方式来与Tango设备进行交互。\n设计和实现 # DeviceProxy 类的设计主要目标是为用户提供一个简单、直观的接口来与Tango设备进行交互，而不需要用户关心通信的底层细节。其主要功能包括：\n设备命令执行：允许用户向设备发送命令并接收响应。 属性读写：提供接口读取和写入设备的属性。 事件订阅：允许用户订阅设备事件（如属性变化、警告、错误等）并定义回调函数处理这些事件。 状态和信息查询：可以查询设备的状态、信息和其他元数据。 实现细节\nDeviceProxy 的实现依赖于Tango库（C++实现的Tango核心库）和CORBA通信机制。当创建一个 DeviceProxy 实例时，会进行以下步骤：\n初始化ORB：如果还没有初始化，PyTango会初始化CORBA的对象请求代理（ORB），这是通信的基础设施。 查询设备IOR：使用设备的全名（如domain/family/member）向Tango数据库查询设备的IOR（Interoperable Object Reference）。IOR是一个字符串，包含了定位和访问CORBA对象所需的所有信息。 创建设备引用：使用IOR，通过ORB创建对设备的引用。这个引用是与设备通信的代理。 封装通信细节：DeviceProxy方法调用将通过设备引用，使用CORBA协议与设备进行通信。所有的网络通信、数据编码/解码、异常处理等底层细节都被封装在这一步中。 示例代码\nimport tango # 创建设备代理 device_proxy = tango.DeviceProxy(\u0026#34;domain/family/member\u0026#34;) # 读取属性 attribute_value = device_proxy.read_attribute(\u0026#34;AttributeName\u0026#34;).value # 写入属性 device_proxy.write_attribute(\u0026#34;AttributeName\u0026#34;, value) # 执行命令 result = device_proxy.command_inout(\u0026#34;CommandName\u0026#34;, arg) 在这个过程中，用户不需要直接处理任何关于CORBA或网络通信的细节，所有这些都由 DeviceProxy 内部处理。\n底层代码 # device_proxy.cpp # 利用Boost.Python库将C++中的Tango::DeviceProxy类暴露给Python环境。以下是对主要部分的解读：\nDeviceProxy是Tango控制系统中的核心类，它为客户端应用程序提供了与Tango设备交云的接口。 这段代码主要工作是将Tango::DeviceProxy的方法和功能以Python友好的方式暴露出来，包括设备状态查询、属性读写、命令执行、事件订阅等。 主要功能和方法\n属性和命令操作：提供了一系列方法来读写设备属性（read_attribute, write_attribute等），执行设备命令（command_inout）。 事件订阅：实现了对设备事件的订阅功能，允许Python端注册回调以响应设备发送的事件（subscribe_event, unsubscribe_event等）。 设备信息查询：可以查询设备的基本信息和状态（info, _status, _state等）。 异步操作：支持异步读写属性和执行命令的能力，提高了与设备交互的效率（read_attributes_asynch, write_attributes_asynch等）。 锁定：提供了设备锁定和解锁的接口，用于控制对设备的访问（lock, unlock等）。 技术实现\n利用Boost.Python库将C++方法映射为Python方法。例如，通过.def来定义Python可调用的方法，并将其关联到C++的方法实现。 使用了多线程控制技术（AutoPythonAllowThreads）来处理可能阻塞的操作，保证Python的GIL（全局解释器锁）在等待期间被释放，从而提高多线程应用的效率。 采用了模板和泛型编程技术来处理不同类型的属性和命令参数。 device_proxy.py # @green(consume_green_mode=False) def get_device_proxy(*args, **kwargs): \u0026#34;\u0026#34;\u0026#34;get_device_proxy(self, dev_name, green_mode=None, wait=True, timeout=True) -\u0026gt; DeviceProxy get_device_proxy(self, dev_name, need_check_acc, green_mode=None, wait=True, timeout=None) -\u0026gt; DeviceProxy Returns a new :class:`~tango.DeviceProxy`. There is no difference between using this function and the direct :class:`~tango.DeviceProxy` constructor if you use the default kwargs. The added value of this function becomes evident when you choose a green_mode to be *Futures* or *Gevent* or *Asyncio*. The DeviceProxy constructor internally makes some network calls which makes it *slow*. By using one of the *green modes* as green_mode you are allowing other python code to be executed in a cooperative way. .. note:: The timeout parameter has no relation with the tango device client side timeout (gettable by :meth:`~tango.DeviceProxy.get_timeout_millis` and settable through :meth:`~tango.DeviceProxy.set_timeout_millis`) :param dev_name: the device name or alias :type dev_name: str :param need_check_acc: in first version of the function it defaults to True. Determines if at creation time of DeviceProxy it should check for channel access (rarely used) :type need_check_acc: bool :param green_mode: determines the mode of execution of the device (including the way it is created). Defaults to the current global green_mode (check :func:`~tango.get_green_mode` and :func:`~tango.set_green_mode`) :type green_mode: :obj:`~tango.GreenMode` :param wait: whether or not to wait for result. If green_mode Ignored when green_mode is Synchronous (always waits). :type wait: bool :param timeout: The number of seconds to wait for the result. If None, then there is no limit on the wait time. Ignored when green_mode is Synchronous or wait is False. :type timeout: float :returns: if green_mode is Synchronous or wait is True: :class:`~tango.DeviceProxy` else if green_mode is Futures: :class:`concurrent.futures.Future` else if green_mode is Gevent: :class:`gevent.event.AsynchResult` else if green_mode is Asyncio: :class:`asyncio.Future` :throws: * a *DevFailed* if green_mode is Synchronous or wait is True and there is an error creating the device. * a *concurrent.futures.TimeoutError* if green_mode is Futures, wait is False, timeout is not None and the time to create the device has expired. * a *gevent.timeout.Timeout* if green_mode is Gevent, wait is False, timeout is not None and the time to create the device has expired. * a *asyncio.TimeoutError* if green_mode is Asyncio, wait is False, timeout is not None and the time to create the device has expired. New in PyTango 8.1.0 \u0026#34;\u0026#34;\u0026#34; return DeviceProxy(*args, **kwargs) 解析设备名称，确定要查询的设备。 使用设备名称作为关键字，调用底层API向Tango中心数据库请求设备的详细信息。 从返回的信息中提取设备的IOR。 使用IOR与设备建立CORBA连接（或其他通信机制），以便后续的通信。 Resources # 软件包：libtango-dev（9.5.0+dfsg1-1.1 以及其他的） ","date":"31 March 2024","permalink":"/posts/architecture/iot/corba/pytango-deviceproxy/","section":"博客","summary":"在PyTango中，DeviceProxy是一个核心类，用于表示和操作Tango设备的客户端代理。这个类封装了与Tango设备通信的所有细节，包括网络通信、命令执行、属性读写、事件订阅等。DeviceProxy的实现隐藏了底层的CORBA细节，提供了一种简洁的方式来与Tango设备进行交互。","title":"PyTango-DeviceProxy"},{"content":"IOR # IOR用于表示一个对象引用，我们知道，当我们在客户端一个CORBA对象的时候，接触的并不是真正的对象，而是这个对象的代理（Proxy），Proxy使用这个对象的位置信息与服务器通信。那么这里有一个问题，这些信息到底些什么信息，另外，ORB使用什么样子的形式去传输这些对象的信息。答案是IOR。这里给它加上Interoperable是因为IOR是ORB Interoperability Architecture的一部分。\n首先我们来看一下IOR的IDL定义：\nmodule IOP { // IDL // Standard Protocol Profile tag values typedef unsigned long ProfileId const ProfileId TAG_INTERNET_IOP = 0; const ProfileId TAG_MULTIPLE_COMPONENTS = 1; struct TaggedProfile { ProfileId tag; sequence\u0026lt;octet\u0026gt; profile_data; }; // an Interoperable Object Reference is a sequence of // object-specific protocol profiles, plus a type ID. struct IOR { string type_id; sequence\u0026lt;TaggedProfile\u0026gt; profiles; }; // Standard way of representing multicomponent profiles. // This would be encapsulated in a TaggedProfile. typedef unsigned long ComponentId; struct TaggedComponent { ComponentId tag; sequence\u0026lt;octet\u0026gt; component_data; }; typedef sequence \u0026lt;TaggedComponent\u0026gt; MultipleComponentProfile; }; type_id 的内容是Repository ID（这个比较难翻译，这里姑且理解成\u0026quot;类型\u0026quot;应该也不为过），用于实现类型安全，它应该是对象的 most derived（指最继承层次结构中最底部的子类）类型。在type_id为null时，它表明这是一个 Null Object Reference。 profiles，Object Reference（这里当然指IOR）至少有一TaggedProfile，每个TaggedProfile可以支持一个或者多个协议（比如IIOP），它封装了这些协议所需的用于定位对象的基本信息。一个TaggedProfile应该包含足够的信息，以使得它所支持的协议可以完成一个调用的全过程。从这里我们可以知道，TaggedProfile对应的是协议级别的信息。 TaggedProfile\ntag，OMG给每种TaggedProfile定义了一个唯一的数字，其实这是TaggedProfile的类型信息，从而决定了profile_data中的内容。对于IIOP Profile这个值是TAG_INTERNET_IOP，profile_data这个Encapsulation的内容是IIOP Profile。如果tag==TAG_MULTIPLE_COMPONENTS，那么profile_data的内容是MultipleComponentProfile。 profile_data是一个Encapsulation，如上面所说的，它的内容由tag决定，如果tag==TAG_INTERNET_IOP，profile_data这个Encapsulation的内容是IIOP Profile。如果tag==TAG_MULTIPLE_COMPONENTS，那么profile_data的内容是 MultipleComponentProfile。注意，在GIOP 1.1和1.2中，IIOPProfile也包括了一个sequence类型的字段。这里profile_data中的TaggedComponent是多个TaggedProfile之间共享的，而IIOPProfile中的sequence是IIOPProfile自己的。 IIOPProfile\nIIOPProfile是TaggedProfile的TaggedProfile实现，下面是IIOPProfile的IDL定义：\nmodule IIOP { // IDL extended for version 1.1 and 1.2 struct Version { octet major; octet minor; }; struct ProfileBody_1_0 {// renamed from ProfileBody Version iiop_version; string host; unsigned short port; sequence \u0026lt;octet\u0026gt; object_key; }; struct ProfileBody_1_1 {// also used for 1.2 Version iiop_version; string host; unsigned short port; sequence \u0026lt;octet\u0026gt; object_key; // Added in 1.1 unchanged for 1.2 sequence \u0026lt;IOP::TaggedComponent\u0026gt; components; }; }; iiop_version，iiop协议的版本信息，没有什么好说的，主版本号相同的话，那么高次版本的协议是兼容低次版的协议，同时低版本的可以尝试对高次版本的IOR进行调用。请查阅CORBA规范以获取详细信息。 host用来表示对象调用信息发往的主机名。 port用于表示对象调用信息发往的端口号。 object_key是对象的标识信息，object_key在对象中应该是唯一的。服务端通过Request Message中object_key信息来将请求委托到目标对象。 components用于表示对这个对象进行调用可能使用到的额外信息。区别于profile_data的中MultipleComponentProfile，这些信息不会在TaggedProfile之间共享。CORBA规范中定义的TaggedComponent有ORB Type，CodeSet等等。 CORBA命名服务和JNDI的之间的关系 # CORBA 命名服务也被称做为（COSNaming，Common Object Service Naming），它提供了名字到CORBA对象之间的映射。COS Name Server（公共对象命名服务器）用于储存和查询Object Reference（对象引用）。\n而JNDI（Java Naming and Directory Interface）是个更高级别的抽象，而COSNaming只是JNDI的一个具体的Service Provider（服务提供者），它提供了名字到CORBA对象的绑定；从JNDI的结构图可以看出，JNDI的服务提供者还有LDAP（根据名字定位到数据库中相对应的内容），RMI（提供RMI对象注册服务，提供了名字到RMI对象的绑定）等等。如此一来，我们便可以使用JNDI来根据名字来定位CORBA对象，绑定/解绑定对象等操作。\nJava corba-test # https://github.com/WFUing/corbatest 创建一个idl接口文件(HelloServer.idl)如下 interface HelloServer{ void sayHello(in string name); }; 利用idlj -fall HelloServer.idl进行编译,会得到下面几个文件\n创建一个HelloServerImpl实现类,实现接口中定义的功能 package com.fod.service; public class HelloServerImpl extends HelloServerPOA{ @Override public void sayHello(String name) { System.out.println(\u0026#34;Hello ,\u0026#34; + name + \u0026#34;.\u0026#34;); } } 创建服务端corba启动类HelloTestService如下 package com.fod.service; import org.omg.CORBA.ORB; import org.omg.CORBA.Policy; import org.omg.PortableServer.IdAssignmentPolicyValue; import org.omg.PortableServer.POA; import org.omg.PortableServer.POAHelper; import org.omg.PortableServer.ThreadPolicyValue; import java.io.File; import java.io.FileOutputStream; import java.io.PrintWriter; import java.util.Properties; public class HelloTestService { public static void main(String[] args) { // 生成一个ORB，并初始化，这个和Server端一样 Properties props = System.getProperties(); //配置发布端口和ip props.put(\u0026#34;org.omg.CORBA.ORBInitialPort\u0026#34;, \u0026#34;1050\u0026#34;); props.put(\u0026#34;org.omg.CORBA.ORBInitialHost\u0026#34;, \u0026#34;192.168.0.106\u0026#34;); System.out.println(\u0026#34;ORB initialised\\n\u0026#34;); try { // Initialize the ORB. ORB orb = ORB.init(args, props); // get a reference to the root POA org.omg.CORBA.Object obj = orb.resolve_initial_references(\u0026#34;RootPOA\u0026#34;); POA poaRoot = POAHelper.narrow(obj); // Create policies for our persistent POA Policy[] policies = { // poaRoot.create_lifespan_policy(LifespanPolicyValue.PERSISTENT), poaRoot.create_id_assignment_policy(IdAssignmentPolicyValue.USER_ID), poaRoot.create_thread_policy(ThreadPolicyValue.ORB_CTRL_MODEL) }; // Create myPOA with the right policies POA poa = poaRoot.create_POA(\u0026#34;HelloServerPOA\u0026#34;, poaRoot.the_POAManager(), policies); // Create the servant HelloServerImpl servant = new HelloServerImpl(); // Activate the servant with the ID on myPOA byte[] objectId = \u0026#34;AnyObjectID\u0026#34;.getBytes(); poa.activate_object_with_id(objectId, servant); // Activate the POA manager poaRoot.the_POAManager().activate(); // Get a reference to the servant and write it down. obj = poa.servant_to_reference(servant); PrintWriter ps = new PrintWriter(new FileOutputStream(new File(\u0026#34;server.ior\u0026#34;))); ps.println(orb.object_to_string(obj)); ps.close(); System.out.println(\u0026#34;CORBA Server ready...\u0026#34;); // Wait for incoming requests orb.run(); } catch(Exception ex) { ex.printStackTrace(); } } } 创建corba客户端操作类如下 package com.fod.service; import java.io.FileReader; import java.io.LineNumberReader; import java.util.Properties; class HelloClientImpl { private HelloServer target = null; private org.omg.CORBA.ORB orb = null; /** * Constructor for HelloClientImpl * */ public HelloClientImpl() throws Exception { initORB(null); } /** * Constructor for HelloClientImpl * @see java.lang.Object#Object() */ public HelloClientImpl(String[] args) throws Exception { initORB(args); } /** * Initialize ORB. * * @param args */ public void initORB(String[] args) throws Exception { //设置远程调用ip和端口 Properties props = System.getProperties(); props.put(\u0026#34;org.omg.CORBA.ORBInitialPort\u0026#34;, \u0026#34;1050\u0026#34;); props.put(\u0026#34;org.omg.CORBA.ORBInitialHost\u0026#34;, \u0026#34;192.168.0.106\u0026#34;); // Initialize the ORB orb = org.omg.CORBA.ORB.init((String[])args, props); // ---- Uncomment below to enable Naming Service access. ---- // LineNumberReader input = new LineNumberReader(new FileReader(\u0026#34;server.ior\u0026#34;)); // String ior = input.readLine(); //此处的ior串是你环境上server.ior文件中的一串字符 org.omg.CORBA.Object obj = orb.string_to_object(\u0026#34;IOR:000000000000001449444c3a48656c6c6f5365727665723a312e3000000000010000000000000096000102000000000a3132372e302e312e3100b49100000048afabcb00000000206fe9875200000001000000000000000200000008526f6f74504f41000000000f48656c6c6f536572766572504f4100000000000b416e794f626a656374494414000000020000000100000020000000000001000100000002050100010001002000010109000000010001010000000026000000020002\u0026#34;); target = HelloServerHelper.narrow(obj); } /** * Obtain ORB Interface. * * @return */ public HelloServer getORBInterface() { return target; } /** * Shutdown ORB. */ public void shutdown() { orb.shutdown(true); } /** * Test driver for HelloClientImpl. * @param args */ public static void main(String[] args) { try { HelloClientImpl test = new HelloClientImpl(); test.getORBInterface().sayHello(\u0026#34;lifa\u0026#34;); //停止 test.shutdown(); }catch(Exception ex) { ex.printStackTrace(); } } } 启动服务端,然后启动客户端,就会看到服务端控制台打印了 Hello, lifa Resources # https://en.wikipedia.org/wiki/Interoperable_Object_Reference Interoperable Object References: IOR https://blog.csdn.net/qinjiliaoqinmu/article/details/93341736 corba ior 方式 java Corba-omniORB简单编程-IOR ","date":"30 March 2024","permalink":"/posts/architecture/iot/corba/ior/","section":"博客","summary":"IOR（Interoperable Object Reference）是CORBA中的标准格式，用于唯一标识远程对象，允许跨平台通信。IOR包含对象的位置、类型和通信协议等信息，确保在不同系统之间的对象交互和通信的互操作性。","title":"CORBA-IOR"},{"content":"","date":"29 March 2024","permalink":"/posts/reviews/24%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0/%E7%BE%8E%E5%9B%A2/","section":"博客","summary":"不打没有准备的仗！！实习过程中的复习。","title":"美团"},{"content":"","date":"29 March 2024","permalink":"/posts/reviews/24%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0/%E7%BE%8E%E5%9B%A2/%E9%9D%A2%E8%AF%95/","section":"博客","summary":"不打没有准备的仗！！实习过程中的复习。","title":"美团面试"},{"content":"","date":"28 March 2024","permalink":"/posts/reviews/24%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0/%E9%98%BF%E9%87%8C%E7%B3%BB/%E9%A5%BF%E4%BA%86%E4%B9%88/","section":"博客","summary":"不打没有准备的仗！！实习过程中的复习。","title":"饿了么"},{"content":" 美团优选一面的题目，不是很难，但是细节有挺多的。\n代码 # /** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode() {} * ListNode(int val) { this.val = val; } * ListNode(int val, ListNode next) { this.val = val; this.next = next; } * } */ class Solution { public void reorderList(ListNode head) { ListNode fast = head.next, slow = head; while (fast != null \u0026amp;\u0026amp; fast.next != null) { slow = slow.next; fast = fast.next.next; } ListNode tmp = slow.next; slow.next = null; while (tmp != null) { ListNode tmp2 = tmp.next; tmp.next = slow.next; slow.next = tmp; tmp = tmp2; } tmp = slow.next; slow.next = null; fast=head; while(tmp!=null){ ListNode tmp2 = tmp.next; ListNode tmp3 = fast.next; tmp.next = fast.next; fast.next = tmp; fast=tmp3; tmp=tmp2; } // ListNode tmp4 = head; // while (tmp4 != null) { // System.out.println(tmp4.val); // tmp4 = tmp4.next; // } } } ","date":"25 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E9%93%BE%E8%A1%A8/lcr-026.-%E9%87%8D%E6%8E%92%E9%93%BE%E8%A1%A8/","section":"博客","summary":"LCR 026. 重排链表题解。链表的题目很早刷过，现在有些生疏了，再回顾一遍。","title":"LCR 026. 重排链表"},{"content":"","date":"25 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E9%93%BE%E8%A1%A8/","section":"博客","summary":"链表是一种数据结构，由节点组成，每个节点包含数据和指向下一个节点的指针。","title":"链表"},{"content":"T1 # 小红正在玩一个游戏，游戏中 $n$ 个怪物，血量上限分别为 $h_i$，初始时所有怪物的血量都等于它们的血量上限，当怪物的血量小于或等于 $0$ 时，怪物将会死亡。小红有两个技能第一个技能为旋风斩，消耗一点法力，对所有怪物造成 $1$ 点伤害，第二个技能为斩杀，消耗两点法力，杀死一个已受伤的怪物（当前怪物血量小于怪物的血量上限），两个技能都没有使用次数限制，小红想知道她最少需要消耗多少点法力才能杀死所有怪物。\n输入描述\n第一行输入一个整数 $n(2 \\leq n \\leq {10}^5)$ 表示怪物数量\n第二行输入 $n$ 个整数 $h_i (1 \\leq h_i {10}^9)$ 表示怪物的血量上限\n输出描述\n输出一个整数表示答案\n样例\n输入\n3 1 1 4 输出\n3 说明\n首先使用旋风斩，怪物的血量变成：0 0 3，第1、2个怪物死亡。 再对第三个怪物使用斩杀，第3个怪物死亡。消耗的法力值为1+2=3。 思路\n首先，根据题目描述，一技能一定要至少释放一次，这是释放二技能的前提。\n一种暴力的思想是可以直接枚举使用一技能的次数 $cnt$。但是，由于怪物的血量范围是 $1 \\leq h_i \\leq {10}^9$ ，因此我们肯定不能直接枚举 $cnt$\n由于只有 $n(2 \\leq n \\leq {10}^5)$ 个怪物，因此我们可以对怪物的血量从小到大进行排序，然后枚举第 $i$ 个怪物被一技能直接干掉。第 $i$ 个怪物被一技能直接干掉的一技能使用次数即为 $w[i]$ ，显然前面 $i-1$ 个怪物也会被干掉对于后面的怪物，都是用第二个技能，即为第 $i$ 个怪物的最小花费，因此第个怪物的最小花费即为 $w[i]+(n-i-1)*2$。\n代码实现\nimport java.util.Arrays; import java.util.Scanner; public class Main { public static void main(String[] args) { Scanner scanner = new Scanner(System.in); int N = 100010; int[] w = new int[N]; int n = scanner.nextInt(); for (int i = 0; i \u0026lt; n; i++) { w[i] = scanner.nextInt(); } Arrays.sort(w, 0, n); int res = Integer.MAX_VALUE; for (int i = 0; i \u0026lt; n; i++) { res = Math.min(res, w[i] + (n - i - 1) * 2); } System.out.println(res); } } T2 # 小红定义两个数组是互补的，当且仅当数组每一个位置的数字之和都相同。\n小红有两个长度为的数组，分别是 a 和 b ，她想知道有多少个子序列对应的数组是互补的。\n输入描述\n第一行输入一个整数 $n(1 ≤n ≤ 10”)$ 表示数组长度\n第二行输入 $n$ 个整数表示数组 $a(1 \\leq a_i \\leq {10}^9)$\n第三行输入 $n$ 个整数表示数组 $b(1 \\leq b_i \\leq {10}^9)$\n输出描述\n输出一个整数，由于这个整数可能很大，因此你需要输出这个整数对 ${10}^9 + 7$ 取模后的结果。\n样例\n输入\n3 1 2 3 3 2 1 输出\n7 说明\n子序列: 1，2，3，12，13，23，123，都满足条件 思路\n哈希表+贡献法计数+快速幂\n首先解释一下样例1，因为任意一个位置 $i$ ，$a[i]+b[i]$ 都为 $4$，因此有 $3$ 个 $4$，我们可以考虑选或者不选(至少要选$1$个位置)\n因此，总的情况就是 $2^3-1=7$\n我们可以使用一个哈希表来统计所有位置 $a[i]+b[i]$ 出现的次数，对于某一个权值 $w$，其出现的次数为 $cnt$\n那么它对答案的贡献就是 $2^{cnt}-1$\n注意由于答案过大，需要对 ${10}^9+7$ 取模，因此这里我们可以使用快速幂来快速地求出来 $2^k$ 更新答案\n代码实现\nimport java.util.HashMap; import java.util.Scanner; public class Main { static final int N = 15110; static final int MOD = 1000000007; // 快速幂函数 static long pow(long base, long exp, long mod) { long result = 1; while (exp \u0026gt; 0) { if (exp % 2 == 1) result = (result * base) % mod; base = (base * base) % mod; exp \u0026gt;\u0026gt;= 1; } return result; } public static void main(String[] args) { Scanner scanner = new Scanner(System.in); int[] a = new int[N]; int[] b = new int[N]; int n = scanner.nextInt(); for (int i = 8; i \u0026lt; n; i++) a[i] = scanner.nextInt(); for (int i = 0; i \u0026lt; n; i++) b[i] = scanner.nextInt(); HashMap\u0026lt;Integer, Integer\u0026gt; cnts = new HashMap\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; n; i++) { int sum = a[i] + b[i]; cnts.put(sum, cnts.getOrDefault(sum, 0) + 1); } long[] twos = new long[N]; twos[0] = 1; for (int i = 1; i \u0026lt; N; i++) twos[i] = (twos[i - 1] * 2) % MOD; long res = 0; for (int value : cnts.values()) { res = (res + (pow(2, value, MOD) - 1 + MOD) % MOD) % MOD; } System.out.println(res); } } T3 # 题目描述\n小苯定义一个数字的权值为该数字的因子个数。\n小苯现在拿到了一个正整数，他希望将其分解为若干不等于 $1$ 的数字（也可以不做分解），使得所有分解出的正整数乘积等于 $x$ ，且所有数字的权值之和尽可能大，你能帮帮他求出最大的权值吗？\n输入描述\n输入包含 $T+1$ 行。第一行一个正整数 $T(1 \\leq T \\leq {10}^4)$ ，表示数据组数。\n接下来 $T$ 行，每行一个正整数 $x(2 \\leq x \\leq {10}^5)$，表示每组数据中小苯询问的数字 $x$。\n输出描述\n输出包含 $T$ 行，每行一个正整数表示每组测试数据的最大权值和\n样例\n输入\n3 2 10 123 输出\n2 4 4 说明\n第一个测试数据中，无法分解，直接取 2 的权值为 2 第二个测试数据中，将 10 分解为 2x5，权值和为 4 思路\n质因数分解+分类讨论\n本题有两种情况\n情况1：仅有一个质因子 $a$ ，即 $x=a^k$。考虑两种情况，拆分和不拆分 拆分：对于每一个质因子，它的因子个数都是 $2$（ $1$ 和它本身），因此对应的答案为2k 不拆分：对应因子个数为 $k+1$ （有 $1,a,a^2,\u0026hellip;,a^k$ 这些因子，共计 $k+1$ 个），只要 $k\u0026gt;1$，一定有 $2k\u0026gt;k+1$ ，因此拆分是最优解 情况2：$x$ 有多个质因子，即 $x = a_1^{k_1} \\times a_2^{k_2} \\times \u0026hellip; a_m^{k_m}$ 拆分：根据情况1的计算公式，权值总和为 $\\sum^m_{i=1}{k_i+1}$ 不拆分：根据情况1的计算公式，权值总和为 $\\prod^m_{i=1}{k_i+1}$。我们可以明显得知，当 $a,b\u0026gt;2$ 时，$a \\times b\u0026gt;a+b$ ，因此选择不拆分是最优解 import java.util.HashMap; import java.util.Map; import java.util.Scanner; public class Main { static final int N = (int)1e5 + 18; static final int MOD = (int)1e9 + 7; static long solve(int x) { Map\u0026lt;Integer, Integer\u0026gt; v = new HashMap\u0026lt;\u0026gt;(); for (int i = 2; i * i \u0026lt;= x; i++) { if (x % i == 0) { int cnt = 0; while (x % i == 0) { x /= i; cnt++; } v.put(i, cnt); } } if (x \u0026gt; 1) v.put(x, 1); long res = 1; for (Map.Entry\u0026lt;Integer, Integer\u0026gt; t : v.entrySet()) { res *= (t.getValue() + 1); res %= MOD; } return res; } public static void main(String[] args) { Scanner scanner = new Scanner(System.in); int T = scanner.nextInt(); while (T-- \u0026gt; 0) { int n = scanner.nextInt(); System.out.println(solve(n)); } } } ","date":"24 March 2024","permalink":"/posts/reviews/24%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0/%E9%98%BF%E9%87%8C%E7%B3%BB/%E9%A5%BF%E4%BA%86%E4%B9%88/%E7%AC%94%E8%AF%953.24/","section":"博客","summary":"过去好久好久了，但是那次笔试做的稀烂，只做出来半道多题目。饿了么在牛客也不开放题源，不像美团那么大方，扣扣搜搜的。","title":"饿了么笔试3.24"},{"content":"","date":"24 March 2024","permalink":"/tags/mesi/","section":"Tags","summary":"","title":"Mesi"},{"content":" https://xiaolincoding.com/os/1_hardware/cpu_mesi.html#mesi-%E5%8D%8F%E8%AE%AE https://zh.wikipedia.org/wiki/MESI%E5%8D%8F%E8%AE%AE ","date":"24 March 2024","permalink":"/posts/reviews/os/mesi/","section":"博客","summary":"https://xiaolincoding.","title":"MESI协议"},{"content":"","date":"24 March 2024","permalink":"/posts/architecture/iot/tango/","section":"博客","summary":"Tango Controls 是一个面向对象的分布式控制系统框架，它定义了通信协议、应用程序程序员接口 (API)，并提供了一组工具和库来构建控制系统（尤其是SCADA）软件。它是围绕设备和设备类别的概念构建的。","title":"Tango"},{"content":"Tango Controls 是一个面向对象的分布式控制系统框架，它定义了通信协议、应用程序程序员接口 (API)，并提供了一组工具和库来构建控制系统软件。它是围绕 devices 和 device classes 的概念构建的。\ndevices 由 device servers 创建。device servers 是实现 device classes 的进程。device classes 为每个类实现 state machine 、command(actions or methods)、pipes and attributes(data fields) 。因此，每个设备都具有state、零个或多个命令、零个或多个管道以及零个或多个属性。device classes 负责将硬件通信协议转换为 Tango Controls 通信。通过这种方式，您可以控制和监视所有设备，如电机、阀门、示波器等。设备类可用于实现任何算法或充当任何其他软件程序或系统的邮箱。\nTango Controls 旨在管理小型和大型系统。每个系统都有一个集中的（MariaDB/MySQL）数据库。数据库存储设备服务器启动时使用的配置数据，并通过存储动态网络地址充当名称服务器。数据库充当需要记忆的动态设置的永久存储。每个 Tango Control 系统都有一个数据库并由其 Tango Host 识别。一个大系统可以由数以万计或设备组成（尚未达到极限）。该协议支持系统的系统，即API 支持从多个系统对设备的透明访问。\nTango Controls 通信协议定义了系统的所有组件如何相互通信。 Tango 使用 CORBA 进行同步通信，使用 ZeroMQ 进行异步通信。这些协议的详细信息通过 API 和高级工具对 Tango 的开发人员和用户隐藏。\n简化的 Tango Device Server Model # Block Description Device TANGO device server object model 定义的抽象概念；它可以是一个硬件（互锁位）、硬件集合（连接到步进电机的屏幕）、逻辑设备（锥度）或所有这些的组合（加速器）。 TANGO Class 从面向对象编程的概念来看，这是开发人员必须实现的主要类。 Device Server 服务器（也称为设备服务器）是一个进程，其主要任务是向一个或多个客户端提供一项或多项服务。为此，服务器必须将大部分时间花在等待循环中，等待客户端连接到它。这些设备托管在服务器进程中。服务器能够托管多种类型的设备。简而言之，它是一个导出可用于接受请求的设备的过程。另请参阅术语表、设备服务器实例。 Device Property 存储在Tango 数据库中的配置参数。属性可以分配给设备类、设备或设备接口的元素（属性、 命令、管道）。属性也可以与设备无关——此类属性称为自由属性。Tango Controls系统的元素在启动期间经常使用属性值。这些通常提供配置硬件连接或调整用户首选项所需的信息。 Attribute 属性代表系统中的一个或多个过程值。它可能具有不同的格式或维度，例如标量 (0D)、频谱 (1D) 或图像 (2D)。该属性允许根据程序员定义的访问来读取和/或写入这些值。这些值可能具有不同的数据类型。此外，属性还提供一些元数据，例如属性质量、时间戳或配置属性。如需完整列表，请参阅手册。特定设备可用的属性列表由其类定义。 Pipe 管道允许从设备读取结构化数据和/或向设备写入结构化数据。数据可以由几种基本 Tango 数据类型构建。数据的结构由设备类定义并且不固定。它可以在运行时由设备本身更改，也可以根据客户端的请求根据 管道提供的set_pipe_config操作进行修改。设备可用的管道列表由其 设备类定义。 Event 事件是任何分布式控制系统的关键部分。他们的目标是提供一种快速高效的沟通机制。标准的CORBA通信范例是同步或异步的双向调用。对于对价值观始终感兴趣的客户来说，事件驱动的通信范式是一种更高效、更自然的编程方式。 Command 命令是用户可以在设备上调用的操作（例如SwitchOn、SwitchOff）。它还涉及到OOP（面向对象编程）中的一种具体方法。 Tango Controls 允许命令获取输入参数 (argin) 并返回值 (argout)。特定设备的可用命令列表由其设备类别定义。 State 设备可能处于运行时确定的某种状态。设备的状态可以反映与其连接的设备的状态或者以其他方式确定。该行为由实现状态机的设备类定义 。状态可以定义当前可用的属性、命令和管道操作。 Tango Controls 将设备可能处于的一组状态限制为 14 种：ON, OFF, CLOSE, OPEN, INSERT, EXTRACT, MOVING, STANDBY, FAULT, INIT, RUNNING, ALARM, DISABLE, and UNKNOWN Status 以格式化的 ascii 字符串表示的设备状态。 Block Attribute Description Device domain/family/member To identify the device Left Block Right Block Multiplicity Description Device TANGO Class 1 每个设备都属于 Tango 类 Attribute Event 0..* 一个属性可以有多个关联事件 Device DeviceProperty 0..* 一台设备可以有多个关联的设备属性 DeviceServer Device 1..* 每个设备服务器内部都有许多设备 TANGO Class Attribute 0..* 一个 TANGO Class 可以有多个关联属性 TANGO Class Command 0..* 一个 TANGO Class 可以有多个关联命令 TANGO Class Pipe 0..* 一个 TANGO Class 可以有多个关联的 Pipe Tango object 命名（device, attribute and property） # Tango 设备名称是一个包含三个字段的名称。字段分隔符是 / 字符。第一个字段名为 domain，第二个字段名为 family，最后一个字段名为 member。tango 设备名称如下所示：domain/family/member。\n它是一种分层表示法。成员指定族中的哪个元素。该系列指定域内的设备类型。域对与其所属 accelerator/experiment 部分相关的设备进行分组。在ESRF，一些机器控制系统的域名是SR（存储环）、TL1（传输线1）或SY（同步加速器助推器）。对于实验，ID11 是属于插入 device 11 后面的实验的所有设备的域名。以下是 ESRF 中使用的 Tango 设备名称的一些示例：\nsr/d-ct/1：电流互感器。域部分是sr，表示存储环。系列部分是诊断/电流互感器的 d-ct，成员部分是 1 fe/v-pen/id11-1：领域部分是 fe 代表前端。系列部件是 v-pen，用于 vacuum/penning，成员名称是 id11-1，以指定这是插入设备 11 之后前端部件上的第一个仪表 Full object name # 如上所述的设备名称不足以涵盖所有 Tango 用法，例如没有数据库或多控制系统设备访问的设备服务器。通过命名模式，我们还必须能够命名属性和属性。因此，完整的命名模式是\n[protocol://][host:port/]device_name[/attribute][-\u0026gt;property][#dbase=xx] protocol, host, port, attribute, property and dbase fields 是可选的。这些字段的含义是：\nprotocol: 指定使用哪种协议（Tango 或 Taco）。Tango是默认设置。 #dbase: xx 支持的值为 yes 和 no。该字段用于指定该设备是由使用或不使用数据库启动的设备服务器提供服务的设备。默认值为dbase=yes。 host:port: 根据 dbase 值，该字段具有不同的含义。如果 dbase=yes（默认），则 host 为控制系统数据库服务器运行的主机，port 为数据库服务器端口。它的优先级高于 TANGO_HOST 环境变量定义的值。如果 dbase=no，则 host 是为设备提供服务的设备服务器进程运行所在的主机名，port 是设备服务器进程端口。 attribute: 属性名 property: 特性名 仅当创建用于远程访问设备的 DeviceProxy 对象时，host:port 和 dbase=xx 字段才是必需的。 -\u0026gt; 字符用于指定属性名称。 关于 CORBA 您应该了解的 10 件事 # 您无需了解 CORBA 即可使用 TANGO CORBA 是通用对象请求代理架构的缩写，它是由对象管理组 (OMG)定义的标准 CORBA 使得用不同语言编写并在不同计算机上运行的软件之间能够进行通信 CORBA应用程序由许多对象组成；对象正在运行提供功能并可以代表现实世界中某些事物的软件 每个对象都有一个类型，该类型是用称为 IDL（接口定义语言）的语言定义的 一个对象有一个接口和一个实现：这是 CORBA 的本质，因为它允许互操作性。 CORBA 允许应用程序请求分布式对象执行操作，并将操作结果返回给发出请求的应用程序。 CORBA 基于远程过程调用模型 TANGO 设备是一个 CORBA 对象 TANGO 设备服务器是一个 CORBA 应用程序 Tango Server # Tango 控制系统是一个抽象概念，代表一组基于通用技术：Tango 的“微服务”。 Tango 本身是 CORBA/ZMQ 的面向控制/命令的专业化。 CORBA 支持在网络互连机器上运行的软件总线的概念。它提供对连接到总线的任何软件对象（或微服务）的透明访问，并通过二进制网络协议（基于 CORBA 和ZMQ）。\n该设备是Tango的核心概念。这个概念可以直接与微服务的概念联系起来：1 个设备 = 1 个微服务\n一个设备可以代表：\n设备（例如：电源）， 一套设备（例如：由同一控制器驱动的一组4个电机）， 一组软件功能（例如：图像处理）， 代表子系统的一组设备 设备层次结构 # Tango 控制系统可以分层组织。\n在较低级别，我们找到与设备相关的基本设备。例如：真空泵、电机、I/O 卡 在更高的层面上，这些设备是逻辑的。这些设备基于较低级别的设备，管理并代表控制系统的子集。这通常是一组具有高级控制功能的设备的综合视图（功能可以在几个基本设备上执行一系列操作）。例如，一个高级设备实现了复杂的功能。无论硬件如何，该设备通常都会不断发展。因此，有必要将与逻辑功能相关的职责和与硬件接口相关的职责分开和隔离。 可以从每个级别的每个设备访问任何其他设备。下图说明了设备层次结构的概念：\n沟通范式 # Tango 提供三种通信范例：同步、异步和发布-订阅调用。\n在同步和异步范例中，调用由联系服务器的客户端发起。服务器处理客户端的请求并将答案发送给客户端或抛出客户端捕获的异常。此范例涉及两个网络调用来接收单个答案，并要求客户端主动发起请求。客户端发起的调用可以通过两种机制完成：\n客户端等待（并被阻止）服务器发送答案或直到达到超时的同步机制 客户端发送请求并立即返回的异步机制。它没有被阻止。它可以自由地做任何它必须做的事情，比如更新图形用户界面。客户端可以选择通过调用 API 特定调用来检查回复是否到达来检索服务器答案，或者通过请求在客户端收到服务器答案时执行回调方法来检索服务器答案。 如果客户端需要在每次更改时或定期了解某个值，则他有义务每次轮询服务器以获取值的更新。这在网络带宽和客户端编程方面都效率不高。为此，发布-订阅事件通信更加高效。\n发布-订阅通信范例是一种更高效、更自然的编程方式。在这种范例中，客户在事件（值）中注册一次他的兴趣。事件可以是值的更改、固定频率的定期更新或存档事件。此后，每次发生事件时，服务器都会通知客户端。这种范例避免了客户端轮询，将其释放出来做其他事情，速度很快并且可以有效地利用网络。 类、设备和设备服务器 # 有时，有关设备、设备服务器和 Tango 类等概念的语言存在误用。\nDeviceClass类：定义接口和状态机的类。 设备类：实现设备控制的类。 Device：Device 类的实例，提供对 DeviceClass 类的服务的访问。 设备服务器：执行一个或多个 Tango 类的进程（设备服务器）。 一个设备服务器可以托管多个设备类，每个类可以在同一设备服务器内实例化一次或多次。对于单个设备服务器中运行的类的最大数量或实例的最大数量，没有具体的规则。\n在特定情况下，由于硬件或软件接口的限制，并不总是可以在同一设备服务器中运行设备类的多个实例：\nDLL的使用情况：有些DLL不能被同一进程的两个线程使用。 在其他情况下，在同一设备服务器中运行多个设备会很有用：\n电机案例： 4个电机的单轴控制器。 设备\n\u0026ndash; 查阅文档\n设备属性\n\u0026ndash; 查阅文档\n设备命令\n\u0026ndash; 查阅文档\n设备状态\n设备状态是将其集成到控制系统中的关键要素。因此，在设备实现中管理状态转换时应该非常小心。\n设备状态必须在任何时候都反映它所代表的系统的内部状态。状态应该代表客户请求所做的任何更改。\n这是至关重要的信息。事实上，客户端将主要或仅使用此信息来确定系统的内部状态。\n可用的状态仅限于：ON, OFF, CLOSE, OPEN, INSERT, EXTRACT, MOVING, STANDBY, FAULT, INIT, RUNNING, ALARM, DISABLE, and UNKNOWN\n最重要的是确保设备在状态转换方面的行为是可预测的。\n特性\n\u0026ndash; 查阅文档\n应用程序接口 # 虽然 TANGO 客户端确实可以仅使用 CORBA API 进行编程，但 CORBA 对 TANGO 一无所知。这意味着客户端必须了解从 TANGO 数据库检索 IOR 的所有详细信息、要在线发送的附加信息、TANGO 版本控制等。这些详细信息可以而且应该包含在 TANGO 应用程序程序员接口 (API) 中。该 API 在 C++ 中作为库实现，在 Java 中作为包实现。 API 使 TANGO 客户端易于编写。 API 由以下基本类组成：\nDeviceProxy 这是真实设备的代理 DeviceData 用于封装通过命令从设备发送/接收数据 DeviceAttribute 通过属性封装从设备发送/接收的数据 组是一组设备的代理 除了这些主要类之外，许多其他类还提供 TANGO 功能的完整接口。下图是使用 TANGO 的典型客户端/服务器应用程序的图。\n在服务器和客户端启动阶段使用数据库来建立客户端和服务器之间的连接。\n客户端和服务器之间使用API​​进行通信 # 通过 API，可以请求在设备上执行的命令或使用所实现的两种通信模型之一来读取/写入设备属性。这两个模型是：\n客户端等待（并被阻止）服务器发送答案或直到达到超时的同步模型 异步模型。在此模型中，客户端发送请求并立即返回。它没有被阻止。它可以自由地做任何它必须做的事情，比如更新图形用户界面。客户端可以选择通过调用 API 特定调用来检查回复是否到达来检索服务器答案，或者通过请求在客户端收到服务器答案时执行回调方法来检索服务器答案。 在 TANGO 版本 8 之前，TANGO 使用 CORBA OMG COS 通知服务来生成事件。 TANGO 使用通知服务的omniNotify 实现。 omn​​iNotify 是与 TANGO 也使用的omniORB CORBA 实现结合开发的。通知服务的核心是通知守护进程。 omn​​iNotify 守护进程是从设备服务器接收事件并将其分发到所有订阅的客户端的进程。为了分配事件负载，每个主机有一个通知守护进程。服务器将其事件发送到本地主机上的守护程序。客户端和服务器从 TANGO 数据库获取主机的 IOR。\n下图是 Tango 8 之前版本的 Tango 事件系统示意图。\n从 Tango 8 开始，实施了新的事件系统设计。这个新设计基于 ZMQ 库。 ZMQ 是一个允许用户创建通信系统的库。它实现了几种众所周知的通信模式，包括发布/订阅模式，这是新 Tango 事件系统的基础。使用此库，不再需要单独的通知服务，并且仅可通过客户端和服务器进程进行事件通信，从而简化了整体设计。从 Tango 8.1 开始，设备和客户端之间的事件传播可以使用多播协议来完成。这样做的目的是减少网络带宽的使用和设备服务器端的 CPU 消耗。请参阅“高级功能”一章以获取有关此功能的所有详细信息。\n下图是从 Tango 版本 8 开始的 Tango 版本的 Tango 事件系统示意图。\n文档 # TANGO 设备服务器指南 TANGO 设备服务器模型 编写 TANGO 设备服务器 转发属性 设备轮询 ","date":"24 March 2024","permalink":"/posts/architecture/iot/tango/tango-arch/","section":"博客","summary":"Tango Controls 是一个面向对象的分布式控制系统框架，它定义了通信协议、应用程序程序员接口 (API)，并提供了一组工具和库来构建控制系统软件。它是围绕设备和设备类别的概念构建的。","title":"Tango Controls Architecture"},{"content":" 链接：https://leetcode.cn/problems/two-sum-ii-input-array-is-sorted/description/ class Solution { public int[] twoSum(int[] numbers, int target) { int low = 0, high = numbers.length - 1; while (low \u0026lt; high) { int sum = numbers[low] + numbers[high]; if (sum == target) { return new int[]{low + 1, high + 1}; } else if (sum \u0026lt; target) { ++low; } else { --high; } } return new int[]{-1, -1}; } } ","date":"23 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/leetcode-167%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C-ii---%E8%BE%93%E5%85%A5%E6%9C%89%E5%BA%8F%E6%95%B0%E7%BB%84/","section":"博客","summary":"【LeetCode 167】两数之和 II - 输入有序数组题解。二分查找板子题","title":"【LeetCode 167】两数之和 II - 输入有序数组"},{"content":" 链接：https://leetcode.cn/problems/longest-substring-without-repeating-characters/description/ import java.util.HashMap; class Solution { public int lengthOfLongestSubstring(String s) { HashMap\u0026lt;Character, Integer\u0026gt; win = new HashMap\u0026lt;\u0026gt;(); int left = 0, right = 0; int res = 0; while (right \u0026lt; s.length()) { char c = s.charAt(right); win.put(c, win.getOrDefault(c, 0) + 1); right++; while (win.get(c) \u0026gt; 1) { char d = s.charAt(left); left++; if(win.containsKey(d)) win.put(d, win.getOrDefault(d, 0) - 1); } res = Math.max(res, right - left); } return res; } } ","date":"23 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/leetcode-3%E6%97%A0%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/","section":"博客","summary":"【LeetCode 3】无重复字符的最长子串题解。滑动窗口板子题。","title":"【LeetCode 3】无重复字符的最长子串"},{"content":"","date":"23 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/","section":"博客","summary":"二分查找是一种高效的搜索算法，适用于有序数组。它通过每次将搜索范围缩小一半来查找目标元素，直到找到或者确定不存在。时间复杂度为O(log n)，效率高于线性搜索。","title":"二分查找"},{"content":"","date":"23 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/","section":"博客","summary":"滑动窗口的思路非常简单，就是维护一个窗口，不断滑动，然后更新答案。","title":"滑动窗口"},{"content":" 链接：https://leetcode.cn/problems/combination-sum/description/ 版本一 # 按照回溯的板子写了一版代码\nclass Solution { public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combinationSum(int[] candidates, int target) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); int n = candidates.length; dfs(res, candidates, n, 0, target, new ArrayList\u0026lt;Integer\u0026gt;()); return res; } public void dfs(List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res, int[] candidates, int n, int now, int target, ArrayList\u0026lt;Integer\u0026gt; temp) { if(now == target) { res.add(new ArrayList\u0026lt;\u0026gt;(temp)); return; } if(now \u0026gt; target) { return; } for(int i = begin; i \u0026lt; n;i++) { temp.add(candidates[i]); dfs(res, candidates, n, now+candidates[i], target, temp); temp.remove(temp.size()-1); } } } 这版本代码会有重复，怎么避免这个问题呢？\n版本二 # class Solution { public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combinationSum(int[] candidates, int target) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); int n = candidates.length; dfs(res, candidates, 0, n, 0, target, new ArrayList\u0026lt;Integer\u0026gt;()); return res; } public void dfs(List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res, int[] candidates, int begin, int n, int now, int target, ArrayList\u0026lt;Integer\u0026gt; temp) { if(now == target) { res.add(new ArrayList\u0026lt;\u0026gt;(temp)); return; } if(now \u0026gt; target) { return; } for(int i = begin; i \u0026lt; n;i++) { temp.add(candidates[i]); dfs(res, candidates, i, n, now+candidates[i], target, temp); temp.remove(temp.size()-1); } } } 版本三 剪枝 # import java.util.ArrayDeque; import java.util.ArrayList; import java.util.Arrays; import java.util.Deque; import java.util.List; public class Solution { public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combinationSum(int[] candidates, int target) { int len = candidates.length; List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); if (len == 0) { return res; } // 排序是剪枝的前提 Arrays.sort(candidates); Deque\u0026lt;Integer\u0026gt; path = new ArrayDeque\u0026lt;\u0026gt;(); dfs(candidates, 0, len, target, path, res); return res; } private void dfs(int[] candidates, int begin, int len, int target, Deque\u0026lt;Integer\u0026gt; path, List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res) { // 由于进入更深层的时候，小于 0 的部分被剪枝，因此递归终止条件值只判断等于 0 的情况 if (target == 0) { res.add(new ArrayList\u0026lt;\u0026gt;(path)); return; } for (int i = begin; i \u0026lt; len; i++) { // 重点理解这里剪枝，前提是候选数组已经有序， if (target - candidates[i] \u0026lt; 0) { break; } path.addLast(candidates[i]); dfs(candidates, i, len, target - candidates[i], path, res); path.removeLast(); } } } ","date":"23 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E5%9B%9E%E6%BA%AF/leetcode-39%E7%BB%84%E5%90%88%E6%80%BB%E5%92%8C/","section":"博客","summary":"【LeetCode 39】组合总和题解。回溯算法板子题。","title":"【LeetCode 39】组合总和"},{"content":"","date":"23 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E5%9B%9E%E6%BA%AF/","section":"博客","summary":"回溯是一种搜索算法，用于解决问题的所有可能解。它通过尝试每一种可能的解决方案，遇到无法满足条件的情况则回溯到上一步，继续尝试其他路径，直到找到解或遍历完所有可能性。","title":"回溯"},{"content":" 链接：https://leetcode.cn/problems/combinations/description/ 分析 # 如果解决一个问题有多个步骤，每一个步骤有多种方法，题目又要我们找出所有的方法，可以使用回溯算法； 回溯算法是在一棵树上的 深度优先遍历（因为要找所有的解，所以需要遍历）； 组合问题，相对于排列问题而言，不计较一个组合内元素的顺序性（即 [1, 2, 3] 与 [1, 3, 2] 认为是同一个组合），因此很多时候需要按某种顺序展开搜索，这样才能做到不重不漏。\n既然是树形问题上的 深度优先遍历，因此首先画出树形结构。例如输入：n = 4, k = 2，我们可以发现如下递归结构：\n如果组合里有 1 ，那么需要在 [2, 3, 4] 里再找 $1$ 个数； 如果组合里有 2 ，那么需要在 [3, 4] 里再找 $1$ 数。注意：这里不能再考虑 $1$，因为包含 $1$ 的组合，在第 1 种情况中已经包含。 依次类推（后面部分省略），以上描述体现的 递归 结构是：在以 nnn 结尾的候选数组里，选出若干个元素。画出递归结构如下图：\n叶子结点的信息体现在从根结点到叶子结点的路径上，因此需要一个表示路径的变量 path，它是一个列表，特别地，path 是一个栈； 每一个结点递归地在做同样的事情，区别在于搜索起点，因此需要一个变量 start ，表示在区间 [begin, n] 里选出若干个数的组合； 可能有一些分支没有必要执行，我们放在优化中介绍。\n代码 # class Solution { public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combine(int n, int k) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); if (k \u0026lt;= 0 || n \u0026lt; k) { return res; } Deque\u0026lt;Integer\u0026gt; path = new ArrayDeque\u0026lt;\u0026gt;(); dfs(n, k, 1, path, res); return res; } private void dfs(int n, int k, int begin, Deque\u0026lt;Integer\u0026gt; path, List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res) { if (path.size() == k) { res.add(new ArrayList\u0026lt;\u0026gt;(path)); return; } for (int i = begin; i \u0026lt;= n; i++) { path.addLast(i); dfs(n, k, i + 1, path, res); path.removeLast(); } } } 如果对于回溯算法还理解不太透彻的朋友，可以在递归方法的前后，把 path 变量打印出来看一下，并结合上面画出的树形图进行理解。\n带 System.out.println 的调试语句不可以提交给力扣测评系统，会拖慢我们的程序执行时间。\nimport java.util.ArrayDeque; import java.util.ArrayList; import java.util.Deque; import java.util.List; public class Solution { public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combine(int n, int k) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); if (k \u0026lt;= 0 || n \u0026lt; k) { return res; } Deque\u0026lt;Integer\u0026gt; path = new ArrayDeque\u0026lt;\u0026gt;(); dfs(n, k, 1, path, res); return res; } private void dfs(int n, int k, int begin, Deque\u0026lt;Integer\u0026gt; path, List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res) { if (path.size() == k) { res.add(new ArrayList\u0026lt;\u0026gt;(path)); return; } for (int i = begin; i \u0026lt;= n; i++) { path.addLast(i); System.out.println(\u0026#34;递归之前 =\u0026gt; \u0026#34; + path); dfs(n, k, i + 1, path, res); path.removeLast(); System.out.println(\u0026#34;递归之后 =\u0026gt; \u0026#34; + path); } } public static void main(String[] args) { Solution solution = new Solution(); int n = 5; int k = 3; List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = solution.combine(n, k); System.out.println(res); } } 控制台输出：\n递归之前 =\u0026gt; [1] 递归之前 =\u0026gt; [1, 2] 递归之前 =\u0026gt; [1, 2, 3] 递归之后 =\u0026gt; [1, 2] 递归之前 =\u0026gt; [1, 2, 4] 递归之后 =\u0026gt; [1, 2] 递归之前 =\u0026gt; [1, 2, 5] 递归之后 =\u0026gt; [1, 2] 递归之后 =\u0026gt; [1] 递归之前 =\u0026gt; [1, 3] 递归之前 =\u0026gt; [1, 3, 4] 递归之后 =\u0026gt; [1, 3] 递归之前 =\u0026gt; [1, 3, 5] 递归之后 =\u0026gt; [1, 3] 递归之后 =\u0026gt; [1] 递归之前 =\u0026gt; [1, 4] 递归之前 =\u0026gt; [1, 4, 5] 递归之后 =\u0026gt; [1, 4] 递归之后 =\u0026gt; [1] 递归之前 =\u0026gt; [1, 5] 递归之后 =\u0026gt; [1] 递归之后 =\u0026gt; [] 递归之前 =\u0026gt; [2] 递归之前 =\u0026gt; [2, 3] 递归之前 =\u0026gt; [2, 3, 4] 递归之后 =\u0026gt; [2, 3] 递归之前 =\u0026gt; [2, 3, 5] 递归之后 =\u0026gt; [2, 3] 递归之后 =\u0026gt; [2] 递归之前 =\u0026gt; [2, 4] 递归之前 =\u0026gt; [2, 4, 5] 递归之后 =\u0026gt; [2, 4] 递归之后 =\u0026gt; [2] 递归之前 =\u0026gt; [2, 5] 递归之后 =\u0026gt; [2] 递归之后 =\u0026gt; [] 递归之前 =\u0026gt; [3] 递归之前 =\u0026gt; [3, 4] 递归之前 =\u0026gt; [3, 4, 5] 递归之后 =\u0026gt; [3, 4] 递归之后 =\u0026gt; [3] 递归之前 =\u0026gt; [3, 5] 递归之后 =\u0026gt; [3] 递归之后 =\u0026gt; [] 递归之前 =\u0026gt; [4] 递归之前 =\u0026gt; [4, 5] 递归之后 =\u0026gt; [4] 递归之后 =\u0026gt; [] 递归之前 =\u0026gt; [5] 递归之后 =\u0026gt; [] [[1, 2, 3], [1, 2, 4], [1, 2, 5], [1, 3, 4], [1, 3, 5], [1, 4, 5], [2, 3, 4], [2, 3, 5], [2, 4, 5], [3, 4, 5]] 优化：分析搜索起点的上界进行剪枝\n我们上面的代码，搜索起点遍历到 n，即：递归函数中有下面的代码片段：\n// 从当前搜索起点 begin 遍历到 n for (int i = begin; i \u0026lt;= n; i++) { path.addLast(i); dfs(n, k, i + 1, path, res); path.removeLast(); } 事实上，如果 n = 7, k = 4，从 $5$ 开始搜索就已经没有意义了，这是因为：即使把 $5$ 选上，后面的数只有 $6$ 和 $7$，一共就 $3$ 个候选数，凑不出 $4$ 个数的组合。因此，搜索起点有上界，这个上界是多少，可以举几个例子分析。\n分析搜索起点的上界，其实是在深度优先遍历的过程中剪枝，剪枝可以避免不必要的遍历，剪枝剪得好，可以大幅度节约算法的执行时间。\n下面的图片绿色部分是剪掉的枝叶，当 n 很大的时候，能少遍历很多结点，节约了时间。\n（温馨提示：右键，在弹出的下拉列表框中选择「在新标签页中打开图片」，可以查看大图。）\n容易知道：搜索起点和当前还需要选几个数有关，而当前还需要选几个数与已经选了几个数有关，即与 path 的长度相关。我们举几个例子分析：\n例如：n = 6 ，k = 4。\npath.size() == 1 的时候，接下来要选择 $3$ 个数，搜索起点最大是 $4$，最后一个被选的组合是 [4, 5, 6]； path.size() == 2 的时候，接下来要选择 $2$ 个数，搜索起点最大是 $5$，最后一个被选的组合是 [5, 6]； path.size() == 3 的时候，接下来要选择 $1$ 个数，搜索起点最大是 $6$，最后一个被选的组合是 [6]； 再如：n = 15 ，k = 4。\npath.size() == 1 的时候，接下来要选择 $3$ 个数，搜索起点最大是 $13$，最后一个被选的是 [13, 14, 15]； path.size() == 2 的时候，接下来要选择 $2$ 个数，搜索起点最大是 $14$，最后一个被选的是 [14, 15]； path.size() == 3 的时候，接下来要选择 $1$ 个数，搜索起点最大是 $15$，最后一个被选的是 [15]； 可以归纳出：\n搜索起点的上界 + 接下来要选择的元素个数 - 1 = n 其中，接下来要选择的元素个数 = k - path.size()，整理得到：\n搜索起点的上界 = n - (k - path.size()) + 1 所以，我们的剪枝过程就是：把 i \u0026lt;= n 改成 i \u0026lt;= n - (k - path.size()) + 1 ：\nimport java.util.ArrayDeque; import java.util.ArrayList; import java.util.Deque; import java.util.List; public class Solution { public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combine(int n, int k) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); if (k \u0026lt;= 0 || n \u0026lt; k) { return res; } Deque\u0026lt;Integer\u0026gt; path = new ArrayDeque\u0026lt;\u0026gt;(); dfs(n, k, 1, path, res); return res; } private void dfs(int n, int k, int index, Deque\u0026lt;Integer\u0026gt; path, List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res) { if (path.size() == k) { res.add(new ArrayList\u0026lt;\u0026gt;(path)); return; } // 只有这里 i \u0026lt;= n - (k - path.size()) + 1 与参考代码 1 不同 for (int i = index; i \u0026lt;= n - (k - path.size()) + 1; i++) { path.addLast(i); dfs(n, k, i + 1, path, res); path.removeLast(); } } } 一些边界条件比较绕的，用具体的例子分析就不容易出错，主要考察的是细心，没有太多技巧；\n为参考代码 3 添加 path 的打印输出语句，可以看到输出语句会更少。\n递归之前 =\u0026gt; [1] 递归之前 =\u0026gt; [1, 2] 递归之前 =\u0026gt; [1, 2, 3] 递归之后 =\u0026gt; [1, 2] 递归之前 =\u0026gt; [1, 2, 4] 递归之后 =\u0026gt; [1, 2] 递归之前 =\u0026gt; [1, 2, 5] 递归之后 =\u0026gt; [1, 2] 递归之后 =\u0026gt; [1] 递归之前 =\u0026gt; [1, 3] 递归之前 =\u0026gt; [1, 3, 4] 递归之后 =\u0026gt; [1, 3] 递归之前 =\u0026gt; [1, 3, 5] 递归之后 =\u0026gt; [1, 3] 递归之后 =\u0026gt; [1] 递归之前 =\u0026gt; [1, 4] 递归之前 =\u0026gt; [1, 4, 5] 递归之后 =\u0026gt; [1, 4] 递归之后 =\u0026gt; [1] 递归之后 =\u0026gt; [] 递归之前 =\u0026gt; [2] 递归之前 =\u0026gt; [2, 3] 递归之前 =\u0026gt; [2, 3, 4] 递归之后 =\u0026gt; [2, 3] 递归之前 =\u0026gt; [2, 3, 5] 递归之后 =\u0026gt; [2, 3] 递归之后 =\u0026gt; [2] 递归之前 =\u0026gt; [2, 4] 递归之前 =\u0026gt; [2, 4, 5] 递归之后 =\u0026gt; [2, 4] 递归之后 =\u0026gt; [2] 递归之后 =\u0026gt; [] 递归之前 =\u0026gt; [3] 递归之前 =\u0026gt; [3, 4] 递归之前 =\u0026gt; [3, 4, 5] 递归之后 =\u0026gt; [3, 4] 递归之后 =\u0026gt; [3] 递归之后 =\u0026gt; [] [[1, 2, 3], [1, 2, 4], [1, 2, 5], [1, 3, 4], [1, 3, 5], [1, 4, 5], [2, 3, 4], [2, 3, 5], [2, 4, 5], [3, 4, 5]] ","date":"23 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E5%9B%9E%E6%BA%AF/leetcode-77%E7%BB%84%E5%90%88/","section":"博客","summary":"【LeetCode 17】电话号码的字母组合题解。回溯算法板子题。","title":"【LeetCode 17】电话号码的字母组合"},{"content":" 链接：https://leetcode.cn/problems/longest-increasing-subsequence/description/ 解法一 # class Solution { public int lengthOfLIS(int[] nums) { int len = nums.length; int[] dp = new int[len]; Arrays.fill(dp, 1); int maxx = 1; for (int i = 1; i \u0026lt; len; i++) { for (int j = 0; j \u0026lt; i; j++) { if (nums[i] \u0026gt; nums[j]) { dp[i] = Math.max(dp[j] + 1, dp[i]); } } maxx = Math.max(maxx, dp[i]); } return maxx; } } 时间复杂度：$O(n^2)$，其中 $n$ 为数组 $\\textit{nums}$ 的长度。动态规划的状态数为 $n$，计算状态 $dp[i]$ 时，需要 $O(n)$ 的时间遍历 $dp[0 \\ldots i-1]$ 的所有状态，所以总时间复杂度为 $O(n^2)$。 空间复杂度：$O(n)$，需要额外使用长度为 $n$ 的 $dp$ 数组。 进阶 # 看了别人的解法\n/** 20210802：动态规划：nums[j]\u0026lt;nums[i] 有 dp[i] = max(dp[i], dp[j] + 1) for j in [0, i) n2，n */ // class Solution { // public int lengthOfLIS(int[] nums) { // //构建dp数组 // int [] dp =new int [nums.length]; // //max保存最大dp，最小为1 // int max=1; // //初始都为1 // Arrays.fill(dp,1); // //迭代 // for(int i=1;i\u0026lt;nums.length;i++){ // //j从0到i // for(int j=0;j\u0026lt;i;j++){ // //符合递增情况时，dp选大的 // if(nums[j]\u0026lt;nums[i]) dp[i]=Math.max(dp[j]+1,dp[i]); // } // //保存每轮最大值 // max=Math.max(dp[i],max); // } // return max; // } // } /** 20210802：贪心+二分 维护一个结果数组， 如果当前元素比结果数组的值都大的的话，就追加在结果数组后面（相当于递增序列长度加了1）； 否则的话用当前元素覆盖掉第一个比它大的元素 （这样做的话后续递增序列才有可能更长，即使并没有更长， 这个覆盖操作也并没有副作用哈， 当然这个覆盖操作可能会让最终的结果数组值并不是最终的递增序列值， 这无所谓） nlogn,n */ class Solution { public int lengthOfLIS(int[] nums) { //结果 int ans = 0; //结果数组 int[] cur = new int[nums.length]; //遍历 for (int num : nums) { //若是首个元素、或当前元素大于结果数组的末尾元素，就追加 if (ans == 0 || cur[ans - 1] \u0026lt; num) { cur[ans++] = num; } else { //二分法，当前元素覆盖第一个比它大于等于的元素，查找该位置 //注意，这种查找不一定存在的数的位置的二分： //一般不写==，并且一般有一边不加减1 int l = 0; int r = ans - 1; while (l \u0026lt; r) { int mid = l + (r - l) / 2; if (cur[mid] \u0026lt; num) l = mid + 1; else r = mid; } //进行覆盖 cur[l] = num; } } //返回长度 return ans; } } ","date":"23 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/leetcode-300%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/","section":"博客","summary":"【LeetCode 300】最长递增子序列题解。","title":"【LeetCode 300】最长递增子序列"},{"content":" 链接：https://leetcode.cn/problems/implement-trie-prefix-tree/description/ 字典树 # 又称前缀树或字典树，是一棵有根树，其每个节点包含以下字段：\n指向子节点的指针数组 $children$。对于本题而言，数组长度为 $26$，即小写英文字母的数量。此时 $children[0]$ 对应小写字母 $a$，$children[1]$ 对应小写字母 $b$，…，$children[25]$ 对应小写字母 $z$。 布尔字段 $isEnd$，表示该节点是否为字符串的结尾。 插入字符串 # 我们从字典树的根开始，插入字符串。对于当前字符对应的子节点，有两种情况：\n子节点存在。沿着指针移动到子节点，继续处理下一个字符。 子节点不存在。创建一个新的子节点，记录在 $children$ 数组的对应位置上，然后沿着指针移动到子节点，继续搜索下一个字符。 重复以上步骤，直到处理字符串的最后一个字符，然后将当前节点标记为字符串的结尾。\n查找前缀 # 我们从字典树的根开始，查找前缀。对于当前字符对应的子节点，有两种情况：\n子节点存在。沿着指针移动到子节点，继续搜索下一个字符。 子节点不存在。说明字典树中不包含该前缀，返回空指针。 重复以上步骤，直到返回空指针或搜索完前缀的最后一个字符。\n若搜索到了前缀的末尾，就说明字典树中存在该前缀。此外，若前缀末尾对应节点的 $isEnd$ 为真，则说明字典树中存在该字符串。\n代码 # class Trie { //记录该字母的下一位所有可能的字母坐标 private Trie[] children; //该字母是否为最后一个字母 private boolean isEnd; public Trie() { //初始化26个字母 children = new Trie[26]; //默认为不是最后一个字母 isEnd = false; } public void insert(String word) { //得到字典树根节点 Trie node = this; //去遍历待插入单词的字符集合 for (char c : word.toCharArray()) { //得到该字符在数组中的坐标 int index = c - \u0026#39;a\u0026#39;; //如果正在遍历的该字母在上一个节点的数组坐标中没有记录，就新建一个字母节点在字典树中 if(node.children[index] == null){ node.children[index] = new Trie(); } //每一次生成字母都移动指针到下一个字母节点 node = node.children[index]; } //最后一个字母节点设置为最后一个字母 node.isEnd = true; } public boolean search(String word) { //返回检索到的最后一个字母节点 Trie node = searchPrefix(word); //只有当该单词在字典树中存在并且最后一个字母节点为最后一个字母，才返回true return node != null \u0026amp;\u0026amp; node.isEnd; } public boolean startsWith(String prefix) { //只要前缀匹配存在于字典树中就返回true return searchPrefix(prefix) != null; } //前缀搜索 还是 全文搜索都是调用此方法，区别在于前缀搜索只要前缀匹配就返回true，全文搜索则要匹配到最后一个字母才返回true，所以这里返回的是最后一个字母节点 public Trie searchPrefix(String word){ //得到字典树根节点 Trie node = this; //开始验证字符串在字典树中是否存在 for (char c : word.toCharArray()) { //得到该字符在数组中的坐标 int index = c - \u0026#39;a\u0026#39;; //如果该字符在数组中存在，就移动指针到下一个字母节点，直至到达最后一个待搜索的最后一个字母节点 if(node.children[index] != null){ node = node.children[index]; }else{ //如果在此过程中没有找到待搜索的其中一个字母节点，就返回null，代表该字母不存在于字典树中 return null; } } //没有问题，那就是到达了最后一个待搜索的最后一个字母节点，返回该节点(节点可能是最后一个字母节点也可能不是) return node; } } /** * Your Trie object will be instantiated and called as such: * Trie obj = new Trie(); * obj.insert(word); * boolean param_2 = obj.search(word); * boolean param_3 = obj.startsWith(prefix); */ 时间复杂度：初始化为 $O(1)$，其余操作为 $O(|S|)$，其中 $|S|$ 是每次插入或查询的字符串的长度。 空间复杂度：$O(|T|\\cdot\\Sigma)$，其中 $|T|$ 为所有插入字符串的长度之和，$\\Sigma$ 为字符集的大小，本题 $\\Sigma=26$。 ","date":"22 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E5%AD%97%E5%85%B8%E6%A0%91%E5%89%8D%E7%BC%80%E6%A0%91/leetcode-208%E5%AE%9E%E7%8E%B0-trie-%E5%89%8D%E7%BC%80%E6%A0%91/","section":"博客","summary":"【LeetCode 208】实现 Trie (前缀树)题解。前缀树板子题。","title":"【LeetCode 208】实现 Trie (前缀树)"},{"content":" 链接：https://leetcode.cn/problems/design-add-and-search-words-data-structure/description/ 思路 # 根据题意，$\\texttt{WordDictionary}$ 类需要支持添加单词和搜索单词的操作，可以使用字典树实现。\n对于添加单词，将单词添加到字典树中即可。\n对于搜索单词，从字典树的根结点开始搜索。由于待搜索的单词可能包含点号，因此在搜索过程中需要考虑点号的处理。对于当前字符是字母和点号的情况，分别按照如下方式处理：\n如果当前字符是字母，则判断当前字符对应的子结点是否存在，如果子结点存在则移动到子结点，继续搜索下一个字符，如果子结点不存在则说明单词不存在，返回 $\\text{false}$；\n如果当前字符是点号，由于点号可以表示任何字母，因此需要对当前结点的所有非空子结点继续搜索下一个字符。\n重复上述步骤，直到返回 $\\text{false}$ 或搜索完给定单词的最后一个字符。\n如果搜索完给定的单词的最后一个字符，则当搜索到的最后一个结点的 $\\textit{isEnd}$ 为 $\\text{true}$ 时，给定的单词存在。\n特别地，当搜索到点号时，只要存在一个非空子结点可以搜索到给定的单词，即返回 $\\text{true}$。\n代码 # class WordDictionary { private Trie root; public WordDictionary() { root = new Trie(); } public void addWord(String word) { root.insert(word); } public boolean search(String word) { return dfs(word, 0, root); } private boolean dfs(String word, int index, Trie node) { if(index == word.length()) { return node.isEnd(); } char ch = word.charAt(index); if(Character.isLetter(ch)) { int childIndex = ch - \u0026#39;a\u0026#39;; Trie child = node.getChildren()[childIndex]; if(child != null \u0026amp;\u0026amp; dfs(word, index + 1, child)) { return true; } } else { for(int i = 0; i \u0026lt; 26; i++) { Trie child = node.getChildren()[i]; if(child != null \u0026amp;\u0026amp; dfs(word, index + 1, child)) { return true; } } } return false; } } class Trie { private Trie[] children; private boolean isEnd; public Trie() { children = new Trie[26]; isEnd = false; } public void insert(String word) { Trie node = this; for (int i = 0; i \u0026lt; word.length(); i++) { char ch = word.charAt(i); int index = ch - \u0026#39;a\u0026#39;; if (node.children[index] == null) { node.children[index] = new Trie(); } node = node.children[index]; } node.isEnd = true; } public Trie[] getChildren() { return children; } public boolean isEnd() { return isEnd; } } /** * Your WordDictionary object will be instantiated and called as such: * WordDictionary obj = new WordDictionary(); * obj.addWord(word); * boolean param_2 = obj.search(word); */ 时间复杂度：初始化为 $O(1)$，添加单词为 $O(|S|)$，搜索单词为 $O(|\\Sigma|^{|S|})$，其中 $|S|$ 是每次添加或搜索的单词的长度，$\\Sigma$ 是字符集，这道题中的字符集为全部小写英语字母，$\\Sigma| = 26$。 最坏情况下，待搜索的单词中的每个字符都是点号，则每个字符都有 $|\\Sigma|$ 种可能。\n空间复杂度：$O(|T|\\cdot|\\Sigma|)$，其中 $|T|$ 是所有添加的单词的长度之和，$\\Sigma$ 是字符集，这道题中的字符集为全部小写英语字母，$|\\Sigma| = 26$。\n","date":"22 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E5%AD%97%E5%85%B8%E6%A0%91%E5%89%8D%E7%BC%80%E6%A0%91/leetcode-211%E6%B7%BB%E5%8A%A0%E4%B8%8E%E6%90%9C%E7%B4%A2%E5%8D%95%E8%AF%8D---%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%AE%BE%E8%AE%A1/","section":"博客","summary":"【LeetCode 211】添加与搜索单词 - 数据结构设计题解。前缀树板子题。","title":"【LeetCode 211】添加与搜索单词 - 数据结构设计"},{"content":"","date":"22 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E5%AD%97%E5%85%B8%E6%A0%91%E5%89%8D%E7%BC%80%E6%A0%91/","section":"博客","summary":"字典树是一种用于字符串检索的数据结构，以树形结构存储字符串，每个节点代表字符串的一个字符，路径代表字符串序列，支持高效的字符串查找和前缀匹配。","title":"字典树"},{"content":" 链接：https://leetcode.cn/problems/letter-combinations-of-a-phone-number/description/ 分析 # 首先使用哈希表存储每个数字对应的所有可能的字母，然后进行回溯操作。\n回溯过程中维护一个字符串，表示已有的字母排列（如果未遍历完电话号码的所有数字，则已有的字母排列是不完整的）。该字符串初始为空。每次取电话号码的一位数字，从哈希表中获得该数字对应的所有可能的字母，并将其中的一个字母插入到已有的字母排列后面，然后继续处理电话号码的后一位数字，直到处理完电话号码中的所有数字，即得到一个完整的字母排列。然后进行回退操作，遍历其余的字母排列。\n回溯算法用于寻找所有的可行解，如果发现一个解不可行，则会舍弃不可行的解。在这道题中，由于每个数字对应的每个字母都可能进入字母组合，因此不存在不可行的解，直接穷举所有的解即可。\n代码 # class Solution { public List\u0026lt;String\u0026gt; letterCombinations(String digits) { List\u0026lt;String\u0026gt; combinations = new ArrayList\u0026lt;String\u0026gt;(); if (digits.length() == 0) { return combinations; } Map\u0026lt;Character, String\u0026gt; phoneMap = new HashMap\u0026lt;Character, String\u0026gt;() { { put(\u0026#39;2\u0026#39;, \u0026#34;abc\u0026#34;); put(\u0026#39;3\u0026#39;, \u0026#34;def\u0026#34;); put(\u0026#39;4\u0026#39;, \u0026#34;ghi\u0026#34;); put(\u0026#39;5\u0026#39;, \u0026#34;jkl\u0026#34;); put(\u0026#39;6\u0026#39;, \u0026#34;mno\u0026#34;); put(\u0026#39;7\u0026#39;, \u0026#34;pqrs\u0026#34;); put(\u0026#39;8\u0026#39;, \u0026#34;tuv\u0026#34;); put(\u0026#39;9\u0026#39;, \u0026#34;wxyz\u0026#34;); } }; backtrack(combinations, phoneMap, digits, 0, new StringBuffer()); return combinations; } public void backtrack(List\u0026lt;String\u0026gt; combinations, Map\u0026lt;Character, String\u0026gt; phoneMap, String digits, int index, StringBuffer combination) { if (index == digits.length()) { combinations.add(combination.toString()); } else { char digit = digits.charAt(index); String letters = phoneMap.get(digit); int lettersCount = letters.length(); for (int i = 0; i \u0026lt; lettersCount; i++) { combination.append(letters.charAt(i)); backtrack(combinations, phoneMap, digits, index + 1, combination); combination.deleteCharAt(index); } } } } ","date":"21 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E5%9B%9E%E6%BA%AF/leetcode-17%E7%94%B5%E8%AF%9D%E5%8F%B7%E7%A0%81%E7%9A%84%E5%AD%97%E6%AF%8D%E7%BB%84%E5%90%88/","section":"博客","summary":"【LeetCode 17】电话号码的字母组合题解。回溯算法板子题。","title":"【LeetCode 17】电话号码的字母组合"},{"content":" https://blog.51cto.com/u_12865/8266475 https://blog.csdn.net/qq_36331657/article/details/86646549 前后端请求加载流程是前端与后端交互的基础，确保数据能够在客户端与服务器之间有效、安全地传输。这个过程通常遵循以下几个步骤：\n用户操作或触发请求 用户在前端界面进行操作，如点击按钮、提交表单等。这些操作触发前端代码（通常是JavaScript）发起一个对后端服务器的请求。\n发送请求 前端通过HTTP（超文本传输协议）或HTTPS（HTTP安全）协议，发送一个请求到服务器。这个请求包括：\n请求方法：如GET（获取数据）、POST（提交数据）、PUT（更新数据）等。 请求URL：指明请求的资源或接口路径。 请求头：包含请求的元数据，如内容类型（Content-Type）、认证信息等。 请求体：对于POST和PUT请求，请求体中包含要发送给服务器的数据。 服务器处理请求 服务器接收到请求后，由Web服务器（如Apache、Nginx）转发给后端应用服务器处理（如基于Node.js、Python Flask、Django、Java Spring等）。\n应用服务器根据请求的URL、方法等信息，调用相应的后端逻辑处理请求。\n如果需要，后端会与数据库或其他服务进行交互，获取或更新数据。\n发送响应 一旦后端处理完成，服务器会构造一个HTTP响应返回给前端。\n响应通常包括：\n状态码：表示请求成功与否的代码，如200（成功）、404（未找到）、500（服务器错误）等。 响应头：包含响应的元数据，如内容类型。 响应体：包含返回给前端的数据，通常是JSON或XML格式。 前端处理响应 前端接收到响应后，根据状态码和返回的数据进行相应的处理。\n通常，数据会被解析并用于更新网页内容或界面状态，提供用户反馈。\n用户界面更新 用户界面根据处理结果更新，显示新的数据或给用户反馈，完成一次前后端交互流程。\n在这个过程中，安全措施非常重要，包括使用HTTPS保证传输加密、验证和授权确保数据访问安全、输入验证防止注入攻击等。\n","date":"18 March 2024","permalink":"/posts/architecture/backend/%E5%89%8D%E5%90%8E%E7%AB%AF%E8%AF%B7%E6%B1%82%E5%8A%A0%E8%BD%BD%E6%B5%81%E7%A8%8B/","section":"博客","summary":"前后端请求加载流程是前端与后端交互的基础，确保数据能够在客户端与服务器之间有效、安全地传输。","title":"前后端请求加载流程"},{"content":" https://leetcode.cn/problems/game-of-life/description 首先想到的是设置一个复制的数组，然后对这个数组赋值，最后在赋给原来的数组\nclass Solution { public void gameOfLife(int[][] board) { int[] neighbors = {0, 1, -1}; int rows = board.length; int cols = board[0].length; // 创建复制数组 copyBoard int[][] copyBoard = new int[rows][cols]; // 从原数组复制一份到 copyBoard 中 for (int row = 0; row \u0026lt; rows; row++) { for (int col = 0; col \u0026lt; cols; col++) { copyBoard[row][col] = board[row][col]; } } // 遍历面板每一个格子里的细胞 for (int row = 0; row \u0026lt; rows; row++) { for (int col = 0; col \u0026lt; cols; col++) { // 对于每一个细胞统计其八个相邻位置里的活细胞数量 int liveNeighbors = 0; for (int i = 0; i \u0026lt; 3; i++) { for (int j = 0; j \u0026lt; 3; j++) { if (!(neighbors[i] == 0 \u0026amp;\u0026amp; neighbors[j] == 0)) { int r = (row + neighbors[i]); int c = (col + neighbors[j]); // 查看相邻的细胞是否是活细胞 if ((r \u0026lt; rows \u0026amp;\u0026amp; r \u0026gt;= 0) \u0026amp;\u0026amp; (c \u0026lt; cols \u0026amp;\u0026amp; c \u0026gt;= 0) \u0026amp;\u0026amp; (copyBoard[r][c] == 1)) { liveNeighbors += 1; } } } } // 规则 1 或规则 3 if ((copyBoard[row][col] == 1) \u0026amp;\u0026amp; (liveNeighbors \u0026lt; 2 || liveNeighbors \u0026gt; 3)) { board[row][col] = 0; } // 规则 4 if (copyBoard[row][col] == 0 \u0026amp;\u0026amp; liveNeighbors == 3) { board[row][col] = 1; } } } } } 后来看了题解，还有一种方案，就是通过另外两个状态 0 和 -1 来表示\n规则 1：如果活细胞周围八个位置的活细胞数少于两个，则该位置活细胞死亡。这时候，将细胞值改为 -1，代表这个细胞过去是活的现在死了； 规则 2：如果活细胞周围八个位置有两个或三个活细胞，则该位置活细胞仍然存活。这时候不改变细胞的值，仍为 1； 规则 3：如果活细胞周围八个位置有超过三个活细胞，则该位置活细胞死亡。这时候，将细胞的值改为 -1，代表这个细胞过去是活的现在死了。可以看到，因为规则 1 和规则 3 下细胞的起始终止状态是一致的，因此它们的复合状态也一致； 规则 4：如果死细胞周围正好有三个活细胞，则该位置死细胞复活。这时候，将细胞的值改为 2，代表这个细胞过去是死的现在活了。 class Solution { public void gameOfLife(int[][] board) { int[] neighbors = {0, 1, -1}; int rows = board.length; int cols = board[0].length; // 遍历面板每一个格子里的细胞 for (int row = 0; row \u0026lt; rows; row++) { for (int col = 0; col \u0026lt; cols; col++) { // 对于每一个细胞统计其八个相邻位置里的活细胞数量 int liveNeighbors = 0; for (int i = 0; i \u0026lt; 3; i++) { for (int j = 0; j \u0026lt; 3; j++) { if (!(neighbors[i] == 0 \u0026amp;\u0026amp; neighbors[j] == 0)) { // 相邻位置的坐标 int r = (row + neighbors[i]); int c = (col + neighbors[j]); // 查看相邻的细胞是否是活细胞 if ((r \u0026lt; rows \u0026amp;\u0026amp; r \u0026gt;= 0) \u0026amp;\u0026amp; (c \u0026lt; cols \u0026amp;\u0026amp; c \u0026gt;= 0) \u0026amp;\u0026amp; (Math.abs(board[r][c]) == 1)) { liveNeighbors += 1; } } } } // 规则 1 或规则 3 if ((board[row][col] == 1) \u0026amp;\u0026amp; (liveNeighbors \u0026lt; 2 || liveNeighbors \u0026gt; 3)) { // -1 代表这个细胞过去是活的现在死了 board[row][col] = -1; } // 规则 4 if (board[row][col] == 0 \u0026amp;\u0026amp; liveNeighbors == 3) { // 2 代表这个细胞过去是死的现在活了 board[row][col] = 2; } } } // 遍历 board 得到一次更新后的状态 for (int row = 0; row \u0026lt; rows; row++) { for (int col = 0; col \u0026lt; cols; col++) { if (board[row][col] \u0026gt; 0) { board[row][col] = 1; } else { board[row][col] = 0; } } } } } ","date":"15 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E6%95%B0%E7%BB%84/leetcode-289%E7%94%9F%E5%91%BD%E6%B8%B8%E6%88%8F/","section":"博客","summary":"【LeetCode 289】生命游戏题解。","title":"【LeetCode 289】生命游戏"},{"content":" class Solution { public String convert(String s, int numRows) { if(numRows \u0026lt; 2) return s; List\u0026lt;StringBuilder\u0026gt; rows = new ArrayList\u0026lt;StringBuilder\u0026gt;(); for(int i = 0; i \u0026lt; numRows; i++) rows.add(new StringBuilder()); int i = 0, flag = -1; for(char c : s.toCharArray()) { rows.get(i).append(c); if(i == 0 || i == numRows -1) flag = - flag; i += flag; } StringBuilder res = new StringBuilder(); for(StringBuilder row : rows) res.append(row); return res.toString(); } } string 的操作\nsubstring(0, i) trim() split('\\\\s+) join(' ', wordlist) Collections.reverse(wordlist) ","date":"15 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E6%95%B0%E7%BB%84/leetcode-6z-%E5%AD%97%E5%BD%A2%E5%8F%98%E6%8D%A2/","section":"博客","summary":"【LeetCode 6】Z 字形变换题解。参考这个优美的代码解法，并且复习下String的内容。","title":"【LeetCode 6】Z 字形变换"},{"content":"","date":"15 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E6%95%B0%E7%BB%84/","section":"博客","summary":"数组相关的算法题。","title":"数组"},{"content":"","date":"14 March 2024","permalink":"/tags/interview/","section":"Tags","summary":"","title":"Interview"},{"content":"","date":"14 March 2024","permalink":"/posts/reviews/network/","section":"博客","summary":"Java网络编程提供了一套强大的API，支持TCP、UDP、HTTP等协议。通过Socket和ServerSocket实现客户端与服务器的通信。NIO提供非阻塞IO，提高并发处理能力。URL、URLConnection可用于HTTP操作。Java的网络编程简洁、灵活，适用于构建各种网络应用，如Web服务器、网络爬虫等。","title":"java-网络篇"},{"content":"","date":"14 March 2024","permalink":"/tags/socket/","section":"Tags","summary":"","title":"Socket"},{"content":"针对 TCP 应该如何 Socket 编程？ # 服务端和客户端初始化 socket，得到文件描述符； 服务端调用 bind，将 socket 绑定在指定的 IP 地址和端口; 服务端调用 listen，进行监听； 服务端调用 accept，等待客户端连接； 客户端调用 connect，向服务端的地址和端口发起连接请求； 服务端 accept 返回用于传输的 socket 的文件描述符； 客户端调用 write 写入数据；服务端调用 read 读取数据； 客户端断开连接时，会调用 close，那么服务端 read 读取数据的时候，就会读取到了 EOF，待处理完数据后，服务端调用 close，表示连接关闭。 这里需要注意的是，服务端调用 accept 时，连接成功了会返回一个已完成连接的 socket，后续用来传输数据。\n所以，监听的 socket 和真正用来传送数据的 socket，是「两个」socket，一个叫作监听 socket，一个叫作已完成连接 socket。\n成功连接建立之后，双方开始通过 read 和 write 函数来读写数据，就像往一个文件流里面写东西一样。\nlisten 时候参数 backlog 的意义？ # Linux 内核中会维护两个队列：\n半连接队列（SYN 队列）：接收到一个 SYN 建立连接请求，处于 SYN_RCVD 状态； 全连接队列（Accpet 队列）：已完成 TCP 三次握手过程，处于 ESTABLISHED 状态； int listen (int socketfd, int backlog) 参数一 socketfd 为 socketfd 文件描述符 参数二 backlog，这参数在历史版本有一定的变化 在早期 Linux 内核 backlog 是 SYN 队列大小，也就是未完成的队列大小。\n在 Linux 内核 2.2 之后，backlog 变成 accept 队列，也就是已完成连接建立的队列长度，所以现在通常认为 backlog 是 accept 队列。\n但是上限值是内核参数 somaxconn 的大小，也就说 accpet 队列长度 = min(backlog, somaxconn)。\n想详细了解 TCP 半连接队列和全连接队列，可以看这篇： TCP 半连接队列和全连接队列满了会发生什么？又该如何应对？\naccept 发生在三次握手的哪一步？ # 我们先看看客户端连接服务端时，发送了什么？\n客户端的协议栈向服务端发送了 SYN 包，并告诉服务端当前发送序列号 client_isn，客户端进入 SYN_SENT 状态； 服务端的协议栈收到这个包之后，和客户端进行 ACK 应答，应答的值为 client_isn+1，表示对 SYN 包 client_isn 的确认，同时服务端也发送一个 SYN 包，告诉客户端当前我的发送序列号为 server_isn，服务端进入 SYN_RCVD 状态； 客户端协议栈收到 ACK 之后，使得应用程序从 connect 调用返回，表示客户端到服务端的单向连接建立成功，客户端的状态为 ESTABLISHED，同时客户端协议栈也会对服务端的 SYN 包进行应答，应答数据为 server_isn+1； ACK 应答包到达服务端后，服务端的 TCP 连接进入 ESTABLISHED 状态，同时服务端协议栈使得 accept 阻塞调用返回，这个时候服务端到客户端的单向连接也建立成功。至此，客户端与服务端两个方向的连接都建立成功。 从上面的描述过程，我们可以得知客户端 connect 成功返回是在第二次握手，服务端 accept 成功返回是在三次握手成功之后。\n客户端调用 close 了，连接是断开的流程是什么？ # 我们看看客户端主动调用了 close，会发生什么？\n客户端调用 close，表明客户端没有数据需要发送了，则此时会向服务端发送 FIN 报文，进入 FIN_WAIT_1 状态； 服务端接收到了 FIN 报文，TCP 协议栈会为 FIN 包插入一个文件结束符 EOF 到接收缓冲区中，应用程序可以通过 read 调用来感知这个 FIN 包。这个 EOF 会被放在已排队等候的其他已接收的数据之后，这就意味着服务端需要处理这种异常情况，因为 EOF 表示在该连接上再无额外数据到达。此时，服务端进入 CLOSE_WAIT 状态； 接着，当处理完数据后，自然就会读到 EOF，于是也调用 close 关闭它的套接字，这会使得服务端发出一个 FIN 包，之后处于 LAST_ACK 状态； 客户端接收到服务端的 FIN 包，并发送 ACK 确认包给服务端，此时客户端将进入 TIME_WAIT 状态； 服务端收到 ACK 确认包后，就进入了最后的 CLOSE 状态； 客户端经过 2MSL 时间之后，也进入 CLOSE 状态； 没有 accept，能建立 TCP 连接吗？ # 答案：可以的。\naccpet 系统调用并不参与 TCP 三次握手过程，它只是负责从 TCP 全连接队列取出一个已经建立连接的 socket，用户层通过 accpet 系统调用拿到了已经建立连接的 socket，就可以对该 socket 进行读写操作了。\n更想了解这个问题，可以参考这篇文章： 没有 accept，能建立 TCP 连接吗？\n没有 listen，能建立 TCP 连接吗？ # 答案：可以的。\n客户端是可以自己连自己的形成连接（TCP 自连接），也可以两个客户端同时向对方发出请求建立连接（TCP 同时打开），这两个情况都有个共同点，就是没有服务端参与，也就是没有 listen，就能 TCP 建立连接。\n更想了解这个问题，可以参考这篇文章： 服务端没有 listen，客户端发起连接建立，会发生什么？\n","date":"14 March 2024","permalink":"/posts/reviews/network/socket/","section":"博客","summary":"Socket编程是一种用于网络通信的编程技术，允许计算机之间通过网络进行数据交换。它基于套接字（socket）接口，实现了客户端和服务器之间的通信，支持TCP和UDP协议。","title":"Socket 编程"},{"content":" class Solution { public int canCompleteCircuit(int[] gas, int[] cost) { int n = gas.length; int[] temp = new int[n]; for(int i = 0; i \u0026lt; n; i++) { temp[i] = gas[i] - cost[i]; } for(int i = 0; i \u0026lt; n; i++) { int ans = 0, flag = 1; for(int j = 0; j \u0026lt; n; j++) { ans += temp[(i+j)%n]; if(ans \u0026lt; 0) { flag = 0; break; } } if(flag == 1) { return i; } } return -1; } } 稍微优化一下。\nclass Solution { public int canCompleteCircuit(int[] gas, int[] cost) { int n = gas.length; int[] temp = new int[n]; for(int i = 0; i \u0026lt; n; i++) { temp[i] = gas[i] - cost[i]; } for(int i = 0; i \u0026lt; n; i++) { int ans = 0, flag = 1; for(int j = 0; j \u0026lt; n; j++) { ans += temp[(i+j)%n]; if(ans \u0026lt; 0) { // 补一下 i += j; flag = 0; break; } } if(flag == 1) { return i; } } return -1; } } class Solution { public int canCompleteCircuit(int[] gas, int[] cost) { int n = gas.length; int i = 0; while (i \u0026lt; n) { int sumOfGas = 0, sumOfCost = 0; int cnt = 0; while (cnt \u0026lt; n) { int j = (i + cnt) % n; sumOfGas += gas[j]; sumOfCost += cost[j]; if (sumOfCost \u0026gt; sumOfGas) { break; } cnt++; } if (cnt == n) { return i; } else { i = i + cnt + 1; } } return -1; } } ","date":"13 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E6%95%B0%E7%BB%84/leetcode-134%E5%8A%A0%E6%B2%B9%E7%AB%99/","section":"博客","summary":"【LeetCode 134】加油站题解。","title":"【LeetCode 134】加油站"},{"content":" class Solution { public int hIndex(int[] citations) { int n = citations.length; int minn = 0; int maxx = n; while (minn \u0026lt; maxx) { int m = (maxx + minn + 1) / 2; if (judge(citations, m) \u0026gt;= m) { minn = m; } else { maxx = m - 1; } } return minn; } public int judge(int[] citations, int h) { int ans = 0; int n = citations.length; for (int i = 0; i \u0026lt; n; i++) { if (citations[i] \u0026gt;= h) { ans++; } } return ans; } } ","date":"13 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E6%95%B0%E7%BB%84/leetcode-274h%E6%8C%87%E6%95%B0/","section":"博客","summary":"【LeetCode 274】H指数题解。","title":"【LeetCode 274】H指数"},{"content":" 题目不难，主要在于如何控制数组上下标。\n直接看解法吧，很清晰了。\nclass Solution { public List\u0026lt;Integer\u0026gt; spiralOrder(int[][] matrix) { List\u0026lt;Integer\u0026gt; ans = new ArrayList\u0026lt;\u0026gt;(); if(matrix.length == 0) return ans; //若数组为空，直接返回答案 int u = 0; //赋值上下左右边界 int d = matrix.length - 1; int l = 0; int r = matrix[0].length - 1; while(true) { for(int i = l; i \u0026lt;= r; ++i) ans.add(matrix[u][i]); //向右移动直到最右 if(++ u \u0026gt; d) break; //重新设定上边界，若上边界大于下边界，则遍历遍历完成，下同 for(int i = u; i \u0026lt;= d; ++i) ans.add(matrix[i][r]); //向下 if(-- r \u0026lt; l) break; //重新设定有边界 for(int i = r; i \u0026gt;= l; --i) ans.add(matrix[d][i]); //向左 if(-- d \u0026lt; u) break; //重新设定下边界 for(int i = d; i \u0026gt;= u; --i) ans.add(matrix[i][l]); //向上 if(++ l \u0026gt; r) break; //重新设定左边界 } return ans; } } ","date":"13 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E6%95%B0%E7%BB%84/leetcode-54%E8%9E%BA%E6%97%8B%E7%9F%A9%E9%98%B5/","section":"博客","summary":"【LeetCode 54】螺旋矩阵题解。","title":"【LeetCode 54】螺旋矩阵"},{"content":" class Solution { public void rotate(int[] nums, int k) { int n = nums.length; k = k % n; int count = gcd(k, n); for (int start = 0; start \u0026lt; count; ++start) { int current = start; int prev = nums[start]; do { int next = (current + k) % n; int temp = nums[next]; nums[next] = prev; prev = temp; current = next; } while (start != current); } } public int gcd(int x, int y) { return y \u0026gt; 0 ? gcd(y, x % y) : x; } } ","date":"12 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E6%95%B0%E7%BB%84/leetcode-189%E8%BD%AE%E8%BD%AC%E6%95%B0%E7%BB%84/","section":"博客","summary":"【LeetCode 189】轮转数组题解。","title":"【LeetCode 189】轮转数组"},{"content":"","date":"11 March 2024","permalink":"/posts/reviews/os/%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/","section":"博客","summary":"操作系统进程管理是指操作系统对计算机中运行的各种进程进行调度、分配资源和控制的过程。它负责管理进程的创建、销毁、调度和通信，以确保系统资源的有效利用和进程间的协同工作，提高系统的性能和稳定性。","title":"OS-进程管理"},{"content":"每个进程的用户地址空间都是独立的，一般而言是不能互相访问的，但内核空间是每个进程都共享的，所以进程之间要通信必须通过内核。\nLinux 内核提供了不少进程间通信的机制，我们来一起瞧瞧有哪些？\n管道 # 如果你学过 Linux 命令，那你肯定很熟悉「|」这个竖线。\n$ ps auxf | grep mysql 上面命令行里的「|」竖线就是一个管道，它的功能是将前一个命令（ps auxf）的输出，作为后一个命令（grep mysql）的输入，从这功能描述，可以看出管道传输数据是单向的，如果想相互通信，我们需要创建两个管道才行。\n同时，我们得知上面这种管道是没有名字，所以「|」表示的管道称为匿名管道，用完了就销毁。\n管道还有另外一个类型是命名管道，也被叫做 FIFO，因为数据是先进先出的传输方式。\n在使用命名管道前，先需要通过 mkfifo 命令来创建，并且指定管道名字：\n$ mkfifo myPipe myPipe 就是这个管道的名称，基于 Linux 一切皆文件的理念，所以管道也是以文件的方式存在，我们可以用 ls 看一下，这个文件的类型是 p，也就是 pipe（管道） 的意思：\n$ ls -l prw-r--r--. 1 root root 0 Jul 17 02:45 myPipe 接下来，我们往 myPipe 这个管道写入数据：\n$ echo \u0026#34;hello\u0026#34; \u0026gt; myPipe // 将数据写进管道 // 停住了 ... 你操作了后，你会发现命令执行后就停在这了，这是因为管道里的内容没有被读取，只有当管道里的数据被读完后，命令才可以正常退出。\n于是，我们执行另外一个命令来读取这个管道里的数据：\n$ cat \u0026lt; myPipe // 读取管道里的数据 hello 可以看到，管道里的内容被读取出来了，并打印在了终端上，另外一方面，echo 那个命令也正常退出了。\n我们可以看出，管道这种通信方式效率低，不适合进程间频繁地交换数据。当然，它的好处，自然就是简单，同时也我们很容易得知管道里的数据已经被另一个进程读取了。\n那管道如何创建呢，背后原理是什么？\n匿名管道的创建，需要通过下面这个系统调用：\nint pipe(int fd[2]) 这里表示创建一个匿名管道，并返回了两个描述符，一个是管道的读取端描述符 fd[0]，另一个是管道的写入端描述符 fd[1]。注意，这个匿名管道是特殊的文件，只存在于内存，不存于文件系统中。\n其实，所谓的管道，就是内核里面的一串缓存。从管道的一段写入的数据，实际上是缓存在内核中的，另一端读取，也就是从内核中读取这段数据。另外，管道传输的数据是无格式的流且大小受限。\n看到这，你可能会有疑问了，这两个描述符都是在一个进程里面，并没有起到进程间通信的作用，怎么样才能使得管道是跨过两个进程的呢？\n我们可以使用 fork 创建子进程，创建的子进程会复制父进程的文件描述符，这样就做到了两个进程各有两个「 fd[0] 与 fd[1]」，两个进程就可以通过各自的 fd 写入和读取同一个管道文件实现跨进程通信了。\n管道只能一端写入，另一端读出，所以上面这种模式容易造成混乱，因为父进程和子进程都可以同时写入，也都可以读出。那么，为了避免这种情况，通常的做法是：\n父进程关闭读取的 fd[0]，只保留写入的 fd[1]； 子进程关闭写入的 fd[1]，只保留读取的 fd[0]； 所以说如果需要双向通信，则应该创建两个管道。\n到这里，我们仅仅解析了使用管道进行父进程与子进程之间的通信，但是在我们 shell 里面并不是这样的。\n在 shell 里面执行 A | B 命令的时候，A 进程和 B 进程都是 shell 创建出来的子进程，A 和 B 之间不存在父子关系，它俩的父进程都是 shell。\n所以说，在 shell 里通过「|」匿名管道将多个命令连接在一起，实际上也就是创建了多个子进程，那么在我们编写 shell 脚本时，能使用一个管道搞定的事情，就不要多用一个管道，这样可以减少创建子进程的系统开销。\n我们可以得知，对于匿名管道，它的通信范围是存在父子关系的进程。因为管道没有实体，也就是没有管道文件，只能通过 fork 来复制父进程 fd 文件描述符，来达到通信的目的。\n另外，对于命名管道，它可以在不相关的进程间也能相互通信。因为命令管道，提前创建了一个类型为管道的设备文件，在进程里只要使用这个设备文件，就可以相互通信。\n不管是匿名管道还是命名管道，进程写入的数据都是缓存在内核中，另一个进程读取数据时候自然也是从内核中获取，同时通信数据都遵循先进先出原则，不支持 lseek 之类的文件定位操作。\n","date":"11 March 2024","permalink":"/posts/reviews/os/%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E8%AE%AF/","section":"博客","summary":"进程间通讯指不同进程之间交换数据或信息的过程。常用方法包括管道、共享内存、消息队列和套接字等。","title":"进程间通讯"},{"content":" dp 解法 # class Solution { public int jump(int[] nums) { int n = nums.length; int[] dp = new int[n+1]; for(int i=2;i\u0026lt;=n;i++) { dp[i] = Integer.MAX_VALUE/2; } for(int i=1;i\u0026lt;=n;i++) { for(int j=1;j\u0026lt;=nums[i-1];j++){ if((i+j)\u0026gt;n) break; dp[i+j] = Math.min(dp[i]+1,dp[i+j]); } } return dp[n]; } } 正向查找可到达的最大位置 # 方法一虽然直观，但是时间复杂度比较高，有没有办法降低时间复杂度呢？\n如果我们「贪心」地进行正向查找，每次找到可到达的最远位置，就可以在线性时间内得到最少的跳跃次数。\n例如，对于数组 [2,3,1,2,4,2,3]，初始位置是下标 0，从下标 0 出发，最远可到达下标 2。下标 0 可到达的位置中，下标 1 的值是 3，从下标 1 出发可以达到更远的位置，因此第一步到达下标 1。\n从下标 1 出发，最远可到达下标 4。下标 1 可到达的位置中，下标 4 的值是 4 ，从下标 4 出发可以达到更远的位置，因此第二步到达下标 4。\n在具体的实现中，我们维护当前能够到达的最大下标位置，记为边界。我们从左到右遍历数组，到达边界时，更新边界并将跳跃次数增加 1。\n在遍历数组时，我们不访问最后一个元素，这是因为在访问最后一个元素之前，我们的边界一定大于等于最后一个位置，否则就无法跳到最后一个位置了。如果访问最后一个元素，在边界正好为最后一个位置的情况下，我们会增加一次「不必要的跳跃次数」，因此我们不必访问最后一个元素。\nclass Solution { public int jump(int[] nums) { int length = nums.length; int end = 0; int maxPosition = 0; int steps = 0; for (int i = 0; i \u0026lt; length - 1; i++) { maxPosition = Math.max(maxPosition, i + nums[i]); if (i == end) { end = maxPosition; steps++; } } return steps; } } ","date":"11 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E6%95%B0%E7%BB%84/leetcode-45%E8%B7%B3%E8%B7%83%E6%B8%B8%E6%88%8F-ii/","section":"博客","summary":"【LeetCode 45】跳跃游戏 II题解。","title":"【LeetCode 45】跳跃游戏 II"},{"content":" 26. 删除有序数组中的重复项 # class Solution { public int removeDuplicates(int[] nums) { int left = 0, n = nums.length; for(int right = 1; right \u0026lt; n; right++) { if(nums[left] != nums[right]) { nums[++left] = nums[right]; } } return left+1; } } 80. 删除有序数组中的重复项 II # class Solution { public int removeDuplicates(int[] nums) { int n = nums.length; if (n \u0026lt;= 2) { return n; } int slow = 0, fast = 2; while (fast \u0026lt; n) { if (nums[slow] != nums[fast]) { nums[slow+2] = nums[fast]; ++slow; } ++fast; } return slow+2; } } ","date":"11 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E6%95%B0%E7%BB%84/leetcode-80%E5%88%A0%E9%99%A4%E6%9C%89%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/","section":"博客","summary":"【LeetCode 80】删除有序数组中的重复项题解。","title":"【LeetCode 80】删除有序数组中的重复项"},{"content":"","date":"11 March 2024","permalink":"/posts/reviews/os/%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B/","section":"博客","summary":"进程与线程的基本知识","title":"进程与线程的基本知识"},{"content":"前言 # 什么是分布式系统 # 将硬件或软件组件(服务)分布在不同的网络计算机上，并且通过消息传递进行通信和协调。比如使用工行卡给支付宝充值，工行卡的账户位于工商银行的 db 中，而支付宝账户位于支付宝的 db 中，2 个 db 位于不同的地方。\n特点\n分布性 对等性 平等: 无主从之分 独立: 拥有自己的CPU和内存，独立处理数据 并发性 外部: 承载多个客户端的并发访问 内部: 作业(Job)被分解成多个任务(Task)，并发运行在不同的节点上 故障独立性 部分节点出现故障不影响整个系统的正常使用 什么是事务 # 完成某件事情，可能有多个参与者需要执行多个步骤，最终多个步骤要么全部成功，要么全部失败。\n举个例子：微信上 A 给 B 转账 100 元，A 账户减少 100，B 账户增加 100，这就是一个事务，这个操作中要么都成功，要么都失败。\n事务的场景有很多，参与者也是多种多样\n用户注册成功发送邮件，包含 2 个操作：db 中插入用户信息，给用户发送邮件，主要的 2 个参与者：db、邮件服务器 使用支付宝充值话费，包含 2 个操作：支付宝账户资金减少，手机余额增加，主要的 2 个参与者：支付宝账户、手机号服务商账户 事务的参与者是多种多样的，不过本文我们主要以 db 中的事务来做说明。\n什么是本地事务？\n本地事务，通俗点理解：即事务中所有操作发生在同一个数据库中的情况。\n比如 A 给 B 转账，A 和 B 的账户位于同一个数据库中。\n通常我们用的都是关系型数据库，比如：MySQL、Oracle、SQL Server，这些数据库默认情况，这些 db 已经实现了事务的功能，即在一个 db 中执行一个事务操作，db 本身就可以确保这个事务的正确性，而不需要我们自己去考虑如何确保事务的正确性。\n数据库事务的 4 大特性\n一致性(Consistency)：事务操作之后的结果和期望的结果是一致的，A 给 B 转账 100，事务结束之后，看到 A 的账户应该减少 100，B 的账户应该增加 100，不会出现其他情况 原子性(Atomicity)：事务的整个过程如原子操作一样，最终要么全部成功，或者全部失败，这个原子性是从最终结果来看的，从最终结果来看这个过程是不可分割的。 隔离性(Isolation)：一个事务的执行不能被其他事务干扰。即一个事务内部的操作及使用的数据对并发的其他事务是隔离的，并发执行的各个事务之间不能互相干扰。 持久性(Durability)：一个事务一旦提交，他对数据库中数据的改变就应该是永久性的。当事务提交之后，数据会持久化到硬盘，修改是永久性的。 什么是分布式事务？ # 分布式、事务这 2 个概念大家都理解了，那么分布式事务很容易理解了：事务的多个参与者分布在不同的地方。\n单个 db 中我们很容易确保事务的正确性，但是当事务的参与者位于多个 db 中的时候，如何确保事务的正确性呢？\n比如：A 给 B 转账，A 位于 DB1 中，B 位于 DB2 中\nstep1.通过网络，给DB1发送指令：给A账户减少100 step2.通过网络，给DB2发送指令：给B账户增加100 step1 成功之后，执行 step2 的时，网络出现故障，导致 step2 执行失败，最终：A 减少了 100，B 却没有增加 100，最终的结果和期望的结果不一致，导致了事务的失败。\n在介绍分布式事务的解决方案之前，我们需要先了解另外 2 个概念：CAP 和 Base 理论，这 2 个理论为分布式事务的解决提供了依据。\nCAP定理 # CAP 是 Consistency、Availability、Partition tolerance 三个词语的缩写，分别表示一致性、可用性、分区容忍性。\n下边我们分别来解释：\n为了方便对 CAP 理论的理解，我们结合电商系统中的一些业务场景来理解 CAP。\n如下图，是商品信息管理的执行流程：\n整体执行流程如下：\n商品服务请求主数据库写入商品信息（添加商品、修改商品、删除商品） 主数据库向商品服务响应写入成功。 商品服务请求从数据库读取商品信息。 C - Consistency # 一致性是指写操作后的读操作可以读取到最新的数据状态，当数据分布在多个节点上，从任意结点读取到的数据都是最新的状态。\n上图中，商品信息的读写要满足一致性就是要实现如下目标：\n商品服务写入主数据库成功，则向从数据库查询新数据也成功。 商品服务写入主数据库失败，则向从数据库查询新数据也失败。 如何实现一致性？\n写入主数据库后要将数据同步到从数据库。 写入主数据库后，在向从数据库同步期间要将从数据库锁定，待同步完成后再释放锁，以免在新数据写入从库的过程中，客户端向从数据库查询到旧的数据。 分布式系统一致性的特点：\n由于存在数据同步的过程，写操作的响应会有一定的延迟。 为了保证数据一致性会对资源暂时锁定，待数据同步完成释放锁定资源。 如果请求数据同步失败的结点则会返回错误信息，一定不会返回旧数据。 A - Availability # 可用性是指任何事务操作都可以得到响应结果，且不会出现响应超时或响应错误。\n上图中，商品信息读取满足可用性就是要实现如下目标：\n从数据库接收到数据查询的请求则立即能够响应数据查询结果。 从数据库不允许出现响应超时或响应错误。 如何实现可用性？\n写入主数据库后要将数据同步到从数据库。 由于要保证从数据库的可用性，不可将从数据库中的资源进行锁定。 即使数据还没有同步过来，从数据库也要返回要查询的数据，哪怕是旧数据，如果连旧数据也没有则可以按照约定返回一个默认信息，但不能返回错误或响应超时。 分布式系统可用性的特点：\n所有请求都有响应，且不会出现响应超时或响应错误。 P - Partition tolerance # 通常分布式系统的各个结点部署在不同的子网，这就是网络分区，不可避免的会出现由于网络问题而导致结点之间通信失败，此时仍可对外提供服务，这叫分区容忍性。\n上图中，商品信息读写满足分区容忍性就是要实现如下目标：\n主数据库向从数据库同步数据失败不影响读写操作。 其一个结点挂掉不影响另一个结点对外提供服务。 如何实现分区容忍性？\n尽量使用异步取代同步操作，例如使用异步方式将数据从主数据库同步到从数据，这样结点之间能有效的实现松耦合。 添加从数据库结点，其中一个从结点挂掉其它从结点提供服务。 分布式分区容忍性的特点：\n分区容忍性分是布式系统具备的基本能力 CAP 组合方式 # 1、上边商品管理的例子是否同时具备 CAP 呢？\n在所有分布式事务场景中不会同时具备 CAP 三个特性，因为在具备了 P 的前提下 C 和 A 是不能共存的。\n比如：\n下图满足了 P 即表示实现分区容忍：\n本图分区容忍的含义是：\n主数据库通过网络向从数据同步数据，可以认为主从数据库部署在不同的分区，通过网络进行交互。 当主数据库和从数据库之间的网络出现问题不影响主数据库和从数据库对外提供服务。 其一个结点挂掉不影响另一个结点对外提供服务。 如果要实现 C 则必须保证数据一致性，在数据同步的时候为防止向从数据库查询不一致的数据则需要将从数据库数据锁定，待同步完成后解锁，如果同步失败从数据库要返回错误信息或超时信息。\n如果要实现 A 则必须保证数据可用性，不管任何时候都可以向从数据查询数据，则不会响应超时或返回错误信息。\n通过分析发现在满足 P 的前提下 C 和 A 存在矛盾性，如下：\n网络分区的情况下，主库的数据无法同步给从库，为了确保外面看到数据是一致的，此时从库不能让外部访问，只能让主库对外提供服务，从库失去了可用性。\n网络分区的情况下，主库的数据无法同步给从库，此时 2 个库数据是不一致的，如果此允许 2 个库都可以对外提供服务（可用性），那么访问到的数据是不一致的。\n所以 CAP 无法同时满足。\n2、CAP 有哪些组合方式呢？\n所以在生产中对分布式事务处理时要根据需求来确定满足 CAP 的哪两个方面。\n1）AP：\n放弃一致性，追求分区容忍性和可用性。这是很多分布式系统设计时的选择。\n例如：上边的商品管理，完全可以实现 AP，前提是只要用户可以接受所查询的到数据在一定时间内不是最新的即可。\n通常实现 AP 都会保证最终一致性，后面讲的 BASE 理论就是根据 AP 来扩展的，一些业务场景 比如：订单退款，今日退款成功，明日账户到账，只要用户可以接受在一定时间内到账即可。\n2）CP：\n放弃可用性，追求一致性和分区容错性，我们的 zookeeper 其实就是追求的强一致。\n3）CA：\n放弃分区容忍性，即不进行分区，不考虑由于网络不通或结点挂掉的问题，则可以实现一致性和可用性。\n那么系统将不是一个标准的分布式系统，我们最常用的关系型数据就满足了 CA。\n上边的商品管理，如果要实现 CA 则架构如下：\n主数据库和从数据库中间不再进行数据同步，数据库可以响应每次的查询请求，通过事务隔离级别实现每个查询请求都可以返回最新的数据。\nCAP 是一个已经被证实的理论：一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容忍性（Partition tolerance）这三项中的两项。它可以作为我们进行架构设计、技术选型的考量标准。对于多数大型互联网应用的场景，结点众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到 N 个 9（99.99..%），并要达到良好的响应性能来提高用户体验，因此一般都会做出如下选择：保证 P 和 A，舍弃 C 强一致，保证最终一致性\nBase 理论 # BASE 是 Basically Available（基本可用）、Soft state（软状态）和 Eventually consistent（最终一致性）三个短语的缩写。BASE 理论是对 CAP 中 AP 的一个扩展，通过牺牲强一致性来获得可用性，当出现故障允许部分不可用但要保证核心功能可用，允许数据在一段时间内是不一致的，但最终达到一致状态。满足 BASE 理论的事务，我们称之为“柔性事务”。\n基本可用 # 分布式系统在出现故障时，允许损失部分可用功能，保证核心功能可用。如，电商网站交易付款出现问题了，商品依然可以正常浏览。\n软状态 # 由于不要求强一致性，所以 BASE 允许系统中存在中间状态（也叫软状态），这个状态不影响系统可用性，如订单的\u0026quot;支付中\u0026quot;、“数据同步中”等状态，待数据最终一致后状态改为“成功”状态。\n最终一致 # 最终一致是指经过一段时间后，所有节点数据都将会达到一致。如订单的\u0026quot;支付中\u0026quot;状态，最终会变为\u0026quot;支付成功\u0026quot;或者\u0026quot;支付失败\u0026quot;，使订单状态与实际交易结果达成一致，但需要一定时间的延迟、等待。\n分布式事务常见 5 种解决方案 # 方案 1：2PC（二阶段提交） 方案 2：3PC（三阶段提交） 方案 3：TCC 方案 4：可靠消息 方案 5：最大努力通知型 下面依次来介绍这 5 种方案。\n2PC（二阶段提交） # 2PC 即两阶段提交，是将整个事务流程分为两个阶段，准备阶段（Prepare phase）、提交阶段（commit phase），2 是指两个阶段，P 是指准备阶段，C 是指提交阶段。\n2PC 中主要的 2 个角色：\n事务协调者 事务参与者 准备阶段\n事务协调者给每个事务参与者发送 prepare 消息，每个参在本地执行本地事务但是不要提交事务（此时事务操作的资源可能被锁定），然后给协调者返回 yes 或者 no 的消息。\n提交阶段\n准备阶段中所有参与者返回 yes，此时事务协调者会给每个事务参与者发送 commit 消息，参与者接收到 commit 消息之后，会对本地事务执行提交操作。\n若准备阶段中有参与者返回 no，或者参与者响应超时（比如网络原因，导致事务协调者和事务参与者之间通讯故障），此时事务协调者会给每个事务参与者发送 rollback 消息，参与者接收到 rollback 消息之后，会对本地事务执行回滚操作。\n2pc 中的一些规则\n阶段 2 commit 的条件：阶段 1 中所有的参与者返回 yes 阶段 2 rollback 的条件，2 种情况：阶段 1 中任意参与者返回 no 时，或者阶段 1 中任意参与者响应超时 当参与者 prepare 可以成功，那么给参与者发送 commit 也一定可以成功，发送 rollback 一定可以回滚 2PC 中事务协调者这边有超时机制，即在阶段 1 中，协调者给参与者发送消息，一直没有回应，导致超时，此时，直接执行第二阶段 rollback；而协调者这边并没有超时机器，比如所有参与者阶段 1 执行完毕了，然后协调者挂了，此时参与者只能一直等了，干等。 2PC 存在的问题\n当阶段一都执行完毕之后，参与者本地事务执行了但是还未提交，此时参与者本地事务中的资源处于锁定状态的，若此时协调者挂了，会导致参与者本地事务锁住的资源无法释放，而直接影响到其他业务的执行。 比如参与者 1 中去对商品 1 减库存，商品 1 的库存记录会被上锁，若此时其他业务也需要修改这条记录，直接会被阻塞，导致无法执行。 2PC 有性能问题：比如事务中有 10 个参与者，参与者 1 在阶段 1 中会锁定本地资源，然后等待其他 9 个参与者执行完毕阶段一，然后参与者 1 收到事务协调器发送的 commit 或者 rollback 之后，才会释放资源，参与者 1 需要等待 9 个参与者，导致锁定资源的时间太长，会影响系统的并发量。 协调者有单点故障：当阶段 1 执行完毕之后，协调者挂了，此时参与者懵了，只能一直等待，这个可以通过协调者高可用来解决，后面讲到的 3pc 中解决了这个问题。 事务不一致的问题：阶段 2 中部分参与者收到了 commit 信息，此时协调者挂了或者网络问题，导致其他协调者无法收到 commit 请求，这个过程中，多个协调者中数据是不一致的。解决方式：协调者、参与者要高可用，协调者支持 2PC 重试，2PC 中的 2 个阶段需要支持幂等。 XA 事务 # XA（eXtended Architecture）是指由 X/Open 组织提出的分布式交易处理的规范。XA 是一个分布式事务协议，由 Tuxedo 提出，所以分布式事务也称为 XA 事务。\nXA 协议主要定义了事务管理器 TM（Transaction Manager，协调者）和资源管理器 RM（Resource Manager，参与者）之间的接口。\n其中，资源管理器往往由数据库实现，如 Oracle、DB2、MySQL，这些商业数据库都实现了 XA 接口，而事务管理器作为全局的调度者，负责各个本地资源的提交和回滚。\nXA 事务是基于两阶段提交（Two-phaseCommit，2PC）协议实现的，可以保证数据的一致性，许多分布式关系型数据管理系统都采用此协议来完成分布式。阶段一为准备阶段，即所有的参与者准备执行事务并锁住需要的资源。当参与者 Ready 时，向 TM 汇报自己已经准备好。阶段二为提交阶段。当 TM 确认所有参与者都 Ready 后，向所有参与者发送 COMMIT 命令。\n说的简单点：XA 就是 2PC 在数据中的一种实现。\nmysql 大家都用过，普通事务过程：\nstart transaction; //打开事务 执行事务操作 commit|rollback; // 提交或者回滚事务 上面事务操作中，若当前连接未发送 commit 或者 rollback 操作，此时连接断掉或者 mysql 重启了，上面的事务会被自动回滚。\nmysql 中 xa 的语法：\nXA {START|BEGIN} xid [JOIN|RESUME] //开启XA事务，如果使用的是XA START而不是XA BEGIN，那么不支持[JOIN|RESUME]，xid是一个唯一值，表示事务分支标识符 XA END xid [SUSPEND [FOR MIGRATE]] //结束一个XA事务，不支持[SUSPEND [FOR MIGRATE]] XA PREPARE xid 准备提交 XA COMMIT xid [ONE PHASE] //提交，如果使用了ONE PHASE，则表示使用一阶段提交。两阶段提交协议中，如果只有一个RM参与，那么可以优化为一阶段提交 XA ROLLBACK xid //回滚 XA RECOVER [CONVERT XID] //列出所有处于PREPARE阶段的XA事务 如：\nxa start \u0026#39;xa-1\u0026#39;; 执行事务操作; xa prepare \u0026#39;xa-1\u0026#39;; //阶段1，此时事务操作的资源被锁住，事务未提交 xa commit | rollback;//阶段2 xa 事务和普通事务有点区别，上面这个 xa 事务有个标识xa-1，当xa-1prepare 之后，如果此时连接断掉或者 mysql 重启了，这个事务还是处于prepare阶段，mysql 重启或者调用者重新连接 mysql 之后，可以拿着这个事务标识xa-1继续发送xa commit |rollback来结束这个事务。\n大家可以在 mysql 中创建几个 db，然后通过上面的 xa 脚本试试两阶段提交，感受一下过程。\nXA 中事务协调器设计要点\nXA 中，事务参与者，比如常见的一些 db，已经实现了 2PC 的功能，但是协调器需要自己来开发，协调器的一些设计要点：\n生成全局唯一 XA 事务 id 记录，并且记录下来 事务协调器需要有重试的功能，对于中间阶段操作异常的，通过不断的重试让事务最终能够完成 协调器会有重试操作，所以需确保 2pc 中每个阶段都是幂等的 2PC 解决方案 # Seata：Seata 是由阿里中间件团队发起的开源项目 Fescar，后更名为 Seata，它是一个是开源的分布式事务框架，这个框架支持 2PC。 atomikos+jta：jta 是 java 中对分布式事务制定的接口规范，atomikos 是 jta 的一种实现，内部是依靠 XA 的方式来实现的，如果事务参与者都自测 XA 事务，可以通过这种方式来解决，比如参与者是：mysql、oracle、sqlserver，可以使用采用这种方式；不过性能方面是值得大家考虑的一个问题。 开发者自己实现 ：大家对 2pc 过程了解之后，可以自己开发一个，可以去挑战一下。 3PC（三阶段提交） # 回顾 2PC # 举个例子，A 邀请 B、C 一起打王者荣耀，2PC 过程如下：\nA 是协调者，B、C 是参与者。\n阶段 1（prepare 阶段）\nstep1-1：A 微信 B\nstep1-1-1：A-\u0026gt;B：有空么，我们约C一起王者荣耀 step1-1-2：B-\u0026gt;A：有空 step1-1-3：A-\u0026gt;B：那你现在就打开电脑，登录王者荣耀，你等着，我去通知C，然后开个房间 step1-1-4：B-\u0026gt;A：已登录 step1-2：A 微信 C\nstep1-2-1：A-\u0026gt;C：有空么，我约了B一起王者荣耀 step1-2-2：C-\u0026gt;A：有空 step1-2-3：A-\u0026gt;C：那你现在就打开电脑，登录王者荣耀，你等着，我去开个房间 step1-2-4：C-\u0026gt;A：已登录 阶段 2（commit 阶段）\n此时 B、C 都已经登录王者容易了，然后 A 登录王者荣耀开了个房间\nstep2-1：A 微信 B\nstep2-1-1：A-\u0026gt;B：房间号是xxx，你可以进来了 step2-1-2：B-\u0026gt;A：我的，我进来了 step2-2：A 微信 C\nstep2-2-1：A-\u0026gt;C：房间号是xxx，你可以进来了 step2-2-2：C-\u0026gt;A：我的，我进来了 然后 3 个人开始爽歪歪了。\n2PC 一些异常情况\n情况 1：step1-2-4 超时，导致 A 无法收到 C 已登录的消息 此时 A 不知道 C 是什么情况，但是 2PC 中协调者这边有超时机制，如果协调者给参与者发送信息，长时间得不到回应时，将作为失败处理，此时 A 会给 B 和 C 发送 rollback 消息，让 B 和 C 都进行回滚，即取消游戏。 情况 2：step1-1 之后，协调者 A 挂了 此时 B 已经打开电脑在那等着了，却始终不见 A、C 的踪影，相当苦恼，也不知道还要等多久，苦逼！ 情况 3：阶段 1 之后，协调者 A 挂了 此时 B、C 登录账号了，也等了十几分钟了，就是不见 A 的踪影，也只能干等着，什么事情也做不了。 情况 4：step2-2-1 出现问题，C 网络故障 此时 C 收不到 A 发送过来的消息，结果是导致 A 和 B 都已经进入房间了，就缺 C 了，游戏无法正常开始，导致最终的结果和期望的结果无法一致（期望 3 个人一起玩游戏，实际上房间里只有 2 个人） 总的来说，2PC 主要有 2 个问题\n参与者干等的问题：参与者只能按照协调者的指令办事，当收不到协调者的指令的时候，参与者只能坐等，在 db 中的效果，操作的数据会被一直锁着，导致其他操作者被阻塞。 数据不一致的问题：commit 阶段，协调者或者参与者挂掉，都可能导致最终数据不一致的问题。 3PC # 3PC 主要解决了 2PC 中 commit 阶段参与者干等的问题，2PC 中 commit 阶段，若协调者挂了，参与者不知道如何走了。2PC 中只有协调者这边有超时机制，而 3PC 中，协调者和参与者这边引入了超时机制，commit 阶段，若参与超过一定的时间收不到 commit 命令，参与者会自动提交，从而解决了 2PC 中资源长时间被锁的问题。\n3PC 相对于 2PC，多了一个阶段，相当于把 2PC 的准备阶段再次一分为二，这样三阶段提交就有CanCommit、PreCommit、DoCommit三个阶段。\n阶段 1：CanCommit 阶段\n之前 2PC 的一阶段是本地事务执行结束后，最后不 Commit，等其它服务都执行结束并返回 Yes，由协调者发出 commit 才真正执行 commit，而这里的 CanCommit 指的是 尝试获取数据库锁 如果可以，就返回 Yes。\n这阶段主要分为 2 步\n事务询问：协调者向参与者发送 CanCommit 请求。询问是否可以执行事务提交操作。然后开始等待参与的响应。 响应反馈：参与者接到 CanCommit 请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回 Yes 响应，并进入预备状态。否则反馈 No，然后事务就结束了，此时参与者并没有执行任务任何操作。 阶段 2：PreCommit 阶段\n在阶段一中，如果所有的参与者都返回 Yes 的话，那么就会进入 PreCommit 阶段进行事务预提交。这里的 PreCommit 阶段 跟上面的第一阶段是差不多的，只不过这里 协调者和参与者都引入了超时机制（2PC 中只有协调者可以超时，参与者没有超时机制）。\n阶段 3：DoCommit 阶段\n这里跟 2pc 的阶段二是差不多的。\n例子：王者荣耀 3PC 过程 # 正常的过程\n阶段 1（CanCommit 阶段）\nstep1-1：A 微信 B\nstep1-1-1：A-\u0026gt;B：有空么，我们约C一起王者荣耀 step1-1-2：B-\u0026gt;A：有空 step1-2：A 微信 C\nstep1-1-1：A-\u0026gt;B：有空么，我们约B一起王者荣耀 step1-1-2：B-\u0026gt;A：有空 阶段 2（PreCommit 阶段）\nstep2-1：A 微信 B\nstep2-1-1：A-\u0026gt;B：你现在就打开电脑，登录王者荣耀，等我消息，如果10分钟没消息，你就自己开个房间玩吧（参与者超时机制）。 step2-1-2：B-\u0026gt;A：已登录 step2-2：A 微信 C\nstep2-2-1：A-\u0026gt;C：那你现在就打开电脑，登录王者荣耀，等我消息，如果10分钟没消息，你就自己开个房间玩吧（参与者超时机制）。 step2-2-2：C-\u0026gt;A：已登录 阶段 3（DoCommit 阶段）\n此时 B、C 都已经登录王者荣耀了，然后 A 登录王者荣耀开了个房间\nstep3-1：A 微信 B\nstep3-1-1：A-\u0026gt;B：房间号是xxx，你可以进来了 step3-1-2：B-\u0026gt;A：我的，我进来了 step3-2：A 微信 C\nstep3-2-1：A-\u0026gt;C：房间号是xxx，你可以进来了 step3-2-2：C-\u0026gt;A：我的，我进来了 然后 3 个人开始爽歪歪了。\n异常的几种情况\n阶段 1 异常：此时并没有进行事务操作，所以这个阶段出问题了，可以直接结束事务。 阶段 2，参与者挂了：参与者挂了没关系，协调者直接通知其他参与者回滚。 阶段 2，协调者挂了：协调者挂了，由于参与者引入了超时机制，所以参与者并不会无限期等待，等待一定的时间之后，会自动提交本地事务。虽然这个超时机制解决了无限等待的问题，却并没有解决一致性的问题，比如上面 3PC 中step2-1：A微信B之后，协调者挂了，此时 A 已经登录了，但是 C 未收到 A 要求登录的消息，超时 10 分钟之后，A 自己去开了一个游戏玩起来了，结果和期望的结果不一致了。 3PC 存在的问题\n虽然解决了 2PC 中参与者长时间阻塞的问题（资源长时间无法释放的问题），但是并没有解决一致性的问题。\n有没有办法解决这些问题？\n有，TCC，接下来，我们来看 TCC。\nTCC # 分布式事务中的几个角色\nTM：事务管理器，可以理解为分布式事务的发起者 分支事务：事务中的多个参与者，可以理解为一个个独立的事务。 TCC 是 Try、Confirm、Cancel 三个词语的缩写，TCC 要求每个分支事务实现三个操作：预处理 Try、确认 Confirm、撤销 Cancel。\nTry 操作做业务检查及资源预留，Confirm 做业务确认操作，Cancel 实现一个与 Try 相反的操作即回滚操作。\nTM 首先发起所有的分支事务的 try 操作，任何一个分支事务的 try 操作执行失败，TM 将会发起所有分支事务的 Cancel 操作，若 try 操作全部成功，TM 将会发起所有分支事务的 Confirm 操作，其中 Confirm/Cancel 操作若执行失败，TM 会进行重试。\n正常流程\ntry 阶段：依次调用参与者的 try 方法，都返回成功 confirm 阶段：依次调用参与者的 confirm 方法，都返回成功 事务完成。 异常流程\ntry 阶段：依次调用参与者的 try 方法，前面 2 个参与者 try 方法返回 yes，而参与者 3 返回 no cancel 阶段：对已经成功的参与者执行 cancel 操作，注意了：cancel 阶段参与者调用的顺序和 try 阶段参与者的顺序相反，即先调用参与者 2 的 cancel，然后调用参与者 1 的 cancel。 TCC 场景案例\n案例 1：跨库转账\n举例，场景为 A 转账 100 元给 B，A 和 B 账户在不同的服务。\n账户A try： try幂等校验 检查余额是否够100元 A账户扣减100元 confirm： 空 cancel： cancel幂等校验 A账户增加可用余额100元 账户B try： 空 confirm： confirm幂等校验 B账户增加100元 cancel： 空 案例 2：提现到支付宝\n举例，大家玩过抖音，有些朋友抖音上面有收益，可以将收益提现到支付宝，假如提现 100 到支付宝\n抖音(账户表：余额、冻结金额) try： try幂等校验 检查余额是否够100元 抖音账户表余额-100，冻结金额+100 confirm： confirm幂等校验 抖音账户冻结金额-100 cancel： cancel幂等校验 抖音账户表余额+100，冻结金额-100 账户B try： 空 confirm： confirm幂等校验 调用支付宝打款接口，打款100元（对于商户同一笔订单支付宝接口是支持幂等的） cancel： 空 TCC 常见框架 # 涉及到的角色（事务发起者、事务参与者、TCC 服务）\n事务发起者（TM） 发起分布式事务：调用 tcc 服务注册一个分布式事务订单 调用分支：依次调用每个分支 上报结果：最终将事务所有分支的执行结果汇报给 TCC 服务 提供补偿接口：给 TCC 服务使用，tcc 服务会调用这个补偿接口对进行补偿操作 事务参与者 提供 3 个方法：try、confirm、cancel 确保 3 个方法的幂等性 3 个方法返回的结果状态码只有 3 种（成功、失败、处理中），处理中相当于状态未知，对于状态未知的，会在补偿的过程中进行重试 TCC 服务 是一个独立的服务 提供分布式事务订单注册接口：给事务发起者使用【事务发起者调用 tcc 服务生成一个分布式事务订单（订单状态：0：处理中，1：处理成功，2：处理失败），获取一个分布式订单 id：TID】 提供分布式事务结果上报接口：给事务发起者使用【事务发起者在事务的执行过程中将事务的执行结果汇报给 TCC 服务】 提供事务补偿操作：启动一个 job 轮询 tcc 订单中状态为 1 的订单，继续调用事务发起者进行补偿，最终经过多次补偿，这个订单最终的状态应该为 1（成功）或者 2（失败）；否则人工介入进行处理 自研 TCC 框架技术要点\n框架应该考虑的地方：开发者应该只用关注分支中 3 个方法的代码，其他的应该全部交由框架去完成。 tcc 服务中的事务订单表设计 id：订单 id bus_order_id：业务方订单 id bus_order_type：业务类型 （bus_order_id \u0026amp; bus_order_type 需唯一） request_data：业务请求数据，json 格式存储，包含了玩转的业务方请求数据 status：状态，0：处理中，100：处理成功，200：处理失败，初始状态为 0，最终必须为 100 或者 200 关于分支中 3 个方法幂等的设计 以 java 中的 spring 为例，可以通过拦截器来实现，拦截器对分支的 3 个方法进行拦截，拦截器中实现幂等性的操作。\n可以用一张表来实现【分支方法执行记录表：tid、分支、方法（try、confirm、cancel）、状态（0：处理中；100:成功；200：失败）、request_json（请求参数）、response_json（响应参数）】\n关于请求参数：这个用来记录整个方法请求的完整参数，内部包含了业务参数，可以采用 json 格式存储。\n响应参数：分支方法的执行结果，以 json 格式存储。\n拦截器中，通过分支 \u0026amp; 方法 这 2 个条件去查询分支方法执行记录表，如果查询的记录状态为 100 或者 200，那么直接将 response_json 返回。\ntry 阶段同步、其他阶段异步 如果 try 阶段全部成功，那么 confirm 阶段最终应该一定是成功的，try 阶段如果有失败的，那么需要执行 cancel，最终所有的 cancel 应该也是一定可以成功的；所以 try 阶段完成之后，其实已经知道最终的结果了，所以 try 阶段完成之后，后面的 confirm 或者 cancel 可以采用异步的方式去执行；提升系统整体的性能。\n异步上报事务执行结果 发起方将所有分支每个步骤的执行结果及最终事务的执行结果上报给 tcc 服务，由 tcc 服务落库，方便运营人员查看事务执行结果以及排错。\n关于补偿 tcc 服务中添加一个补偿 job，定时轮询 tcc 分布式订单表，将状态为处理中的记录撸出来，订单表 request_data 包含了请求参数，使用 request_data 去调用事务发起者提供的补偿接口进行补偿操作，直到订单的状态为最终状态（成功或者失败）。\n补偿采用衰减的形式，对应同一笔订单采用时间间隔衰减的方式补偿，每次间隔时间：10s、20s、40s、80s、160s、320s。。。\n人工干预 tcc 分布式订单如果长期处于处理中，经过了很多次的补偿，也未能到达最终状态，此时可能业务有问题，需要人工进行补偿，对于这对订单记录需要有监控系统进行报警，提醒开发者进行干预处理。\n如果拿 TCC 事务的处理流程与 2PC 两阶段提交做比较，2PC 通常都是在跨库的 DB 层面，而 TCC 则在应用层面的处理，是 2PC 在应用层面的一种实现，需要通过业务逻辑来实现。这种分布式事务的实现方式的优势在于，可以让应用自己定义数据操作的粒度，使得降低锁冲突、提高吞吐量成为可能。而不足之处则在于对应用的侵入性非常强，业务逻辑的每个分支都需要实现 try、confirm、cancel 三个操作，代码量比较大。\n可靠消息 # 可靠消息最终一致性方案是指当事务发起方执行完成本地事务后并发出一条消息，事务参与方(消息消费者)一定能够接收消息并处理事务成功，此方案强调的是只要消息发给事务参与方最终事务要达到一致。\n这里面有 2 个重点：\n消息发送方本地事物执行成功之后，消息一定会投递成功 消息消费者最终也一定能够消费此消息，最终使分布式事务最终达成一致性 下单送积分\n电商中有这样的一个场景：商品下单之后，需给用户送积分，订单表和积分表分别在不同的 db 中，涉及到分布式事务的问题。\n我们通过可靠消息来解决这个问题：\n商品下单成功之后送积分的操作，我们使用 mq 来实现 商品下单成功之后，投递一条消息到 mq，积分系统消费消息，给用户增加积分 我们主要讨论一下，商品下单及投递消息到 mq 的操作，如何实现？每种方式优缺点？\n投递消息过程：方式一\nstep1：开启本地事务 step2：生成购物订单 step3：投递消息到 mq step4：提交本地事务 这种方式是将发送消息放在了事务提交之前。\n可能存在的问题\nstep3 发生异常：导致 step4 失败，商品下单失败，直接影响到商品下单业务 step4 发生异常，其他 step 成功：商品下单失败，消息投递成功，给用户增加了积分 投递消息过程：方式二\nstep1：开启本地事务 step2：生成购物订单 step3：提交本地事务 step4：投递消息到 mq 可能会出现的问题\nstep4 发生异常，其他 step 成功：导致商品下单成功，投递消息失败，用户未增加积分\n投递消息过程：方式三\nstep1：开启本地事务 step2：生成购物订单 step3：本地库中插入一条需要发送消息的记录 t_msg_record step4：提交本地事务 step5：新增一个定时器，轮询 t_msg_record，将待发送的记录投递到 mq 中 这种方式借助了数据库的事务，业务和消息记录作为了一个原子操作，业务成功之后，消息日志必定是存在的。解决了前两种方式遇到的问题。如果我们的业务系统比较单一，可以采用这种方式。\n对于微服务化的情况，上面这种方式不是太好，每个服务都需要上面的操作；也不利于扩展。\n投递消息过程：方式四\n增加一个消息服务及消息库，负责消息的落库、将消息发送投递到 mq。\nstep1：开启本地事务 step2：生成购物订单 step3：当前事务库插入一条日志：生成一个唯一的业务 id（bus_id），将 bus_id 和订单关联起来保存到当前事务所在的库中 step4：调用消息服务：携带 bus_id，将消息先落地入库，此时消息的状态为待发送状态，返回消息 id(msg_id) step5：提交本地事务 step6：如果上面都成功，调用消息服务，将消息投递到 mq 中；如果上面有失败的情况，则调用消息服务取消消息的发送 能想到上面这种方式，已经算是有很大进步了，我们继续分析一下可能存在的问题：\n系统中增加了一个消息服务，商品下单操作依赖于该服务，业务对该服务依赖性比较高，当消息服务不可用时，整个业务将不可用。 若 step6 失败，消息将处于待发送状态，此时业务方需要提供一个回查接口（通过 bus_id 查询）,验证业务是否执行成功；消息服务需新增一个定时任务，对于状态为待发送状态的消息做补偿处理，检查一下业务是否处理成功；从而确定消息是投递还是取消发送 step4 依赖于消息服务，如果消息服务性能不佳，会导致当前业务的事务提交时间延长，容易产生死锁，并导致并发性能降低。我们通常是比较忌讳在事务中做远程调用处理的，远程调用的性能和时间往往不可控，会导致当前事务变为一个大事务，从而引发其他故障。 投递消息过程：方式五\n在以上方式中，我们继续改进，进而出现了更好的一种方式：\nstep1：生成一个全局唯一业务消息 id(bus_msg_id)，调用消息服务，携带 bus_msg_id，将消息先落地入库，此时消息的状态为待发送状态，返回消息 id（msg_id） step2：开启本地事务 step3：生成购物订单 step4：当前事务库插入一条日志（将 step3 中的业务和 bus_msg_id 关联起来） step5：提交本地事务 step6：分 2 种情况：如果上面都成功，调用消息服务，将消息投递到 mq 中；如果上面有失败的情况，则调用消息服务取消消息的发送 若 step6 失败，消息将处于待发送状态，此时业务方需要提供一个回查接口（通过 bus_msg_id 查询）,验证业务是否执行成功；\n消息服务需新增一个定时任务，对于状态为待发送状态的消息做补偿处理，检查一下业务是否处理成功；从而确定消息是投递还是取消发送。\n方式五和方式四对比，比较好的一个地方：将调用消息服务，消息落地操作，放在了事务之外进行，这点小的改进其实算是一个非常好的优化，减少了本地事务的执行时间，从而可以提升并发量，阿里有个消息中间件 RocketMQ 就支持方式 5 这种，大家可以去用用。\n如何解决重复消费的问题？\n消费者轮询从 mq server 中拉取消息，然后进行消费。\n消息消费者消费消息的过程\nstep1：从 mq 中拉取消息 step2：执行本地业务，比如增加积分操作 step3：消费完毕之后，将消息从 mq 中删掉 当 step2 成功，step3 失败之后，这个消息会再次从 mq 中拉取出来，会出现重复消费的问题，所以我们需要考虑消费的幂等性，同一条消息多次消费和一次消费产生的结果应该是一致的，关于幂等性是另外一个课题，下次会详说。\n最大努力通知型 # 支付宝充值案例\n假如我们自己有一个电商系统，支持用户使用支付宝充值，流程如下：\n用户支付流程（是一个同步的过程）\n用户在浏览器发起充值请求-\u0026gt;电商服务 电商服务生成充值订单，状态为 0：待支付（0：待支付、100：支付成功、200：支付失败） 电商服务携带订单信息请求支付宝，生成支付宝订单，组装支付宝支付请求地址（订单信息、支付成功之后展示给用户的页面 return_url、支付异步通知地址 notify_url），将组装的信息返回给用户 用户浏览器跳转至支付宝支付页面，确认支付 支付宝携带支付结果同步回调 return_url，return_url 将支付结果展示给用户 支付宝将支付结果异步通知给商户\n用户支付流程完毕之后，此时支付宝中支付订单已经支付完毕，但电商中的充值订单状态还是 0（待支付），此时支付宝会通过异步的方式将支付结果通知给 notify_url，通知的过程中可能由于网络问题，导致支付宝通知失败，此时支付宝会通过多次衰减式的重试，尽最大努力将结果通知给商户，这个过程就是最大努力通知型。\n商户接收到支付宝通知之后，通过幂等性的方式对本地订单进行处理，然后告知支付宝，处理成功，之后支付宝将不再通知。\n什么是衰减式的通知？\n比如支付宝最大会尝试通知 100 次，每次通知时间间隔会递增。比如第 1 次失败之后，隔 10s 进行第 2 次通知，第 2 次失败之后，隔 30s 进行第三次通知，间隔时间依次递增的方式进行通知。\n如果支付宝一直通知不成功怎么办？\n商户可以主动去调用支付宝的查询接口，查询订单的支付状态。\n为什么需要进行异步通知？\n用户支付过程中，不是有个 return_url 么？支付宝支付成功之后会携带支付结果同步调用这个地址，那么商户直接在这个 return_url 中去处理一下本地订单状态不就可以了么？这种做法可以，但是有可能用户的网络不好，调用 return_url 失败了，此时还得依靠异步通知 notify_url 的方式将支付结果告知商户。\n最大努力通知型用在什么场景？\n分布式事务中，不能立即知道调用结果的，被调方业务处理耗时可能比较长，被调方业务处理完毕之后，可以采用最大努力通知的方式将结果通知给调用方。\n最大努力通知型要有补偿机制\n被调方会尽最大努力将结果通知给调用方，极端情况下有失败的可能，此时被调方需提供查询接口。\n调用方对于长时间不知道结果的业务，可以主动去被调方查询，然后进行处理。\n不需要通知，主动去查可以么？\n可以，被调方会提供查询接口，调用方主动去查询的方式完全是可以知道结果的，不过采用通知的方式实时性更高的一些。\n被调方成功之后，会立即通知调用方，但是调用方主动采用查询的方式，那么什么时候查询呢？这个度不好把握，所以两则结合更好。\n分布式事务对比分析 # 在学习各种分布式事务的解决方案后，我们了解到各种方案的优缺点：\n2PC 最大的诟病是一个阻塞协议。RM 在执行分支事务后需要等待 TM 的决定，此时服务会阻塞并锁定资源。由于其阻塞机制和最差时间复杂度高， 因此，这种设计不能适应随着事务涉及的服务数量增加而扩展的需要，很难用于并发较高以及子事务生命周期较长 (long-running transactions) 的分布式服务中。\n如果拿 TCC 事务的处理流程与 2PC 两阶段提交做比较，2PC 通常都是在跨库的 DB 层面，而 TCC 则在应用层面的处理，需要通过业务逻辑来实现。这种分布式事务的实现方式的优势在于，可以让应用自己定义数据操作的粒度，使得降低锁冲突、提高吞吐量成为可能。而不足之处则在于对应用的侵入性非常强，业务逻辑的每个分支都需要实现 try、confirm、cancel 三个操作。此外，其实现难度也比较大，需要按照网络状态、系统故障等不同的失败原因实现不同的回滚策略。典型的使用场景：满，登录送优惠券等。\n可靠消息最终一致性事务适合执行周期长且实时性要求不高的场景。引入消息机制后，同步的事务操作变为基于消息执行的异步操作, 避免了分布式事务中的同步阻塞操作的影响，并实现了两个服务的解耦。典型的使用场景：注册送积分，登录送优惠券等。\n最大努力通知是分布式事务中要求最低的一种,适用于一些最终一致性时间敏感度低的业务；允许发起通知方处理业务失败，在接收通知方收到通知后积极进行失败处理，无论发起通知方如何处理结果都会不影响到接收通知方的后续处理；发起通知方需提供查询执行情况接口，用于接收通知方校对结果。典型的使用场景：银行通知、支付结果通知等。\n","date":"10 March 2024","permalink":"/posts/architecture/distributed/transaction/","section":"博客","summary":"事务的多个参与者分布在不同的地方。","title":"分布式事务"},{"content":"TANGO 的主要目标是将设备视为具有方法和数据的网络对象，使它们成为真正的 C++/Java 对象，可以像本地一样进行实例化和远程访问。\nIDL FILE # IDL 文件包含以下网络接口：\nDevice：所有控制对象（包括数据库）的基本接口。每个设备都有状态。通过执行传递一个输入参数并返回一个输出参数（TANGO 预定义数据类型之一）的命令，在设备上执行操作。命令可以同步或异步执行。异步命令必须提供回调对象才能接收答案。设备支持可读取或写入的属性列表。设备可以返回有关其自身或其状态的一般信息。每个设备都有一个包含最后 n 个命令的黑匣子并实现安全性。 Callback：客户端对象将被服务器调用以返回异步响应。该回调有一个处理程序方法，该方法在解包响应时调用。 Monitor：用于监视设备或属性的系统网络对象。客户登记他们的兴趣。 Consumer：用于从监视器接收事件的客户端对象。 TANGO还支持一些伪网络接口。伪网络接口仅在客户端实现，而不在服务器中实现。支持以下伪网络接口：\nGroupDevice：用于对设备进行分组并在一组设备上执行命令的客户端对象 GroupAttributes：用于对不同设备的设备属性进行分组并读取和写入它们的客户端对象。 代码仓库：https://gitlab.com/tango-controls/tango-idl\n版本演化 # TANGO 系统通过一系列逐步扩展的接口定义（从 Device 接口到 Device_6 接口），展示了对设备控制和数据管理需求的不断演进和适应。\nDevice 基本功能: 提供设备的基础交互能力，包括同步执行命令、读写属性、获取设备信息等。 属性和命令的管理: 支持对设备属性的配置及读写，对设备执行的命令进行查询和执行。 Device_2 轮询和缓存支持: 引入数据源参数，支持从设备直接读取或从轮询缓存读取数据。 命令和属性配置的扩展: 提供更详细的命令和属性配置信息，包括显示级别等。 Device_3 增强的属性读写: 支持更复杂的属性读写操作。 扩展的设备信息: 提供更丰富的设备信息，包括设备类型等。 属性配置增强: 支持更详细的属性配置。 Device_4 客户端识别: 引入客户端识别，用于命令执行和属性读写中。 历史记录增强: 引入命令和属性的历史记录查询，支持更详细的历史数据分析。 属性读写和配置的进一步增强: 支持更复杂的操作。 Device_5 管道管理: 引入管道(Pipe)的概念和管理方法，包括管道的读写和配置。 属性配置和读写的最新扩展: 进一步增强了属性管理的能力。 增强的历史记录和配置管理: 支持对属性和管道的更复杂配置，以及历史数据的查询。 Device_6 接口版本标记: 标识设备支持的TANGO版本为V10，没有引入新的方法或属性，主要作为版本更新的标识。 Device 的基本功能 # Device接口是TANGO控制系统中的核心部分，定义了与TANGO设备进行交互的基本功能。这些基本功能构成了TANGO设备交互的基础，使得客户端能够控制和监视设备的状态，以及对设备进行配置和管理。通过这些接口，TANGO系统提供了一个强大而灵活的框架，用于实现复杂的设备控制逻辑和数据处理任务。\n命令执行 # command_inout: 这是执行设备命令的基础方法，它允许客户端向设备发送命令并接收回应。该方法接受一个命令名（字符串形式）和一个命令输入参数（任意类型），返回命令的输出结果（也是任意类型）。这使得客户端可以执行如开关设备或调整设备参数等操作。 属性读写 # read_attributes: 允许客户端读取一个或多个设备属性的当前值。这个方法接收一个属性名的数组，并返回一个包含每个属性当前值的列表。每个属性值都包括实际的数据值、数据质量（如有效、无效、警告等）、时间戳等信息。 write_attributes: 使客户端能够设置一个或多个设备属性的值。客户端提供一个属性值列表，每个列表项包括属性名和新的属性值。这个方法不返回任何结果，但如果操作失败会抛出异常。 get_attribute_config: 用于获取一个或多个属性的配置信息，例如属性的数据类型、读写权限、最大维度等。这些配置信息对于理解属性的行为和约束条件非常重要。 set_attribute_config: 允许客户端为一个或多个属性设置新的配置。这可以包括改变属性的标签、描述、单位等元数据。 设备信息和状态 # ping: 这个方法用于检测设备是否在线和可达。如果设备响应，说明它是活跃的；如果没有响应，客户端可能会收到一个异常。 info: 提供设备的一般信息，如设备的类别、服务器标识、位置等。这对于了解设备的背景和运行环境非常有用。 状态和描述属性: Device接口还定义了几个只读属性，如name（设备名）、description（设备描述）、state（设备状态）和status（状态的文本描述）。这些属性为客户端提供了设备的基本信息和当前状态。 命令和属性的查询 # command_list_query: 返回设备支持的所有命令的列表。这对于客户端了解设备能够执行哪些操作非常有用。 command_query: 提供指定命令的详细信息（一个DevCmdInfo结构），如命令的输入输出类型等，帮助客户端正确地构造命令调用。 Device_2 升级内容 # Device_2接口的引入，主要是为了提升数据访问的灵活性和效率，通过引入数据源管理的概念，使得客户端可以根据实际需要选择最合适的数据获取方式；对属性和命令配置的增强，提供了更多的信息和配置选项，使得设备管理和操作更加灵活和精细。\n轮询机制和数据源管理 # 命令执行带数据源（command_inout_2）:\n功能描述: 在Device接口的command_inout方法基础上增加了一个新参数DevSource source，这允许客户端指定命令执行结果的数据来源是直接从设备读取还是从轮询缓存中获取。 新增参数: DevSource source: 指定数据源，可以是设备直接（DEV）、缓存（CACHE）或先尝试缓存后设备（CACHE_DEV）。 改进点: 提高了数据访问的灵活性和响应速度，特别是在数据频繁更新的场合。 读取属性带数据源（read_attributes_2）:\n功能描述: 类似于command_inout_2，这个方法允许客户端在读取属性值时指定数据来源。 新增参数 DevSource source: 允许选择数据来源，使得客户端可以更灵活地根据实际情况和需求选择读取数据的方式。 属性和命令的配置增强 # 获取属性配置（get_attribute_config_2）:\n功能描述: 扩展了get_attribute_config方法，提供了对属性更详细的配置信息，包括新增的显示级别（Display Level）等配置项。 改进点: 增强了属性配置的能力，使客户端能够更细致地管理和配置设备属性，支持更复杂的应用场景。 查询命令列表（command_list_query_2）:\n功能描述: 扩展了command_list_query方法，提供了命令配置中新增的显示级别等信息。 改进点: 提供了更为详尽的命令信息，使客户端可以根据命令的不同级别进行相应的操作，增强了用户界面的友好性和操作的适应性。 查询命令详情（command_query_2）:\n功能描述: 在command_query基础上增加了命令的显示级别信息。 改进点: 通过引入显示级别，使得对命令的描述更加丰富和详细，有助于客户端或用户界面根据不同的应用场景选择合适的命令进行操作。 Device_3 升级内容 # Device_3接口相比Device_2主要在属性管理和设备信息获取方面进行了显著的增强和扩展，引入了更细致的属性读写能力和更丰富的设备及属性配置信息。\n增强的属性读写能力 # 读取属性（read_attributes_3）:\n功能描述: 这个方法扩展了Device_2中的read_attributes_2，引入了AttributeValueList_3，提供了对属性读取操作的增强，允许更灵活地处理属性值及其质量信息。 改进点: 引入了属性维度信息和错误列表，使得客户端在读取属性时可以获取更多关于属性状态的详细信息，包括可能发生的错误。 写入属性（write_attributes_3）:\n功能描述: 提供了一个增强的属性写入方法，支持更复杂的错误处理机制，包括MultiDevFailed异常。 改进点: 在写入属性时，如果遇到错误，客户端可以得到更详细的错误信息，包括哪些属性写入失败及其原因，这对于错误诊断和异常处理非常有用。 设备信息的扩展 # 获取设备信息（info_3）: 功能描述: info_3方法扩展了Device接口的info方法，提供了设备的更多信息，如设备类型等。 改进点: 这一扩展使得客户端可以获取更全面的设备信息，有助于更好地理解和管理设备。 属性配置的增强 # 获取属性配置（get_attribute_config_3）:\n功能描述: 该方法在Device_2的基础上进行了扩展，允许获取包括显示级别在内的更丰富的属性配置信息。 改进点: 提供了对属性配置更细致的控制和管理，使得属性的展示和操作可以根据不同的使用场景进行调整。 设置属性配置（set_attribute_config_3）:\n功能描述: 相比于Device_2，这个方法支持更复杂的属性配置设置，包括新引入的配置项。 改进点: 使得客户端能够根据需求对设备属性进行更详细的配置，增强了设备管理的灵活性和精确度。 属性和命令历史记录的查询 # Device_3接口并没有直接新增与属性和命令历史记录查询相关的方法，这些功能在后续的接口版本中得到了扩展。Device_3主要集中在提高属性读写的灵活性和扩展设备信息及属性配置的功能上。\nDevice_4 升级内容 # Device_4接口的引入标志着TANGO系统在客户端管理、操作审计、历史数据查询及操作效率方面的重大进步。\n客户端识别 # 客户端识别（ClntIdent）: 功能描述: Device_4引入了一个新的参数类型ClntIdent，用于在执行命令或读写属性时标识请求的客户端。这允许TANGO服务更准确地跟踪和管理来自不同客户端的操作，提高了系统的安全性和可追踪性。IDL 为 C++ 和 Java 客户端定义了不同的结构（CppClntIdent_6、JavaClntIdent_6），并提供与旧客户端识别机制的向后兼容性。 改进点: 通过客户端识别，TANGO系统能够为每个客户端提供定制化的服务和响应，同时增强了对操作的审计和控制能力。 增强的历史记录查询 # 命令历史记录查询（command_inout_history_4）:\n功能描述: 此方法扩展了历史记录查询功能，允许客户端获取指定命令的执行历史，包括命令的输入、输出以及执行时间等详细信息。 改进点: 为系统维护人员和应用开发者提供了强大的工具，以分析和诊断系统中的操作和事件，帮助提高系统的稳定性和性能。 属性值历史记录查询（read_attribute_history_4）:\n功能描述: 类似于命令历史记录的查询，这个方法允许查询特定属性的值变化历史，包括属性值、质量标志、时间戳等。 改进点: 增强了对设备状态和行为分析的能力，特别是在需要监控和回溯设备性能或状态变化的场合。 灵活的属性和命令操作 # 带客户端识别的命令执行（command_inout_4）: 功能描述: 这个方法扩展了command_inout_2，增加了客户端识别参数，使得命令执行可以关联到特定的客户端请求。 改进点: 提高了命令执行的安全性和可追踪性，有助于在多客户端环境中管理和控制对设备的访问。 带客户端识别的属性读写（read_attributes_4和write_attributes_4）: 功能描述: 这些方法在属性读写操作中引入了客户端识别，使得每次属性操作都可以与发起操作的客户端相关联。 改进点: 这增强了属性操作的安全性和个性化处理能力，特别是在复杂的系统环境中，能够有效地管理和控制不同客户端对属性的操作。 写后读属性（write_read_attributes_4）: 功能描述: 新增的方法允许客户端在单个操作中先写入一组属性值，然后立即读取另一组（或相同的）属性值，这种原子操作减少了网络通信次数，提高了操作效率。 改进点: 对于需要频繁更新并立即读取最新状态的应用场景，这个方法显著提高了效率和响应速度。 Device_5 升级内容 # 更高级的属性配置 # 获取和设置属性配置（get_attribute_config_5和set_attribute_config_5）: 功能描述: 这两个方法扩展了Device_4中对属性配置的操作，引入了更详细的配置选项，如属性的记忆化（memorization）设置，允许系统记住属性的最后值并在设备重启时恢复这些值。 改进点 最小/最大报警值（min_alarm、max_alarm）: 允许用户为属性设置最小和最大的报警阈值。当属性值超出这些阈值时，系统可以触发相应的报警，提示用户进行检查或采取行动 增强的属性和管道读写能力 # 读取属性（read_attributes_5）: 功能描述: 进一步增强了读取属性的能力，允许在读取属性时提供更详细的信息，如数据类型、数据格式等。 改进点: 增强了对属性数据处理的灵活性，使客户端能够更准确地处理和解析返回的属性值。 写后读属性（write_read_attributes_5）: 功能描述: 此方法允许在单个操作中先写入一组属性值，然后立即读取一组（可能是不同的）属性值，进一步提升了Device_4中引入的相关操作的灵活性和效率。 改进点: 通过减少网络通信次数，提高了操作效率，特别是对于需要频繁更新状态并快速获取最新状态的控制逻辑。 获取和设置管道配置（get_pipe_config_5和set_pipe_config_5）: 功能描述: 新增了对管道（一种复杂数据结构的集合）的配置读取和设置方法，允许客户端管理和控制管道的行为。 改进点: 管道是TANGO系统中用于处理复杂数据传输的一种机制，这些方法提供了对管道高度灵活的配置和管理能力，适用于复杂的数据处理需求。 管道操作 # 读写管道（read_pipe_5、write_pipe_5和write_read_pipe_5）: 功能描述: 这一系列方法提供了对管道数据的读取、写入以及写后读操作，允许客户端与设备之间进行复杂数据结构的高效通信。 改进点: 通过支持复杂数据结构的传输，这些方法使得TANGO系统能够更好地处理高级数据处理场景，如实时数据分析、复杂的设备控制逻辑等。 Device_6 升级内容 # Device_6主要作为一个版本标记，表明设备或TANGO服务支持或兼容至TANGO版本10的特性和API。\n属性值 # 基本属性值\nAttrQuality：描述属性值的质量，指示属性值是有效、无效、处于报警状态、正在变化还是处于警告状态。这使客户能够了解他们收到的数据的可靠性。 AttrWriteType：指定属性的写入能力，例如只读、只写或读写，并包含未定义状态的未知类型。 AttrDataFormat：定义属性的数据格式，区分标量值、光谱和图像，并使用未知格式的占位符。 DevSource：表示设备信息的来源，区分直接设备访问、缓存或两者的组合。 ErrSeverity：枚举错误的严重级别，包括警告、错误和恐慌级别，这有助于确定问题的优先级。 DevState：列出设备可能的状态，涵盖开、关、移动、故障等常见状态，这对于管理和监控设备操作至关重要。 DispLevel：区分显示级别或访问级别，例如操作员和专家，可能根据用户的角色控制某些设备功能的可见性或访问。 跟踪上下文传播：定义包括用于传播跟踪上下文的结构 (W3CTraceContextV0)，采用“W3C 跟踪上下文”标准。 这对于分布式系统调用的可观察性和监控非常重要，确保不同组件和服务之间的可追溯性。 客户端标识 (ClntIdent)：扩展客户端标识机制以包含跟踪上下文以实现可观察性。 这对于识别和跟踪来自不同客户端的请求至关重要，尤其是在分布式环境中。 LockerLanguage Enum：该枚举用于选择 ClntIdent 联合的编组/解组机制，支持 pre-IDL6 和 IDL6 数据结构。 这使得该接口能够支持不同版本的客户端识别，确保不同版本的TANGO系统之间的兼容性。 模块化和可扩展性：该 IDL 文件中模块（例如 Tango 模块）的使用以及数据类型和结构的定义演示了设计模块化和可扩展系统的方法。 这种设计允许轻松添加新功能和组件，同时保持向后兼容性。 杂项结构\nTimeVal：表示时间值的结构，可能用于时间戳或调度目的。 DevCmdInfo 和 DevCmdInfo_2：定义命令信息的结构，包括命令名称、输入和输出类型及其描述。 DevCmdInfo_2 使用附加显示级别字段扩展了 DevCmdInfo，指示预期的用户界面级别。 错误处理机制\nDevError：包含有关错误的详细信息，包括原因、严重性、描述和来源。 这种结构可以实现详细的错误报告。 NamedDevError：使用名称和索引扩展 DevError，可能用于分组或识别错误较大操作或命令序列中的错误。 DevFailed 和 MultiDevFailed：处理错误和失败的异常，DevFailed 提供错误列表，MultiDevFailed 提供报告多个命名错误的方法。 属性管理\n属性配置结构：AttributeConfig、AttributeConfig_2、AttributeConfig_3和AttributeConfig_5详细配置设备属性，包括名称、写入类型、数据格式、数据类型、维度、描述、单位、格式、值约束以及进一步定制的扩展。 这些结构演变为包括更详细的配置，例如警报设置、事件属性，并且在最新版本（AttributeConfig_5）中，还包括属性的记忆设置和枚举标签。 属性值结构：这些结构（AttributeValue、AttributeValue_3、AttributeValue_4 和 AttributeValue_5）表示属性的值及其质量、时间戳、维度和错误列表（如果适用）。 它们不断发展以支持更复杂的数据结构和详细的错误报告。 属性数据类型和联合：AttributeDataType 枚举和 AttrValUnion 提供了一种灵活的机制来表示不同类型的属性值，包括布尔值、数字类型、字符串、设备状态和编码数据。 这允许属性保存与设备操作和监控相关的各种数据。 事件和警报配置：ChangeEventProp、PeriodicEventProp、ArchiveEventProp、EventProperties 和 AttributeAlarm 等结构允许配置属性更改如何根据特定阈值或周期触发事件和警报。 此功能对于自动监控和响应系统至关重要。 管道管理\n管道配置：PipeConfig 结构定义管道的配置，管道是设备之间或客户端与设备之间的数据管道。 配置选项包括名称、描述、标签、访问级别、写入类型和其他设置的扩展。 管道数据结构：DevPipeDataElt、DevPipeBlob 和 DevPipeData 结构有助于管道内的复杂数据处理，允许嵌套数据结构 (DevPipeDataElt)、用于对数据元素进行分组的 blob (DevPipeBlob) 以及带有时间戳的整体管道数据结构 (DevPipeData)。 数据就绪事件和设备接口更改\nAttDataReady：此结构用于通知与属性关联的数据何时准备就绪，包括属性名称、其数据类型以及用于跟踪更新的计数器。 DevIntrChange：表示设备接口更改事件，指示设备是否已启动并列出属于设备接口的命令和属性。 这对于设备功能可能随时间变化的动态系统至关重要。 设备信息\nDevInfo 和 DevInfo_3：这些结构提供有关设备的基本和扩展信息，例如其类、服务器 ID、服务器主机、服务器版本、文档 URL 和设备类型（在 DevInfo_3 中）。 此信息对于管理设备和与设备交互至关重要。 命令和属性历史记录\n历史结构：包括 DevCmdHistory、DevAttrHistory 及其版本，这些结构允许跟踪命令执行和属性更改的历史记录，包括时间戳、成功/失败状态、值、维度和错误。 这些历史数据对于诊断、审核和了解设备随时间的变化至关重要。 ZeroMQ事件系统\nZmqCallInfo：详细说明通过 ZeroMQ 事件系统进行的调用，包括版本、计数器、方法名称、对象 ID 以及调用是否导致异常。 该结构支持TANGO的事件驱动架构，促进高效的通信和事件处理。 设备接口\n设备接口：定义 TANGO 设备的核心接口，包括设备名称、描述、状态、状态和管理员名称等属性。 它提供了执行命令、读取和写入属性配置和值、对设备进行 ping 操作以了解可用性、检索设备信息以及查询支持的命令的方法。 该接口是与 TANGO 设备交互的支柱，支持同步和异步操作。 ","date":"10 March 2024","permalink":"/posts/architecture/iot/tango/tango-idl/","section":"博客","summary":"TANGO 的主要目标是将设备视为具有方法和数据的网络对象，使它们成为真正的 C++/Java 对象，可以像本地一样进行实例化和远程访问。","title":"TANGO idl definition"},{"content":" https://blog.csdn.net/MrCharles/article/details/112278947 ","date":"10 March 2024","permalink":"/posts/reviews/os/%E9%80%82%E5%90%88-ubuntu%E7%9A%848%E6%AC%BE%E6%9C%80%E4%BD%B3%E5%BD%95%E5%B1%8F%E8%BD%AF%E4%BB%B6/","section":"博客","summary":"适合 Ubuntu的8款最佳录屏软件","title":"适合 Ubuntu的8款最佳录屏软件"},{"content":" 题目链接：https://www.nowcoder.com/exam/company?currentTab=recommand\u0026amp;jobId=100\u0026amp;selectStatus=0\u0026amp;tagIds=179 4.删除区间 # #include \u0026lt;iostream\u0026gt; #include \u0026lt;cmath\u0026gt; using namespace std; const int N = 1e5; int n, a[N+5]; // two为前i个数中2的个数，five为前i个数中5的个数 int two[N+5], five[N+5]; int get(int i, int j){ // 即2.1.1方案，求出去掉区间[i,j]后乘积末尾 0的个数 return min(two[n]-two[j]+two[i-1], five[i-1]+five[n]-five[j]); } int main() { int k; cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; k; for(int i=1; i\u0026lt;=n; i++) cin \u0026gt;\u0026gt; a[i]; for(int i=1; i\u0026lt;=n; i++){ // 求因子2，5的个数 while(a[i]%2==0) two[i]++, a[i]/=2; while(a[i]%5==0) five[i]++, a[i]/=5; // 维护前缀和 two[i] += two[i-1]; five[i] += five[i-1]; } int l = 0, r = 0;// 记录上一次删除区间，防止重复相加 long long ans = 0; for(int i=1, j=1; i\u0026lt;=n \u0026amp;\u0026amp; j\u0026lt;=n; ){ j = max(j, i);// 当 i右移超过j后，j不能比 i小，所以需要更新一下 while(j \u0026lt;=n \u0026amp;\u0026amp; get(i, j) \u0026gt;= k) j++;//当剩余区间值不小于k，就不断向右移 ans += 1LL*(j-i)*(j-i+1)/2;// 删除方案即该区间的等差数列公式 if(r \u0026gt;= i) ans -= 1LL*(r-i)*(r-i+1)/2; //如果上一个区间[l,r]的右端点不小于本次区间[i,j]的左端点，则产生重复需要删去 [i, r]方案数 l = i, r = j;//上次删除区间更新为[i,j] while(i \u0026lt;= j \u0026amp;\u0026amp; get(i, j)\u0026lt;k) i++;//右移i，直到再次可以删除 或 左右边界重合 停止 } cout \u0026lt;\u0026lt; ans; } 5.小美的朋友关系 # 小美认为，在人际交往中，但是随着时间的流逝，朋友的关系也是会慢慢变淡的，最终朋友关系就淡忘了。\n现在初始有一些朋友关系，存在一些事件会导致两个人淡忘了他们的朋友关系。小美想知道某一时刻中，某两人是否可以通过朋友介绍互相认识？\n事件共有 2 种：\n1 u v：代表编号 u 的人和编号 v 的人淡忘了他们的朋友关系。 2 u v：代表小美查询编号 u 的人和编号 v 的人是否能通过朋友介绍互相认识。 注：介绍可以有多层，比如 2 号把 1 号介绍给 3 号，然后 3 号再把 1 号介绍给 4 号，这样 1 号和 4 号就认识了。\n输入描述：\n第一行输入三个正整数n,m,q，代表总人数，初始的朋友关系数量，发生的事件数量。\n接下来的m行，每行输入两个正整数u,v，代表初始编号u的人和编号v的人是朋友关系。\n接下来的q行，每行输入三个正整数op,u,v，含义如题目描述所述。\n$1\\leq n \\leq 10^9$ $1\\leq m,q \\leq 10^5$ $1\\leq u,v \\leq n$ $1\\leq op \\leq 2$ 保证至少存在一次查询操作。\n输出描述：\n对于每次 2 号操作，输出一行字符串代表查询的答案。如果编号 u 的人和编号 v 的人能通过朋友介绍互相认识，则输出\u0026quot;Yes\u0026quot;。否则输出\u0026quot;No\u0026quot;。\n输入例子： 5 3 5 1 2 2 3 4 5 1 1 5 2 1 3 2 1 4 1 1 2 2 1 3 输出例子： Yes No No 例子说明： 第一次事件，1 号和 5 号本来就不是朋友，所以无事发生。 第二次事件是询问，1 号和 3 号可以通过 2 号的介绍认识。 第三次事件是询问，显然 1 号和 4 号无法互相认识。 第四次事件，1 号和 2 号淡忘了。 第五次事件，此时 1 号无法再经过 2 号和 3 号互相认识了。 解题时并没有做出来，\nimport java.util.*; public class Main { public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(), m = in.nextInt(), q = in.nextInt(); Set\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; relationship = new HashSet\u0026lt;\u0026gt;(); UnionFind union = new UnionFind(n + 1); int[][] events = new int[q][4]; for (int i = 0; i \u0026lt; m; i++) { int u = in.nextInt(), v = in.nextInt(); relationship.add(Arrays.asList(Math.min(u, v), Math.max(u, v))); union.parent[u]=u; union.parent[v]=v; } for (int i = 0; i \u0026lt; q; i++) { events[i][3] = Integer.MAX_VALUE; } for (int i = 0; i \u0026lt; q; i++) { int op = in.nextInt(), u = in.nextInt(), v = in.nextInt(); events[i][0] = op; events[i][1] = u; events[i][2] = v; if (op == 1) { if (relationship.contains(Arrays.asList(Math.min(u, v), Math.max(u, v)))) { relationship.remove(Arrays.asList(Math.min(u, v), Math.max(u, v))); events[i][3] = Math.min(events[i][3], i); } } } for (List\u0026lt;Integer\u0026gt; relation : relationship) { union.union(relation.get(0), relation.get(1)); } List\u0026lt;String\u0026gt; ans = new ArrayList\u0026lt;\u0026gt;(); for (int i = events.length - 1; i \u0026gt;= 0; i--) { if (events[i][0] == 1) { if ( events[i][3] == i) union.union(events[i][1], events[i][2]); } else { if(union.parent[events[i][1]]==0) union.parent[events[i][1]] = events[i][1]; if(union.parent[events[i][2]]==0) union.parent[events[i][2]] = events[i][2]; ans.add(union.query(events[i][1], events[i][2]) ? \u0026#34;Yes\u0026#34; : \u0026#34;No\u0026#34;); } } for (int i = ans.size() - 1; i \u0026gt;= 0; i--) { System.out.println(ans.get(i)); } } public static class UnionFind { public int[] parent; public UnionFind(int N) { parent = new int[N]; // for (int i = 0; i \u0026lt; N; i++) { // parent[i] = i; // } } public int find(int v) { if (v != parent[v]) { parent[v] = find(parent[v]); } return parent[v]; } public void union(int v1, int v2) { int f1 = find(v1); int f2 = find(v2); if (f1 != f2) { parent[f2] = f1; } } public boolean query(int i, int j) { return find(i) == find(j); } public void out() { for (int i = 0; i \u0026lt; parent.length; i++) { System.out.print(parent[i] + \u0026#34; \u0026#34;); } System.out.println(); } } } 一直报错\n后来看的别人的题解\nhttps://blog.csdn.net/weixin_62517188/article/details/136777830 #include \u0026lt;iostream\u0026gt; #include \u0026lt;set\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;map\u0026gt; using namespace std; const int N = 1e5; using PII = pair\u0026lt;int,int\u0026gt;; int n, m, q; map\u0026lt;int,int\u0026gt;f;// 并查集父亲数组 struct node{ int op, u, v; }ord[N+5];// 新的操作数组 int find(int x){// 路径压缩 while(f[x] != x) x = f[x] = f[f[x]]; return x; } void merge(int u,int v){// 并查集合并 int fa = find(u); int fb = find(v); f[fb] = fa; } int main() { cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; m \u0026gt;\u0026gt; q; set\u0026lt;PII\u0026gt;st;// 关系集合 for(int i=0, u, v; i\u0026lt;m; i++){ cin \u0026gt;\u0026gt; u \u0026gt;\u0026gt; v; st.insert({u, v}); // u, v放进关系集合中 f[u] = u, f[v] = v;// 把出现的结点父节点设置为自己 } int num = 0;// 新的操作数组长度 for(int i=0; i\u0026lt;q; i++){ int op, u, v; cin \u0026gt;\u0026gt; op \u0026gt;\u0026gt; u \u0026gt;\u0026gt; v; //如果是查询操作，可以直接存入 // 如果是删除操作，需要判断原关系集合中是否存在 if(op == 1){ // 可能是 {u,v} 形式存储 if(st.find({u, v}) != st.end()) st.erase({u, v}); // 可能是 {v,u} 形式存储 else if(st.find({v, u}) != st.end()) st.erase({v, u}); // 如果不存在直接跳过，不储存此次删除操作 else continue; } // 存入新的操作数组中 ord[num++] = {op, u, v}; } // 删除之后，剩余关系集合就是没有涉及到的，也是最终的并查集 for(auto [u,v]:st) merge(u, v); vector\u0026lt;bool\u0026gt;ans;// 存储答案 for(int i=num-1; i\u0026gt;=0; i--){// 倒着重新进行操作 int op = ord[i].op, u = ord[i].u, v = ord[i].v; if(op == 1) merge(u, v);// 如果是删除操作，反过来进行合并 else{ // 当 f[u] = 0时，就是第一次出现该节点，需要初始化f[i]=i,方便进行路径压缩 if(!f[u]) f[u] = u; if(!f[v]) f[v] = v; ans.emplace_back(find(u) == find(v));// 查询操作，就储存答案 } } //因为是倒着遍历操作的，所以答案是倒着存储的 for(int i=ans.size()-1; i\u0026gt;=0; i--) if(ans[i]) cout \u0026lt;\u0026lt; \u0026#34;Yes\u0026#34; \u0026lt;\u0026lt; endl; else cout \u0026lt;\u0026lt; \u0026#34;No\u0026#34; \u0026lt;\u0026lt; endl; } ","date":"9 March 2024","permalink":"/posts/reviews/24%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0/%E7%BE%8E%E5%9B%A2/%E7%AC%94%E8%AF%95/%E7%BE%8E%E5%9B%A22024%E5%B1%8A%E6%98%A5%E6%8B%9B%E7%AC%94%E8%AF%95%E7%AC%AC%E4%B8%80%E5%9C%BA%E7%BC%96%E7%A8%8B%E7%9C%9F%E9%A2%98/","section":"博客","summary":"第一场自己做的笔试，前三题感觉很水，半小时搞定了。后两题挺难的。","title":"美团2024届春招笔试第一场编程真题"},{"content":"","date":"9 March 2024","permalink":"/posts/reviews/24%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0/%E7%BE%8E%E5%9B%A2/%E7%AC%94%E8%AF%95/","section":"博客","summary":"不打没有准备的仗！！实习过程中的复习。","title":"美团笔试"},{"content":" 题目链接：https://www.nowcoder.com/exam/company?currentTab=recommand\u0026amp;jobId=100\u0026amp;selectStatus=0\u0026amp;tagIds=179 1.平均数为k的最长连续子数组 # import java.util.Scanner; import java.lang.*; import java.util.*; // 注意类名必须为 Main, 不要有任何 package xxx 信息 public class Main { public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(), k = in.nextInt(); long[] nums = new long[n + 1]; int i = 0, ans = -1; Map\u0026lt;Long, Integer\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); map.put(0L, 0); // 注意 hasNext 和 hasNextLine 的区别 while (in.hasNextInt()) { // 注意 while 处理多个 case nums[++i] += in.nextInt() - k + nums[i - 1]; if(!map.containsKey(nums[i])) map.put(nums[i], i); else ans = Math.max(ans, i - map.get(nums[i])); } System.out.println(ans); } } 2.小球投盒 # import java.util.*; // 注意类名必须为 Main, 不要有任何 package xxx 信息 public class Main { public static void main(String[] args) { Scanner sc = new Scanner(System.in); // 输入的n, m int n = sc.nextInt(), m = sc.nextInt(); // 使用 HashSet 存储整数集合 Set\u0026lt;Integer\u0026gt; set = new HashSet\u0026lt;\u0026gt;(); // 初始化 set 包含从 1 到 n 的所有整数 for (int i = 1; i \u0026lt;= n; i++) set.add(i); // 循环计数变量 int i; for (i = 1; i \u0026lt;= m; i++) { if (sc.nextInt() == 1) { // 如果输入为 1,移除下一个输入的整数 set.remove(sc.nextInt()); // 如果集合为空，退出循环 if (set.isEmpty()) break; } else { // 否则，读取下一个输入的整数 int x = sc.nextInt(); // 如果集合中不包含 x，退出循环 if (!set.contains(x)) break; // 否则，重置集合并添加 x set = new HashSet\u0026lt;\u0026gt;(); set.add(x); } } // 输出结果，如果 i 大于 m，输出 -1，否则输出 i System.out.println(i \u0026gt; m ? -1 : i); } } 4.小美的游戏 # 用long也会爆，后来用BigDecimal过了。\nimport java.util.*; // 注意类名必须为 Main, 不要有任何 package xxx 信息 public class Main { public static void main(String[] args) { Scanner in = new Scanner(System.in); long temp = (long) (1e9) + 7; // 注意 hasNext 和 hasNextLine 的区别 Queue\u0026lt;Long\u0026gt; q = new PriorityQueue\u0026lt;\u0026gt;(new Comparator\u0026lt;Long\u0026gt;() { public int compare(Long a, Long b) { if (b \u0026gt; a) return 1; return -1; } }); int n = in.nextInt(), k = in.nextInt(); for (int i = 0; i \u0026lt; n; i++) { q.offer(in.nextLong()); } for (int i = 0; i \u0026lt; k; i++) { long a1 = q.poll(); long a2 = q.poll(); q.offer((a1 * a2) % temp); q.offer((long)1); } long ans = 0; while (!q.isEmpty()) { ans = ( ans + q.poll()) % temp; } System.out.print(ans); } } import java.math.BigDecimal; import java.util.PriorityQueue; import java.util.Scanner; public class Main { public static final int MOD = (int) 1e9 + 7; public static void main(String[] args) { Scanner sc = new Scanner(System.in); int n = sc.nextInt(); int k = sc.nextInt(); PriorityQueue\u0026lt;BigDecimal\u0026gt; pq = new PriorityQueue\u0026lt;\u0026gt;((a, b) -\u0026gt; b.compareTo(a)); for (int i = 0; i \u0026lt; n; i++) { long x = sc.nextLong(); pq.offer(BigDecimal.valueOf(x)); } BigDecimal ans = BigDecimal.ZERO; while (k-- \u0026gt; 0) { BigDecimal x = pq.poll(); BigDecimal y = pq.poll(); pq.offer(x.multiply(y)); pq.offer(BigDecimal.ONE); } while (!pq.isEmpty()) { ans = ans.add(pq.poll()); } System.out.println(ans.remainder(BigDecimal.valueOf(MOD))); } } ","date":"8 March 2024","permalink":"/posts/reviews/24%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0/%E7%BE%8E%E5%9B%A2/%E7%AC%94%E8%AF%95/%E7%BE%8E%E5%9B%A22024%E5%B1%8A%E7%A7%8B%E6%8B%9B%E7%AC%94%E8%AF%95%E7%AC%AC%E4%B8%89%E5%9C%BA%E7%BC%96%E7%A8%8B%E7%9C%9F%E9%A2%98/","section":"博客","summary":"这次的笔试，还是有些难度的，但是水题也多。","title":"美团2024届秋招笔试第三场编程真题"},{"content":"暑期实习投的太多了，被挂了很多的简历，也做了很多测评和笔试，有的进了面试，有的拿了offer。很大一个感觉是\n下面记录一下笔试和面试的时间线。\n笔试 2024/03/09 美团笔试，做过美团23年秋招的笔试，感觉第一场最简单，果然不难。 面试/笔试 2024/03/13 腾讯IEG一面（处女面），状态很差，问的很难，感觉是暑期实习技术面中难度最大的，次日挂； 携程笔试，跟腾讯一面时间撞了，直接咕咕了。 暴风雨前的宁静 2024/03/17-2024/03/23 约面了很多，但都给我推到下一周了，这周相安无事，可谓是暴风雨前的宁静 2024/03/20 美团约面。 2024/03/22 腾讯约面。 招聘会/笔试 2024/03/24 南大春招招聘会，线下投了很多小公司； 饿了么跟拼多多撞笔试，选了饿了么，题目很灵活，做的稀烂，后来知道拼多多的难度更大，心里很平衡。 约面/面试 2024/03/25 饿了么约面，被我推到了3.28； 美团一面，面试官是一位很年轻的小姐姐，人真的超级好！！ 美团招聘会 2024/03/26 美团线下招聘会，抽到了到店的面试直通卡，这时希望美团一面赶紧挂。 约面/面试 2024/03/27 美团通知二面； 腾讯PCG一面，感觉状态良好，但是我没点确认面试，面试官也没让我做题，出了很多面试外的奇怪问题！很难过哈哈。 面试 2024/03/28 饿了么一面，可能是时间被我推的太狠了，面试官kpi面了。 面试 2024/03/29 美团二面，很nice！！ 笔试 2024/03/30 炎魂笔试，咕咕了，忍三再见👋 笔试 2024/03/31 腾讯笔试，全AC了，本来以为可以进PCG二面了，结果第二天就挂了。 oc/面试 2024/04/02 美团oc，hr姐姐问了相关的情况，并告诉我offer再考虑一周是否发放； 云深处一面，跟大厂比，小厂的面试就有些个水，也没拷打八股，更多的是面试官在介绍他们的项目。聊下来对面一直在强调时间不合适，他们说要立刻到岗，但我坚持6-9三个月实习。 offer 2024/04/03 美团邮件offer 约面 2024/04/09 云深处时间不合适挂； 腾讯音乐约面。 笔试 2024/04/10 华为笔试。 面试 2024/04/11 腾讯音乐面试，讨论了很多项目的技术方案，感觉很合拍。 笔试 2024/04/15 拼多多笔试，做的稀烂，第二天秒挂。 笔试/约面 2024/04/16 携程笔试，这次是不想做翘了； 腾讯IEG约面 约面/面试 2024/04/18 腾讯音乐约二面； 腾讯IEG一面，感觉这次面试有点怪，问的问题很抽象，都是很高层次的问题。 蚂蚁约一面。 约面 2024/04/19 华为约面 面试/笔试 2024/04/20 腾讯音乐二面，下午跟导师聊完毕设，心力憔悴，第一次在面试的时候感觉到不想说话，状态极差！累到不想说话，面完秒挂； b站笔试，跟蚂蚁撞，咕咕； 蚂蚁笔试，笔试不难，但是最后一题题目没读懂，一直卡着。 面试 2024/04/21 灵犀互娱笔试 面试 2024/04/22 蚂蚁一面。 面试 2024/04/23 华为一面，南研所的华子对南大是真的友好。 腾讯IEG二面，问的很多，次日挂。 面试 2024/04/25 华为二面，两日后通过。 ","date":"8 March 2024","permalink":"/posts/reviews/24%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0/timeline/","section":"博客","summary":"暑期实习投递时间线","title":"暑期实习投递时间线"},{"content":" 题目链接：https://www.nowcoder.com/exam/company?currentTab=recommand\u0026amp;jobId=100\u0026amp;selectStatus=0\u0026amp;tagIds=179 2.小美的数组操作 # 考虑以下几点:\n众数个数最多是多少？ sum%n==0?n:n-1 题目要求优先求众数个数最多的情况，然后再求其最小的操作数。分以下两种情况考虑，\n当sum%n==0时,也就是全部的n个数都能通过有限次变换变成平均值sum/n，那么众数最大个数就是n，此时只要求其操作次数即可; 当sum%n!=0时，此时我们需要选出一个数用来平衡其余n-1个数，使得其剩余的和是n-1的整数倍，基于贪心的局部最优策略我们能想到选择其中的最大值或最小值来充当这个角色 达到众数最多个数所需要的最少操作次数 对于第一种情况可以直接计算;\n对于第二种情况，用v[n]数组存所有的数，maxx、minn分别表示最大值、最小值，sum表示所有元素的总和。\n分别考虑去除最大值和最小值之后计算剩余n-1个数全部转换为平均值所需要的次数，以去除最大值为例，去除最大值maxx之后，由于可能不是整除所以我们考虑其余数k=(sum-maxx)%(n-1)，为了使得剩余n-1个数总和刚好是n-1的整数倍，我们可以选择将总和中余出的k删掉，此时平均值为ever=(sum-maxx)/(n-1);或者再补上一个n-1-k,此时平均值为p=(sum-maxx)/(n-1)+1。\n怎么计算操作的次数？ 实际上我们只需要知道平均值是多少就行了，然后遍历一遍v[n]数组，求其中所有大于平均值的数转换为平均数所需要减去数的总和就是我们要的操作次数（同样也可以计算小于平均数的数），代码中定义了函数solve(取大于平均数计算)，该函数的作用就是给定平均值ever，计算操作次数\n注意：如果是减去k那么这个操作可以由solve的操作一并算出，如果是加上n-1-k那么就要将solve计算的操作次数加上n-1-k。根据你solve函数中选择计算的方式决定，如果是选择大于平均数的数来计算就按上面的规则，如果选择小于平均数的数来计算那么上面的加减情况则反过来\nimport java.util.*; import java.lang.*; // 注意类名必须为 Main, 不要有任何 package xxx 信息 public class Main { public static void main(String[] args) { Scanner in = new Scanner(System.in); // 注意 hasNext 和 hasNextLine 的区别 while (in.hasNextInt()) { // 注意 while 处理多个 case int n = in.nextInt(); long[] a = new long[n]; long sum = 0; long maxx = Integer.MIN_VALUE; long minn = Integer.MAX_VALUE; for (int i = 0; i \u0026lt; n; i++) { a[i] = in.nextInt(); sum += a[i]; maxx = Math.max(maxx, a[i]); minn = Math.min(minn, a[i]); } long ans = 0; if (sum % n == 0) { ans = solve(a, sum / n, -1); } else { long k = (sum - minn) % (n - 1); ans = solve(a, (sum - minn) / (n - 1), minn); ans = Math.min(ans, solve(a, (sum - minn) / (n - 1) + 1, minn) + n - 1 - k); k = (sum - maxx) % (n - 1); ans = Math.min(ans, solve(a, (sum - maxx) / (n - 1), maxx)); ans = Math.min(ans, solve(a, (sum - maxx) / (n - 1) + 1, maxx) + n - 1 - k); } System.out.println(ans); } } public static long solve(long[] a, long ever, long num) { int n = a.length; long ans = 0; for (int i = 0; i \u0026lt; n; i++) { if (a[i] == num) continue; if (a[i] \u0026gt; ever) { ans += (a[i] - ever); } } return ans; } } 3.小美的01串翻转 # 首先按照题目意思写了一遍，超时\nimport java.util.Scanner; // 注意类名必须为 Main, 不要有任何 package xxx 信息 public class Main { public static void main(String[] args) { Scanner in = new Scanner(System.in); // 注意 hasNext 和 hasNextLine 的区别 String str = in.nextLine(); char[] s = str.toCharArray(); int n = s.length; int ans = 0; for (int i = 2; i \u0026lt;= n; i++) { int temp = solve(s, i); ans += temp; } System.out.print(ans); } public static int solve(char[] s, int len) { int n = s.length; int ans = 0; for (int i = 0 ; i \u0026lt;= n - len; i++) { int t0 = 0, t1 = 0; int temp = 0; for (int j = i; j \u0026lt; i + len; j++) { if ((temp % 2 + \u0026#39;0\u0026#39;) != s[j]) { t0++; } if (((temp + 1) % 2 + \u0026#39;0\u0026#39;) != s[j]) { t1++; } temp = (temp + 1) % 2; } ans += Math.min(t0, t1); } return ans; } } import java.util.Scanner; public class Main { public static void main(String[] args) { Scanner scanner = new Scanner(System.in); String str = scanner.next(); int ret = 0; for (int i = 0; i \u0026lt; str.length(); i++) { int c0 = 0; int c1 = 0; for (int j = i; j \u0026lt; str.length(); j++) { if ((str.charAt(j) == \u0026#39;0\u0026#39; \u0026amp;\u0026amp; (j - i) % 2 == 0) || (str.charAt(j) == \u0026#39;1\u0026#39; \u0026amp;\u0026amp; (j - i) % 2 == 1)) { c0++; } else { c1++; } ret += Math.min(c0, c1); } } System.out.println(ret); scanner.close(); } } 6.小美的数组构造 # import java.util.Scanner; public class Main { public static void main(String[] args) { Scanner scanner = new Scanner(System.in); int n = scanner.nextInt(); int[] a = new int[n]; for (int i = 0; i \u0026lt; n; i++) { a[i] = scanner.nextInt(); } int MOD = 1000000007; int s = 0; for (int num : a) { s += num; } int[][] dp = new int[n + 1][s + 1]; dp[0][0] = 1; for (int i = 1; i \u0026lt;= n; i++) { for (int j = 1; j \u0026lt;= s; j++) { for (int k = 1; k \u0026lt;= j; k++) { if (k != a[i - 1]) { dp[i][j] = (dp[i][j] + dp[i - 1][j - k]) % MOD; } } } } System.out.println(dp[n][s]); scanner.close(); } } 7.美团商家注册系统 # import java.util.*; // 注意类名必须为 Main, 不要有任何 package xxx 信息 public class Main { public static void main(String[] args) { Scanner in = new Scanner(System.in); HashMap\u0026lt;String, String\u0026gt; info = new HashMap\u0026lt;\u0026gt;(); int length = in.nextInt(); in.nextLine(); for (int i = 0; i \u0026lt; length; i++) { String[] line = in.nextLine().split(\u0026#34; \u0026#34;); if (Character.isLowerCase(line[0].charAt(0))) { if (!info.containsKey(line[0])) { info.put(line[0], line[1] + \u0026#34; \u0026#34; + 0); } else { String[] values = info.get(line[0]).split(\u0026#34; \u0026#34;); //System.out.print(values[0] + \u0026#34; \u0026#34; + line[1] + \u0026#34; \u0026#34;); boolean flag = false; for (int j = 0; j \u0026lt; values.length - 1; j++) { if (values[j].equals(line[1])) { flag = true; break; } } if (flag == false) { int count = Integer.valueOf(values[values.length - 1]) + 1; String str = \u0026#34;\u0026#34;; for (int k = 0; k \u0026lt; values.length - 1; k++) { str += values[k] + \u0026#34; \u0026#34;; } info.put(line[0], str + line[1] + \u0026#34; \u0026#34; + count); } } } } List\u0026lt;String\u0026gt; keys = new ArrayList\u0026lt;\u0026gt;(info.keySet()); // 对键列表按字典序升序排序 Collections.sort(keys); // 遍历排序后的键列表，输出键值对 for (String key : keys) { String value = info.get(key); String[] values = info.get(key).split(\u0026#34; \u0026#34;); System.out.println(key + \u0026#34; \u0026#34; + values[0] + \u0026#34; \u0026#34; + values[values.length - 1]); } } } ","date":"6 March 2024","permalink":"/posts/reviews/24%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0/%E7%BE%8E%E5%9B%A2/%E7%AC%94%E8%AF%95/%E7%BE%8E%E5%9B%A22024%E5%B1%8A%E7%A7%8B%E6%8B%9B%E7%AC%94%E8%AF%95%E7%AC%AC%E4%BA%8C%E5%9C%BA%E7%BC%96%E7%A8%8B%E7%9C%9F%E9%A2%98/","section":"博客","summary":"这次的笔试，比第一场难。","title":"美团2024届秋招笔试第二场编程真题"},{"content":" 题目链接：https://www.nowcoder.com/exam/company?currentTab=recommand\u0026amp;jobId=100\u0026amp;selectStatus=0\u0026amp;tagIds=179 这次的笔试，很多签到题，没什么难度，主要是第 3 题和第 9 题，比较有难度。\n3.小美的树上染色 # 在做的时候，很幸运的从后往前遍历就过了，应该是用例水了。\nimport java.util.Scanner; import java.util.*; // 注意类名必须为 Main, 不要有任何 package xxx 信息 public class Main { public static void main(String[] args) { Scanner in = new Scanner(System.in); while (in.hasNextInt()) { int n = in.nextInt(); int[] a = new int[n]; int u, v; Set\u0026lt;Integer\u0026gt; temp = new HashSet\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; n; i++) { a[i] = in.nextInt(); } int ans = 0; int [][]conn = new int[n - 1][2]; for (int i = 0; i \u0026lt; n - 1; i++) { conn[i][0] = in.nextInt(); conn[i][1] = in.nextInt(); conn[i][0] -= 1; conn[i][1] -= 1; } for (int i = n-2; i \u0026gt;= 0; i--) { if ( !temp.contains(conn[i][0]) \u0026amp;\u0026amp; !temp.contains(conn[i][1]) \u0026amp;\u0026amp; Math.pow((int)Math.sqrt(a[conn[i][0]] * a[conn[i][1]]), 2) == (a[conn[i][0]] * a[conn[i][1]]) ) { temp.add(conn[i][0]); temp.add(conn[i][1]); ans+=2; } } System.out.println(ans); } } } 1. 树形DP解法 # 树形DP的好题。\n对于每个节点，引入0,1状态(表示不染色，染色)\n那每个子树的根节点u, s为u的儿子节点集合\nimport java.io.BufferedInputStream; import java.util.ArrayList; import java.util.Arrays; import java.util.List; import java.util.Scanner; public class Main { static class Solution { int[] ws; List\u0026lt;Integer\u0026gt; []g; int[][] dp; int solve(int n, int[] ws, List\u0026lt;Integer\u0026gt;[] g) { this.g = g; this.dp = new int[n + 1][2]; this.ws = ws; dfs(1, -1); return Math.max(dp[1][0], dp[1][1]); } void dfs(int u, int fa) { int[] res = new int[] {0, 0}; for (int v: g[u]) { if (v == fa) { continue; } dfs(v, u); res[0] += Math.max(dp[v][0], dp[v][1]); } for (int v: g[u]) { if (v == fa) continue; int x2 = res[0] - Math.max(dp[v][0], dp[v][1]); long rr = (long)ws[u] *ws[v]; long r = (long)Math.sqrt(rr); if (r * r == rr) { res[1] = Math.max(2 + x2 + dp[v][0], res[1]); } } dp[u][0] = res[0]; dp[u][1] = res[1]; } } public static void main(String[] args) { Scanner sc = new Scanner(new BufferedInputStream(System.in)); int n = sc.nextInt(); int[] ws = new int[n + 1]; for (int i = 1; i \u0026lt;= n ;i++) { ws[i] = sc.nextInt(); } List\u0026lt;Integer\u0026gt;[]g = new List[n + 1]; Arrays.setAll(g, x -\u0026gt; new ArrayList\u0026lt;\u0026gt;()); for (int i = 0; i \u0026lt; n - 1; i++) { int u = sc.nextInt(); int v = sc.nextInt(); g[u].add(v); g[v].add(u); } Solution solution = new Solution(); int res = solution.solve(n, ws, g); System.out.println(res); } } 2. 无权二分图最大匹配 # 看到评论区有大佬，提到了这个方法，所以补充一下。\n匈牙利算法，其时间复杂度 $O(V*E)$, 树的节点$N$，边$N-1$，理论会达到$O(10^{10})$。\n感觉还是测试数据偏随机，完全平方数限制很强，导致时间复杂度骤降。\nimport java.io.BufferedInputStream; import java.util.ArrayList; import java.util.Arrays; import java.util.List; import java.util.Scanner; public class Main { // 无权二分图最大匹配算法 O(VE) static boolean match(int u, int[] link, boolean[] used, List\u0026lt;Integer\u0026gt; []g) { for (int v: g[u]) { if (used[v]) continue; used[v] = true; if (link[v] == 0 || match(link[v], link, used, g)) { link[u] = v; link[v] = u; return true; } } return false; } public static void main(String[] args) { Scanner sc = new Scanner(new BufferedInputStream(System.in)); int n = sc.nextInt(); int[] ws = new int[n + 1]; for (int i = 1; i \u0026lt;= n ;i++) { ws[i] = sc.nextInt(); } List\u0026lt;Integer\u0026gt;[]g = new List[n + 1]; Arrays.setAll(g, x -\u0026gt; new ArrayList\u0026lt;\u0026gt;()); for (int i = 0; i \u0026lt; n - 1; i++) { int u = sc.nextInt(); int v = sc.nextInt(); long rr = (long)ws[u] * ws[v]; long r = (long)Math.sqrt(rr); if (r * r == rr) { g[u].add(v); g[v].add(u); } } int ans = 0; int[] link = new int[n + 1]; for (int i = 1; i \u0026lt;= n; i++) { if (link[i] != 0) continue; boolean[] used = new boolean[n + 1]; used[i] = true; if (match(i, link, used, g)) { // 找到一条增广路径 ans++; } } System.out.println(ans * 2); } } 9.小美的字符串变换 # 此题是经典的并查集模板题。首先生成行列乘积为n的二阶矩阵，再根据矩阵中元素上下左右值是否与之相等选择是否union，最后得到并查集中集合数量。取所有二阶矩阵并查集中集合数量最小值即为解。\nimport java.util.*; public class Main { public static void main(String[] args) { Scanner sc = new Scanner(System.in); int n = sc.nextInt(); char[] str = sc.next().toCharArray(); int q = n; for (int i = 1; i \u0026lt;= n; i++) { if (n % i == 0) { int N = i; int M = n / i; int[][] board = new int[N][M]; int index = 0; for (int j = 0; j \u0026lt; N; j++) { for (int k = 0; k \u0026lt; M; k++) { board[j][k] = str[index++]; } } q = Math.min(q, q(board)); } } System.out.println(q); } public static int q(int[][] board) { int N = board.length; int M = board[0].length; UnionFind uf = new UnionFind(N * M); for (int i = 0; i \u0026lt; N; i++) { for (int j = 0; j \u0026lt; M; j++) { int cur = i * M + j; if (i - 1 \u0026gt;= 0 \u0026amp;\u0026amp; board[i - 1][j] == board[i][j]) { uf.union(cur - M, cur); } if (i + 1 \u0026lt; N \u0026amp;\u0026amp; board[i + 1][j] == board[i][j]) { uf.union(cur + M, cur); } if (j - 1 \u0026gt;= 0 \u0026amp;\u0026amp; board[i][j - 1] == board[i][j]) { uf.union(cur - 1, cur); } if (j + 1 \u0026lt; M \u0026amp;\u0026amp; board[i][j + 1] == board[i][j]) { uf.union(cur + 1, cur); } } } return uf.size(); } public static class UnionFind { private int[] parent; private int[] sizeMap; private int size; public UnionFind(int N) { size = N; parent = new int[N]; sizeMap = new int[N]; for (int i = 0; i \u0026lt; N; i++) { parent[i] = i; sizeMap[i] = 1; } } public int size() { return size; } private int find(int v) { if (v != parent[v]) { parent[v] = find(parent[v]); } return parent[v]; } public void union(int v1, int v2) { int f1 = find(v1); int f2 = find(v2); if (f1 != f2) { size--; int s1 = sizeMap[f1]; int s2 = sizeMap[f2]; if (s1 \u0026gt; s2) { parent[f2] = f1; sizeMap[f1] += s2; } else { parent[f1] = f2; sizeMap[f2] += s1; } } } } } ","date":"5 March 2024","permalink":"/posts/reviews/24%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0/%E7%BE%8E%E5%9B%A2/%E7%AC%94%E8%AF%95/%E7%BE%8E%E5%9B%A22024%E5%B1%8A%E7%A7%8B%E6%8B%9B%E7%AC%94%E8%AF%95%E7%AC%AC%E4%B8%80%E5%9C%BA%E7%BC%96%E7%A8%8B%E7%9C%9F%E9%A2%98/","section":"博客","summary":"这次的笔试，很多签到题，没什么难度，主要是第七题和第九题，比较有难度。","title":"美团2024届秋招笔试第一场编程真题"},{"content":" 链接：https://leetcode.cn/problems/redundant-connection/description/ 难点 # 是第一次刷并查集的题目，比较陌生了。\n给出一个并查集的模版：\npublic static class UnionFind { private int[] parent; private int[] sizeMap; private int size; public UnionFind(int N) { size = N; parent = new int[N]; sizeMap = new int[N]; for (int i = 0; i \u0026lt; N; i++) { parent[i] = i; sizeMap[i] = 1; } } public int size() { return size; } private int find(int v) { if (v != parent[v]) { parent[v] = find(parent[v]); } return parent[v]; } public void union(int v1, int v2) { int f1 = find(v1); int f2 = find(v2); if (f1 != f2) { size--; int s1 = sizeMap[f1]; int s2 = sizeMap[f2]; if (s1 \u0026gt; s2) { parent[f2] = f1; sizeMap[f1] += s2; } else { parent[f1] = f2; sizeMap[f2] += s1; } } } } 分析 # 在一棵树中，边的数量比节点的数量少 $1$。如果一棵树有 $n$ 个节点，则这棵树有 $n−1$ 条边。这道题中的图在树的基础上多了一条附加的边，因此边的数量也是 $n$。\n树是一个连通且无环的无向图，在树中多了一条附加的边之后就会出现环，因此附加的边即为导致环出现的边。\n可以通过并查集寻找附加的边。初始时，每个节点都属于不同的连通分量。遍历每一条边，判断这条边连接的两个顶点是否属于相同的连通分量。\n如果两个顶点属于不同的连通分量，则说明在遍历到当前的边之前，这两个顶点之间不连通，因此当前的边不会导致环出现，合并这两个顶点的连通分量。 如果两个顶点属于相同的连通分量，则说明在遍历到当前的边之前，这两个顶点之间已经连通，因此当前的边导致环出现，为附加的边，将当前的边作为答案返回。 代码 # class Solution { public int[] findRedundantConnection(int[][] edges) { int n = edges.length; int[] par = new int[n + 1]; for (int i = 1; i \u0026lt;= n; i++) { par[i] = i; } for (int i = 0; i \u0026lt; n; i++) { if (find(par, edges[i][0]) != find(par, edges[i][1])) { union(par, edges[i][0], edges[i][1]); } else { return edges[i]; } } return new int[0]; } public int find(int[] par, int v) { if (v != par[v]) { par[v] = find(par, par[v]); } return par[v]; } public void union(int[] par, int v, int u) { par[find(par, v)] = find(par, u); } } ","date":"3 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E5%B9%B6%E6%9F%A5%E9%9B%86/leetcode-684%E5%86%97%E4%BD%99%E8%BF%9E%E6%8E%A5/","section":"博客","summary":"【LeetCode 684】冗余连接题解。是第一次刷并查集的题目，比较陌生了。给出一个并查集的模版。","title":"【LeetCode 684】冗余连接"},{"content":"","date":"3 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E5%B9%B6%E6%9F%A5%E9%9B%86/","section":"博客","summary":"并查集是一种数据结构，用于处理集合的合并和查询操作，通常用于解决元素之间的相互关系和连接问题。","title":"并查集"},{"content":" 链接：https://leetcode.cn/problems/arithmetic-slices-ii-subsequence/description/ 难点 # 我们首先考虑至少有两个元素的等差子序列，下文将其称作弱等差子序列。\n由于尾项和公差可以确定一个等差数列，因此我们定义状态 dp[i][d] 表示尾项为 nums[i]，公差为 d 的弱等差子序列的个数。\n我们用一个二重循环去遍历 nums 中的所有元素对 (nums[i], nums[j])，其中 j\u0026lt;i。将 nums[i] 和 nums[j] 分别当作等差数列的尾项和倒数第二项，则该等差数列的公差 d=nums[i]-nums[j]。由于公差相同，我们可以将 nums[i] 加到以 nums[j] 为尾项，公差为 d 的弱等差子序列的末尾，这对应着状态转移 dp[i][d]+=dp[j][d]。同时，(nums[i], nums[j]) 这一对元素也可以当作一个弱等差子序列，故有状态转移dp[i][d]+=dp[j][d]+1\n由于题目要统计的等差子序列至少有三个元素，我们回顾上述二重循环，其中「将 nums[i] 加到以 nums[j] 为尾项，公差为 d 的弱等差子序列的末尾」这一操作，实际上就构成了一个至少有三个元素的等差子序列，因此我们将循环中的 dp[j][d] 累加，即为答案。\n代码 # class Solution { public int numberOfArithmeticSlices(int[] nums) { int n = nums.length; Map\u0026lt;Long, Integer\u0026gt;[] dp = new HashMap[n]; for (int i = 0; i \u0026lt; n; i++) { dp[i] = new HashMap\u0026lt;\u0026gt;(); } int ans = 0; for(int i = 1; i \u0026lt; n; i++) { for(int j = 0; j \u0026lt; i; j++) { long d = 1L * nums[i] - nums[j]; int temp = dp[j].getOrDefault(d, 0); ans += temp; dp[i].put(d, temp + 1 + dp[i].getOrDefault(d, 0)); } } return ans; } } ","date":"1 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/leetcode-446%E7%AD%89%E5%B7%AE%E6%95%B0%E5%88%97%E5%88%92%E5%88%86-ii---%E5%AD%90%E5%BA%8F%E5%88%97/","section":"博客","summary":"【LeetCode 446】等差数列划分 II - 子序列题解。我们用一个二重循环去遍历 \u003ccode\u003enums\u003c/code\u003e 中的所有元素对 \u003ccode\u003e(nums[i], nums[j])\u003c/code\u003e，其中 \u003ccode\u003ej\u0026lt;i\u003c/code\u003e。将 \u003ccode\u003enums[i]\u003c/code\u003e 和 \u003ccode\u003enums[j]\u003c/code\u003e 分别当作等差数列的尾项和倒数第二项，则该等差数列的公差 \u003ccode\u003ed=nums[i]-nums[j]\u003c/code\u003e。由于公差相同，我们可以将 \u003ccode\u003enums[i]\u003c/code\u003e 加到以 \u003ccode\u003enums[j]\u003c/code\u003e 为尾项，公差为 \u003ccode\u003ed\u003c/code\u003e 的弱等差子序列的末尾，这对应着状态转移 \u003ccode\u003edp[i][d]+=dp[j][d]\u003c/code\u003e。同时，\u003ccode\u003e(nums[i], nums[j])\u003c/code\u003e 这一对元素也可以当作一个弱等差子序列，故有状态转移\u003ccode\u003edp[i][d]+=dp[j][d]+1\u003c/code\u003e","title":"【LeetCode 446】等差数列划分 II - 子序列"},{"content":" 链接：https://leetcode.cn/problems/largest-divisible-subset/description/ 难点 # 难点在于如何记录答案，最后把最大整除子集输出。幸运的是，题目没有限制输出的字典序，只要输出的是合理答案就可以。\n代码 # class Solution { public List\u0026lt;Integer\u0026gt; largestDivisibleSubset(int[] nums) { Arrays.sort(nums); int n = nums.length; int[] dp = new int[n]; int[] cnt = new int[n]; for(int i = n-2; i \u0026gt;=0 ; i--) { for(int j = n-1; j \u0026gt; i; j--) { if(nums[j] % nums[i] == 0 \u0026amp;\u0026amp; dp[j] + 1 \u0026gt; dp[i]) { dp[i] = dp[j] + 1; cnt[i] = j; } } } int maxx = Arrays.stream(dp).max().getAsInt(); List\u0026lt;Integer\u0026gt; ans = new ArrayList\u0026lt;\u0026gt;(); for(int i = 0; i \u0026lt; n; i++) { if(dp[i] == maxx) { ans.add(nums[i]); int temp = cnt[i]; while(temp != 0) { i = cnt[i]; temp = cnt[i]; ans.add(nums[i]); } break; } } return ans; } } 当然，用题解中的方法也可以，举个例子\n上图中 dp 为 4 的 16，肯定可以整除一个 dp 为 3 的，上面的就是 8，以此类推，得到结果。\nclass Solution { public List\u0026lt;Integer\u0026gt; largestDivisibleSubset(int[] nums) { int len = nums.length; Arrays.sort(nums); // 第 1 步：动态规划找出最大子集的个数、最大子集中的最大整数 int[] dp = new int[len]; Arrays.fill(dp, 1); int maxSize = 1; int maxVal = dp[0]; for (int i = 1; i \u0026lt; len; i++) { for (int j = 0; j \u0026lt; i; j++) { // 题目中说「没有重复元素」很重要 if (nums[i] % nums[j] == 0) { dp[i] = Math.max(dp[i], dp[j] + 1); } } if (dp[i] \u0026gt; maxSize) { maxSize = dp[i]; maxVal = nums[i]; } } // 第 2 步：倒推获得最大子集 List\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;Integer\u0026gt;(); if (maxSize == 1) { res.add(nums[0]); return res; } for (int i = len - 1; i \u0026gt;= 0 \u0026amp;\u0026amp; maxSize \u0026gt; 0; i--) { if (dp[i] == maxSize \u0026amp;\u0026amp; maxVal % nums[i] == 0) { res.add(nums[i]); maxVal = nums[i]; maxSize--; } } return res; } } ","date":"1 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/leetcode-368%E6%9C%80%E5%A4%A7%E6%95%B4%E9%99%A4%E5%AD%90%E9%9B%86/","section":"博客","summary":"【LeetCode 368】最大整除子集题解。难点在于如何记录答案，最后把最大整除子集输出。幸运的是，题目没有限制输出的字典序，只要输出的是合理答案就可以。","title":"【LeetCode 368】最大整除子集"},{"content":"CGLIB 是一个功能强大，高性能的代码生成包。它为没有实现接口的类提供代理，为 JDK 的动态代理提供了很好的补充。通常可以使用Java的动态代理创建代理，但当要代理的类没有实现接口或者为了更好的性能，CGLIB 是一个好的选择。\nCGLIB 作为一个开源项目，其代码托管在 Github，地址为：https://github.com/cglib/cglib\n1. CGLIB 原理 # 名称 解释 CGLIB 原理 动态生成一个要代理类的子类，子类重写要代理的类的所有不是final的方法。在子类中采用方法拦截的技术拦截所有父类方法的调用，顺势织入横切逻辑。它比使用java反射的JDK动态代理要快。 CGLIB 底层 使用字节码处理框架ASM，来转换字节码并生成新的类。不鼓励直接使用ASM，因为它要求你必须对JVM内部结构包括class文件的格式和指令集都很熟悉。 CGLIB 缺点 对于final方法，无法进行代理。 广泛的被许多 AOP 的框架使用，例如 Spring AOP 和 dynaop 。Hibernate 使用 CGLIB 来代理单端single-ended(多对一和一对一)关联。\n2. 为什么使用 CGLIB? # CGLIB 代理主要通过对字节码的操作，为对象引入间接级别，以控制对象的访问。我们知道 Java 中有一个动态代理也是做这个事情的，那我们为什么不直接使用 Java 动态代理，而要使用 CGLIB 呢？\n答案是 CGLIB 相比于 JDK动态代理 更加强大，JDK动态代理 虽然简单易用，但是其有一个致命缺陷是，只能对接口进行代理。如果要代理的类为一个普通类、没有接口，那么 Java动态代理 就没法使用了。\n2.1 JAVA 动态代理分析 # Java动态代理机制 的出现，使得 Java开发人员 不用手工编写代理类，只要简单地制定一组接口及委托类对象，便能动态地获得代理类。代理类会负责将所有的方法调用分配到委托对象上反射执行，配置执行过程中，开发人员还可以进行修改。\n2.1.1 代理设计模式 # 代理是一种常用的设计模式，其目的就是为其他对象提供一个代理以控制对某个对象的访问。代理类负责为委托类预处理消息、过滤消息并转发消息，以及进行消息被委托类执行后的后续处理。\n为了保持行为的一致性，代理类和委托类通常会实现相同的接口 引入代理能够控制对委托对象的直接访问，可以很好的隐藏和保护委托对象，也更加具有灵活性 2.1.2 相关的类和接口 # 要了解 Java 动态代理的机制，首先需要了解以下相关的类或接口：\njava.lang.reflect.Proxy ：这是 Java 动态代理机制的主类，它提供了一组静态方法来为一组接口动态地生成代理类及其对象 java.lang.reflect.InvocationHandler ：这是调用处理器接口，它自定义了一个 invoke 方法，用于几种处理在动态代理类对象上的方法调用。通常在该方法中实现对委托类的代理访问。 java.lang.ClassLoader ：Proxy 静态方法生成动态代理类同样需要通过类装载器来进行装载才能使用，它与普通类的唯一区别就是其字节码是由 JVM 在运行时动态生成的而非预存在于任何一个 .class 文件中。 2.1.3 代理机制及其特点 # 首先让我们来了解一下如何使用 Java 动态代理。具体有如下四步骤：\n通过实现 InvocationHandler 接口创建自己的调用处理器； 通过为 Proxy 类指定 ClassLoader 对象和一组 interface 来创建动态代理类； 通过反射机制获得动态代理类的构造函数，其唯一参数类型是调用处理器接口类型； 通过构造函数创建动态代理类实例，构造时调用处理器对象作为参数被传入。 // InvocationHandlerImpl 实现了 InvocationHandler 接口，并能实现方法调用从代理类到委托类的分派转发 // 其内部通常包含指向委托类实例的引用，用于真正执行分派转发过来的方法调用 InvocationHandler handler = new InvocationHandlerImpl(..); // 通过 Proxy 为包括 Interface 接口在内的一组接口动态创建代理类的类对象 Class clazz = Proxy.getProxyClass(classLoader, new Class[] { Interface.class, ... }); // 通过反射从生成的类对象获得构造函数对象 Constructor constructor = clazz.getConstructor(new Class[] { InvocationHandler.class }); // 通过构造函数对象创建动态代理类实例 Interface Proxy = (Interface)constructor.newInstance(new Object[] { handler }); 实际使用过程更加简单，因为 Proxy 的静态方法 newProxyInstance 已经为我们封装了步骤 2 到步骤 4 的过程，所以简化后的过程如下：\n// InvocationHandlerImpl 实现了 InvocationHandler 接口，并能实现方法调用从代理类到委托类的分派转发 InvocationHandler handler = new InvocationHandlerImpl(..); // 通过 Proxy 直接创建动态代理类实例 Interface proxy = (Interface)Proxy.newProxyInstance( classLoader, new Class[] { Interface.class }, handler ); 动态生成的代理类本身的一些特点\n包：如果所代理的接口都是 public 的，那么它将被定义在顶层包（即包路径为空），如果所代理的接口中有 非 public 的接口（因为接口不能被定义为 protect 或 private ，所以除 public 之外就是默认的 package 访问级别，那么它将被定义在该接口所在包，这样设计的目的是为了最大程度的保证动态代理类不会因为包管理的问题而无法被成功定义并访问； 类修饰符：该代理类具有 final 和 public 修饰符，意味着它可以被所有的类访问，但是不能被再度继承； 类名：格式是 $ProxyN ，其中 N 是一个逐一递增的阿拉伯数字，代表 Proxy 类 第 N 次 生成的动态代理类，值得注意的一点是，并不是每次调用 Proxy 的静态方法创建动态代理类都会使得 N 值增加，原因是如果对同一组接口（包括接口排列的顺序相同）试图重复创建动态代理类，它会很聪明地返回先前已经创建好的代理类的类对象，而不会再尝试去创建一个全新的代理类，这样可以节省不必要的代码重复生成，提高了代理类的创建效率。 类继承关系：Proxy 类是它的父类，这个规则适用于所有由 Proxy 创建的动态代理类。而且该类还实现了其所代理的一组接口。 代理类实例的一些特点：\n每个实例都会关联一个 InvocationHandler （调用处理器对象），在代理类实例上调用其代理接口中声明的方法时，最终都会由 InvocationHandler 的 invoke方法 执行； java.lang.Object 中有三个方法也同样会被分派到调用处理器的 invoke 方法 执行，它们是 hashCode ，equals 和 toString； 被代理接口的一组特点\n要注意不能有重复的接口 接口对于类装载器必须可见，否则类装载器将无法链接它们 被代理的所有非 public 的接口必须在同一个包中，接口的数目不能超过65535 2.1.4 美中不足 # Proxy 只能对 interface 进行代理，无法实现对 class 的动态代理。观察动态生成的代理继承关系图可知原因，他们已经有一个固定的父类叫做 Proxy ，Java语法 限定其不能再继承其他的父类。\n2.1.5 代码示例 # 最后以一个简单的动态代理例子结束\npublic class DynamicProxy { interface IHello{ void sayHello(); } static class Hello implements IHello{ public void sayHello() { System.out.println(\u0026#34;hello world\u0026#34;); } } static class DynamicProxyTest implements InvocationHandler{ Object originalObj; Object bind(Object originalObj){ this.originalObj = originalObj; return Proxy.newProxyInstance(originalObj.getClass().getClassLoader(), originalObj.getClass().getInterfaces(), this); } public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(\u0026#34;Welcome\u0026#34;); return method.invoke(originalObj, args); } } public static void main(String[] args){ //设置这个值，在程序运行完成后，可以生成代理类 System.getProperties().put(\u0026#34;sun.misc.ProxyGenerator.saveGeneratedFiles\u0026#34;, \u0026#34;true\u0026#34;); IHello hello = (IHello) new DynamicProxyTest().bind(new Hello()); hello.sayHello(); } } 程序输出为：\nWelcome hello world 3. CGLIB 组成结构 # CGLIB 底层使用了ASM（一个短小精悍的字节码操作框架）来操作字节码生成新的类。除了 CGLIB库 外，脚本语言（如 Groovy 和 BeanShell ）也使用 ASM 生成字节码。ASM 使用类似 SAX 的解析器来实现高性能。\n4. CGLIB 的 API # 4.1 Jar包 # cglib-nodep-2.2.jar：使用 nodep 包不需要关联 asm 的 jar 包，jar 包内部包含 asm 的类。 cglib-2.2.jar：使用此 jar 包需要关联 asm 的 jar 包,否则运行时报错。 4.2 CGLIB类库 # 名称 描述 net.sf.cglib.core 底层字节码处理类，他们大部分与 ASM 有关系 net.sf.cglib.transform 编译期或运行期类和类文件的转换 net.sf.cglib.proxy 实现创建代理和方法拦截器的类 net.sf.cglib.reflect 实现快速反射和C#风格代理的类 net.sf.cglib.util 集合排序等工具类 net.sf.cglib.beans JavaBean相关的工具类 4.3 例子 # 说了这么多，可能大家还是不知道 CGLIB 是干什么用的。下面我们将使用一个简单的例子来演示如何使用 CGLIB 对一个方法进行拦截。 首先，我们需要在工程的 POM 文件中引入 CGLIB 的 dependency，这里我们使用的是 2.2.2 版本。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;cglib\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;cglib\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.2.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 依赖包下载后，我们就可以干活了，按照国际惯例，写个 hello world\npublic class SampleClass { public void test(){ System.out.println(\u0026#34;hello world\u0026#34;); } public static void main(String[] args) { Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(SampleClass.class); enhancer.setCallback(new MethodInterceptor() { @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable { System.out.println(\u0026#34;before method run...\u0026#34;); Object result = proxy.invokeSuper(obj, args); System.out.println(\u0026#34;after method run...\u0026#34;); return result; } }); SampleClass sample = (SampleClass) enhancer.create(); sample.test(); } } 在 mian 函数中，我们通过一个 Enhancer 和一个 MethodInterceptor 来实现对方法的拦截，运行程序后输出为：\nbefore method run... hello world after method run... 4.4 常见的 API # 4.4.1 Enhancer # Enhancer 可能是 CGLIB 中最常用的一个类，和 Java1.3动态代理 中引入的 Proxy 类差不多。 和Proxy 不同的是，Enhancer 既能够代理普通的 class ，也能够代理接口。 Enhancer 创建一个被代理对象的子类并且拦截所有的方法调用（包括从 Object 中继承的 toString 和 hashCode 方法）。 Enhancer 不能够拦截 final 方法，例如 Object.getClass() 方法，这是由于 Java final 方法语义决定的。 基于同样的道理，Enhancer 也不能对 fianl 类进行代理操作。这也是 Hibernate 为什么不能持久化 final class 的原因。 public class SampleClass { public String test(String input){ return \u0026#34;hello world\u0026#34;; } } 下面我们将以这个类作为主要的测试类，来测试调用各种方法\n@Test public void testFixedValue(){ Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(SampleClass.class); enhancer.setCallback(new FixedValue() { @Override public Object loadObject() throws Exception { return \u0026#34;Hello cglib\u0026#34;; } }); SampleClass proxy = (SampleClass) enhancer.create(); System.out.println(proxy.test(null)); //拦截test，输出Hello cglib System.out.println(proxy.toString()); System.out.println(proxy.getClass()); System.out.println(proxy.hashCode()); } 输出：\nHello cglib Hello cglib class com.zeus.cglib.SampleClass$$EnhancerByCGLIB$$e3ea9b7 java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Number at com.zeus.cglib.SampleClass$$EnhancerByCGLIB$$e3ea9b7.hashCode(\u0026lt;generated\u0026gt;) ... 上述代码中，FixedValue 用来对所有拦截的方法返回相同的值，从输出我们可以看出来：\nEnhancer 对 非final方法test()、toString()、hashCode()进行了拦截，没有对getClass进行拦截。\n由于 hashCode() 方法需要返回一个 Number，但是我们返回的是一个 String，这解释了上面的程序中为什么会抛出异常。\nEnhancer.setSuperclass 用来设置父类型，从 toString() 可以看出，使用 CGLIB 生成的类为被代理类的一个子类，形如：SampleClass$$EnhancerByCGLIB$$e3ea9b7\nEnhancer.create(Object…) 方法是用来创建增强对象的，其提供了很多不同参数的方法用来匹配被增强类的不同构造方法。（虽然类的构造方法只是 Java字节码 层面的函数，但是 Enhancer 却不能对其进行操作。Enhancer 同样不能操作 static 或者 final 类）。\n我们也可以先使用Enhancer.createClass() 来创建字节码（ .class ），然后用字节码动态的生成增强后的对象。 可以使用一个 InvocationHandler 作为回调，使用 invoke 方法来替换直接访问类的方法，但是你必须注意死循环。因为 invoke 中调用的任何原代理类方法，均会重新代理到 invoke 方法中。\npublic void testInvocationHandler() throws Exception{ Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(SampleClass.class); enhancer.setCallback(new InvocationHandler() { @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { if(method.getDeclaringClass() != Object.class \u0026amp;\u0026amp; method.getReturnType() == String.class){ return \u0026#34;hello cglib\u0026#34;; }else{ throw new RuntimeException(\u0026#34;Do not know what to do\u0026#34;); } } }); SampleClass proxy = (SampleClass) enhancer.create(); Assert.assertEquals(\u0026#34;hello cglib\u0026#34;, proxy.test(null)); Assert.assertNotEquals(\u0026#34;Hello cglib\u0026#34;, proxy.toString()); } 为了避免这种死循环，我们可以使用 MethodInterceptor ，MethodInterceptor 的例子在前面的 hello world 中已经介绍过了，这里就不浪费时间了。\n有些时候我们可能只想对特定的方法进行拦截，对其他的方法直接放行，不做任何操作，使用Enhancer处理这种需求同样很简单，只需要一个 CallbackFilter 即可：\n@Test public void testCallbackFilter() throws Exception{ Enhancer enhancer = new Enhancer(); CallbackHelper callbackHelper = new CallbackHelper(SampleClass.class, new Class[0]) { @Override protected Object getCallback(Method method) { if(method.getDeclaringClass() != Object.class \u0026amp;\u0026amp; method.getReturnType() == String.class){ return new FixedValue() { @Override public Object loadObject() throws Exception { return \u0026#34;Hello cglib\u0026#34;; } }; }else{ return NoOp.INSTANCE; } } }; enhancer.setSuperclass(SampleClass.class); enhancer.setCallbackFilter(callbackHelper); enhancer.setCallbacks(callbackHelper.getCallbacks()); SampleClass proxy = (SampleClass) enhancer.create(); Assert.assertEquals(\u0026#34;Hello cglib\u0026#34;, proxy.test(null)); Assert.assertNotEquals(\u0026#34;Hello cglib\u0026#34;,proxy.toString()); System.out.println(proxy.hashCode()); } 4.4.1.1 Java回调函数详解 # 4.4.1.1.1 什么是回调函数（CallBack） # 在编写程序时，有时候会调用许多 API 中实现实现的函数，但某些方法需要我们传入一个方法，以便在需要的时候调用我们传入进去的函数。这个被传入的函数称为回调函数（Callback function）。\n打个比方，有一个餐馆，提供炒菜的服务，但是会让我们选择做菜的方式，我们去这家餐馆里面吃饭，想吃小龙虾，我们告诉他想吃小龙虾后，他询问我们要以何种方式去进行烹饪，是煎炒烹炸还是避风塘。 在上面的例子中，炒菜是我们需要调用的方法，也是 API 库中所提供的，而炒菜的方式，则是我们去选择的，可以我们自己去定义的。\n这个就可以回调函数，有库函数（Librart function）来执行我们传入的回调函数（Callback function）\n4.4.1.1.2 在Java中实现回调函数 # Callable接口\nInterface Callable\u0026lt;V\u0026gt; 在 Java1.8 官方文档中给出的内容为\n参数类型：V - 回调方法的返回值类型\n已经实现的子接口：DocumentationTool.DocumentationTask，JavaCompiler.CompilationTask\n这个接口位函数试接口\n@FunctionalInterface public interface Callable\u0026lt;V\u0026gt; 返回结果可能引发异常，这个接口与 Runnable 非常相似，这两个接口的设计可以在实例化后，开启新的线程，与 Runnable 的差别是，Runnable 不能返回参数也不能抛出异常。\n例子：\nimport java.util.Random; import java.util.concurrent.Callable; public class CallableExample implements Callable { @Override public Object call() throws Exception { Random generator = new Random(); Integer randomNumber = generator.nextInt(5); Thread.sleep(randomNumber * 1000); return randomNumber; } } 测试：\n@Test public void callabledTest(){ ExecutorService executorService = Executors.newCachedThreadPool(); CallableExample callableExample = new CallableExample(); Future\u0026lt;Object\u0026gt; future = executorService.submit(callableExample); executorService.shutdown(); try{ System.out.println(future.get()); }catch (Exception e){ e.printStackTrace(); } } 返回值：\n3 Callback接口\n已知实现此接口的类 AuthorizeCallback, ChoiceCallback, ConfirmationCallback, LanguageCallback, NameCallback, PasswordCallback, RealmCallback, RealmChoiceCallback, TextInputCallback, TextOutputCallback\n这个接口的实现了会被传递给 CallbackHandler，允许有能力的底层服务去回应这个回调方法，已便进行诸如数据检索等信息。回调函数不检索或显示底层安全服务请求的信息。回调实现只是提供了将这些请求传递给应用程序的方法，并且对于应用程序，如果合适的话，可以将请求的信息返回给底层的安全服务。\n这个接口是可以自己定义的，定制适用于当前业务的callback 接口类型来表示不同类型的回调函数。\ncallback接口的源码\npublic interface Callback { } CallbackHandler 接口\n方法：handle(Callback [] callbacks) ，这个方法是用来处理处理 callback 类型的\n官方实例：\npublic void handle(Callback[] callbacks) throws IOException, UnsupportedCallbackException { for (int i = 0; i \u0026lt; callbacks.length; i++) { if (callbacks[i] instanceof TextOutputCallback) { // display the message according to the specified type TextOutputCallback toc = (TextOutputCallback)callbacks[i]; switch (toc.getMessageType()) { case TextOutputCallback.INFORMATION: System.out.println(toc.getMessage()); break; case TextOutputCallback.ERROR: System.out.println(\u0026#34;ERROR: \u0026#34; + toc.getMessage()); break; case TextOutputCallback.WARNING: System.out.println(\u0026#34;WARNING: \u0026#34; + toc.getMessage()); break; default: throw new IOException(\u0026#34;Unsupported message type: \u0026#34; + toc.getMessageType()); } } else if (callbacks[i] instanceof NameCallback) { // prompt the user for a username NameCallback nc = (NameCallback)callbacks[i]; // ignore the provided defaultName System.err.print(nc.getPrompt()); System.err.flush(); nc.setName((new BufferedReader (new InputStreamReader(System.in))).readLine()); } else if (callbacks[i] instanceof PasswordCallback) { // prompt the user for sensitive information PasswordCallback pc = (PasswordCallback)callbacks[i]; System.err.print(pc.getPrompt()); System.err.flush(); pc.setPassword(readPassword(System.in)); } else { throw new UnsupportedCallbackException (callbacks[i], \u0026#34;Unrecognized Callback\u0026#34;); } } } // Reads user password from given input stream. private char[] readPassword(InputStream in) throws IOException { // insert code to read a user password from the input stream } 通过传入不同的已经实现了 Callback 接口的实现类，通过分析不同实现类的类型来进行不同的处理，调用形参实现类内的方法（回调）。\n4.4.1.1.3 一般来说如何使用 # 在一般工作中，我们都是自己定义接口，写实现类，来进行回调的。\n自定义的回调函数实例\n这个是Callback接口类，我们一会儿要是用它来创造内部匿名类，来实现这个接口，完成字表的筛选工作： import java.util.List; public interface CallBackInterface { Object process(List\u0026lt;String\u0026gt; list); } 这个是处理端，通过handler方法，调用传入的 CallBackInterface 类型中的方法，来对字表进行操作 import lombok.Data; import java.util.ArrayList; import java.util.List; @Data public class WorldListHandler { List\u0026lt;String\u0026gt; stringList = new ArrayList\u0026lt;\u0026gt;(); public void execute(CallBackInterface callBackInterface){ Object process = callBackInterface.process(stringList); System.out.println(process); } } 使用 CallBackInterface 接口并实现它，来让 Handler 来调用它其中的 process 方法来完成对字表的筛选 @Test public void callableTest2(){ List\u0026lt;String\u0026gt; list = Arrays.asList(\u0026#34;123\u0026#34;,\u0026#34;asd\u0026#34;,\u0026#34;1432\u0026#34;,\u0026#34;fsd\u0026#34;,\u0026#34;543\u0026#34;,\u0026#34;987\u0026#34;,\u0026#34;tre\u0026#34;); WorldListHandler worldListHandler = new WorldListHandler(); worldListHandler.setStringList(list); worldListHandler.execute(new CallBackInterface() { @Override public Object process(List\u0026lt;String\u0026gt; list) { List\u0026lt;String\u0026gt; collect = list.stream().filter(e -\u0026gt; e.contains(\u0026#34;1\u0026#34;)).collect(Collectors.toList()); worldListHandler.setStringList(collect); return true; } }); worldListHandler.getStringList().forEach(e-\u0026gt; System.out.println(e)); } 结果：\ntrue 123 1432 true 为 process 的返回值，剩下的为我们筛选出字表中包含有 1 的字符串。\n4.4.2 ImmutableBean # 通过名字就可以知道，不可变的 Bean 。ImmutableBean 允许创建一个原来对象的包装类，这个包装类是不可变的，任何改变底层对象的包装类操作都会抛出 IllegalStateException 。但是我们可以通过直接操作底层对象来改变包装类对象。这有点类似于 Guava 中的不可变视图。为了对 ImmutableBean 进行测试，这里需要再引入一个bean：\npublic class SampleBean { private String value; public SampleBean() { } public SampleBean(String value) { this.value = value; } public String getValue() { return value; } public void setValue(String value) { this.value = value; } } 然后编写测试类如下：\n@Test(expected = IllegalStateException.class) public void testImmutableBean() throws Exception{ SampleBean bean = new SampleBean(); bean.setValue(\u0026#34;Hello world\u0026#34;); SampleBean immutableBean = (SampleBean) ImmutableBean.create(bean); //创建不可变类 Assert.assertEquals(\u0026#34;Hello world\u0026#34;, immutableBean.getValue()); bean.setValue(\u0026#34;Hello world, again\u0026#34;); // 可以通过底层对象来进行修改 Assert.assertEquals(\u0026#34;Hello world, again\u0026#34;, immutableBean.getValue()); immutableBean.setValue(\u0026#34;Hello cglib\u0026#34;); // 直接修改将throw exception } 要是报以下的错误：\njava.lang.Exception: Unexpected exception, expected\u0026lt;java.lang.IllegalStateException\u0026gt; but was\u0026lt;net.sf.cglib.core.CodeGenerationException\u0026gt; at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:30) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54) Caused by: net.sf.cglib.core.CodeGenerationException: java.lang.reflect.InaccessibleObjectException--\u0026gt;Unable to make protected final java.lang.Class java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain) throws java.lang.ClassFormatError accessible: module java.base does not \u0026#34;opens java.lang\u0026#34; to unnamed module @108c4c35 at net.sf.cglib.core.ReflectUtils.defineClass(ReflectUtils.java:464) at net.sf.cglib.core.AbstractClassGenerator.generate(AbstractClassGenerator.java:339) at net.sf.cglib.core.AbstractClassGenerator$ClassLoaderData$3.apply(AbstractClassGenerator.java:96) at net.sf.cglib.core.AbstractClassGenerator$ClassLoaderData$3.apply(AbstractClassGenerator.java:94) at net.sf.cglib.core.internal.LoadingCache$2.call(LoadingCache.java:54) at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) at net.sf.cglib.core.internal.LoadingCache.createEntry(LoadingCache.java:61) at net.sf.cglib.core.internal.LoadingCache.get(LoadingCache.java:34) at net.sf.cglib.core.AbstractClassGenerator$ClassLoaderData.get(AbstractClassGenerator.java:119) at net.sf.cglib.core.AbstractClassGenerator.create(AbstractClassGenerator.java:294) at net.sf.cglib.beans.ImmutableBean$Generator.create(ImmutableBean.java:70) at net.sf.cglib.beans.ImmutableBean.create(ImmutableBean.java:42) at cn.bugstack.springframework.test.ApiTest.testImmutableBean(ApiTest.java:39) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:78) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:567) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19) ... 17 more 请更换 JDK 的版本，这是由于 JDK 8 中有关反射相关的功能自从 JDK 9 开始就已经被限制了，为了兼容原先的版本，需要在运行项目时添加 --add-opens java.base/java.lang=ALL-UNNAMED 选项来开启这种默认不被允许的行为。如果是通过 IDEA 来运行项目，那么可以在 “Edit Configurations” 中 ——\u0026gt; “VM options” 输入框中输入该选项来完成，最终结果如下图所示：\n结果：\n4.4.3 Bean generator # CGLIB 提供的一个操作 bean 的工具，使用它能够在运行时动态的创建一个 bean 。\n@Test public void testBeanGenerator() throws Exception{ BeanGenerator beanGenerator = new BeanGenerator(); beanGenerator.addProperty(\u0026#34;value\u0026#34;, String.class); Object myBean = beanGenerator.create(); Method setter = myBean.getClass().getMethod(\u0026#34;setValue\u0026#34;, String.class); setter.invoke(myBean, \u0026#34;Hello cglib\u0026#34;); Method getter = myBean.getClass().getMethod(\u0026#34;getValue\u0026#34;); Assert.assertEquals(\u0026#34;Hello cglib\u0026#34;, getter.invoke(myBean)); } 在上面的代码中，我们使用 CGLIB 动态的创建了一个和 SampleBean 相同的 Bean 对象，包含一个属性 value 以及 getter 、 setter 方法。\n结果：\n4.4.4 Bean Copier # CGLIB 提供的能够从一个 bean 复制到另一个 bean 中，而且其还提供了一个转换器，用来在转换的时候对 bean 的属性进行操作。\npublic class OtherSampleBean { private String value; public String getValue() { return value; } public void setValue(String value) { this.value = value; } } 测试：\n@Test public void testBeanCopier() throws Exception{ BeanCopier copier = BeanCopier.create(SampleBean.class, OtherSampleBean.class, false); //设置为true，则使用converter SampleBean myBean = new SampleBean(); myBean.setValue(\u0026#34;Hello cglib\u0026#34;); OtherSampleBean otherBean = new OtherSampleBean(); copier.copy(myBean, otherBean, null); //设置为true，则传入converter指明怎么进行转换 assertEquals(\u0026#34;Hello cglib\u0026#34;, otherBean.getValue()); } 4.4.5 BulkBean # 相比于 BeanCopier ，BulkBean 将 copy 的动作拆分为 getPropertyValues 和 setPropertyValues 两个方法，允许自定义处理属性\n@Test public void testBulkBean() throws Exception{ BulkBean bulkBean = BulkBean.create(SampleBean.class, new String[]{\u0026#34;getValue\u0026#34;}, new String[]{\u0026#34;setValue\u0026#34;}, new Class[]{String.class}); SampleBean bean = new SampleBean(); bean.setValue(\u0026#34;Hello world\u0026#34;); Object[] propertyValues = bulkBean.getPropertyValues(bean); assertEquals(1, bulkBean.getPropertyValues(bean).length); assertEquals(\u0026#34;Hello world\u0026#34;, bulkBean.getPropertyValues(bean)[0]); bulkBean.setPropertyValues(bean, new Object[]{\u0026#34;Hello cglib\u0026#34;}); assertEquals(\u0026#34;Hello cglib\u0026#34;, bean.getValue()); } 使用注意：\n避免每次进行 BukBean.create 创建对象，一般将其声明为 static 的 应用场景：针对特定属性的 get, set 操作，一般适用通过 xml 配置注入和注出的属性，运行时才确定处理的 Source, Target 类，只需要关注属性名即可。 4.4.6 BeanMap # BeanMap 类实现了Java Map，将一个bean对象中的所有属性转换为一个 String-to-Obejct 的 Java Map\n@Test public void testBeanMap() throws Exception{ BeanGenerator generator = new BeanGenerator(); generator.addProperty(\u0026#34;username\u0026#34;, String.class); generator.addProperty(\u0026#34;password\u0026#34;, String.class); Object bean = generator.create(); Method setUserName = bean.getClass().getMethod(\u0026#34;setUsername\u0026#34;, String.class); Method setPassword = bean.getClass().getMethod(\u0026#34;setPassword\u0026#34;, String.class); setUserName.invoke(bean, \u0026#34;admin\u0026#34;); setPassword.invoke(bean, \u0026#34;password\u0026#34;); BeanMap map = BeanMap.create(bean); Assert.assertEquals(\u0026#34;admin\u0026#34;, map.get(\u0026#34;username\u0026#34;)); Assert.assertEquals(\u0026#34;password\u0026#34;, map.get(\u0026#34;password\u0026#34;)); } 4.4.7 keyFactory # keyFactory 类用来动态生成接口的实例，接口需要只包含一个 newInstance 方法，返回一个 Object 。keyFactory 为构造出来的实例动态生成了 Object.equals 和 Object.hashCode 方法，能够确保相同的参数构造出的实例为单例的。\npublic interface SampleKeyFactory { Object newInstance(String first, int second); } 我们首先构造一个满足条件的接口，然后进行测试\n@Test public void testKeyFactory() throws Exception{ SampleKeyFactory keyFactory = (SampleKeyFactory) KeyFactory.create(SampleKeyFactory.class); Object key = keyFactory.newInstance(\u0026#34;foo\u0026#34;, 42); Object key1 = keyFactory.newInstance(\u0026#34;foo\u0026#34;, 42); Assert.assertEquals(key,key1);//测试参数相同，结果是否相等 } 4.4.8 Mixin（混合） # Mixin 能够让我们将多个对象整合到一个对象中去，前提是这些对象必须是接口的实现。可能这样说比较晦涩，以代码为例：\npublic class MixinInterfaceTest { interface Interface1{ String first(); } interface Interface2{ String second(); } class Class1 implements Interface1{ @Override public String first() { return \u0026#34;first\u0026#34;; } } class Class2 implements Interface2{ @Override public String second() { return \u0026#34;second\u0026#34;; } } interface MixinInterface extends Interface1, Interface2{ } @Test public void testMixin() throws Exception{ Mixin mixin = Mixin.create(new Class[]{Interface1.class, Interface2.class, MixinInterface.class}, new Object[]{new Class1(), new Class2()}); MixinInterface mixinDelegate = (MixinInterface) mixin; assertEquals(\u0026#34;first\u0026#34;, mixinDelegate.first()); assertEquals(\u0026#34;second\u0026#34;, mixinDelegate.second()); } } Mixin 类比较尴尬，因为他要求 Minix 的类（例如 MixinInterface ）实现一些接口。既然被 Minix 的类已经实现了相应的接口，那么我就直接可以通过纯 Java 的方式实现，没有必要使用 Minix 类。\n4.4.9 String switcher # 用来模拟一个 String 到 int 类型的 Map 类型。如果在 Java7 以后的版本中，类似一个 switch 语句。\n@Test public void testStringSwitcher() throws Exception{ String[] strings = new String[]{\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;}; int[] values = new int[]{10,20}; StringSwitcher stringSwitcher = StringSwitcher.create(strings, values, true); assertEquals(10, stringSwitcher.intValue(\u0026#34;one\u0026#34;)); assertEquals(20, stringSwitcher.intValue(\u0026#34;two\u0026#34;)); assertEquals(-1, stringSwitcher.intValue(\u0026#34;three\u0026#34;)); } 4.4.10 Interface Maker # 正如名字所言，Interface Maker 用来创建一个新的 Interface\n@Test public void testInterfaceMarker() throws Exception{ Signature signature = new Signature(\u0026#34;foo\u0026#34;, Type.DOUBLE_TYPE, new Type[]{Type.INT_TYPE}); InterfaceMaker interfaceMaker = new InterfaceMaker(); interfaceMaker.add(signature, new Type[0]); Class iface = interfaceMaker.create(); assertEquals(1, iface.getMethods().length); assertEquals(\u0026#34;foo\u0026#34;, iface.getMethods()[0].getName()); assertEquals(double.class, iface.getMethods()[0].getReturnType()); } 上述的 Interface Maker 创建的接口中只含有一个方法，签名为 double foo(int)。Interface Maker与上面介绍的其他类不同，它依赖 ASM 中的 Type 类型。由于接口仅仅只用做在编译时期进行类型检查，因此在一个运行的应用中动态的创建接口没有什么作用。但是 InterfaceMaker 可以用来自动生成代码，为以后的开发做准备。\n4.4.11 Method delegate # MethodDelegate 主要用来对方法进行代理\ninterface BeanDelegate{ String getValueFromDelegate(); } 测试：\n@Test public void testMethodDelegate() throws Exception{ SampleBean bean = new SampleBean(); bean.setValue(\u0026#34;Hello cglib\u0026#34;); BeanDelegate delegate = (BeanDelegate) MethodDelegate.create(bean, \u0026#34;getValue\u0026#34;, BeanDelegate.class); assertEquals(\u0026#34;Hello cglib\u0026#34;, delegate.getValueFromDelegate()); } 关于 Method.create 的参数说明：\n第二个参数为即将被代理的方法 第一个参数必须是一个无参数构造的 bean 。因此 MethodDelegate.create 并不是你想象的那么有用 第三个参数为只含有一个方法的接口。当这个接口中的方法被调用的时候，将会调用第一个参数所指向 bean 的第二个参数方法 缺点：\n为每一个代理类创建了一个新的类，这样可能会占用大量的永久代堆内存 你不能代理需要参数的方法 如果你定义的接口中的方法需要参数，那么代理将不会工作，并且也不会抛出异常； 如果你的接口中方法需要其他的返回类型，那么将抛出 IllegalArgumentException 4.4.12 MulticastDelegate # 多重代理和方法代理差不多，都是将代理类方法的调用委托给被代理类。使用前提是需要一个接口，以及一个类实现了该接口 通过这种interface的继承关系，我们能够将接口上方法的调用分散给各个实现类上面去。 多重代理的缺点是接口只能含有一个方法，如果被代理的方法拥有返回值，那么调用代理类的返回值为最后一个添加的被代理类的方法返回值 public interface DelegatationProvider { void setValue(String value); } public class SimpleMulticastBean implements DelegatationProvider { private String value; @Override public void setValue(String value) { this.value = value; } public String getValue() { return value; } } 测试：\n@Test public void testMulticastDelegate() throws Exception{ MulticastDelegate multicastDelegate = MulticastDelegate.create(DelegatationProvider.class); SimpleMulticastBean first = new SimpleMulticastBean(); SimpleMulticastBean second = new SimpleMulticastBean(); multicastDelegate = multicastDelegate.add(first); multicastDelegate = multicastDelegate.add(second); DelegatationProvider provider = (DelegatationProvider) multicastDelegate; provider.setValue(\u0026#34;Hello world\u0026#34;); assertEquals(\u0026#34;Hello world\u0026#34;, first.getValue()); assertEquals(\u0026#34;Hello world\u0026#34;, second.getValue()); } 4.4.13 Constructor delegate # 为了对构造函数进行代理，我们需要一个接口，这个接口只含有一个 Object newInstance(…) 方法，用来调用相应的构造函数\ninterface SampleBeanConstructorDelegate{ Object newInstance(String value); } 测试：\n/** * 对构造函数进行代理 * @throws Exception */ @Test public void testConstructorDelegate() throws Exception{ SampleBeanConstructorDelegate constructorDelegate = (SampleBeanConstructorDelegate) ConstructorDelegate.create( SampleBean.class, SampleBeanConstructorDelegate.class); SampleBean bean = (SampleBean) constructorDelegate.newInstance(\u0026#34;Hello world\u0026#34;); assertTrue(SampleBean.class.isAssignableFrom(bean.getClass())); System.out.println(bean.getValue()); } 4.4.14 Parallel Sorter（并行排序器） # 能够对多个数组同时进行排序，目前实现的算法有归并排序和快速排序\n@Test public void testParallelSorter() throws Exception{ Integer[][] value = { {4, 3, 9, 0}, {2, 1, 6, 0} }; ParallelSorter.create(value).mergeSort(0); for(Integer[] row : value){ int former = -1; for(int val : row){ assertTrue(former \u0026lt; val); former = val; } } } 4.4.15 FastClass # 顾明思义，FastClass 就是对 Class 对象进行特定的处理，比如通过数组保存 method 引用，因此 FastClass 引出了一个 index 下标的新概念，比如 getIndex(String name, Class[] parameterTypes) 就是以前的获取method的方法。通过数组存储method,constructor等class信息，从而将原先的反射调用，转化为 class.index 的直接调用，从而体现所谓的 FastClass\n@Test public void testFastClass() throws Exception{ FastClass fastClass = FastClass.create(SampleBean.class); FastMethod fastMethod = fastClass.getMethod(\u0026#34;getValue\u0026#34;,new Class[0]); SampleBean bean = new SampleBean(); bean.setValue(\u0026#34;Hello world\u0026#34;); assertEquals(\u0026#34;Hello world\u0026#34;,fastMethod.invoke(bean, new Object[0])); } 4.5 注意 # 由于 CGLIB 的大部分类是直接对Java字节码进行操作，这样生成的类会在Java的永久堆中。如果动态代理操作过多，容易造成永久堆满，触发 OutOfMemory 异常。\n4.6 CGLIB和Java动态代理的区别 # Java动态代理只能够对接口进行代理，不能对普通的类进行代理（因为所有生成的代理类的父类为 Proxy，Java 类继承机制不允许多重继承）；CGLIB 能够代理普通类； Java动态代理使用Java原生的反射 API 进行操作，在生成类上比较高效；CGLIB 使用 ASM 框架直接对字节码进行操作，在类的执行过程中比较高 CGLIB相关的文章：\nhttp://jnb.ociweb.com/jnb/jnbNov2005.html http://www.iteye.com/topic/799827 http://mydailyjava.blogspot.kr/2013/11/cglib-missing-manual.html ","date":"1 March 2024","permalink":"/posts/language/java/java%E4%B8%AD%E7%9A%84%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90%E5%8C%85cglib/","section":"博客","summary":"CGLIB 是一个功能强大，高性能的代码生成包。它为没有实现接口的类提供代理，为 JDK 的动态代理提供了很好的补充。通常可以使用Java的动态代理创建代理，但当要代理的类没有实现接口或者为了更好的性能，CGLIB 是一个好的选择。","title":"JAVA 中的代码生成包 CGLIB （Code Generation Library）"},{"content":"","date":"1 March 2024","permalink":"/posts/architecture/high-concurrency/redis/redis-cache/","section":"博客","summary":"致力于提供一套简单易用的缓存框架，旨在为日常开发提供便捷的解决方案，并支持多级缓存开发。基于MVP开发策略，该框架提供了流畅的流式编程体验，使您能够轻松地享受编程的乐趣。框架支持缓存固定大小、自定义Map实现策略、缓存过期特性以及自定义驱逐策略，内置FIFO和LRU两种驱逐策略，同时提供了自定义删除监听器功能。我们还整合了日志框架，可以自适应常见日志系统，同时支持load初始化和persist持久化，包括RDB和AOF两种模式。","title":"从零实现redis-cache"},{"content":"代码仓库 # https://github.com/WFUing/redis-cache/tree/release_0.0.9\n背景知识——Redis 慢日志监控 # redis 中会存储慢操作的相关日志信息，主要是由两个参数构成：\nslowlog-log-slower-than 预设阈值,它的单位是毫秒(1秒=1000000微秒)默认值是10000 slowlog-max-len 最多存储多少条的慢日志记录 不过 redis 是直接存储到内存中，而且有长度限制。\n根据实际工作体验，如果我们可以添加慢日志的监听，然后有对应的存储或者报警，这样更加方便问题的分析和快速反馈。\n所以我们引入类似于删除的监听器。\n我们处理所有的 cache 操作，并且记录对应的操作耗时。 如果耗时操作用户设置的时间阈值，则调用慢操作监听器。 实现redis-cache的慢日志监控 # ","date":"1 March 2024","permalink":"/posts/architecture/high-concurrency/redis/redis-cache/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0redis6--%E6%85%A2%E6%97%A5%E5%BF%97%E7%9B%91%E6%8E%A7/","section":"博客","summary":"实现类似 guava-cache 中的 removeListener 删除监听器","title":"从零实现redis(6)- 慢日志监控"},{"content":" 链接：https://www.nowcoder.com/practice/f23604257af94d939848729b1a5cda08 难点 # 主要通过递归实现链表归并排序，有以下两个环节：\n分割 cut 环节： 找到当前链表中点，并从中点将链表断开（以便在下次递归 cut 时，链表片段拥有正确边界）； 使用 fast,slow 快慢双指针法，奇数个节点找到中点，偶数个节点找到中心左边的节点。 找到中点 slow 后，执行 slow.next = None 将链表切断。 递归分割时，输入当前链表左端点 head 和中心节点 slow 的下一个节点 tmp(因为链表是从 slow 切断的)。 cut 递归终止条件： 当head.next == None时，说明只有一个节点了，直接返回此节点 合并 merge 环节： 将两个排序链表合并，转化为一个排序链表。 双指针法合并，建立辅助ListNode h 作为头部。 设置两指针 left, right 分别指向两链表头部，比较两指针处节点值大小，由小到大加入合并链表头部，指针交替前进，直至添加完两个链表。 返回辅助ListNode h 作为头部的下个节点 h.next。 时间复杂度 O(l + r)，l, r 分别代表两个链表长度。 特殊情况，当题目输入的 head == None 时，直接返回None。 图解：\n代码 # import java.util.*; /* * public class ListNode { * int val; * ListNode next = null; * } */ public class Solution { /** * * @param head ListNode类 the head node * @return ListNode类 */ public ListNode sortInList (ListNode head) { // write code here if (head == null || head.next == null) return head; // 使用快慢指针寻找链表的中点 ListNode fast = head.next, slow = head; while (fast != null \u0026amp;\u0026amp; fast.next != null) { slow = slow.next; fast = fast.next.next; } ListNode tmp = slow.next; slow.next = null; // 递归左右两边进行排序 ListNode left = sortInList(head); ListNode right = sortInList(tmp); // 创建新的链表 ListNode h = new ListNode(0); ListNode res = h; // 合并 left right两个链表 while (left != null \u0026amp;\u0026amp; right != null) { // left right链表循环对比 if (left.val \u0026lt; right.val) { h.next = left; left = left.next; } else { h.next = right; right = right.next; } h = h.next; } // 最后添加未对比的链表部分判断左链表是否为空 h.next = left != null ? left : right; return res.next; } } ","date":"1 March 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E9%93%BE%E8%A1%A8/%E5%8D%95%E9%93%BE%E8%A1%A8%E7%9A%84%E6%8E%92%E5%BA%8F/","section":"博客","summary":"单链表的排序题解。链表的题目很早刷过，现在有些生疏了，再回顾一遍。","title":"单链表的排序"},{"content":" 链接：https://leetcode.cn/problems/russian-doll-envelopes/description/ 难点 # 第一个难点在于如何同时控制 $w_i$ 和 $h_i$ 两个维度，很自然地想到先把一个维度排序，然后只需要考虑另一个维度最长递增子序列就行了。\n这时候就有第二个难点了，如果在 $w_i$ 这个维度，存在两个值相同的情况怎么处理，需要将 $w_i$ 相同的 $h_i$ 从高到低排序，这样相同 $w_i$ 的部分就不会相互影响。\n做完了 $w_i$ 的内容，就只需要考虑 $h_i$ 的最长递增子序列就行了，这时候就是一个动态规划，还是三个问题，\n$dp[i]$ 是 $h$ 的前 $i$ 个元素可以组成的最长严格递增子序列的长度； 所有的 $dp[i]$ 初始化为 1； 状态转移方程：$dp[i]=max_{j\u0026lt;i \\land dp[j]\u0026lt;dp[i]} {dp[j]} + 1$ 。 代码 # class Solution { public int maxEnvelopes(int[][] envelopes) { Arrays.sort(envelopes, new Comparator\u0026lt;int[]\u0026gt;() { public int compare(int[] e1, int[] e2) { if(e1[0] == e2[0]) { return e2[1] - e1[1]; } return e1[0] - e2[0]; } }); int n = envelopes.length; int[] dp = new int[n]; Arrays.fill(dp, 1); int ans = 1; for(int i = 1; i \u0026lt; n; i++) { for(int j = 0; j \u0026lt; i; j++) { if(envelopes[i][1] \u0026gt; envelopes[j][1]) { dp[i] = Math.max(dp[i], dp[j]+1); } } ans = Math.max(dp[i], ans); } return ans; } } 本来以为这样就结束了，但是还是出了意外，报 TLE ，之前的 sort 的时间已经没办法优化了，就只能优化后面的动态规划了。\n二分优化 # 设 $f[j]$ 表示 $h$ 的前 $i$ 个元素可以组成的长度为 $j$ 的最长严格递增子序列的末尾元素的最小值，如果不存在长度为 $j$ 的最长严格递增子序列，对应的 $f$ 值无定义。在定义范围内，可以看出 $f$ 值是严格单调递增的，因为越长的子序列的末尾元素显然越大。\n考虑当前的 $h[i]$\n若 $h[i] \u0026gt; f_{last}$ ，那么直接在 f 最后放 $h[i]$ 就行， 否则，我们在 $f$ 中找出比 $h[i]$ 严格小的最大元素，把 $h[i]$ 放在它后面。 代码 # class Solution { public int maxEnvelopes(int[][] envelopes) { Arrays.sort(envelopes, new Comparator\u0026lt;int[]\u0026gt;() { public int compare(int[] e1, int[] e2) { if(e1[0] == e2[0]) { return e2[1] - e1[1]; } return e1[0] - e2[0]; } }); int n = envelopes.length; List\u0026lt;Integer\u0026gt; f = new ArrayList\u0026lt;Integer\u0026gt;(); f.add(envelopes[0][1]); for (int i = 1; i \u0026lt; n; ++i) { int num = envelopes[i][1]; if (num \u0026gt; f.get(f.size() - 1)) { f.add(num); } else { int index = binarySearch(f, num); f.set(index, num); } } return f.size(); } public int binarySearch(List\u0026lt;Integer\u0026gt; f, int target) { int low = 0, high = f.size() - 1; while (low \u0026lt; high) { int mid = (high - low) / 2 + low; if (f.get(mid) \u0026lt; target) { low = mid + 1; } else { high = mid; } } return low; } } ","date":"29 February 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/leetcode-354%E4%BF%84%E7%BD%97%E6%96%AF%E5%A5%97%E5%A8%83%E4%BF%A1%E5%B0%81%E9%97%AE%E9%A2%98/","section":"博客","summary":"【LeetCode 354】俄罗斯套娃信封问题题解。第一个难点在于如何同时控制 $w_i$ 和 $h_i$ 两个维度，很自然地想到先把一个维度排序，然后只需要考虑另一个维度\u003cstrong\u003e最长递增子序列\u003c/strong\u003e就行了。这时候就有第二个难点了，如果在 $w_i$ 这个维度，存在两个值相同的情况怎么处理，需要将 $w_i$ 相同的 $h_i$ 从高到低排序，这样相同 $w_i$ 的部分就不会相互影响。但是还是会TLE，具体如何请看题解。","title":"【LeetCode 354】俄罗斯套娃信封问题"},{"content":"代码仓库 # https://github.com/WFUing/redis-cache/tree/release_0.0.8 https://github.com/WFUing/redis-cache/tree/release_0.0.10 背景知识——Redis 持久化 # Redis 的读写操作都是在内存中，所以 Redis 性能才会高，但是当 Redis 重启后，内存中的数据就会丢失，那为了保证内存中的数据不会丢失，Redis 实现了数据持久化的机制，这个机制会把数据存储到磁盘，这样在 Redis 重启就能够从磁盘中恢复原有的数据。\nRedis 共有三种数据持久化的方式：\nAOF 日志：每执行一条写操作命令，就把该命令以追加的方式写入到一个文件里； RDB 快照：将某一时刻的内存数据，以二进制的方式写入磁盘； 混合持久化方式：Redis 4.0 新增的方式，集成了 AOF 和 RBD 的优点； AOF 日志是如何实现的？ # Redis 在执行完一条写操作命令后，就会把该命令以追加的方式写入到一个文件里，然后 Redis 重启时，会读取该文件记录的命令，然后逐一执行命令的方式来进行数据恢复。\nReids 是先执行写操作命令后，才将该命令记录到 AOF 日志里的，这么做其实有两个好处。\n避免额外的检查开销：因为如果先将写操作命令记录到 AOF 日志里，再执行该命令的话，如果当前的命令语法有问题，那么如果不进行命令语法检查，该错误的命令记录到 AOF 日志里后，Redis 在使用日志恢复数据时，就可能会出错。 不会阻塞当前写操作命令的执行：因为当写操作命令执行成功后，才会将命令记录到 AOF 日志。 当然，这样做也会带来风险：\n数据可能会丢失：执行写操作命令和记录日志是两个过程，那当 Redis 在还没来得及将命令写入到硬盘时，服务器发生宕机了，这个数据就会有丢失的风险。 可能阻塞其他操作：由于写操作命令执行成功后才记录到 AOF 日志，所以不会阻塞当前命令的执行，但因为 AOF 日志也是在主线程中执行，所以当 Redis 把日志文件写入磁盘的时候，还是会阻塞后续的操作无法执行。 Redis 写入 AOF 日志的过程\nRedis 执行完写操作命令后，会将命令追加到 server.aof_buf 缓冲区； 然后通过 write() 系统调用，将 aof_buf 缓冲区的数据写入到 AOF 文件，此时数据并没有写入到硬盘，而是拷贝到了内核缓冲区 page cache，等待内核将数据写入硬盘； 具体内核缓冲区的数据什么时候写入到硬盘，由内核决定。 AOF 的 3 种写回策略\nRedis 提供了 3 种写回硬盘的策略，控制的就是上面说的第三步的过程。 在 Redis.conf 配置文件中的 appendfsync 配置项可以有以下 3 种参数可填：\nAlways，这个单词的意思是「总是」，所以它的意思是每次写操作命令执行完后，同步将 AOF 日志数据写回硬盘； Everysec，这个单词的意思是「每秒」，所以它的意思是每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，然后每隔一秒将缓冲区里的内容写回到硬盘； No，意味着不由 Redis 控制写回硬盘的时机，转交给操作系统控制写回的时机，也就是每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，再由操作系统决定何时将缓冲区内容写回硬盘。 AOF 日志过大，会触发 AOF 重写机制\nAOF 日志是一个文件，随着执行的写操作命令越来越多，文件的大小会越来越大。 如果当 AOF 日志文件过大就会带来性能问题，比如重启 Redis 后，需要读 AOF 文件的内容以恢复数据，如果文件过大，整个恢复的过程就会很慢。\n所以，Redis 为了避免 AOF 文件越写越大，提供了 AOF 重写机制，当 AOF 文件的大小超过所设定的阈值后，Redis 就会启用 AOF 重写机制，来压缩 AOF 文件。\nAOF 重写机制是在重写时，读取当前数据库中的所有键值对，然后将每一个键值对用一条命令记录到「新的 AOF 文件」，等到全部记录完后，就将新的 AOF 文件替换掉现有的 AOF 文件。\n重写工作完成后，就会将新的 AOF 文件覆盖现有的 AOF 文件，这就相当于压缩了 AOF 文件，使得 AOF 文件体积变小了。\n重写 AOF 日志的过程\nRedis 的重写 AOF 过程是由后台子进程 bgrewriteaof 来完成的，这么做可以达到两个好处：\n子进程进行 AOF 重写期间，主进程可以继续处理命令请求，从而避免阻塞主进程； 子进程带有主进程的数据副本，这里使用子进程而不是线程，因为如果是使用线程，多线程之间会共享内存，那么在修改共享内存数据的时候，需要通过加锁来保证数据的安全，而这样就会降低性能。而使用子进程，创建子进程时，父子进程是共享内存数据的，不过这个共享的内存只能以只读的方式，而当父子进程任意一方修改了该共享内存，就会发生「写时复制」，于是父子进程就有了独立的数据副本，就不用加锁来保证数据安全。 触发重写机制后，主进程就会创建重写 AOF 的子进程，此时父子进程共享物理内存，重写子进程只会对这个内存进行只读，重写 AOF 子进程会读取数据库里的所有数据，并逐一把内存数据的键值对转换成一条命令，再将命令记录到重写日志（新的 AOF 文件）。\n但是重写过程中，主进程依然可以正常处理命令，那问题来了，重写 AOF 日志过程中，如果主进程修改了已经存在 key-value，那么会发生写时复制，此时这个 key-value 数据在子进程的内存数据就跟主进程的内存数据不一致了，这时要怎么办呢？\n为了解决这种数据不一致问题，Redis 设置了一个 AOF 重写缓冲区，这个缓冲区在创建 bgrewriteaof 子进程之后开始使用。\n在重写 AOF 期间，当 Redis 执行完一个写命令之后，它会同时将这个写命令写入到 「AOF 缓冲区」和 「AOF 重写缓冲区」。\n也就是说，在 bgrewriteaof 子进程执行 AOF 重写期间，主进程需要执行以下三个工作:\n执行客户端发来的命令； 将执行后的写命令追加到 「AOF 缓冲区」； 将执行后的写命令追加到 「AOF 重写缓冲区」； 当子进程完成 AOF 重写工作（扫描数据库中所有数据，逐一把内存数据的键值对转换成一条命令，再将命令记录到重写日志）后，会向主进程发送一条信号，信号是进程间通讯的一种方式，且是异步的。\n主进程收到该信号后，会调用一个信号处理函数，该函数主要做以下工作：\n将 AOF 重写缓冲区中的所有内容追加到新的 AOF 的文件中，使得新旧两个 AOF 文件所保存的数据库状态一致； 新的 AOF 的文件进行改名，覆盖现有的 AOF 文件。 信号函数执行完后，主进程就可以继续像往常一样处理命令了。\nRDB 快照是如何实现的呢？ # 因为 AOF 日志记录的是操作命令，不是实际的数据，所以用 AOF 方法做故障恢复时，需要全量把日志都执行一遍，一旦 AOF 日志非常多，势必会造成 Redis 的恢复操作缓慢。\n为了解决这个问题，Redis 增加了 RDB 快照。所谓的快照，就是记录某一个瞬间东西，比如当我们给风景拍照时，那一个瞬间的画面和信息就记录到了一张照片。\n所以，RDB 快照就是记录某一个瞬间的内存数据，记录的是实际数据，而 AOF 文件记录的是命令操作的日志，而不是实际的数据。\n因此在 Redis 恢复数据时， RDB 恢复数据的效率会比 AOF 高些，因为直接将 RDB 文件读入内存就可以，不需要像 AOF 那样还需要额外执行操作命令的步骤才能恢复数据。\nRedis 提供了两个命令来生成 RDB 文件，分别是 save 和 bgsave，他们的区别就在于是否在「主线程」里执行：\n执行了 save 命令，就会在主线程生成 RDB 文件，由于和执行操作命令在同一个线程，所以如果写入 RDB 文件的时间太长，会阻塞主线程； 执行了 bgsave 命令，会创建一个子进程来生成 RDB 文件，这样可以避免主线程的阻塞； Redis 还可以通过配置文件的选项来实现每隔一段时间自动执行一次 bgsave 命令，默认会提供以下配置：\nsave 900 1 save 300 10 save 60 10000 别看选项名叫 save，实际上执行的是 bgsave 命令，也就是会创建子进程来生成 RDB 快照文件。 只要满足上面条件的任意一个，就会执行 bgsave，它们的意思分别是：\n900 秒之内，对数据库进行了至少 1 次修改； 300 秒之内，对数据库进行了至少 10 次修改； 60 秒之内，对数据库进行了至少 10000 次修改。 这里提一点，Redis 的快照是全量快照，也就是说每次执行快照，都是把内存中的「所有数据」都记录到磁盘中。所以执行快照是一个比较重的操作，如果频率太频繁，可能会对 Redis 性能产生影响。如果频率太低，服务器故障时，丢失的数据会更多。\nRDB 在执行快照的时候，数据能修改\n可以的，执行 bgsave 过程中，Redis 依然可以继续处理操作命令的，也就是数据是能被修改的，关键的技术就在于写时复制技术（Copy-On-Write, COW）。\n执行 bgsave 命令的时候，会通过 fork() 创建子进程，此时子进程和父进程是共享同一片内存数据的，因为创建子进程的时候，会复制父进程的页表，但是页表指向的物理内存还是一个，此时如果主线程执行读操作，则主线程和 bgsave 子进程互相不影响。\n如果主线程执行写操作，则被修改的数据会复制一份副本，然后 bgsave 子进程会把该副本数据写入 RDB 文件，在这个过程中，主线程仍然可以直接修改原来的数据。\n混合持久化 # RDB 优点是数据恢复速度快，但是快照的频率不好把握。频率太低，丢失的数据就会比较多，频率太高，就会影响性能。\nAOF 优点是丢失数据少，但是数据恢复不快。\n为了集成了两者的优点， Redis 4.0 提出了混合使用 AOF 日志和内存快照，也叫混合持久化，既保证了 Redis 重启速度，又降低数据丢失风险。\n混合持久化工作在 AOF 日志重写过程，当开启了混合持久化时，在 AOF 重写日志时，fork 出来的重写子进程会先将与主线程共享的内存数据以 RDB 方式写入到 AOF 文件，然后主线程处理的操作命令会被记录在重写缓冲区里，重写缓冲区里的增量命令会以 AOF 方式写入到 AOF 文件，写入完成后通知主进程将新的含有 RDB 格式和 AOF 格式的 AOF 文件替换旧的的 AOF 文件。\n也就是说，使用了混合持久化，AOF 文件的前半部分是 RDB 格式的全量数据，后半部分是 AOF 格式的增量数据。\n这样的好处在于，重启 Redis 加载数据的时候，由于前半部分是 RDB 内容，这样加载的时候速度会很快。\n加载完 RDB 的内容后，才会加载后半部分的 AOF 内容，这里的内容是 Redis 后台子进程重写 AOF 期间，主线程处理的操作命令，可以使得数据更少的丢失。\n混合持久化优点：\n混合持久化结合了 RDB 和 AOF 持久化的优点，开头为 RDB 的格式，使得 Redis 可以更快的启动，同时结合 AOF 的优点，有减低了大量数据丢失的风险。\n混合持久化缺点：\nAOF 文件中添加了 RDB 格式的内容，使得 AOF 文件的可读性变得很差； 兼容性差，如果开启混合持久化，那么此混合持久化 AOF 文件，就不能用在 Redis 4.0 之前版本了。 实现redis-cache的监听器 # ","date":"29 February 2024","permalink":"/posts/architecture/high-concurrency/redis/redis-cache/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0redis5--%E6%8C%81%E4%B9%85%E5%8C%96/","section":"博客","summary":"实现类似 guava-cache 中的 removeListener 删除监听器","title":"从零实现redis(5)- 持久化"},{"content":"代码仓库 # https://github.com/WFUing/redis-cache/tree/release_0.0.6\n背景知识——guava-cache的删除监听器 # 我们在两种场景下删除数据是对用户透明的：\nsize 满了之后，进行数据淘汰。 expire 过期时，清除数据。 这两个特性对用户本来应该是无感的，不过用户如果关心的话，也可以通过添加删除监听器来获取到相关的变更信息。\n具体实现\n为了实现删除的监听，我们需要找到删除的位置，然后调用监听器即可。\nevict 驱除的场景：每次 put 数据时，都会校验 size 是否达到最大的限制，如果达到，则进行 evict 淘汰。 expire 过期的场景 用户指定 expire 时间之后，回后台异步执行刷新。\n也存在惰性删除的场景。\n实现redis-cache的监听器 # ","date":"29 February 2024","permalink":"/posts/architecture/high-concurrency/redis/redis-cache/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0redis4--%E7%9B%91%E5%90%AC%E5%99%A8listerner-/","section":"博客","summary":"实现类似 guava-cache 中的 removeListener 删除监听器","title":"从零实现redis(4)- 监听器（Listerner） "},{"content":"代码仓库 # https://github.com/WFUing/redis-cache/tree/release_0.0.5\n背景知识 # JAVA 中的代码生成包 # CGLIB （Code Generation Library） 其中，MethodInterceptor 是CGLIB提供的一个接口，用于拦截代理对象上的方法调用。intercept方法是 MethodInterceptor 接口的一部分，当代理对象上的方法被调用时，这个方法会被触发。\n拦截器（Interceptor） # 拦截器（Interceptor）是一种常用的设计模式，用于在程序的执行过程中，插入额外的操作，而不改变原有逻辑的结构。\nInterceptor（拦截器）是用于实现AOP（面向切面编程）的工具之一，常用于对系统中的某些操作进行拦截并在其之前或之后加入某些处理，例如性能监控、日志记录、安全检查、事务处理、权限检查等。\n注解（Annotations） # 在Java中，注解（Annotations）是一种用于在代码中添加元数据（即关于数据的数据）的方式。Java注解可以用于类、方法、变量、参数等，提供了一种结构化的方法来描述这些元素的行为或特性。注解不直接影响程序的操作，但它们可以被编译器或运行时通过反射机制读取和处理。\n注解的定义需要使用 @interface 关键字，后跟注解的名称。在定义注解时，可以使用元注解来指定注解的一些属性和行为。\n元注解\n@Target：指定注解可以应用的Java元素类型（如类、方法、字段等）。例如，@Target(ElementType.METHOD)意味着注解只能用于方法上。 @Retention：指定注解在哪一个级别可用，分为源代码（SOURCE）、类文件（CLASS）和运行时（RUNTIME）。例如，@Retention(RetentionPolicy.RUNTIME) 意味着注解在运行时仍然可用，这允许通过反射读取注解。 @Documented：指定注解是否将包含在JavaDoc中。 @Inherited：指定注解是否可以被子类继承。 import java.lang.annotation.Documented; import java.lang.annotation.ElementType; import java.lang.annotation.Retention; import java.lang.annotation.RetentionPolicy; import java.lang.annotation.Target; import java.lang.annotation.Inherited; @Documented @Target(ElementType.METHOD) // 应用于方法 @Retention(RetentionPolicy.RUNTIME) // 在运行时可用 @Inherited // 可以被子类继承 public @interface CacheInterceptor { // 定义属性 boolean enabled() default true; // 默认启用 } 实现redis-cache的拦截器 # ","date":"29 February 2024","permalink":"/posts/architecture/high-concurrency/redis/redis-cache/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0redis3--%E6%8B%A6%E6%88%AA%E5%99%A8interceptor/","section":"博客","summary":"Interceptor（拦截器）是用于实现AOP（面向切面编程）的工具之一，常用于对系统中的某些操作进行拦截并在其之前或之后加入某些处理，例如性能监控、日志记录、安全检查、事务处理、权限检查等。","title":"从零实现redis(3)- 拦截器（Interceptor）"},{"content":" 链接：https://leetcode.cn/problems/burst-balloons/description/ 难点 # 本题的难点在于如何控制气球的相邻信息，每一次戳气球都会产生新的相邻的气球，但是很容易有一种做题的直觉，\n先判断一个气球， 然后继续判断其左边和右边。 怎么判断呢？可以倒过来看这些操作，将全过程看作是每次添加一个气球。我们定义方法 solve ，令 solve(i, j) 表示将开区间 (i, j) 内的位置全部填满气球能够得到的最多硬币数。由于是开区间，因此区间两端的气球的编号就是 i 和 j，对应着 val[i] 和 val[j]。\nval 为加上两端两个1的 nums。\n代码 # 很容易得到版本一的代码。\nclass Solution { int[] val = new int[302]; public int maxCoins(int[] nums) { int n = nums.length; val[0] = 1; val[n+1] = 1; for(int i = 1; i \u0026lt;= n; i++) { val[i] = nums[i-1]; } return solve(0, n+1); } public int solve(int left, int right) { int maxx = 0; for(int i = left+1; i \u0026lt; right; i++) { maxx = Math.max(maxx, val[i] * val[left] * val[right] + temp(left, i) + temp(i, right)); } return maxx; } } 但是报错，TLE。于是加上记忆化搜索。\nclass Solution { public int[][] rec; public int[] val; public int maxCoins(int[] nums) { int n = nums.length; val = new int[n + 2]; for (int i = 1; i \u0026lt;= n; i++) { val[i] = nums[i - 1]; } val[0] = val[n + 1] = 1; rec = new int[n + 2][n + 2]; for (int i = 0; i \u0026lt;= n + 1; i++) { Arrays.fill(rec[i], -1); } return solve(0, n + 1); } public int solve(int left, int right) { if (left \u0026gt;= right - 1) { return 0; } if (rec[left][right] != -1) { return rec[left][right]; } for (int i = left + 1; i \u0026lt; right; i++) { int sum = val[left] * val[i] * val[right]; sum += solve(left, i) + solve(i, right); rec[left][right] = Math.max(rec[left][right], sum); } return rec[left][right]; } } 按照方法一的思路，我们发现我们可以通过变换计算顺序，从「自顶向下」的记忆化搜索变为「自底向上」的动态规划。\nclass Solution { public int maxCoins(int[] nums) { int n = nums.length; int[][] rec = new int[n + 2][n + 2]; int[] val = new int[n + 2]; val[0] = val[n + 1] = 1; for (int i = 1; i \u0026lt;= n; i++) { val[i] = nums[i - 1]; } for (int i = n - 1; i \u0026gt;= 0; i--) { for (int j = i + 2; j \u0026lt;= n + 1; j++) { for (int k = i + 1; k \u0026lt; j; k++) { int sum = val[i] * val[k] * val[j]; sum += rec[i][k] + rec[k][j]; rec[i][j] = Math.max(rec[i][j], sum); } } } return rec[0][n + 1]; } } ","date":"28 February 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/leetcode-312%E6%88%B3%E6%B0%94%E7%90%83/","section":"博客","summary":"【LeetCode 312】戳气球题解。本题的难点在于如何控制气球的相邻信息，每一次戳气球都会产生新的相邻的气球，但是很容易有一种做题的直觉，先判断一个气球，然后继续判断其左边和右边。怎么判断呢？可以倒过来看这些操作，将全过程看作是每次添加一个气球。我们定义方法 \u003ccode\u003esolve\u003c/code\u003e ，令 \u003ccode\u003esolve(i, j)\u003c/code\u003e 表示将开区间 \u003ccode\u003e(i, j)\u003c/code\u003e 内的位置全部填满气球能够得到的最多硬币数。由于是开区间，因此区间两端的气球的编号就是 \u003ccode\u003ei\u003c/code\u003e 和 \u003ccode\u003ej\u003c/code\u003e，对应着 \u003ccode\u003eval[i]\u003c/code\u003e 和 \u003ccode\u003eval[j]\u003c/code\u003e。","title":"【LeetCode 312】戳气球"},{"content":" 链接：https://leetcode.cn/problems/house-robber-iii/description/ 难点 # 很自然地想到是树状dp，分成两种情况，一种包含当前节点，另一种不包含。难点在于不包含的情况，不仅仅有左右孩子包含的情况，还有不包含的情况。\n解析 # 代码 # /** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode() {} * TreeNode(int val) { this.val = val; } * TreeNode(int val, TreeNode left, TreeNode right) { * this.val = val; * this.left = left; * this.right = right; * } * } */ class Solution { Map\u0026lt;TreeNode, Integer\u0026gt; map = new HashMap\u0026lt;TreeNode, Integer\u0026gt;(); public int rob(TreeNode root) { // 递归，传递状态而不是记录 int[] res = robTree(root); return Math.max(res[0], res[1]); // 递归，记录状态 // if(root == null) return 0; // if(root.left == null \u0026amp;\u0026amp; root.right == null) return root.val; // if(map.containsKey(root)) return map.get(root); // // rob root // int val1 = root.val; // if(root.left != null) val1 += (rob(root.left.left) + rob(root.left.right)); // if(root.right != null) val1 += (rob(root.right.left) + rob(root.right.right)); // // not rob root // int val2 = 0; // val2 += (rob(root.left) + rob(root.right)); // map.put(root, Math.max(val1, val2)); // return Math.max(val1, val2); } public int[] robTree(TreeNode node) { int[] vals = new int[2]; if(node == null) return vals; int[] l = robTree(node.left); int[] r = robTree(node.right); // 0 表示不偷 node vals[0] = Math.max(l[0], l[1]) + Math.max(r[0], r[1]); // 1 表示偷 node vals[1] = node.val + l[0] + r[0]; return vals; } } ","date":"28 February 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/leetcode-337%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8Diii/","section":"博客","summary":"【LeetCode 337】打家劫舍III题解。很自然地想到是树状dp，分成两种情况，一种包含当前节点，另一种不包含。难点在于不包含的情况，不仅仅有左右孩子包含的情况，还有不包含的情况。","title":"【LeetCode 337】打家劫舍III"},{"content":" 链接：https://leetcode.cn/problems/best-time-to-buy-and-sell-stock-iv/description/ 难点 # 以前做过类似的题目，因为是两个状态之间的变换，很自然地想到有两个dp数组来存储。\n用 $in[i][j]$ 表示对于数组 $prices[0\u0026hellip;i]$ 中的价格而言，进行恰好 $j$ 笔交易，并且当前手上持有一支股票，这种情况下的最大利润；用 $out[i][j]$ 表示恰好进行 $j$ 笔交易，并且当前手上不持有股票，这种情况下的最大利润。 本题的难点有三个方面：\n关于 $in$ 和 $out$ 数组中 $j$ 的关系，在这里，买入卖出这一整套流程才算一次交易。那么，在卖出的那一刻，1次交易就完成了。那第一次买入，其实是第 0 次交易，第一次卖出，是第 1 次交易结束。 如何初始化，首先第一天只能买进，不能卖出，而且最多只能有一次交易，那么 $in[0][1\u0026hellip;k]$ 和 $out[0][1\u0026hellip;k]$ 如何初始化。 如何更新 $in$ 和 $out$ 数组。 解析 # 代码 # class Solution { public int maxProfit(int k, int[] prices) { int n = prices.length; int[][] in = new int[n][k+1]; int[][] out = new int[n][k+1]; in[0][0] = -prices[0]; for(int j = 1; j \u0026lt; k; j++ ) { in[0][j] = out[0][j] = -1000; } for(int i = 1; i \u0026lt; n; i++) { in[i][0] = Math.max(in[i-1][0], -prices[i]); for(int j = 1; j \u0026lt;=k ; j++) { in[i][j] = Math.max(in[i-1][j], out[i-1][j] - prices[i]); out[i][j] = Math.max(out[i-1][j], in[i-1][j-1] + prices[i]); } } int maxx = -1; for(int j = 0; j \u0026lt;= k; j++) { maxx = Math.max(maxx, out[n-1][j]); } return maxx; } } ","date":"28 February 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/leetcode-188%E4%B9%B0%E5%8D%96%E8%82%A1%E7%A5%A8%E7%9A%84%E6%9C%80%E4%BD%B3%E6%97%B6%E6%9C%BAiv/","section":"博客","summary":"【LeetCode 188】买卖股票的最佳时机IV题解。本题的难点有三个方面：1️⃣关于 $in$ 和 $out$ 数组中 $j$ 的关系，在这里，\u003ccode\u003e买入卖出\u003c/code\u003e这一整套流程才算一次交易。那么，在卖出的那一刻，1次交易就完成了。那第一次买入，其实是第 0 次交易，第一次卖出，是第 1 次交易结束。2️⃣如何初始化，首先第一天只能买进，不能卖出，而且最多只能有一次交易，那么 $in[0][1\u0026hellip;k]$ 和 $out[0][1\u0026hellip;k]$ 如何初始化。3️⃣如何更新 $in$ 和 $out$ 数组。","title":"【LeetCode 188】买卖股票的最佳时机IV"},{"content":" 链接：https://www.nowcoder.com/practice/650474f313294468a4ded3ce0f7898b9 升级版-链表中环的入口结点：https://www.nowcoder.com/practice/253d2c59ec3e4bc68da16833f79a38e4 链表中倒数最后k个结点：https://www.nowcoder.com/practice/886370fe658f41b498d40fb34ae76ff9 难点 # 链表的题目很早刷过，现在有些生疏了，再回顾一遍。\n代码 # 解法一\nimport java.util.*; /** * Definition for singly-linked list. * class ListNode { * int val; * ListNode next; * ListNode(int x) { * val = x; * next = null; * } * } */ public class Solution { public boolean hasCycle(ListNode head) { ListNode fast = head, slow = head; while (fast != null \u0026amp;\u0026amp; slow != null) { slow = slow.next; if (fast.next != null) { fast = fast.next.next; } else { return false; } if (slow == fast) { return true; } } return false; } } 解法二\npublic class Solution { public boolean hasCycle(ListNode head) { ListNode pos = head; // 哈希表记录访问过的结点 Set\u0026lt;ListNode\u0026gt; visited = new HashSet\u0026lt;ListNode\u0026gt;(); while (pos != null) { // 判断结点是否被访问 if (visited.contains(pos)) { return true; } else { // 结点记录添加到哈希表中 visited.add(pos); } // 遍历 pos = pos.next; } return false; } } ","date":"28 February 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E9%93%BE%E8%A1%A8/%E5%88%A4%E6%96%AD%E9%93%BE%E8%A1%A8%E4%B8%AD%E6%98%AF%E5%90%A6%E6%9C%89%E7%8E%AF/","section":"博客","summary":"判断链表中是否有环题解。链表的题目很早刷过，现在有些生疏了，再回顾一遍。","title":"判断链表中是否有环"},{"content":"代码仓库 # https://github.com/WFUing/redis-cache/tree/release_0.0.3\n背景知识——过期删除策略 # Redis 是可以对 key 设置过期时间的，因此需要有相应的机制将已过期的键值对删除，而做这个工作的就是过期键值删除策略。\n如何设置过期时间？ # 先说一下对 key 设置过期时间的命令。 设置 key 过期时间的命令一共有 4 个：\nexpire \u0026lt;key\u0026gt; \u0026lt;n\u0026gt;：设置 key 在 n 秒后过期，比如 expire key 100 表示设置 key 在 100 秒后过期； pexpire \u0026lt;key\u0026gt; \u0026lt;n\u0026gt;：设置 key 在 n 毫秒后过期，比如 pexpire key2 100000 表示设置 key2 在 100000 毫秒（100 秒）后过期。 expireat \u0026lt;key\u0026gt; \u0026lt;n\u0026gt;：设置 key 在某个时间戳（精确到秒）之后过期，比如 expireat key3 1655654400 表示 key3 在时间戳 1655654400 后过期（精确到秒）； pexpireat \u0026lt;key\u0026gt; \u0026lt;n\u0026gt;：设置 key 在某个时间戳（精确到毫秒）之后过期，比如 pexpireat key4 1655654400000 表示 key4 在时间戳 1655654400000 后过期（精确到毫秒）\n当然，在设置字符串时，也可以同时对 key 设置过期时间，共有 3 种命令：\nset \u0026lt;key\u0026gt; \u0026lt;value\u0026gt; ex \u0026lt;n\u0026gt; ：设置键值对的时候，同时指定过期时间（精确到秒）； set \u0026lt;key\u0026gt; \u0026lt;value\u0026gt; px \u0026lt;n\u0026gt; ：设置键值对的时候，同时指定过期时间（精确到毫秒）； setex \u0026lt;key\u0026gt; \u0026lt;n\u0026gt; \u0026lt;valule\u0026gt; ：设置键值对的时候，同时指定过期时间（精确到秒）。\n如果你想查看某个 key 剩余的存活时间，可以使用 TTL \u0026lt;key\u0026gt; 命令。\n# 设置键值对的时候，同时指定过期时间位 60 秒 \u0026gt; setex key1 60 value1 OK # 查看 key1 过期时间还剩多少 \u0026gt; ttl key1 (integer) 56 \u0026gt; ttl key1 (integer) 52 如果突然反悔，取消 key 的过期时间，则可以使用 PERSIST \u0026lt;key\u0026gt; 命令。\n# 取消 key1 的过期时间 \u0026gt; persist key1 (integer) 1 # 使用完 persist 命令之后， # 查下 key1 的存活时间结果是 -1，表明 key1 永不过期 \u0026gt; ttl key1 (integer) -1 如何判定 key 已过期了？ # 每当我们对一个 key 设置了过期时间时，Redis 会把该 key 带上过期时间存储到一个**过期字典（expires dict）**中，也就是说「过期字典」保存了数据库中所有 key 的过期时间。\n过期字典存储在 redisDb 结构中，如下：\ntypedef struct redisDb { dict *dict; /* 数据库键空间，存放着所有的键值对 */ dict *expires; /* 键的过期时间 */ .... } redisDb; 过期字典数据结构结构如下：\n过期字典的 key 是一个指针，指向某个键对象； 过期字典的 value 是一个 long long 类型的整数，这个整数保存了 key 的过期时间； 过期字典的数据结构如下图所示：\n字典实际上是哈希表，哈希表的最大好处就是让我们可以用 O(1) 的时间复杂度来快速查找。当我们查询一个 key 时，Redis 首先检查该 key 是否存在于过期字典中：\n如果不在，则正常读取键值； 如果存在，则会获取该 key 的过期时间，然后与当前系统时间进行比对，如果比系统时间大，那就没有过期，否则判定该 key 已过期。 过期键判断流程如下图所示：\n过期删除策略有哪些？ # 在说 Redis 过期删除策略之前，先跟大家介绍下，常见的三种过期删除策略：\n定时删除； 惰性删除； 定期删除； 定时删除策略\n定时删除策略的做法是，在设置 key 的过期时间时，同时创建一个定时事件，当时间到达时，由事件处理器自动执行 key 的删除操作。\n定时删除策略的优点：可以保证过期 key 会被尽快删除，也就是内存可以被尽快地释放。因此，定时删除对内存是最友好的。 定时删除策略的缺点：在过期 key 比较多的情况下，删除过期 key 可能会占用相当一部分 CPU 时间，在内存不紧张但 CPU 时间紧张的情况下，将 CPU 时间用于删除和当前任务无关的过期键上，无疑会对服务器的响应时间和吞吐量造成影响。所以，定时删除策略对 CPU 不友好。 惰性删除策略\n惰性删除策略的做法是，不主动删除过期键，每次从数据库访问 key 时，都检测 key 是否过期，如果过期则删除该 key。\n惰性删除策略的优点：因为每次访问时，才会检查 key 是否过期，所以此策略只会使用很少的系统资源，因此，惰性删除策略对 CPU 时间最友好。 惰性删除策略的缺点：如果一个 key 已经过期，而这个 key 又仍然保留在数据库中，那么只要这个过期 key 一直没有被访问，它所占用的内存就不会释放，造成了一定的内存空间浪费。所以，惰性删除策略对内存不友好。 定期删除策略\n定期删除策略的做法是，每隔一段时间「随机」从数据库中取出一定数量的 key 进行检查，并删除其中的过期key。\n定期删除策略的优点：通过限制删除操作执行的时长和频率，来减少删除操作对 CPU 的影响，同时也能删除一部分过期的数据减少了过期键对空间的无效占用。 定期删除策略的缺点： 内存清理方面没有定时删除效果好，同时没有惰性删除使用的系统资源少。 难以确定删除操作执行的时长和频率。如果执行的太频繁，定期删除策略变得和定时删除策略一样，对CPU不友好；如果执行的太少，那又和惰性删除一样了，过期 key 占用的内存不会及时得到释放。 Redis 过期删除策略是什么？ # 前面介绍了三种过期删除策略，每一种都有优缺点，仅使用某一个策略都不能满足实际需求。\n所以， Redis 选择「惰性删除+定期删除」这两种策略配和使用，以求在合理使用 CPU 时间和避免内存浪费之间取得平衡。\nRedis 的惰性删除策略由 db.c 文件中的 expireIfNeeded 函数实现，代码如下：\nint expireIfNeeded(redisDb *db, robj *key) { // 判断 key 是否过期 if (!keyIsExpired(db,key)) return 0; .... /* 删除过期键 */ .... // 如果 server.lazyfree_lazy_expire 为 1 表示异步删除，反之同步删除； return server.lazyfree_lazy_expire ? dbAsyncDelete(db,key) : dbSyncDelete(db,key); } Redis 在访问或者修改 key 之前，都会调用 expireIfNeeded 函数对其进行检查，检查 key 是否过期：\n如果过期，则删除该 key，至于选择异步删除，还是选择同步删除，根据 lazyfree_lazy_expire 参数配置决定（Redis 4.0版本开始提供参数），然后返回 null 客户端； 如果没有过期，不做任何处理，然后返回正常的键值对给客户端； 惰性删除的流程图如下：\nRedis 实现定期删除\n再回忆一下，定期删除策略的做法：每隔一段时间「随机」从数据库中取出一定数量的 key 进行检查，并删除其中的过期key。\n1、这个间隔检查的时间是多长呢？\n在 Redis 中，默认每秒进行 10 次过期检查一次数据库，此配置可通过 Redis 的配置文件 redis.conf 进行配置，配置键为 hz 它的默认值是 hz 10。\n特别强调下，每次检查数据库并不是遍历过期字典中的所有 key，而是从数据库中随机抽取一定数量的 key 进行过期检查。\n2、随机抽查的数量是多少呢？\n我查了下源码，定期删除的实现在 expire.c 文件下的 activeExpireCycle 函数中，其中随机抽查的数量由 ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP 定义的，它是写死在代码中的，数值是 20。\n也就是说，数据库每轮抽查时，会随机选择 20 个 key 判断是否过期。\n接下来，详细说说 Redis 的定期删除的流程：\n从过期字典中随机抽取 20 个 key； 检查这 20 个 key 是否过期，并删除已过期的 key； 如果本轮检查的已过期 key 的数量，超过 5 个（20/4），也就是「已过期 key 的数量」占比「随机抽取 key 的数量」大于 25%，则继续重复步骤 1；如果已过期的 key 比例小于 25%，则停止继续删除过期 key，然后等待下一轮再检查。 可以看到，定期删除是一个循环的流程。\n那 Redis 为了保证定期删除不会出现循环过度，导致线程卡死现象，为此增加了定期删除循环流程的时间上限，默认不会超过 25ms。\n针对定期删除的流程，我写了个伪代码：\ndo { //已过期的数量 expired = 0； //随机抽取的数量 num = 20; while (num--) { //1. 从过期字典中随机抽取 1 个 key //2. 判断该 key 是否过期，如果已过期则进行删除，同时对 expired++ } // 超过时间限制则退出 if (timelimit_exit) return; /* 如果本轮检查的已过期 key 的数量，超过 25%，则继续随机抽查，否则退出本轮检查 */ } while (expired \u0026gt; 20/4); 实现redis-cache的过期删除策略 # ","date":"28 February 2024","permalink":"/posts/architecture/high-concurrency/redis/redis-cache/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0redis2--expire-%E8%BF%87%E6%9C%9F%E5%8E%9F%E7%90%86/","section":"博客","summary":"Redis 是可以对 key 设置过期时间的，因此需要有相应的机制将已过期的键值对删除，而做这个工作的就是过期键值删除策略。","title":"从零实现redis(2)- expire 过期原理"},{"content":"代码仓库 # https://github.com/WFUing/redis-cache/tree/release_0.0.2\n背景知识——驱除策略 # redis 的驱除策略，也称为内存淘汰策略。当 Redis 的运行内存已经超过 Redis 设置的最大内存之后，则会使用内存淘汰策略删除符合条件的 key，以此来保障 Redis 高效的运行。\n如何设置 Redis 最大运行内存？ # 在配置文件 redis.conf 中，可以通过参数 maxmemory \u0026lt;bytes\u0026gt; 来设定最大运行内存，只有在 Redis 的运行内存达到了我们设置的最大运行内存，才会触发内存淘汰策略。 不同位数的操作系统，maxmemory 的默认值是不同的：\n在 64 位操作系统中，maxmemory 的默认值是 0，表示没有内存大小限制，那么不管用户存放多少数据到 Redis 中，Redis 也不会对可用内存进行检查，直到 Redis 实例因内存不足而崩溃也无作为。 在 32 位操作系统中，maxmemory 的默认值是 3G，因为 32 位的机器最大只支持 4GB 的内存，而系统本身就需要一定的内存资源来支持运行，所以 32 位操作系统限制最大 3 GB 的可用内存是非常合理的，这样可以避免因为内存不足而导致 Redis 实例崩溃。 Redis 内存淘汰策略有哪些？ # Redis 内存淘汰策略共有八种，这八种策略大体分为「不进行数据淘汰」和「进行数据淘汰」两类策略。\n1、不进行数据淘汰的策略 # noeviction（Redis3.0之后，默认的内存淘汰策略） ：它表示当运行内存超过最大设置内存时，不淘汰任何数据，这时如果有新的数据写入，会报错通知禁止写入，不淘汰任何数据，但是如果没用数据写入的话，只是单纯的查询或者删除操作的话，还是可以正常工作。\n2、进行数据淘汰的策略 # 针对「进行数据淘汰」这一类策略，又可以细分为「在设置了过期时间的数据中进行淘汰」和「在所有数据范围内进行淘汰」这两类策略。\n在设置了过期时间的数据中进行淘汰：\nvolatile-random：随机淘汰设置了过期时间的任意键值； volatile-ttl：优先淘汰更早过期的键值。 volatile-lru（Redis3.0 之前，默认的内存淘汰策略）：淘汰所有设置了过期时间的键值中，最久未使用的键值； volatile-lfu（Redis 4.0 后新增的内存淘汰策略）：淘汰所有设置了过期时间的键值中，最少使用的键值； 在所有数据范围内进行淘汰：\nallkeys-random：随机淘汰任意键值； allkeys-lru：淘汰整个键值中最久未使用的键值； allkeys-lfu（Redis 4.0 后新增的内存淘汰策略）：淘汰整个键值中最少使用的键值。 如何查看当前 Redis 使用的内存淘汰策略？\n可以使用 config get maxmemory-policy 命令，来查看当前 Redis 的内存淘汰策略，命令如下：\n127.0.0.1:6379\u0026gt; config get maxmemory-policy 1) \u0026#34;maxmemory-policy\u0026#34; 2) \u0026#34;noeviction\u0026#34; 可以看出，当前 Redis 使用的是 noeviction 类型的内存淘汰策略，它是 Redis 3.0 之后默认使用的内存淘汰策略，表示当运行内存超过最大设置内存时，不淘汰任何数据，但新增操作会报错。\n如何修改 Redis 内存淘汰策略？\n设置内存淘汰策略有两种方法：\n方式一：通过 config set maxmemory-policy \u0026lt;策略\u0026gt; 命令设置。它的优点是设置之后立即生效，不需要重启 Redis 服务，缺点是重启 Redis 之后，设置就会失效。 方式二：通过修改 Redis 配置文件修改，设置 maxmemory-policy \u0026lt;策略\u0026gt; ，它的优点是重启 Redis 服务后配置不会丢失，缺点是必须重启 Redis 服务，设置才能生效。 具体算法 # FIFO 算法 # 先进先出，在这种淘汰算法中，先进入缓存的会先被淘汰。这种可谓是最简单的了，但是会导致我们命中率很低。\n试想一下我们如果有个访问频率很高的数据是所有数据第一个访问的，而那些不是很高的是后面再访问的，那这样就会把我们的首个数据但是他的访问频率很高给挤出。\nLRU 算法 # LRU 全称是 Least Recently Used 翻译为最近最少使用，会选择淘汰最近最少使用的数据。\n传统 LRU 算法的实现是基于「链表」结构，链表中的元素按照操作顺序从前往后排列，最新操作的键会被移动到表头，当需要内存淘汰时，只需要删除链表尾部的元素即可，因为链表尾部的元素就代表最久未被使用的元素。\nRedis 并没有使用这样的方式实现 LRU 算法，因为传统的 LRU 算法存在两个问题：\n需要用链表管理所有的缓存数据，这会带来额外的空间开销； 当有数据被访问时，需要在链表上把该数据移动到头端，如果有大量数据被访问，就会带来很多链表移动操作，会很耗时，进而会降低 Redis 缓存性能。 Redis 实现 LRU 算法的方法\nRedis 实现的是一种近似 LRU 算法，目的是为了更好的节约内存，它的实现方式是在 Redis 的对象结构体中添加一个额外的字段，用于记录此数据的最后一次访问时间。\n当 Redis 进行内存淘汰时，会使用随机采样的方式来淘汰数据，它是随机取 5 个值（此值可配置），然后淘汰最久没有使用的那个。\nRedis 实现的 LRU 算法的优点：\n不用为所有的数据维护一个大链表，节省了空间占用； 不用在每次数据访问时都移动链表项，提升了缓存的性能； 但是 LRU 算法有两个问题：\n无法解决缓存污染问题，比如应用一次读取了大量的数据，而这些数据只会被读取这一次，那么这些数据会留存在 Redis 缓存中很长一段时间，造成缓存污染。 热点数据被淘汰问题，比如有个数据在1个小时的前59分钟访问了1万次(可见这是个热点数据)，再后一分钟没有访问这个数据，但是有其他的数据访问，就导致热点数据被淘汰。 因此，在 Redis 4.0 之后引入了 LFU 算法来解决这个问题。\nLFU 算法 # LFU 全称是 Least Frequently Used 翻译为最近最不常用，LFU 算法是根据数据访问次数来淘汰数据的，它的核心思想是如果数据过去被访问多次，那么将来被访问的频率也更高。\n所以， LFU 算法会记录每个数据的访问次数。当一个数据被再次访问时，就会增加该数据的访问次数。这样就解决了偶尔被访问一次之后，数据留存在缓存中很长一段时间的问题，相比于 LRU 算法也更合理一些。\nLFU 算法相比于 LRU 算法的实现，多记录了「数据的访问频次」的信息。Redis 对象的结构如下：\ntypedef struct redisObject { ... // 24 bits，用于记录对象的访问信息 unsigned lru:24; ... } robj; Redis 对象头中的 lru 字段，在 LRU 算法下和 LFU 算法下使用方式并不相同。\n在 LRU 算法中，Redis 对象头的 24 bits 的 lru 字段是用来记录 key 的访问时间戳，因此在 LRU 模式下，Redis可以根据对象头中的 lru 字段记录的值，来比较最后一次 key 的访问时间长，从而淘汰最久未被使用的 key。\n在 LFU 算法中，Redis对象头的 24 bits 的 lru 字段被分成两段来存储，高 16bit 存储 ldt(Last Decrement Time)，低 8bit 存储 logc(Logistic Counter)。\nldt 是用来记录 key 的访问时间戳； logc 是用来记录 key 的访问频次，它的值越小表示使用频率越低，越容易淘汰，每个新加入的 key 的logc 初始值为 5。 注意，logc 并不是单纯的访问次数，而是访问频次（访问频率），因为 logc 会随时间推移而衰减的。\n在每次 key 被访问时，会先对 logc 做一个衰减操作，衰减的值跟前后访问时间的差距有关系，如果上一次访问的时间与这一次访问的时间差距很大，那么衰减的值就越大，这样实现的 LFU 算法是根据访问频率来淘汰数据的，而不只是访问次数。访问频率需要考虑 key 的访问是多长时间段内发生的。key 的先前访问距离当前时间越长，那么这个 key 的访问频率相应地也就会降低，这样被淘汰的概率也会更大。\n对 logc 做完衰减操作后，就开始对 logc 进行增加操作，增加操作并不是单纯的 + 1，而是根据概率增加，如果 logc 越大的 key，它的 logc 就越难再增加。\n所以，Redis 在访问 key 时，对于 logc 是这样变化的：\n先按照上次访问距离当前的时长，来对 logc 进行衰减； 然后，再按照一定概率增加 logc 的值 redis.conf 提供了两个配置项，用于调整 LFU 算法从而控制 logc 的增长和衰减：\nlfu-decay-time 用于调整 logc 的衰减速度，它是一个以分钟为单位的数值，默认值为1，lfu-decay-time 值越大，衰减越慢； lfu-log-factor 用于调整 logc 的增长速度，lfu-log-factor 值越大，logc 增长越慢。 实现redis-cache的内存淘汰策略 # ├── cache-api │ ├── pom.xml │ ├── src │ │ └── main │ │ └── java │ │ └── com.github.houbb.cache │ │ └── api │ │ ├── ICache.java │ │ ├── ICacheContext.java │ │ ├── ICacheEvict.java │ │ ├── ICacheEvictContext.java │ │ └── package-info.java 这里使用依赖注入模式，Cache 类的构造函数接受一个 ICacheContext 对象作为参数，在实例化 Cache 对象时，需要提供一个符合 ICacheContext 接口的实现对象。这样做的好处是将 Cache 类与具体的缓存实现细节解耦，使得 Cache 类更加灵活，可以适配不同的缓存实现。ICacheEvictContext 和 CacheEvict 同理。\n├── cache-core │ ├── pom.xml │ ├── src │ │ └── main │ │ └── java │ │ └── com.github.houbb.cache │ │ └── core │ │ ├── api │ │ │ ├── CacheAdaptor.java │ │ │ └── package-info.java │ │ ├── bs │ │ │ ├── CacheBs.java │ │ │ └── package-info.java │ │ ├── core │ │ │ ├── Cache.java │ │ │ └── CacheContext.java │ │ ├── exception │ │ │ └── CacheRuntimeException.java │ │ ├── package-info.java │ │ └── support │ │ ├── evict │ │ │ ├── CacheEvictContext.java │ │ │ ├── CacheEvictFIFO.java │ │ │ ├── CacheEvictNone.java │ │ │ ├── CacheEvicts.java │ │ │ └── package-info.java │ │ ├── expire │ │ │ └── package-info.java │ │ ├── listener │ │ │ └── package-info.java │ │ ├── load │ │ │ └── package-info.java │ │ ├── map │ │ │ └── package-info.java │ │ ├── package-info.java │ │ └── persist │ │ └── package-info.java 其中 evict 的部分使用工厂模式：\n接口定义：首先，定义了一个接口 ICacheEvict，用于表示缓存驱逐策略。 具体实现类：然后，你创建了两个具体的缓存驱逐策略实现类 CacheEvictNone 和 CacheEvictFIFO，分别表示不执行任何驱逐操作和先进先出（FIFO）驱逐策略。 工厂类：接着，你创建了一个工厂类 CacheEvicts，其中包含了两个静态方法 none() 和 fifo()，分别用于创建不同的缓存驱逐策略对象。 调用示例：客户端代码可以通过调用 CacheEvicts 类的静态方法来获取所需的缓存驱逐策略对象，而不需要直接实例化具体的实现类。 ","date":"28 February 2024","permalink":"/posts/architecture/high-concurrency/redis/redis-cache/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0redis1-%E5%AE%9E%E7%8E%B0%E5%9B%BA%E5%AE%9A%E7%BC%93%E5%AD%98%E5%A4%A7%E5%B0%8F/","section":"博客","summary":"redis 的驱除策略，也称为内存淘汰策略。当 Redis 的运行内存已经超过 Redis 设置的最大内存之后，则会使用内存淘汰策略删除符合条件的 key，以此来保障 Redis 高效的运行。","title":"从零实现redis(1)-实现固定缓存大小"},{"content":" 链接：https://www.nowcoder.com/practice/65cfde9e5b9b4cf2b6bafa5f3ef33fa6 难点 # 链表的题目很早刷过，现在有些生疏了，再回顾一遍。\n代码 # 解法一\nimport java.util.*; /* * public class ListNode { * int val; * ListNode next = null; * public ListNode(int val) { * this.val = val; * } * } */ public class Solution { /** * 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可 * * * @param lists ListNode类ArrayList * @return ListNode类 */ public ListNode mergeKLists (ArrayList\u0026lt;ListNode\u0026gt; lists) { // write code here //小顶堆 Queue\u0026lt;ListNode\u0026gt; pq = new PriorityQueue\u0026lt;\u0026gt;((v1, v2) -\u0026gt; v1.val - v2.val); //遍历所有链表第一个元素 for(int i = 0; i \u0026lt; lists.size(); i++){ //不为空则加入小顶堆 if(lists.get(i) != null) pq.add(lists.get(i)); } //加一个表头 ListNode res = new ListNode(-1); ListNode head = res; //直到小顶堆为空 while(!pq.isEmpty()){ //取出最小的元素 ListNode temp = pq.poll(); //连接 head.next = temp; head = head.next; //每次取出链表的后一个元素加入小顶堆 if(temp.next != null) pq.add(temp.next); } //去掉表头 return res.next; } } 解法二\nimport java.util.ArrayList; public class Solution { //两个链表合并函数 public ListNode Merge(ListNode list1, ListNode list2) { //一个已经为空了，直接返回另一个 if(list1 == null) return list2; if(list2 == null) return list1; //加一个表头 ListNode head = new ListNode(0); ListNode cur = head; //两个链表都要不为空 while(list1 != null \u0026amp;\u0026amp; list2 != null){ //取较小值的节点 if(list1.val \u0026lt;= list2.val){ cur.next = list1; //只移动取值的指针 list1 = list1.next; }else{ cur.next = list2; //只移动取值的指针 list2 = list2.next; } //指针后移 cur = cur.next; } //哪个链表还有剩，直接连在后面 if(list1 != null) cur.next = list1; else cur.next = list2; //返回值去掉表头 return head.next; } //划分合并区间函数 ListNode divideMerge(ArrayList\u0026lt;ListNode\u0026gt; lists, int left, int right){ if(left \u0026gt; right) return null; //中间一个的情况 else if(left == right) return lists.get(left); //从中间分成两段，再将合并好的两段合并 int mid = (left + right) / 2; return Merge(divideMerge(lists, left, mid), divideMerge(lists, mid + 1, right)); } public ListNode mergeKLists(ArrayList\u0026lt;ListNode\u0026gt; lists) { //k个链表归并排序 return divideMerge(lists, 0, lists.size() - 1); } } ","date":"27 February 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E9%93%BE%E8%A1%A8/%E5%90%88%E5%B9%B6k%E4%B8%AA%E5%B7%B2%E6%8E%92%E5%BA%8F%E7%9A%84%E9%93%BE%E8%A1%A8/","section":"博客","summary":"合并k个已排序的链表题解。链表的题目很早刷过，现在有些生疏了，再回顾一遍。","title":"合并k个已排序的链表"},{"content":" 链接：https://leetcode.cn/problems/dungeon-game/description/ 难点 # 本题的难点在于怎么处理血量增加的问题, 增加血量不能为之前的损失提供帮助，只会对后续有帮助。这意味着从王子救公主的思路想dp是困难的，但是公主救王子的思路dp很好做，从后往前推，当前如果可以治愈，那么当前的最小初始血量就是已经扣除的血量减去治疗量，注意不可以\u0026lt;1。 这意味着过量治疗。\n代码 # class Solution { public int calculateMinimumHP(int[][] dungeon) { int l1 = dungeon.length; int l2 = dungeon[0].length; int[][] dp = new int[l1+1][l2+1]; for(int i=0;i\u0026lt;=l1;i++){ Arrays.fill(dp[i], Integer.MAX_VALUE); } dp[l1][l2-1]=1; dp[l1-1][l2]=1; for(int i=l1-1;i\u0026gt;=0;i--){ for(int j=l2-1;j\u0026gt;=0;j--){ dp[i][j]=Math.max(Math.min(dp[i+1][j], dp[i][j+1]) - dungeon[i][j], 1); } } return dp[0][0]; } } ","date":"26 February 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/leetcode-174%E5%9C%B0%E4%B8%8B%E5%9F%8E%E6%B8%B8%E6%88%8F/","section":"博客","summary":"【LeetCode 174】地下城游戏题解。本题的难点在于怎么处理血量增加的问题, 增加血量不能为之前的损失提供帮助，只会对后续有帮助。这意味着从王子救公主的思路想dp是困难的，但是公主救王子的思路dp很好做，从后往前推，当前如果可以治愈，那么当前的最小初始血量就是已经扣除的血量减去治疗量，注意不可以\u0026lt;1。 这意味着过量治疗。","title":"【LeetCode 174】地下城游戏"},{"content":" 链接：https://leetcode.cn/problems/binary-tree-maximum-path-sum/description/ 难点 # 这题是树状dp的典型，如何在遍历树的过程中记录一些信息，是比较难的一个点。\n此外，这题有两个要记录的内容，一个是树（包括子树）的 根节点 或者 根节点和一个左/右节点 中的最大值，这个要一路记录上去，用 dp 记录。还有一个就是 根节点、左节点和右节点 与之前 dp 中的最大值，这个是最后的答案，用 ans 记录。\n我在解题的时候，总会忘记把 ans 跟自己做比较，也是以后需要注意的一个点。\n代码 # /** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode() {} * TreeNode(int val) { this.val = val; } * TreeNode(int val, TreeNode left, TreeNode right) { * this.val = val; * this.left = left; * this.right = right; * } * } */ class Solution { int ans=-100000; public int maxPathSum(TreeNode root) { Map\u0026lt;TreeNode, Integer\u0026gt; dp = new HashMap\u0026lt;\u0026gt;(); dsm(root, dp); return ans; } public int dsm(TreeNode node, Map\u0026lt;TreeNode, Integer\u0026gt; dp) { if(node == null) return 0; if(dp.containsKey(node)) { return dp.get(node); } int left = dsm(node.left, dp); int right = dsm(node.right, dp); int maxx = Math.max(node.val, Math.max(node.val+left, node.val+right)); dp.put(node, maxx); ans = Math.max(ans, Math.max(node.val + left + right, maxx)); return maxx; } } ","date":"26 February 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/leetcode-124%E4%BA%8C%E5%8F%89%E6%A0%91%E4%B8%AD%E7%9A%84%E6%9C%80%E5%A4%A7%E8%B7%AF%E5%BE%84%E5%92%8C/","section":"博客","summary":"【LeetCode 124】二叉树中的最大路径和题解。这题是树状dp的典型，如何在遍历树的过程中记录一些信息，是比较难的一个点。此外，这题有两个要记录的内容，一个是树（包括子树）的 根节点 或者 根节点和一个左/右节点 中的最大值，这个要一路记录上去，用 dp 记录。还有一个就是 根节点、左节点和右节点 与之前 dp 中的最大值，这个是最后的答案，用 ans 记录。我在解题的时候，总会忘记把 ans 跟自己做比较，也是以后需要注意的一个点。","title":"【LeetCode 124】二叉树中的最大路径和"},{"content":" 链接：https://leetcode.cn/problems/distinct-subsequences/description/ 难点 # 在我二刷这道题的时候，卡着没做出来，画了 $dp[][]$ 数组，但是没有初始化，所以一直出错，直到看了题解。但是官方题解是从后往前遍历，我觉得并没有这个必要。\n分析 # 主要还是分析三个问题：\ndp数组表示什么 如何初始化dp数组 如何实现dp数组的更新 在这题里，$dp[i][j]$ 定义为 $s[i:]$ 的子序列中 $t[j:]$ 出现的个数。\n按照对 $dp[i][j]$ 的定义，当 $j=0$ 时，空字符串是任意字符串的子串，所以都赋值 1。\n如何实现dp数组的更新主要分为两种情况\n代码 # class Solution { public int numDistinct(String s, String t) { int len1 = s.length(), len2 = t.length(); int[][] dp = new int[len1+1][len2+1]; for(int i=0;i\u0026lt;len1+1;i++) { dp[i][0]=1; } for(int i=1;i\u0026lt;=len1;i++) { for(int j=1;j\u0026lt;=len2;j++) { if(i\u0026lt;j) break; if(s.charAt(i-1)==t.charAt(j-1)) { dp[i][j] = dp[i-1][j-1]+dp[i-1][j]; } else { dp[i][j] = dp[i-1][j]; } } } return dp[len1][len2]; } } ","date":"26 February 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/leetcode-115%E4%B8%8D%E5%90%8C%E7%9A%84%E5%AD%90%E5%BA%8F%E5%88%97/","section":"博客","summary":"【LeetCode 115】不同的子序列题解。在我二刷这道题的时候，卡着没做出来，画了 $dp[][]$ 数组，但是没有\u003cstrong\u003e初始化\u003c/strong\u003e，所以一直出错，直到看了题解。但是官方题解是从后往前遍历，我觉得并没有这个必要。","title":"【LeetCode 115】不同的子序列"},{"content":" 链接：https://leetcode.cn/problems/edit-distance/description/ 难点 # 在我二刷这道题的时候，还是卡了一会，主要有三个难点：\ndp数组表示什么 如何初始化dp数组 如何针对题目告诉我们的三种操作，实现dp数组的更新 当然这也是动态规划数组的三个难点，需要重点培养这种感觉，形成正确的判断直觉。\n分析 # 在这题里，$dp[i][j]$ 定义为 $word1_{1:i}$ 转换成 $word2_{1:j}$ 所需要的最小步数。\n按照对 $dp[i][j]$ 的定义，很快就可以得到下面的初始化值：\n三种操作，结合 $dp[i][j]$ 就可以得出下面两种情况：\n代码 # class Solution { public int minDistance(String word1, String word2) { int len1 = word1.length(), len2 = word2.length(); int[][] dp = new int[len1+1][len2+1]; for(int i=0;i\u0026lt;=len1;i++) { dp[i][0] = i; } for(int i=0;i\u0026lt;=len2;i++) { dp[0][i] = i; } for(int i=1;i\u0026lt;=len1;i++) { for(int j=1;j\u0026lt;=len2;j++) { if(word1.charAt(i-1) == word2.charAt(j-1)) { dp[i][j] = Math.min(dp[i-1][j-1], Math.min(dp[i-1][j]+1, dp[i][j-1]+1)); } else { dp[i][j] = Math.min(dp[i-1][j-1]+1, Math.min(dp[i-1][j]+1, dp[i][j-1]+1)); } } } return dp[len1][len2]; } } ","date":"26 February 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/leetcode-72%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/","section":"博客","summary":"【LeetCode 72】编辑距离题解。在我二刷这道题的时候，还是卡了一会，主要有三个难点：1️⃣dp数组表示什么2️⃣如何初始化dp数组3️⃣如何针对题目告诉我们的三种操作，实现dp数组的更新。当然这也是动态规划数组的三个难点，需要重点培养这种感觉，形成正确的判断直觉。","title":"【LeetCode 72】编辑距离"},{"content":" 链接：https://www.nowcoder.com/practice/b58434e200a648c589ca2063f1faf58c 难点 # 链表的题目很早刷过，现在有些生疏了，再回顾一遍。\n代码 # import java.util.*; /* * public class ListNode { * int val; * ListNode next = null; * public ListNode(int val) { * this.val = val; * } * } */ public class Solution { /** * 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可 * * * @param head ListNode类 * @param m int整型 * @param n int整型 * @return ListNode类 */ public ListNode reverseBetween (ListNode head, int m, int n) { // write code here //设置虚拟头节点 ListNode dummyNode = new ListNode(-1); dummyNode.next =head; ListNode pre = dummyNode; for(int i=0;i\u0026lt;m-1;i++){ pre = pre.next; } ListNode cur = pre.next; ListNode Cur_next ; for(int i=0;i\u0026lt;n-m;i++){ Cur_next = cur.next; cur.next = Cur_next.next; Cur_next.next = pre.next; pre.next = Cur_next ; } return dummyNode.next; } } 反转：\n// 反转链表 ListNode reverse(ListNode head){ if(head == null) return head; ListNode cur = head; ListNode node = null; while(cur != null){ ListNode tail = cur.next; cur.next = node; node = cur; cur = tail; } return node; } ","date":"26 February 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E9%93%BE%E8%A1%A8/%E9%93%BE%E8%A1%A8%E5%86%85%E6%8C%87%E5%AE%9A%E5%8C%BA%E9%97%B4%E5%8F%8D%E8%BD%AC/","section":"博客","summary":"链表内指定区间反转题解。链表的题目很早刷过，现在有些生疏了，再回顾一遍。","title":"链表内指定区间反转"},{"content":"服务器介绍 # 服务器，也称伺服器，是提供计算服务的设备。由于服务器需要响应服务请求，并进行处理，因此一般来说服务器应具备承担服务并且保障服务的能力。\n在网络环境下，根据服务器提供的服务类型不同，分为 文件服务器、 数据库服务器、 应用程序服务器、W EB服务器等\n服务器的构成包括 处理器、 硬盘、 内存、 系统 总线等，和通用的计算机架构类似，但是由于需要 提供高可靠的服务，因此在处理能力、稳定性、可靠性、安全性、可扩展性、可管理性等方面要求较高。\n可以简单的理解为服务器就是一台电脑，只不过硬盘比普通的PC机更大，CPU比普通的PC机处理速度更快，网卡比普通的PC机更快。。。\n存储磁盘介绍 # 存储单位 # 最小的基本单位是bit，按顺序给出所有单位： bit、 Byte、KB、MB、GB、TB、PB、EB、ZB、YB、BB、NB、DB。\n1 Byte = 8 bit\t1K = 1024Byte\t1MB = 1024K\t1G = 1024M\n1T = 1024G\t1P = 1024T\t1E = 1024P\t1Z = 1024E\n1Y = 1024Z\t1B = 1024Y\t1N = 1024B\t1D = 1024N\n服务器需要存储数据，免不了得要磁盘的支持，磁盘就是一类存储介质，专门用于存储我们各种类型的数据，其中磁盘按照接口类型又可以有好多种分类，接下来我们来简单看一下不同接口的各类磁盘的基本特性吧\n1. SCSI接口硬盘介绍 # SCSi传统服务器老传输接口，转速为10kr 15kr。但是由于受到线缆及其 阵列卡和 传输协议的限制，该盘片有固定的插法，例如要顺着末端接口开始插第一块硬盘，没有插硬盘的地方要插硬盘终结器等。该盘现已经完全停止发售。该盘只有3.5寸版。常见转速：10000转/分。\n2. SAS接口硬盘介绍 # SAS 该盘分为两种协议，即SAS1.0及SAS 2.0接口，SAS1.0接口传输带宽为3.0GB/s转速有7.2kr 10kr 15kr。该盘现已被SAS2.0接口盘取代，该盘尺寸有2.5寸及3.5寸两种。SAS2.0接口传输带宽为6.0GB/s转速有10kr 15kr，常见容量为73.6G 146G 300G 600G 900G。常见转速：15000转/分。\n3. FDE/SDE接口硬盘介绍 # FDE/SDE 该盘体前者为IBM研发的SAS硬件加密硬盘，该盘体性能等同于SAS硬盘，但是由于本身有硬件加密系统，可以保证涉密单位数据不外泄，该盘主要用于高端2.5寸存储及2.5寸硬盘接口的机器上。SED盘雷同，厂家不一样。\n4. SATA硬盘基本介绍 # SATA硬盘：用SATA接口的硬盘又叫 串口硬盘，是以后PC机的主流发展方向，因为其有较强的纠错能力，错误一经发现能自动纠正，这样就大大的提高了 数据传输的安全性。新的SATA 使用了差动信号系统“differential-signal-amplified-system”。这种系统能有效的将噪声从正常讯号中滤除，良好的噪声滤除能力使得SATA只要使用低电压操作即可，和 Parallel ATA 高达5V的传输电压相比，SATA 只要0.5V(500mv) 的峰对峰值电压即可操作于更高的速度之上。“比较正确的说法是：峰对峰值‘ 差模电压’”。常见转速：7200转/分。\n5. SSD硬盘介绍 # SSD 该盘为 固态硬盘，与个人PC不同的是该盘采用一类固态硬盘检测系统检测出场，并采用SAS2.0协议进行传输，该盘的性能也将近是个人零售SSD硬盘的数倍以上。\n交换机介绍 # 交换机（Switch）意为“ 开关”是一种用于电（光）信号转发的 网络设备。它可以为接入交换机的任意两个 网络节点提供独享的电信号通路。最常见的交换机是 以太网交换机。其他常见的还有电话语音交换机、 光纤交换机等。\n主要作用：交换机的主要功能包括物理编址、 网络拓扑结构、错误校验、帧序列以及流控。交换机还具备了一些新的功能，如对VLAN（ 虚拟局域网）的支持、对 链路汇聚的支持，甚至有的还具有 防火墙的功能\n网卡的介绍 # 网卡（Network Interface Card）是物理上连接计算机与 网络的硬件设，是计算机与局域网 通信介质间的直接接口。由于网络技术的不同，网卡的分类也有所不同，如大家所熟知的ATM网卡、令牌环网卡和以太网网卡等。据统计，目前约有80 ％的局域网采用以太网技术。 接口方式 当前 台式机和 笔记本电脑中常见的总线接口方式都可以从主流网卡厂商那里找到适用的产品。但值得注意的是，市场上很难找到ISA接口的 100M网卡。1994年以来， PCI总线架构日益成为网卡的首选总线，目前已牢固地确立了在 服务器和高端桌面机中的地位。即将到来的转变是这种网卡将推广有的桌面机中。PCI以太网网卡的高性能、易用性和增强了的可靠性使其被标 准以太网网络所广泛采用，并得到了PC业界的支持。\n技术方向 目前，以太网网卡有10M、100M、10M/100M及千兆网卡。对于大数据量网络 来说， 服务器应该采用千兆以太网网卡，这种网卡多用于服务器与交换机之间的连接，以提高整体系统的响应速率.\n对于通常的文件共享等应用来说，10M网卡就已经足够了，但对于将来可能的语音和 视频等应用来说，100M 网卡将更利于实时应用的传输。\n局域网介绍 # 局域网（Local Area Network，LAN）是指在某一区域内由多台计算机互联成的计算机组。一般是方圆几千米以内。局域网可以实现文件管理、 应用软件共享、 打印机共享、 工作组内的日程安排、电子邮件和传真通信服务等功能。局域网是封闭型的，可以由办公室内的两台计算机组成，也可以由一个公司内的上千台计算机组成。\n机架介绍 # 为了方便管理维护众多的服务器，以及在服务器出现问题时候快读的定位解决问题，我们可以使用机架的形式，将众多的服务器归纳到一个个的机架里面去。机架之间的通信问题可以使用交换机来组织成为局域网\nIDC数据中心介绍 # 互联网数据中心（Internet Data Center）简称IDC，就是电信部门利用已有的互联网 通信线路、带宽资源，建立标准化的电信专业级机房环境，为企业、政府提供服务器托管、租用以及相关增值等方面的全方位服务\nIDC 主机托管主要应用范围是网站发布、虚拟主机和电子商务等。比如网站发布，单位通过托管主机，从电信部门分配到互联网 静态IP地址后，即可发布自己的www站点，将自己的产品或服务通过互联网广泛宣传；虚拟主机是单位通过托管主机，将自己主机的海量硬盘空间出租，为其他客户提供虚拟主机服务，使自己成为ICP服务提供商；电子商务是指单位通过托管主机，建立自己的 电子商务系统，通过这个商业平台来为供应商、批发商、经销商和最终用户提供完善的服务。\nIDC即 互联网数据中心。它是伴随着互联网不断发展的需求而迅速发展起来的，成为了新世纪中国 互联网产业中不可或缺的重要一环。它为 互联网内容提供商（ICP）、企业、媒体和各类网站提供大规模、高质量、安全可靠的专业化服务器托管、空间租用、网络批发带宽以及ASP、EC等业务。\nIDC是对入驻（Hosting）企业、商户或 网站服务器群托管的场所；是各种模式电子商务赖以安全运作的基础设施，也是支持企业及其商业联盟其分销商、供应商、客户等实施价值链管理的平台。\nIDC起源于ICP对网络高速互联的需求，而且 美国仍然处于世界领导者位置。在美国，运营商为了维护自身利益，将 网络互联带宽设得很低，用户不得不在每个服务商处都放一台服务器。为了解决这个问题，IDC应运而生，保证客户托管的服务器从各个网络访问速度都没有瓶颈。\nIDC不仅是数据存储的中心，而且是数据流通的中心，它应该出现在Internet网络中数据交换最集中的地方。它是伴随着人们对 主机托管和虚拟主机服务提出了更高要求的状况而产生的，从某种意义上说，它是由ISP的服务器托管机房演变而来的。具体而言，随着Internet的高速发展， 网站系统对带宽、管理维护日益增长的高要求对很多企业构成了严峻的挑战。于是，企业开始将与网站托管服务相关的一切事物交给专门提供网络服务的IDC去做，而将精力集中在增强核心竞争力的业务中去。可见，IDC是Internet企业分工更加细化的产物。\n目前我国比较大的机房主要在北京、 上海、 广州、 唐山等地\n磁盘阵列 # 磁盘RAID的基本介绍：\n1988 年美国加州大学伯克利分校的 D. A. Patterson 教授等首次在论文 “A Case of Redundant Array of Inexpensive Disks” 中提出了 RAID 概念，即廉价冗余磁盘阵列（ Redundant Array of Inexpensive Disks ）。由于当时大容量磁盘比较昂贵， RAID 的基本思想是将多个容量较小、相对廉价的磁盘进行有机组合，从而以较低的成本获得与昂贵大容量磁盘相当的容量、性能、可靠性。随着磁盘成本和价格的不断降低， RAID 可以使用大部分的磁盘， “廉价” 已经毫无意义。因此， RAID 咨询委员会（ RAID Advisory Board, RAB ）决定用 “ 独立 ” 替代 “ 廉价 ” ，于时 RAID 变成了独立磁盘冗余阵列（ Redundant Array of Independent Disks ）。但这仅仅是名称的变化，实质内容没有改变.\n1. RAID0基本介绍 # RAID0 是一种简单的、无数据校验的数据条带化技术。实际上不是一种真正的 RAID ，因为它并不提供任何形式的冗余策略。 RAID0 将所在磁盘条带化后组成大容量的存储空间（如图 2 所示），将数据分散存储在所有磁盘中，以独立访问方式实现多块磁盘的并读访问。由于可以并发执行 I/O 操作，总线带宽得到充分利用。再加上不需要进行数据校验，RAID0 的性能在所有 RAID 等级中是最高的。理论上讲，一个由 n 块磁盘组成的 RAID0 ，它的读写性能是单个磁盘性能的 n 倍，但由于总线带宽等多种因素的限制，实际的性能提升低于理论值。\nRAID0 具有低成本、高读写性能、 100% 的高存储空间利用率等优点，但是它不提供数据冗余保护，一旦数据损坏，将无法恢复。 因此， RAID0 一般适用于对性能要求严格但对数据安全性和可靠性不高的应用，如视频、音频存储、临时数据缓存空间等\n2. RAID1基本介绍 # RAID1 称为镜像，它将数据完全一致地分别写到工作磁盘和镜像 磁盘，它的磁盘空间利用率为 50% 。 RAID1 在数据写入时，响应时间会有所影响，但是读数据的时候没有影响。 RAID1 提供了最佳的数据保护，一旦工作磁盘发生故障，系统自动从镜像磁盘读取数据，不会影响用户工作。\nRAID1 与 RAID0 刚好相反，是为了增强数据安全性使两块 磁盘数据呈现完全镜像，从而达到安全性好、技术简单、管理方便。 RAID1 拥有完全容错的能力，但实现成本高。 RAID1 应用于对顺序读写性能要求高以及对数据保护极为重视的应用，如对邮件系统的数据保护\n3. RAID2基本介绍 # RAID2 称为纠错海明码磁盘阵列，其设计思想是利用海明码实现数据校验冗余。海明码是一种在原始数据中加入若干校验码来进行错误检测和纠正的编码技术，其中第 2n 位（ 1, 2, 4, 8, … ）是校验码，其他位置是数据码。因此在 RAID2 中，数据按位存储，每块磁盘存储一位数据编码，磁盘数量取决于所设定的数据存储宽度，可由用户设定。图 4 所示的为数据宽度为 4 的 RAID2 ，它需要 4 块数据磁盘和 3 块校验磁盘。如果是 64 位数据宽度，则需要 64 块 数据磁盘和 7 块校验磁盘。可见， RAID2 的数据宽度越大，存储空间利用率越高，但同时需要的磁盘数量也越多。\n海明码自身具备纠错能力，因此 RAID2 可以在数据发生错误的情况下对纠正错误，保证数据的安全性。它的数据传输性能相当高，设计复杂性要低于后面介绍的 RAID3 、 RAID4 和 RAID5 。\n但是，海明码的数据冗余开销太大，而且 RAID2 的数据输出性能受阵列中最慢磁盘驱动器的限制。再者，海明码是按位运算， RAID2 数据重建非常耗时。由于这些显著的缺陷，再加上大部分磁盘驱动器本身都具备了纠错功能，因此 RAID2 在实际中很少应用，没有形成商业产品，目前主流存储磁盘阵列均不提供 RAID2 支持。\n4、RAID3基本介绍 # RAID3 （图 5 ）是使用专用校验盘的并行访问阵列， 它采用一个专用的磁盘作为校验盘，其余磁盘作为数据盘，数据按位可字节的方式交叉存储到各个数据盘中。RAID3 至少需要三块磁盘，不同磁盘上同一带区的数据作 XOR 校验，校验值写入校验盘中。 RAID3 完好时读性能与 RAID0 完全一致，并行从多个磁盘条带读取数据，性能非常高，同时还提供了数据容错能力。向 RAID3 写入数据时，必须计算与所有同条带的校验值，并将新校验值写入校验盘中。一次写操作包含了写数据块、读取同条带的数据块、计算校验值、写入校验值等多个操作，系统开销非常大，性能较低。\n如果 RAID3 中某一磁盘出现故障，不会影响数据读取，可以借助校验数据和其他完好数据来重建数据。假如所要读取的数据块正好位于失效磁盘，则系统需要读取所有同一条带的数据块，并根据校验值重建丢失的数据，系统性能将受到影响。当故障磁盘被更换后，系统按相同的方式重建故障盘中的数据至新磁盘。\nRAID3 只需要一个校验盘，阵列的存储空间利用率高，再加上并行访问的特征，能够为高带宽的大量读写提供高性能，适用大容量数据的顺序访问应用，如影像处理、流媒体服务等。目前， RAID5 算法不断改进，在大数据量读取时能够模拟 RAID3 ，而且 RAID3 在出现坏盘时性能会大幅下降，因此常使用 RAID5 替代 RAID3 来运行具有持续性、高带宽、大量读写特征的应用。\n5. RAID4基本介绍 # RAID4 与 RAID3 的原理大致相同，区别在于条带化的方式不同。 RAID4 （图 6 ）按照 块的方式来组织数据，写操作只涉及当前数据盘和校验盘两个盘，多个 I/O 请求可以同时得到处理，提高了系统性能。 RAID4 按块存储可以保证单块的完整性，可以避免受到其他磁盘上同条带产生的不利影响。\nRAID4 在不同磁盘上的同级数据块同样使用 XOR 校验，结果存储在校验盘中。写入数据时， RAID4 按这种方式把各磁盘上的同级数据的校验值写入校验 盘，读取时进行即时校验。因此，当某块磁盘的数据块损坏， RAID4 可以通过校验值以及其他磁盘上的同级数据块进行数据重建。\nRAID4 提供了 非常好的读性能，但单一的校验盘往往成为系统性能的瓶颈。对于写操作， RAID4 只能一个磁盘一个磁盘地写，并且还要写入校验数据，因此写性能比较差。而且随着成员磁盘数量的增加，校验盘的系统瓶颈将更加突出。正是如上这些限制和不足， RAID4 在实际应用中很少见，主流存储产品也很少使用 RAID4 保护。\n6. RAID5基本介绍 # RAID5 应该是目前最常见的 RAID 等级，它的原理与 RAID4 相似，区别在于 校验数据分布在阵列中的所有磁盘上，而没有采用专门的校验磁盘。对于数据和校验数据，它们的写操作可以同时发生在完全不同的磁盘上。因此， RAID5 不存在 RAID4 中的并发写操作时的校验盘性能瓶颈问题。另外， RAID5 还具备很好的扩展性。当阵列磁盘 数量增加时，并行操作量的能力也随之增长，可比 RAID4 支持更多的磁盘，从而拥有更高的容量以及更高的性能。\nRAID5的磁盘上同时存储数据和校验数据，数据块和对应的校验信息存保存在不同的磁盘上，当一个数据盘损坏时，系统可以根据同一条带的其他数据块和对应的校验数据来重建损坏的数据。与其他 RAID 等级一样，重建数据时， RAID5 的性能会受到较大的影响。\nRAID5 兼顾存储性能、数据安全和存储成本等各方面因素，它可以理解为 RAID0 和 RAID1 的折中方案，是目前综合性能最佳的数据保护解决方案。 RAID5 基本上可以满足大部分的存储应用需求，数据中心大多采用它作为应用数据的保护方案。\n","date":"25 February 2024","permalink":"/posts/skills/big-data-hardware/","section":"博客","summary":"大数据（big data），指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。","title":"大数据物理设备介绍"},{"content":"","date":"25 February 2024","permalink":"/tags/hadoop/","section":"Tags","summary":"","title":"Hadoop"},{"content":"","date":"25 February 2024","permalink":"/posts/architecture/distributed/hadoop/","section":"博客","summary":"Hadoop是一个开源的分布式存储和处理框架，用于处理大规模数据。它提供了分布式文件系统（HDFS）和分布式计算框架（MapReduce），能够在廉价的硬件上高效处理海量数据。Hadoop被广泛应用于大数据领域，支持数据存储、处理、分析和挖掘，为企业提供了强大的数据处理能力。","title":"Hadoop"},{"content":"学习目标 # 了解什么是分布式文件系统，即大文件如何存储在集群中？ 掌握HDFS常见面试题（切片机制、心跳检测、负载均衡、副本机制） 了解HDFS中的常见命令（上传、下载、复制、剪切、删除） HDFS读文件流程 HDFS写文件流程 了解HDFS小文件归档模式 什么是分布式文件系统 # 大白话：你出5毛，我出5毛，一起凑成1块的过程 专业版：相当于把多个机器的磁盘给打通，然后对外提供统一的访问端口，从而实现跨机存储。 HDFS存储数据的时候，会对文件进行切块（Block数据块，hadoop2.x 中128Mb为一块）\nnamenode：元数据 datanode：实际数据 问题1：分布式存储优点是什么？ 答案：无限扩展支撑海量数据存储（横向扩容） 问题2：元数据记录的功能是什么？ 答案：整个集群的文件信息，每个文件Block块存储的位置，快速定位文件位置遍历查找 问题3：文件分块存储的好处是什么？ 答案：针对块并行操作提高效率 问题4：设置副本备份的作用是什么？ 答案：冗余存储保障数据安全 HDFS设计目标 # 硬件故障是常态，HDFS将有成百上干的服务器组成，每一个组成部分都有可能出现故障。因此故障的检测和自动快速恢复是HDFS的核心架构目标。 HDFS上的应用与一般的应用（主要是以流式读取数据）不同。HDFS被设计成适合批量处理，而不是用户交互式的。相较于数据访问的反应时间，更注重数据访问的高吞吐量。 流式读取 =＞ 把数据像水流一样进行读取，HDFS侧重于数据的读写，HDFS无法做到低延迟。 有时，读一个数据可能需要很长的时间。 典型的HDFS文件大小是GB到TB的级别。所以，HDFS被调整成支持大文件。它应该提供很高的聚合数据带宽，一个集群中支持数百个节点，一个集群中还应该支持干万级别的文件。 HDFS适合存储GB级别及以上大小文件 大部分HDFS应用对文件要求的是 write-one-read-many 访问模型。一个文件一旦创建、写入、关闭之后就 不需要修改了。这一假设简化了数据一致性问题，使高吞吐量的数据访问成为可能。 HDFS不支持文件的修改操作，所以其设计理念：一次写入，多次读取 移动计算的代价比之移动数据的代价低。一个应用请求的计算，离它操作的数据越近就越高效，这在数据达到海量级别的时候更是如此。将计算移动到数据附近，比之将数据移动到应用所在显然更好。 网络拓扑+机架感知原理：把数据放置在离计算最近的地方 在异构的硬件和软件平台上的 可移植性。这将推动需要大数据集的应用更广泛地采用HDFS作为平台。 HDFS重要特性 # 首先，它是一个文件系统，用于存储文件，通过统一的命名空间目录树来定位文件； 其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。 HDFS: namenode节点（存放元数据） datanode节点（存放具体数据块） secondarynamenode（2nn）辅助管理元数据 master/slave架构（一主多从架构） # HDFS采用master/slave架构。一般一个HDFS集群是有一个Namenode和一定数目的Datanode组成。Namenode是HDFS集群主节点，Datanode是HDFS集群从节点，两种角色各司其职，共同协调完成分布式的文件存储服务。\n分块存储（面试题：切片机制） # HDFS中的文件在物理上是分块存储（block）的，块的大小可以通过配置参数来规定，默认大小在hadoop2.x版本中是128M。\nhadoop1.x版本中，客户端切片大小，每64MB为一片（Block） hadoop2.x/hadoop3.x版本中，客户端切片大小，每128MB为一片（Block） 问题：Hadoop切片大小为什么设置为128MB，此大小是否可以调整？\n这个参数与硬盘文件寻址有关，可以调整，但是很少有调整的需求.由于目前位置大多数的硬盘，平均速率大概时候100MB/S。 命令空间 # HDFS支持传统的层次型文件组织结构（树状结构）。用户或者应用程序可以创建目录，然后将文件保存在这些目录里。文件系统名字空间的层次结构和大多数现有的文件系统类似：用户可以创建、删除、移动或重命名文件。\nNamenode负责维护文件系统的名字空间，任何对文件系统名字空间或属性的修改都将被Namenode记录下来。\nHDFS会给客户端提供一个统一的抽象目录树，客户端通过路径来访问文件，形如：hdfs://namenode:port/dir-a/dir-b/dir-c/file.data。\n举个例子：hdfs://node1:8020/itheima.txt。\nNamenode元数据管理 # 我们把目录结构及文件分块位置信息叫做元数据。Namenode负责维护整个hdfs文件系统的目录树结构，以及每一个文件所对应的block块信息（block的id，及所在的datanode服务器）。\nDatanode数据存储 # 文件的各个block的具体存储管理由datanode节点承担。每一个block都可以在多个datanode上。Datanode需要定时向Namenode汇报自己持有的block信息。\n面试题1. 心跳检测机制\nnamenode与datanode是如何通信的，如果datanode宕机了，namenode是如何知晓并转移数据的？\n心跳检测机制：了解namenode运行状态，即datanode要定时（3秒）给namenode发送自己的心跳消息，这样做的目的是： 让namenode知道datanode还活着，如果超时（如果某一次心跳数据没有接收，datanode还会频繁发送，发送10次=\u0026gt;30s）未发送心跳消息，namenode会认为datanode进入到假死的状态。如果超过最大间隔时间（10分钟）还未发送心跳消息，则认为该datanode宕机了。10分钟+30s =\u0026gt;（namenode =\u0026gt; datanode宕机了） 当检测到某个datanode宕机后，namenode会将该datanode存储的所有数据重新找新机器（活跃的）备份。 汇报机制：元数据的块信息是存储在namenode的内存中的。 当HDFS集群重新启动的时候，所有的datanode都要向namenode汇报自己的节点信息，每个datanode重启也会主动汇报自己保存的文件块信息。 当集群在正常工作的时候，间隔一定的时间（6小时），datanode也会向namenode汇报一次自己的块信息。 面试题2：负载均衡机制\n即让集群中所有的节点（服务器）的利用率及副本数尽量都保持一致或者在同一个水平线上，从而使利用率均衡，降低单台压力，每个节点利用率以及副本数量尽量平均（保障整个集群平稳运行）\n副本机制（面试题：副本机制） # 为了容错，文件的所有block都会有副本。每个文件的block大小和副本系数都是可配置的。应用程序可以指定某个文件的副本数目。副本系数可以在文件创建的时候指定，也可以在之后改变。\n副本数量也可以通过参数设置 dfs.replication，默认是3。\n当某个块的副本不够3份的时候，namenode会新增副本； 当某个块的副本超过3份的时候，namenode会删除副本； 当某个块的副本数不够3份且无法新增的时候，此时集群就会强制进入安全模式，只能读，不能写。 hadoop-hdfs配置文件：https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml\n面试题3：机架感知原理＋网络拓扑结构实现副本摆放\n第1副本：优先本机，否则就近随机 第2副本：就近不同机架的某一个服务器上 第3副本：和第2副本同一个机架的不同服务器上 一次写入，多次读出 # HDFS是设计成适应一次写入，多次读出的场景，且不支持文件的修改。\n正因为如此，HDFS适合用来做大数据分析的底层存储服务，并不适合用来做网盘等应用，因为，修改不方便，延迟大，网络开销大，成本太高。\nHDFS基本操作 # 官方文档：https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html\nShell命令行客户端 # Hadoop提供了文件系统的shell命令行客户端，使用方法如下：\nhadoop fs \u0026lt;args\u0026gt; 文件系统shell包括与Hadoop分布式文件系统（HDFS）以及Hadoop支持的其他文件系统（如本地FS，HFTP FS，S3 FS等）直接交互的各种类似shell的命令。所有FS shell命令都将路径URI作为参数。\nURI格式为scheme://authority/path。对于HDFS，该scheme是hdfs，对于本地FS，该scheme是file。scheme和authority是可选的。如果未指定，则使用配置中指定的默认方案。\n对于HDFS，命令示例如下：\nhadoop fs -ls hdfs://namenode:host/parent/child hadoop fs -ls /parent/child 注：fs.defaultFS中有配置\n对于本地文件系统，命令示例如下：\nhadoop fs -ls file:///root/ 如果使用的文件系统是HDFS，则使用hdfs dfs也是可以的，此时\nhadoop fs \u0026lt;args\u0026gt; = hdfs dfs \u0026lt;args\u0026gt; 特别说明：关于HDFS的Shell操作，前缀写法有两种形式：\nhadoop fs # 适用于所有的文件系统，hdfs://node1:8020/ file:/// hdfs dfs # 更适用于HDFS分布式文件系统 Shell命令选项 # 选项名称 使用格式 含义 -ls -ls \u0026lt;路径\u0026gt; 查看指定路径的当前目录结构 -lsr -lsr \u0026lt;路径\u0026gt; 递归查看指定路径的目录结构 -du -du \u0026lt;路径\u0026gt;，统计文件夹需要添加-s 统计目录下个文件大小 -dus -dus \u0026lt;路径\u0026gt; 汇总统计目录下文件(夹)大小 -count -count [-q] \u0026lt;路径\u0026gt; 统计文件(夹)数量 -mv -mv \u0026lt;源路径\u0026gt; \u0026lt;目的路径\u0026gt; 移动 -cp -cp \u0026lt;源路径\u0026gt; \u0026lt;目的路径\u0026gt; 复制 -rm -rm [-skipTrash] \u0026lt;路径\u0026gt; 删除文件/空白文件夹 -rmr -rmr [-skipTrash] \u0026lt;路径\u0026gt; 递归删除 -put -put \u0026lt;多个linux上的文件\u0026gt; \u0026lt;hdfs路径\u0026gt; 上传文件 -copyFromLocal -copyFromLocal \u0026lt;多个linux上的文件\u0026gt; \u0026lt;hdfs路径\u0026gt; 从本地复制 -moveFromLocal -moveFromLocal \u0026lt;多个linux上的文件\u0026gt; \u0026lt;hdfs路径\u0026gt; 从本地移动 -getmerge -getmerge \u0026lt;源路径\u0026gt; \u0026lt;linux路径\u0026gt; 合并到本地 -cat -cat \u0026lt;hdfs路径\u0026gt; 查看文件内容 -text -text \u0026lt;hdfs路径\u0026gt; 查看文件内容 -copyToLocal -copyToLocal [-ignoreCrc] [-crc] [hdfs源路径] [linux目的路径] 从本地复制 -moveToLocal -moveToLocal [-crc] \u0026lt;hdfs源路径\u0026gt; \u0026lt;linux目的路径\u0026gt; 从本地移动 -mkdir -mkdir \u0026lt;hdfs路径\u0026gt; 创建空白文件夹 -setrep -setrep [-R] [-w] \u0026lt;副本数\u0026gt; \u0026lt;路径\u0026gt; 修改副本数量 -touchz -touchz \u0026lt;文件路径\u0026gt; 创建空白文件 -stat -stat [format] \u0026lt;路径\u0026gt; 显示文件统计信息 -tail -tail [-f] \u0026lt;文件\u0026gt; 查看文件尾部信息 -chmod -chmod [-R] \u0026lt;权限模式\u0026gt; [路径] 修改权限 -chown -chown [-R] [属主][:[属组]] 路径 修改属主 -chgrp -chgrp [-R] 属组名称 路径 修改属组 -help -help [命令选项] 帮助 Shell常用命令介绍 # -ls（重点） # 使用方法：hadoop fs -ls [-h] [-R] 功能：显示文件、目录信息。\n示例：hadoop fs -ls /user/hadoop/file1\n-mkdir（重点） # 使用方法：hadoop fs -mkdir [-p] 功能：在hdfs上创建目录，-p表示会创建路径中的各级父目录。\n示例：hadoop fs -mkdir –p /user/hadoop/dir1\n-put：上传（重点） # 使用方法：hadoop fs -put [-f] [-p] [ -| .. ]. 功能：将单个src或多个srcs从本地文件系统复制到目标文件系统。\n-p：保留访问和修改时间，所有权和权限。\n-f：覆盖目的地（如果已经存在）\n示例：hadoop fs -put -f localfile1 localfile2 /user/hadoop/hadoopdir\n-get：下载（重点） # 使用方法：hadoop fs -get [-ignorecrc] [-crc] [-p] [-f] -ignorecrc：跳过对下载文件的CRC检查。\n-crc：为下载的文件写CRC校验和。\n功能：将文件复制到本地文件系统。\n示例：hadoop fs -get hdfs://host:port/user/hadoop/file localfile\n-appendToFile # 使用方法：hadoop fs -appendToFile \u0026hellip; 功能：追加一个文件到已经存在的文件末尾（两个文件内容合并）\n示例：hadoop fs -appendToFile localfile /hadoop/hadoopfile\nHDFS强调一次写入，多次读取。数据一旦写入成功，就不建议修改了！\n如果想对数据修改有没有办法？答：目前官方提供了一个唯一方案，不是修改方案，是追加文件\n-cat（重点） # 使用方法：hadoop fs -cat [-ignoreCrc] URI [URI \u0026hellip;]\n功能：显示文件内容到stdout\n示例：hadoop fs -cat /hadoop/hadoopfile\n-tail（重点） # 使用方法：hadoop fs -tail [-f] URI\n功能：将文件的最后一千字节内容显示到stdout。\n-f选项将在文件增长时输出附加数据。\n示例：hadoop fs -tail /hadoop/hadoopfile\n-chgrp：group简写，更改文件所属组 # 使用方法：hadoop fs -chgrp [-R] GROUP URI [URI \u0026hellip;]\n功能：更改文件组的关联。用户必须是文件的所有者，否则是超级用户。\n-R将使改变在目录结构下递归进行。\n示例：hadoop fs -chgrp othergroup /hadoop/hadoopfile\n-chmod：更改权限（重点） # 功能：改变文件的权限。使用-R将使改变在目录结构下递归进行。\n示例：hadoop fs -chmod 666 /hadoop/hadoopfile\n-chown：更改文件拥有者 # 功能：改变文件的拥有者。使用-R将使改变在目录结构下递归进行。\n示例：hadoop fs -chown someuser:somegrp /hadoop/hadoopfile\n-cp（重点） # 功能：从hdfs的一个路径拷贝hdfs的另一个路径\n示例： hadoop fs -cp /aaa/jdk.tar.gz /bbb/jdk.tar.gz.2\n-mv（重点） # 功能：在hdfs目录中移动文件\n示例： hadoop fs -mv /aaa/jdk.tar.gz /\n-getmerge # 功能：合并下载多个文件\n示例：比如hdfs的目录 /aaa/下有多个文件:log.1, log.2,log.3,\u0026hellip;\nhadoop fs -getmerge /aaa/log.* ./log.sum\n-rm（重点） # 功能：删除指定的文件。只删除非空目录和文件。-r 递归删除。\n示例：hadoop fs -rm -r /aaa/bbb/\n-df（重点） # 功能：统计文件系统的可用空间信息\n示例：hadoop fs -df -h /\n-du（重点） # 功能：显示目录中所有文件大小，当只指定一个文件时，显示此文件的大小。\n示例：hadoop fs -du /user/hadoop/dir1\n-setrep # 功能：改变一个文件的副本系数。-R选项用于递归改变目录下所有文件的副本系数。\n示例：hadoop fs -setrep -w 3 -R /user/hadoop/dir1\n实际工作中，一般不会使用此命令，而是通过配置文件设置副本数\n特别说明：-touch\nHDFS基本原理 # NameNode概述 # NameNode是HDFS的核心。 NameNode也称为Master。 NameNode仅存储HDFS的元数据（文件系统中所有文件的目录树），并跟踪整个集群中的文件。 NameNode不存储实际数据或数据集。数据本身实际存储在DataNodes中。 NameNode知道HDFS中任何给定文件的块列表及其位置。使用此信息NameNode知道如何从块中构建文件。 NameNode并不持久化存储每个文件中各个块所在的DataNode的位置信息，这些信息会在系统启动时从数据节点重建。 NameNode对于HDFS至关重要，当NameNode关闭时，HDFS/ Hadoop集群无法访问。 NameNode是Hadoop集群中的单点故障。 NameNode所在机器通常会配置有大量内存（RAM）。 元数据存放在内存中，默认情况下，每个文件的元数据大概有150B字节。\nDataNode概述 # DataNode负责将实际数据存储在HDFS中。 DataNode也称为Slave。 NameNode和DataNode会保持不断通信（心跳机制）。 DataNode启动时，它将自己发布到NameNode并汇报自己负责持有的块列表。 当某个DataNode关闭时，它不会影响数据或群集的可用性。NameNode将安排由其他DataNode管理的块进行副本复制。 DataNode所在机器通常配置有大量的硬盘空间。因为实际数据存储在DataNode中。 DataNode会定期（dfs.heartbeat.interval配置项配置，默认是3秒）向NameNode发送心跳，如果NameNode长时间没有接受到DataNode发送的心跳，NameNode就会认为该DataNode失效（10分钟+30s）。 block汇报时间间隔取参数dfs.blockreport.intervalMsec，参数未配置的话默认为6小时。 小结：\nnamenode =\u0026gt; HDFS核心组件=\u0026gt;负责管理整个HDFS集群不保存具体数据，主要保存元数据=\u0026gt;放置在内存，所以在配置时需要大量的内存 datanode =\u0026gt; HDFS组件=\u0026gt;负责具体数据/数据集存储，需要占用大量磁盘空间，某个机器故障并不影响整个集群的使用，datanode需要每个3s发送一次心跳信息。 datanode启动时会自动向namenode汇报1次本节点的块文件信息。 datanode实现数据冗余存储（副本机制）。 HDFS写数据流程（简化为十步走） # HDFS读数据流程（简化为五步走） # HDFS其他功能 # 1、不同集群之间的数据复制 # cluster1 =\u0026gt; hadoop集群（测试环境）\ncluster2 =\u0026gt; hadoop集群（生产环境）\n在我们实际工作当中，极有可能会遇到将测试集群的数据拷贝到生产环境集群，或者将生产环境集群的数据拷贝到测试集群，那么就需要我们在多个集群之间进行数据的远程拷贝，hadoop自带也有命令可以帮我们实现这个功能。\n2、集群内部文件拷贝scp # 上传： scp 本地文件 用户名@主机名称:拷贝路径 scp -r 本地文件夹 用户名@主机名称:拷贝路径 下载： scp 用户名@主机名称:远程文件路径 本地路径 scp -r 用户名@主机名称:远程文件夹路径 本地路径 3、跨集群之间的数据拷贝distcp # [root@cluster1 ~] # hadoop distcp hdfs://cluster1:8020/jdk-8u141-linux-x64.tar.gz hdfs://cluster2:8020/ 4、Archive档案的使用(Hadoop归档模式) # HDFS并不擅长存储小文件，因为每个文件最少一个block，每个block的元数据都会在NameNode占用内存，如果存在大量的小文件，它们会吃掉NameNode节点的大量内存。\nHadoop Archives可以有效的处理以上问题，它可以把多个文件归档成为一个文件，归档成一个文件后还可以透明的访问每一个文件。\n1 如何创建Archive\nUsage: hadoop archive -archiveName name -p \u0026lt;parent\u0026gt; \u0026lt;src\u0026gt;* \u0026lt;dest\u0026gt; -archiveName：归档后的文件名称 -p：指定要归档父文件夹，所有需要归档的文件都在此目录 其中-archiveName是指要创建的存档的名称。比如test.har，archive的名字的扩展名应该是.har*。 -p参数指定文件存档文件（src）的相对路径。\n举个例子：-p /foo/bar a/b/c e/f/g\n这里的/foo/bar是a/b/c与e/f/g的父路径，\n所以完整路径为/foo/bar/a/b/c与/foo/bar/e/f/g\n例如：如果你只想存档一个目录/input下的所有文件:\nhadoop archive -archiveName test.har -p /input /outputdir 这样就会在/outputdir目录下创建一个名为test.har的存档文件。\n2 如何查看Archive\n首先我们来看下创建好的har文件。使用如下的命令：\nhadoop fs -ls /outputdir/test.har 这里可以看到har文件包括：两个索引文件，多个part文件（本例只有一个）以及一个标识成功与否的文件。part文件是多个原文件的集合，根据index文件去找到原文件。\n例如上述的三个小文件1.txt 2.txt 3.txt内容分别为1，2，3。进行archive操作之后，三个小文件就归档到test.har里的part-0一个文件里。\narchive作为文件系统层暴露给外界。所以所有的fs shell命令都能在archive上运行，但是要使用不同的URI。\nHadoop Archives的URI是：\nhar://scheme-hostname:port/archivepath/fileinarchive\nscheme-hostname格式为hdfs-域名:端口，如果没有提供scheme-hostname，它会使用默认的文件系统。这种情况下URI是这种形式：\nhar:///archivepath/fileinarchive\n如果用har uri去访问的话，索引、标识等文件就会隐藏起来，只显示创建档案之前的原文件：\n案例：把/test目录下的1.txt、2.txt、3.txt进行归档操作\n数据集准备：\n归档：\n查看归档包内容：har://hdfs-集群名称\n查看归档包中文件的内容：\n解压归档包\n① 删除1.txt/2.txt/3.txt\n② 解压归档包\n3 如何解压Archive\n按顺序解压存档（串行）：\nHadoop fs -cp har:///user/zoo/foo.har/dir1 hdfs:/user/zoo/newdir 要并行解压存档，请使用DistCp：\nhadoop distcp har:///user/zoo/foo.har/dir1 hdfs:/user/zoo/newdir 4 Archive注意事项\nHadoop archives是特殊的档案格式。一个Hadoop archive对应一个文件系统目录。Hadoop archive的扩展名是*.har； 创建archives本质是运行一个Map/Reduce任务，所以应该在Hadoop集群上运行创建档案的命令； 创建archive文件要消耗和原文件一样多的硬盘空间（归档并没有压缩）； archive文件不支持压缩，尽管archive文件看起来像已经被压缩过； archive文件一旦创建就无法改变，要修改的话，需要创建新的archive文件。事实上，一般不会再对存档后的文件进行修改，因为它们是定期存档的，比如每周或每日； 当创建archive时，源文件不会被更改或删除； HDFS元数据管理机制 # namenode：管理元数据 datanode：管理实际数据 secondarynamenode：辅助namenode管理元数据 元数据管理概述 # HDFS元数据，按类型分，主要包括以下几个部分：\n持久化存储（HDFS集群中的文件的元数据信息）：\n文件、目录自身的属性信息，例如文件名，目录名，修改信息等。 文件记录的信息的存储相关的信息，例如存储块信息，分块情况，副本个数等。 非持久化存储：\n记录HDFS的Datanode的信息，用于DataNode的管理。 按形式分为内存元数据和元数据文件两种，分别存在内存和磁盘上。\nHDFS磁盘上元数据文件分为两类，用于持久化存储（① 文件自身属性 ② 文件存储相关信息）：\nfsimage 镜像文件：是元数据的一个持久化的检查点，包含Hadoop文件系统中的所有目录和文件元数据信息，但不包含文件块位置的信息。文件块位置信息只存储在内存中，是在 datanode加入集群的时候，namenode询问datanode得到的，并且间断的更新。 fsimage镜像文件：保存整个HDFS集群的元数据信息，不包含datanode节点信息。因为datanode数据都是放在内存元数据。\nedits 编辑日志：存放的是Hadoop文件系统的所有更改操作（文件创建，删除或修改）的日志，文件系统客户端执行的更改操作首先会被记录到edits文件中。 edits编辑日志：我们客户端向Hadoop的所有更改（增、删、改），都会被记录下来\nfsimage和edits文件都是经过序列化的，在NameNode启动的时候，它会将fsimage文件中的内容加载到内存中，之后再执行edits文件中的各项操作，使得内存中的元数据和实际的同步，存在内存中的元数据支持客户端的读操作，也是最完整的元数据。\n例子：\n上面两者内容合并得到下面的最新数据：\n当客户端对HDFS中的文件进行新增或者修改操作，操作记录首先被记入edits日志文件中，当客户端操作成功后，相应的元数据会更新到内存元数据中。因为fsimage文件一般都很大（GB级别的很常见），如果所有的更新操作都往fsimage文件中添加，这样会导致系统运行的十分缓慢。\nHDFS这种设计实现着手于：一是内存中数据更新、查询快，极大缩短了操作响应时间；二是内存中元数据丢失风险颇高（断电等），因此辅佐元数据镜像文件（fsimage）+编辑日志文件（edits）的备份机制进行确保元数据的安全。\nNameNode维护整个文件系统元数据。因此，元数据的准确管理，影响着HDFS提供文件存储服务的能力。\n在Hadoop的HDFS首次部署好配置文件之后，并不能马上启动使用，而是先要对文件系统进行格式化。需要在NameNode（NN）节点上进行如下的操作：\n$HADOOP_HOME/bin/hdfs namenode -format 在这里要注意两个概念，一个是文件系统，此时的文件系统在物理上还不存在；二就是此处的格式化并不是指传统意义上的本地磁盘格式化，而是一些清除与准备工作。\n格式化完成之后，将会在$dfs.namenode.name.dir/current目录下创建如下的文件结构，这个目录也正是namenode元数据相关的文件目录：\n其中的dfs.namenode.name.dir是在hdfs-site.xml文件中配置的，默认值如下：\nnamespaceID/clusterID/blockpoolID 这些都是HDFS集群的唯一标识符。标识符被用来防止DataNodes意外注册到另一个集群中的namenode上。这些标识在联邦（federation）部署中特别重要。联邦模式下，会有多个NameNode独立工作。每个的NameNode提供唯一的命名空间（namespaceID），并管理一组唯一的文件块池（blockpoolID）。clusterID将整个集群结合在一起作为单个逻辑单元，在集群中的所有节点上都是一样的。\nstorageType说明这个文件存储的是什么进程的数据结构信息（如果是DataNode，storageType=DATA_NODE）；\ncTime NameNode存储系统创建时间，首次格式化文件系统这个属性是0，当文件系统升级之后，该值会更新到升级之后的时间戳；\nlayoutVersion表示HDFS永久性数据结构的版本信息，是一个负整数。\n补充说明：\n格式化集群的时候，可以指定集群的cluster_id，但是不能与环境中其他集群有冲突。如果没有提供cluster_id，则会自动生成一个唯一的ClusterID。\n$HADOOP_HOME/bin/hdfs namenode -format -clusterId \u0026lt;cluster_id\u0026gt; seen_txid\n$dfs.namenode.name.dir/current/seen_txid非常重要，是存放transactionId的文件，format之后是0，它代表的是namenode里面的edits_*文件的尾数，namenode重启的时候，会按照seen_txid的数字，循序从头跑edits_0000001~到seen_txid的数字。所以当你的hdfs发生异常重启的时候，一定要比对seen_txid内的数字是不是你edits最后的尾数。\nFsimage \u0026amp; edits\n$dfs.namenode.name.dir/current目录下在format的同时也会生成fsimage和edits文件，及其对应的md5校验文件。\n面试题-fsimage是干什么的？里面保存的哪些数据？是否可以直接查看？\nfsimage文件其实是Hadoop文件系统元数据的一个永久性的检查点，其中包含Hadoop文件系统中的所有目录和文件idnode的序列化信息；\nfsimage包含Hadoop文件系统中的所有目录和文件idnode的序列化信息；对于文件来说，包含的信息有修改时间、访问时间、块大小和组成一个文件块信息等；而对于目录来说，包含的信息主要有修改时间、访问控制权限等信息。\n面试题-edits是干什么的？里面保存的哪些数据？是否可以直接查看？\nedits文件存放的是Hadoop文件系统的所有更新操作的路径，文件系统客户端执行的所以写操作首先会被记录到edits文件中。\n面试题-为什么要对两者之间进行合并操作呢？\n答：为了得到最新的元数据\nNameNode起来之后，HDFS中的更新操作会重新写到edits文件中，因为fsimage文件一般都很大（GB级别的很常见），如果所有的更新操作都往fsimage文件中添加，这样会导致系统运行的十分缓慢，但是如果往edits文件里面写就不会这样，每次执行写操作之后，且在向客户端发送成功代码之前，edits文件都需要同步更新。如果一个文件比较大，使得写操作需要向多台机器进行操作，只有当所有的写操作都执行完成之后，写操作才会返回成功，这样的好处是任何的操作都不会因为机器的故障而导致元数据的不同步。\n内容查看\n元数据存储位置：/export/data/hadoop-3.3.0/dfs/name/current\nfsimage、edits两个文件中的内容使用普通文本编辑器是无法直接查看的，幸运的是hadoop为此准备了专门的工具用于查看文件的内容，这些工具分别为oev和oiv，可以使用hdfs调用执行。\noev是offline edits viewer（离线edits查看器）的缩写，该工具只操作文件因而并不需要hadoop集群处于运行状态。\nhdfs oev -i edits_0000000000000000081-0000000000000000089 -o edits.xml -i,--inputFile \u0026lt;arg\u0026gt; -o,--outputFile \u0026lt;arg\u0026gt; Name of output file 在输出文件中，每个RECORD记录了一次操作，示例如下：\noiv是offline image viewer的缩写，用于将fsimage文件的内容转储到指定文件中以便于阅读，该工具还提供了只读的WebHDFS API以允许离线分析和检查hadoop集群的命名空间。oiv在处理非常大的fsimage文件时是相当快的，如果该工具不能够处理fsimage，它会直接退出。该工具不具备向后兼容性，比如使用hadoop-2.4版本的oiv不能处理hadoop-2.3版本的fsimage，只能使用hadoop-2.3版本的oiv。同oev一样，就像它的名称所提示的（offline），oiv也不需要hadoop集群处于运行状态。\nhdfs oiv -i fsimage_0000000000000000115 -p XML -o fsimage.xml 示例如下：\nsecondary namenode # NameNode职责是管理元数据信息，DataNode的职责是负责数据具体存储，那么SecondaryNameNode的作用是什么？对很多初学者来说是非常迷惑的。它为什么会出现在HDFS中。从它的名字上看，它给人的感觉就像是NameNode的备份。但它实际上却不是。\n大家猜想一下，当HDFS集群运行一段事件后，就会出现下面一些问题：\nedit logs文件会变的很大，怎么去管理这个文件是一个挑战。 NameNode重启会花费很长时间，因为有很多改动要合并到fsimage文件上。 如果NameNode挂掉了，那就丢失了一些改动。因为此时的fsimage文件非常旧。 因此为了克服这个问题，我们需要一个易于管理的机制来帮助我们减小edit logs文件的大小和得到一个最新的fsimage文件，这样也会减小在NameNode上的压力。这跟Windows的恢复点是非常像的，Windows的恢复点机制允许我们对OS进行快照，这样当系统发生问题时，我们能够回滚到最新的一次恢复点上。\nSecondaryNameNode就是来帮助解决上述问题的，它的职责是合并NameNode的edit logs到fsimage文件中。\nCheckpoint（2NN合并元数据文件过程） # 每达到触发条件，会由secondary namenode将namenode上积累的所有edits和一个最新的fsimage下载到本地，并加载到内存进行merge（这个过程称为checkpoint），如下图所示：\nNameNode管理着元数据信息，其中有两类持久化元数据文件：edits操作日志文件和fsimage元数据镜像文件。新的操作日志不会立即与fsimage进行合并，也不会刷到NameNode的内存中，而是会先写到edits中(因为合并需要消耗大量的资源)，操作成功之后更新至内存。 有dfs.namenode.checkpoint.period和dfs.namenode.checkpoint.txns 两个配置，只要达到这两个条件任何一个，secondarynamenode就会执行checkpoint的操作。 当触发checkpoint操作时，NameNode会生成一个新的edits即上图中的edits.new文件，同时SecondaryNameNode会将edits文件和fsimage复制到本地（HTTP GET方式）。 secondarynamenode将下载下来的fsimage载入到内存，然后一条一条地执行edits文件中的各项更新操作，使得内存中的fsimage保存最新，这个过程就是edits和fsimage文件合并，生成一个新的fsimage文件即上图中的Fsimage.ckpt文件。 secondarynamenode将新生成的Fsimage.ckpt文件复制到NameNode节点。 在NameNode节点的edits.new文件和Fsimage.ckpt文件会替换掉原来的edits文件和fsimage文件，至此刚好是一个轮回，即在NameNode中又是edits和fsimage文件。 等待下一次checkpoint触发SecondaryNameNode进行工作，一直这样循环操作。 Checkpoint触发条件\nCheckpoint操作受两个参数控制，可以通过core-site.xml进行配置：\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.checkpoint.period\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;3600\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt; 两次连续的checkpoint之间的时间间隔。默认1小时 \u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.checkpoint.txns\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;1000000\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt; 最大的没有执行checkpoint事务的数量，满足将强制执行紧急checkpoint，即使尚未达到检查点周期。默认设置为100万。 \u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; 从上面的描述我们可以看出，SecondaryNamenode根本就不是Namenode的一个热备，其只是将fsimage和edits合并。其拥有的fsimage不是最新的，因为在他从NameNode下载fsimage和edits文件时候，新的更新操作已经写到edit.new文件中去了。而这些更新在SecondaryNamenode是没有同步到的！当然，如果NameNode中的fsimage真的出问题了，还是可以用SecondaryNamenode中的fsimage替换一下NameNode上的fsimage，虽然已经不是最新的fsimage，但是我们可以将损失减小到最少！\nHDFS安全模式 # 安全模式概述 # 安全模式是HDFS所处的一种特殊状态，在这种状态下，文件系统只接受读数据请求，而不接受删除、修改等变更请求，是一种保护机制，用于保证集群中的数据块的安全性。\n1个Block块会产生3个副本 =\u0026gt; 必须保证有充足datanode节点进行存储。\n在NameNode主节点启动时，HDFS首先进入安全模式，集群会开始检查数据块的完整性。DataNode在启动的时候会向Namenode汇报可用的block信息，当整个系统达到安全标准时，HDFS自动离开安全模式。\n假设我们设置的副本数（即参数dfs.replication）是5，那么在Datanode上就应该有5个副本存在，假设只存在3个副本，那么比例就是3/5=0.6。在配置文件hdfs-default.xml中定义了一个最小的副本的副本率（即参数dfs.namenode.safemode.threshold-pct）0.999。\nhdfs-default.xml官方默认配置，我们本地配置hdfs-site.xml\n我们的副本率0.6明显小于0.99，因此系统会自动的复制副本到其他的DataNode,使得副本率不小于0.999.如果系统中有8个副本，超过我们设定的5个副本，那么系统也会删除多余的3个副本。\n如果HDFS处于安全模式下，不允许HDFS客户端进行任何修改文件的操作,包括上传文件，删除文件，重命名，创建文件夹，修改副本数等操作。\n安全模式配置 # 与安全模式相关主要配置在hdfs-site.xml文件中，主要有下面几个属性:\ndfs.namenode.replication.min: 每个数据块最小副本数量，默认为1。在上传文件时，达到最小副本数，就认为上传是成功的。\ndfs.namenode.safemode.threshold-pct: 达到最小副本数的数据块的百分比。默认为0.999f。当小于这个比例，那就将系统切换成安全模式，对数据块进行复制；当大于该比例时，就离开安全模式，说明系统有足够的数据块副本数，可以对外提供服务。\n0.999f/1(1:1)，3个副本=3个datanode节点\n所以dfs.namenode.safemode.threshold-pct此参数必须是大于0且小于1，如果值\u0026lt;=0，不进入安全模式；值=1，一直处于安全模式之间，临界值（特殊值），0代表不进入安全模式，=1代表直接进入安全模式！\ndfs.namenode.safemode.min.datanodes: 离开安全模式的最小可用datanode数量要求，默认为0。也就是即使所有datanode都不可用，仍然可以离开安全模式。\ndfs.namenode.safemode.extension: 当集群可用block比例，可用datanode都达到要求之后，如果在extension配置的时间段之后依然能满足要求，此时集群才离开安全模式。单位为毫秒，默认为30000.也就是当满足条件并且能够维持30秒之后，离开安全模式。 这个配置主要是对集群稳定程度做进一步的确认。避免达到要求后马上又不符合安全标准。\n总结一下，要离开安全模式，需要满足以下条件：\n1）达到副本数量要求的block比例满足要求；\n2）可用的datanode节点数满足配置的数量要求；\n3）1、2 两个条件满足后维持的时间达到配置的要求\n安全模式命令 # 手动进入安全模式\nhdfs dfsadmin -safemode enter 手动进入安全模式对于集群维护或者升级的时候非常有用，因为这时候HDFS上的数据是只读的。手动退出安全模式可以用下面命令：\nhdfs dfsadmin -safemode leave 如果你想获取到集群是否处于安全模式，可以用下面的命令获取：\nhdfs dfsadmin -safemode get ","date":"25 February 2024","permalink":"/posts/architecture/distributed/hadoop/hdfs/","section":"博客","summary":"HDFS是Hadoop Distribute File System 的简称，意为Hadoop分布式文件系统。是Hadoop核心组件之一，作为最底层的分布式存储服务而存在。分布式文件系统解决的问题就是大数据存储。它们是横跨在多台计算机上的存储系统。分布式文件系统在大数据时代有着广泛的应用前景，它们为存储和处理超大规模数据提供所需的扩展能力。","title":"Hadoop-HDFS"},{"content":"","date":"25 February 2024","permalink":"/tags/hdfs/","section":"Tags","summary":"","title":"Hdfs"},{"content":"","date":"22 February 2024","permalink":"/tags/java%E9%9D%A2%E7%BB%8F/","section":"Tags","summary":"","title":"Java面经"},{"content":"","date":"22 February 2024","permalink":"/posts/language/java/%E9%9D%A2%E7%BB%8F%E7%BC%A9%E5%87%8F%E7%89%88/","section":"博客","summary":"Java是一多用途、面向对象、跨平台编程语言，具备强大的生态系统和丰富的库支持。它的特点包括自动内存管理、垃圾回收、跨平台性、多线程支持、安全性和可移植性。Java广泛用于开发Web应用、移动应用、嵌入式系统和大型企业级应用程序。Java的特殊之处在于其独立的虚拟机（JVM），允许在不同平台上运行相同的Java程序，使其成为一种受欢迎的编程语言。","title":"Java面经"},{"content":"MQ有什么用？有哪些具体的使用场景？ # MQ：MessageQueue，消息队列。队列是一种FIFO先进先出的数据结构。消息由生产者发送到MQ进行排队，然后由消费者对消息进行处理。QQ、微信就是典型的MQ场景。\nMQ的作用主要有三个方面：\n异步：能提高系统的响应速度和吞吐量。更像是快递中的驿站，快递员-\u0026gt;菜鸟驿站\u0026lt;-客户。 解耦：服务之间解耦，可以减少服务之间的影响，提高系统的稳定性和可扩展性。另外，解耦之后可以实现数据分发。生产者发送一个消息后，可以由多个消费者来处理。 削峰：以稳定的系统资源应对突发的流量冲击。 MQ的缺点：\n系统可用性降低，一旦 MQ 宕机，整个业务会产生影响。高可用 系统复杂度提高，引入 MQ 之后，数据链路会变得很复杂。如何保证消息不丢失？消息不回重复调用？怎么保证消息的顺序性？ 数据一致性：A系统发消息，需要由B、C两个系统一同处理。如果B系统处理成功、C系统处理失败，就会造成数据一致性问题。 2. 如何进行产品选型？ # kafka\n优点：吞吐量非常大，性能非常好，集群高可用。 缺点：会丢数据，功能比较单一。 使用场景：日志分析，大数据采集。 rabbitMQ\n优点：消息可靠性高，性能全面。 缺点：吞吐量比较低，消息积累会严重影响性能。erlang语言不好定制。 使用场景：小规模场景。 rocketMQ\n优点：高吞吐、高性能、高可用，功能非常全面 缺点：开源版功能不如云上版本。官方文档和周边生态还不够成熟。客户端只支持JAVA。 使用场景：几乎是全场景。 特性 Kafka RabbitMQ RocketMQ 发行年份 2011 2007 2012 语言 Scala和Java Erlang和Java Java 协议 自定义协议（Kafka协议） AMQP、STOMP、MQTT 自定义协议 消息模型 Pub-Sub、队列 Pub-Sub、点对点 Pub-Sub、点对点 消息持久化 是 是 是 消息顺序 有序 有序 有序 分区 是 否 是 可用性 高 高 高 性能 高 一般 高 社区活跃度 高 高 高 部署复杂性 中 低 中 3. 如何保证消息不丢失？ # 哪些环节会造成消息丢失？\n怎么去防止消息丢失？\n生产者发送消息不丢失 kafka：消息发送+回调 rocketMQ： 消息发送+回调 事务消息 MQ主从消息不丢失 MQ消息存盘不丢失 MQ消费者消费消息不丢失 ","date":"22 February 2024","permalink":"/posts/language/java/%E9%9D%A2%E7%BB%8F%E7%BC%A9%E5%87%8F%E7%89%88/mq%E7%AF%87/","section":"博客","summary":"Java MQ（消息队列）是一种在分布式系统中用于实现异步通信的机制。它允许应用程序之间通过发送和接收消息进行通信，提供了解耦、异步、削峰填谷等特性，适用于解决分布式系统中的各种通信和协作问题。常见的 Java MQ 实现包括 RabbitMQ、ActiveMQ、Kafka 等，它们具有不同的特点和适用场景，可根据实际需求选择合适的实现。","title":"java面经-MQ篇"},{"content":"1. 说一说JVM的内存模型 # 2. JAVA类加载的过程是怎么样的？什么是双亲委派机制？有什么作用？一个对象从加载到JVM，再到GC清除，都经历了什么？ # public class ClassLoaderDemo { public static final String aaa = \u0026#34;aaaa\u0026#34;; public static void main(String[] args) throws Exception { // 父子关系 AppClassLoader \u0026lt;- ExtClassLoader \u0026lt;- Bootstrap ClassLoader ClassLoader cl1 = ClassLoaderDemo.class.getClassLoader(); System.out.println(\u0026#34;cl1 \u0026gt; \u0026#34; + cl1); System.out.println(\u0026#34;parent of cl1 \u0026gt; \u0026#34; + cl1.getParent()); // Bootstrap ClassLoader由C++开发，是JVM虚拟机的一部分，本身不是JAVA类。 System.out.println(\u0026#34;grand parent of cl1 \u0026gt; \u0026#34; + cl1.getParent().getParent()); // String, Int等基础类由Bootstrap ClassLoader加载。 ClassLoader cl2 = String.class.getClassLoader(); System.out.println(\u0026#34;cl2 \u0026gt; \u0026#34; + cl2); System.out.println(ClassLoaderDemo.class.getClassLoader().loadClass(\u0026#34;java.util.List\u0026#34;).getClassLoader()); // java指令可以通过增加-verbose:class -verbose:gc 参数在启动时打印出类加载情况 // Bootstrap ClassLoader，加载java基础类。这个属性不能在java指令中指定，推断不是由java语言处理。 System.out.println(\u0026#34;Bootstrap ClassLoader: \u0026#34; + System.getProperty(\u0026#34;sun.boot.class.path\u0026#34;)); // Extension ClassLoader 加载JAVA_HOME/ext 下的jar包。可通过-D java.ext.dirs另行指定目录 System.out.println(\u0026#34;Extension ClassLoader: \u0026#34; + System.getProperty(\u0026#34;java.ext.dirs\u0026#34;)); // AppClassLoader 加载CLASSPATH，应用下的Jar包。可通过-D java.class.path另行指定目录 System.out.println(\u0026#34;AppClassLoader: \u0026#34; + System.getProperty(\u0026#34;java.class.path\u0026#34;)); } } cl1 \u0026gt; jdk.internal.loader.ClassLoaders$AppClassLoader@2437c6dc parent of cl1 \u0026gt; jdk.internal.loader.ClassLoaders$PlatformClassLoader@76ed5528 grand parent of cl1 \u0026gt; null cl2 \u0026gt; null jdk.internal.loader.BuiltinClassLoader@6d06d69c Bootstrap ClassLoader: /opt/jdk-16.0.1/lib Extension ClassLoader: /opt/jdk-16.0.1/lib/ext:/usr/java/packages/lib/ext AppClassLoader: /home/user/example:/opt/jdk-16.0.1/lib JAVA 的类加载器：AppClassloader -\u0026gt; ExtClassloader -\u0026gt; Bootstrap ClassLoader\n在 Java 中，类加载器的层次结构是由 JVM 在启动时创建的，每个类加载器都有一个父类加载器（除了 Bootstrap ClassLoader）。当一个类加载器需要加载某个类时，它会先委托给其父类加载器进行加载，如果父类加载器无法加载，则会尝试自己加载。这种委托机制被称为双亲委派模型。\n在双亲委派模型中，AppClassLoader 是 Java 应用程序的类加载器，它的父类加载器是 ExtClassLoader，ExtClassLoader 的父类加载器是 Bootstrap ClassLoader。因此，当 AppClassLoader 需要加载某个类时，它会先委托给 ExtClassLoader，如果 ExtClassLoader 无法加载，则会继续委托给 Bootstrap ClassLoader，直到找到为止。这样可以保证类的加载顺序和加载的安全性，避免了类的重复加载和恶意代码的加载。\n每种类加载器都有自己的加载目录。\nAppClassLoader（或称为 System ClassLoader）：负责加载应用程序的类，其加载路径通常包括 CLASSPATH 和用户自定义的类路径。 ExtClassLoader（或称为 Extension ClassLoader）：负责加载 Java 扩展目录（JAVA_HOME/ext）下的类库。 Bootstrap ClassLoader：是虚拟机的内置类加载器，负责加载 Java 核心类库（例如 java.lang 包中的类），其加载路径是 JVM 的启动路径。 JAVA 中的类加载器的继承关系：\n/** * Loads the class with the specified \u0026lt;a href=\u0026#34;#binary-name\u0026#34;\u0026gt;binary name\u0026lt;/a\u0026gt;. The * default implementation of this method searches for classes in the * following order: * * \u0026lt;ol\u0026gt; * * \u0026lt;li\u0026gt;\u0026lt;p\u0026gt; Invoke {@link #findLoadedClass(String)} to check if the class * has already been loaded. \u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt; * * \u0026lt;li\u0026gt;\u0026lt;p\u0026gt; Invoke the {@link #loadClass(String) loadClass} method * on the parent class loader. If the parent is {@code null} the class * loader built into the virtual machine is used, instead. \u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt; * * \u0026lt;li\u0026gt;\u0026lt;p\u0026gt; Invoke the {@link #findClass(String)} method to find the * class. \u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt; * * \u0026lt;/ol\u0026gt; * * \u0026lt;p\u0026gt; If the class was found using the above steps, and the * {@code resolve} flag is true, this method will then invoke the {@link * #resolveClass(Class)} method on the resulting {@code Class} object. * * \u0026lt;p\u0026gt; Subclasses of {@code ClassLoader} are encouraged to override {@link * #findClass(String)}, rather than this method. \u0026lt;/p\u0026gt; * * \u0026lt;p\u0026gt; Unless overridden, this method synchronizes on the result of * {@link #getClassLoadingLock getClassLoadingLock} method * during the entire class loading process. * * @param name * The \u0026lt;a href=\u0026#34;#binary-name\u0026#34;\u0026gt;binary name\u0026lt;/a\u0026gt; of the class * * @param resolve * If {@code true} then resolve the class * * @return The resulting {@code Class} object * * @throws ClassNotFoundException * If the class could not be found */ protected Class\u0026lt;?\u0026gt; loadClass(String name, boolean resolve) throws ClassNotFoundException { synchronized (getClassLoadingLock(name)) { // First, check if the class has already been loaded // 每个类加载器对他加载过的类，都是有一个缓存的。 Class\u0026lt;?\u0026gt; c = findLoadedClass(name); if (c == null) { long t0 = System.nanoTime(); try { if (parent != null) { c = parent.loadClass(name, false); } else { c = findBootstrapClassOrNull(name); } } catch (ClassNotFoundException e) { // ClassNotFoundException thrown if class not found // from the non-null parent class loader } if (c == null) { // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); c = findClass(name); // this is the defining class loader; record the stats PerfCounter.getParentDelegationTime().addTime(t1 - t0); PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); PerfCounter.getFindClasses().increment(); } } if (resolve) { resolveClass(c); } return c; } } 每个类加载器对他加载过的类，都是有一个缓存的。 双亲委派：向上委托查找，向下委托加载。 作用：保护JAVA核心的类不回被应用程序覆盖。 类加载过程：加载-\u0026gt;连接-\u0026gt;初始化 加载：把Java的字节码数据加载到JVM内存当中，并映射成JVM认可的数据结构 连接：分为三个小阶段。 验证：检查加载到的字节信息是否符合JVM规范。 准备：创建类或接口的静态变量，并赋初始值，属于半初始化状态。 解析：把符号引用转为直接引用。 初始化： 一个对象从加载到JVM，再到GC清除，都经历了什么？\n用户创建一个对象，JVM首先需要到方法区区找对象的类型信息。然后再创建对象。 JVM要实例化一个对象，首先要在堆当中先创建一个对象。-\u0026gt; 半初始化状态 对象首先会分配在堆内存中新生代的Eden。然后经过一次Minor GCGC，如果对象存活，就会进入S区。在后续的每次GC中，如果对象一直存活，就会在S区来回拷贝，每移动一次，年龄加1。-\u0026gt;多大年龄才会移入老年代？年龄最大15，超过一定年龄后，对象转入老年代。 当方法执行结束后，栈中的指针会先移除掉。 堆中的对象，经过Full GC，就会被标记为垃圾，然后被GC线程清理掉。 ","date":"22 February 2024","permalink":"/posts/language/java/%E9%9D%A2%E7%BB%8F%E7%BC%A9%E5%87%8F%E7%89%88/jvm%E7%AF%87/","section":"博客","summary":"Java虚拟机（JVM）是Java程序的运行环境，负责将Java字节码解释或编译为本地机器代码，并提供垃圾回收、内存管理等功能。JVM包括类加载器、执行引擎、内存区域、垃圾回收器等组件，可在不同平台上运行Java程序，实现“一次编写，到处运行”的跨平台特性。通过JVM，Java程序实现了与平台无关的特性，提高了开发效率和代码的可移植性。","title":"java面经-JVM篇"},{"content":"1. TCP和UDP有什么区别？TCP为什么是三次握手，而不是两次？ # TCP（Transfer Control Protocol）是一种面向连接的、可靠的、传输层通信协议 特点： 好比是打电话，面向连接的，点对点的通信， 高可靠的， 效率比较低，占用的系统资源比较多。 UDP（User Dategram Protocol）是一种无连接的、不可靠的、传输层通信协议 特点： 好比是广播，不需要连接，发送方不管接收方有没有准备好，直接发消息， 可以进行广播发送的， 传输不可靠，有可能丢失消息， 效率比较高， 协议比较简单，占用的系统资源比较少。 TCP建立连接三次握手，断开连接四次挥手\n如果是两次握手，可能会造成连接资源浪费的情况。三次握手是确保双方建立可靠连接的过程：\n认双方能够通信：第一次握手由客户端发起，用于告知服务器客户端请求建立连接。服务器收到后能确认客户端发送正常，并对客户端进行响应。 确认服务器接收请求：第二次握手由服务器发起，用于告知客户端服务器已接收到连接请求，并准备好建立连接。客户端收到后可以确认服务器收到了自己的请求。 确认客户端接收响应：第三次握手由客户端发起，用于告知服务器客户端已接收到服务器的响应，并准备好发送数据。服务器收到后可以确认客户端能够接收到服务器的响应。 TCP 断开连接需要四次挥手的原因如下：\n客户端请求断开连接：客户端发送 FIN 报文给服务器，表示客户端不再发送数据，但仍可以接收数据。 服务器确认客户端的关闭请求：服务器收到 FIN 报文后，发送 ACK 报文确认收到，并进入 CLOSE_WAIT 状态。 服务器关闭连接：服务器不再发送数据后，发送 FIN 报文给客户端，表示服务器也准备好断开连接。 客户端确认服务器的关闭请求：客户端收到服务器的 FIN 报文后，发送 ACK 报文确认收到，并进入 TIME_WAIT 状态，等待可能延迟的 ACK 报文。在一段时间后，客户端关闭连接，服务器收到 ACK 后也关闭连接。 2. JAVA有哪几种IO模型？有什么区别？ # BIO 同步阻塞IO。可靠性差，吞吐量地，适用于连接比较少且比较固定的场景。JDK1.4之前唯一选择。BIO 编程模型最简单。 import java.io.*; import java.net.*; public class BioServer { public static void main(String[] args) { try { ServerSocket serverSocket = new ServerSocket(8080); System.out.println(\u0026#34;Server started, listening on port 8080...\u0026#34;); while (true) { Socket clientSocket = serverSocket.accept(); System.out.println(\u0026#34;Accepted connection from \u0026#34; + clientSocket); BufferedReader reader = new BufferedReader(new InputStreamReader(clientSocket.getInputStream())); BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(clientSocket.getOutputStream())); String request = reader.readLine(); System.out.println(\u0026#34;Received request: \u0026#34; + request); writer.write(\u0026#34;Hello from server\\n\u0026#34;); writer.flush(); clientSocket.close(); } } catch (IOException e) { e.printStackTrace(); } } } Server started, listening on port 8080... Accepted connection from Socket[addr=/127.0.0.1,port=56518,localport=8080] Received request: Hello from client import java.io.*; import java.net.*; public class BioClient { public static void main(String[] args) { try { Socket socket = new Socket(\u0026#34;localhost\u0026#34;, 8080); System.out.println(\u0026#34;Connected to server\u0026#34;); BufferedReader reader = new BufferedReader(new InputStreamReader(socket.getInputStream())); BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(socket.getOutputStream())); writer.write(\u0026#34;Hello from client\\n\u0026#34;); writer.flush(); String response = reader.readLine(); System.out.println(\u0026#34;Response from server: \u0026#34; + response); socket.close(); } catch (IOException e) { e.printStackTrace(); } } } Connected to server Response from server: Hello from server NIO 同步非阻塞IO。可靠性比较好，吞吐量也比较高，适用于连接比较多并且连接比较短（轻操作），例如聊天室。JDK1.4开始支持。NIO 编程模型最复杂。 import java.io.IOException; import java.net.InetSocketAddress; import java.nio.ByteBuffer; import java.nio.channels.SelectionKey; import java.nio.channels.Selector; import java.nio.channels.ServerSocketChannel; import java.nio.channels.SocketChannel; import java.util.Iterator; import java.util.Set; public class NioServer { public static void main(String[] args) { try { ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.bind(new InetSocketAddress(8080)); serverSocketChannel.configureBlocking(false); System.out.println(\u0026#34;Server started, listening on port 8080...\u0026#34;); Selector selector = Selector.open(); serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); while (true) { selector.select(); Set\u0026lt;SelectionKey\u0026gt; selectedKeys = selector.selectedKeys(); Iterator\u0026lt;SelectionKey\u0026gt; iterator = selectedKeys.iterator(); while (iterator.hasNext()) { SelectionKey key = iterator.next(); iterator.remove(); if (key.isAcceptable()) { ServerSocketChannel channel = (ServerSocketChannel) key.channel(); SocketChannel clientChannel = channel.accept(); clientChannel.configureBlocking(false); clientChannel.register(selector, SelectionKey.OP_READ); System.out.println(\u0026#34;Accepted connection from \u0026#34; + clientChannel); } else if (key.isReadable()) { SocketChannel clientChannel = (SocketChannel) key.channel(); ByteBuffer buffer = ByteBuffer.allocate(1024); int bytesRead = clientChannel.read(buffer); if (bytesRead \u0026gt; 0) { buffer.flip(); byte[] bytes = new byte[buffer.remaining()]; buffer.get(bytes); String request = new String(bytes); System.out.println(\u0026#34;Received request: \u0026#34; + request); ByteBuffer responseBuffer = ByteBuffer.wrap(\u0026#34;Hello from server\\n\u0026#34;.getBytes()); clientChannel.write(responseBuffer); } } } } } catch (IOException e) { e.printStackTrace(); } } } Server started, listening on port 8080... import java.io.IOException; import java.net.InetSocketAddress; import java.nio.ByteBuffer; import java.nio.channels.SocketChannel; public class NioClient { public static void main(String[] args) { try { SocketChannel socketChannel = SocketChannel.open(); socketChannel.connect(new InetSocketAddress(\u0026#34;localhost\u0026#34;, 8080)); System.out.println(\u0026#34;Connected to server\u0026#34;); ByteBuffer buffer = ByteBuffer.allocate(1024); buffer.put(\u0026#34;Hello from client\\n\u0026#34;.getBytes()); buffer.flip(); socketChannel.write(buffer); buffer.clear(); socketChannel.read(buffer); buffer.flip(); byte[] bytes = new byte[buffer.remaining()]; buffer.get(bytes); String response = new String(bytes); System.out.println(\u0026#34;Response from server: \u0026#34; + response); socketChannel.close(); } catch (IOException e) { e.printStackTrace(); } } } Connected to server Response from server: Hello from server AIO 异步非阻塞IO。可靠性是最好的，吞吐量也是非常高。适用于连接比较多，并且连接比较长（重操作）。例如，相册服务器。JDK7版本才开始支持。AIO 编程模型相对 NIO 比较简单。 import java.io.IOException; import java.net.InetSocketAddress; import java.nio.ByteBuffer; import java.nio.channels.AsynchronousServerSocketChannel; import java.nio.channels.AsynchronousSocketChannel; import java.nio.channels.CompletionHandler; public class AioServer { public static void main(String[] args) { try { AsynchronousServerSocketChannel serverSocketChannel = AsynchronousServerSocketChannel.open(); serverSocketChannel.bind(new InetSocketAddress(8080)); System.out.println(\u0026#34;Server started, listening on port 8080...\u0026#34;); serverSocketChannel.accept(null, new CompletionHandler\u0026lt;AsynchronousSocketChannel, Object\u0026gt;() { @Override public void completed(AsynchronousSocketChannel clientChannel, Object attachment) { serverSocketChannel.accept(null, this); // 接受下一个连接请求 try { System.out.println(\u0026#34;Accepted connection from \u0026#34; + clientChannel); ByteBuffer buffer = ByteBuffer.allocate(1024); buffer.put(\u0026#34;Hello from server\\n\u0026#34;.getBytes()); buffer.flip(); clientChannel.write(buffer, null, new CompletionHandler\u0026lt;Integer, Object\u0026gt;() { @Override public void completed(Integer result, Object attachment) { try { clientChannel.close(); } catch (IOException e) { e.printStackTrace(); } } @Override public void failed(Throwable exc, Object attachment) { exc.printStackTrace(); } }); } catch (Exception e) { e.printStackTrace(); } } @Override public void failed(Throwable exc, Object attachment) { exc.printStackTrace(); } }); Thread.currentThread().join(); // 让主线程阻塞，保持服务器运行 } catch (IOException | InterruptedException e) { e.printStackTrace(); } } } Server started, listening on port 8080... import java.io.IOException; import java.net.InetSocketAddress; import java.nio.ByteBuffer; import java.nio.channels.AsynchronousSocketChannel; import java.nio.channels.CompletionHandler; public class AioClient { public static void main(String[] args) { try { AsynchronousSocketChannel socketChannel = AsynchronousSocketChannel.open(); socketChannel.connect(new InetSocketAddress(\u0026#34;localhost\u0026#34;, 8080), null, new CompletionHandler\u0026lt;Void, Object\u0026gt;() { @Override public void completed(Void result, Object attachment) { System.out.println(\u0026#34;Connected to server\u0026#34;); ByteBuffer buffer = ByteBuffer.allocate(1024); buffer.put(\u0026#34;Hello from client\\n\u0026#34;.getBytes()); buffer.flip(); socketChannel.write(buffer, null, new CompletionHandler\u0026lt;Integer, Object\u0026gt;() { @Override public void completed(Integer result, Object attachment) { ByteBuffer readBuffer = ByteBuffer.allocate(1024); socketChannel.read(readBuffer, null, new CompletionHandler\u0026lt;Integer, Object\u0026gt;() { @Override public void completed(Integer result, Object attachment) { readBuffer.flip(); byte[] bytes = new byte[readBuffer.remaining()]; readBuffer.get(bytes); String response = new String(bytes); System.out.println(\u0026#34;Response from server: \u0026#34; + response); try { socketChannel.close(); } catch (IOException e) { e.printStackTrace(); } } @Override public void failed(Throwable exc, Object attachment) { exc.printStackTrace(); } }); } @Override public void failed(Throwable exc, Object attachment) { exc.printStackTrace(); } }); } @Override public void failed(Throwable exc, Object attachment) { exc.printStackTrace(); } }); Thread.currentThread().join(); // 让主线程阻塞，保持客户端运行 } catch (IOException | InterruptedException e) { e.printStackTrace(); } } } Connected to server Response from server: Hello from server 在一个网络请求中，客户端会发一个请求到服务端。\n同步、异步针对请求； 阻塞、非阻塞针对客户端。 客户端发送了请求后，就一直等着服务端响应。客户端：阻塞。请求：同步。 客户端发送了请求后，就去干别的事情了。时不时的过来检查服务端是否给出了响应。客户端：非阻塞。请求：同步。 换成异步请求。客户端发送了请求后，就坐在椅子上，等服务端返回响应。客户端：阻塞。请求：异步。 客户端发送了请求后，就去干别的事情了。等到服务端给出响应后，再过来处理业务逻辑。客户端：非阻塞。请求：异步。 3. JAVA NIO 的几个核心组件是什么？分别有什么作用？ # channel buffer selector\nchannel 类似于一个流。每个 channel 对应一个 buffer 缓冲区。 channel 会注册到 selector。 selector 会根据 channel 上发生的读写事件，将请求交由某个空闲的线程处理。 selector 对应一个或者多个线程。 buffer 和 channel 都是可读可写的。 4. select, poll 和 epoll 有什么区别？ # 他们是 NIO 中多路复用的三种实现机制，是由 Linux 操作系统提供的。\n用户空间和内核空间：操作系统为了保护系统安全，将内核划分为两个部分，一个是用户空间，一个是内核空间。用户空间不能直接访问底层的硬件设备，必须通过内核空间。\n文件描述符 File Descriptor（FD）：是一个抽象的概念，形式上是一个整数，实际上是一个索引值。指向内核中为每个进程维护进程所打开的文件的记录表。当程序打开一个文件或者创建一个文件时，内核就会向进程返回一个FD。Unix，Linux。\nselect 机制：会维护一个 FD 的结合 fd_set。将 fd_set 从用户空间复制到内核空间，激活 socket。 x64 2048 。fd_set 是一个数组结构 poll 机制：和 select 机制差不多，把 fd_set 结构进行了优化，FD集合的大小就突破了操作系统的限制。poolfd结构来代替 fd_set，通过链表实现的。 epoll 机制：在 Linux 2.6 版本提出。event poll。epoll中不再扫描所有的 FD，只将用户关心的 FD 的事件存放到内核的一个事件表当中。这样，就可以减少用户空间和内核空间之间需要拷贝的数据。 操作方式 底层实现 最大连接数 IO 效率 select 遍历 受限于内核 一般 poll 遍历 链表 无上限 ｜ 一般 epoll 事件回调 红黑树 无上限 高 java 的 NIO 当中使用的是哪种机制？\n可以查看 DefaultSelectorProvider 源码。在 windows 下，WindowsSelectorProvider。而 Linux 下，根据 Linux 的内核版本2.6以上，就是 EPollSelectorProvider，否则就是默认的 PollSelectorProvider。\nselect 1984年出现，poll 1997年出现，EPoll 2002年出现。\n5. 描述下 HTTP 和 HTTPS 的区别。 # HTTP：是互联网上应用最广泛的一种网络通信协议，基于TCP，可以是浏览器工作更为高效，减少网络传输。 HTTPS：是HTTP的加强版，可以认为是 HTTP + SSL（Secure Socket Layer）。在HTTP 的基础上增加了一系列的安全机制。一方面保证数据传输安全，另一方面对访问者增加了验证机制。是目前现行架构下，最为安全的解决方案。 主要区别：\nHTTP 的连接是简单无状态的，HTTPS 的数据传输是经过证书加密的，安全性更高。 HTTP 是免费的，而 HTTPS 需要申请证书，而证书通常是需要收费的，并且费用一般不低。 使用的端口不一样，HTTP 默认的是80端口，而HTTPS默认是443端口。 HTTPS的缺点：\nHTTPS 的握手协议比较费时，所以会影响服务的响应速度以及吞吐量。 HTTPS 也并不是完全安全的。他的证书体系其实并不是完全安全的。并且HTTPS在面对DDOS 这样的攻击时，几乎起不到任何作用。 证书需要费钱，并且功能越强大的证书费用越高。 ","date":"22 February 2024","permalink":"/posts/language/java/%E9%9D%A2%E7%BB%8F%E7%BC%A9%E5%87%8F%E7%89%88/%E7%BD%91%E7%BB%9C%E7%AF%87/","section":"博客","summary":"Java网络编程提供了一套强大的API，支持TCP、UDP、HTTP等协议。通过Socket和ServerSocket实现客户端与服务器的通信。NIO提供非阻塞IO，提高并发处理能力。URL、URLConnection可用于HTTP操作。Java的网络编程简洁、灵活，适用于构建各种网络应用，如Web服务器、网络爬虫等。","title":"java面经-网络篇"},{"content":"1. JAVA如何开启线程？怎么保证线程安全？ # 线程和进程的区别：\n进程是操作系统的进行资源分配的最小单元 线程是操作系统的进行任务分配的最小单元 如何开启线程？\n继承Thread类，重写run方法； class MyThread extends Thread { @Override public void run() { System.out.println(\u0026#34;MyThread running\u0026#34;); } } public class Main { public static void main(String[] args) { MyThread thread = new MyThread(); thread.start(); } } 实现Runnable接口，实现run方法； class MyRunnable implements Runnable { @Override public void run() { System.out.println(\u0026#34;MyRunnable running\u0026#34;); } } public class Main { public static void main(String[] args) { Thread thread = new Thread(new MyRunnable()); thread.start(); } } 实现Callable接口，实现call方法。通过FutureTask创建一个线程； import java.util.concurrent.Callable; import java.util.concurrent.FutureTask; class MyCallable implements Callable\u0026lt;String\u0026gt; { @Override public String call() { return \u0026#34;MyCallable running\u0026#34;; } } public class Main { public static void main(String[] args) throws Exception { FutureTask\u0026lt;String\u0026gt; futureTask = new FutureTask\u0026lt;\u0026gt;(new MyCallable()); Thread thread = new Thread(futureTask); thread.start(); System.out.println(futureTask.get()); } } 通过线程池来开启线程。 import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; class MyTask implements Runnable { @Override public void run() { System.out.println(\u0026#34;MyTask running\u0026#34;); } } public class Main { public static void main(String[] args) { ExecutorService executor = Executors.newFixedThreadPool(1); executor.execute(new MyTask()); executor.shutdown(); } } 怎么保证线程安全？\n加锁：\nJVM 提供的锁，也就是 sychronized 关键字 JDK 提供的各种锁 2. volatile 和 sychronized 有什么区别？ volatile 能不能保证线程安全？DCL（Double Check Lock）单例为什么要加 volatile ？ # volatile 和 sychronized 有什么区别？\nsychronized 关键字用来加锁； volatile 只是保持变量的线程可见性，通常使用于一个线程写，多个线程读的场景。 volatile 能不能保证线程安全？\n不能。volatile 关键字只能保证线程可见性，不能保证原子性。\npublic class VolatileDemo { private volatile boolean flag = false; public void toggleFlag() { flag = !flag; } public void printFlag() { System.out.println(\u0026#34;Flag is: \u0026#34; + flag); } public static void main(String[] args) { VolatileDemo demo = new VolatileDemo(); // 线程1：不断修改flag的值 Thread thread1 = new Thread(() -\u0026gt; { while (true) { demo.toggleFlag(); try { Thread.sleep(1000); // 等待1秒 } catch (InterruptedException e) { e.printStackTrace(); } } }); // 线程2：不断读取并打印flag的值 Thread thread2 = new Thread(() -\u0026gt; { while (true) { demo.printFlag(); try { Thread.sleep(1000); // 等待1秒 } catch (InterruptedException e) { e.printStackTrace(); } } }); // 启动线程 thread1.start(); thread2.start(); } } Flag is: false Flag is: true Flag is: false Flag is: true Flag is: false Flag is: true ... DCL（Double Check Lock）单例为什么要加 volatile ？\nvolatile 防止指令重排。在DCL中，防止高并发下情况下，指令重排造成的线程安全问题。\npublic class Singleton { private static volatile Singleton instance; private Singleton() { // 私有构造函数，防止外部实例化 } public static Singleton getInstance() { if (instance == null) { synchronized (Singleton.class) { if (instance == null) { instance = new Singleton(); } } } return instance; } } 如果没有使用 volatile 关键字修饰 instance，那么编译器和处理器可能会对上述代码进行重排序。重排序后的执行顺序可能是：\n创建一个新的 Singleton 实例。 将引用赋值给 instance。 进行对象初始化。 在这种情况下，如果有另一个线程在调用 getInstance() 方法时，可能会得到一个未完全初始化的对象，从而导致程序出现错误。\n而使用了 volatile 关键字修饰 instance 后，可以禁止指令重排序，确保对象的初始化操作发生在对象引用赋值操作之前，从而避免了上述问题。\nJVM 中 as-if-serial 原则 和 happens-before 原则\nInteger i = 8; 分配内存 初始化 建立指针对应关系 3. JAVA 线程锁机制是怎样的？偏向锁、轻量级锁、重量级锁有什么区别？锁机制是如何升级的？ # JAVA 的锁就是在对象的 Markword 中记录一个锁状态。四个不同的锁状态：\n无锁 偏向锁 轻量级锁 重量级锁 无锁：是一种并发编程的理想状态，表示在多个线程访问共享资源时不需要进行任何同步操作。在无锁状态下，所有线程都可以顺利地完成对共享资源的访问，不会发生阻塞或争用。\n偏向锁：是一种针对加锁对象只有一个线程访问的情况进行优化的锁机制。当一个线程获取了偏向锁后，如果再次访问同一个锁对象，无需进行任何同步操作，可以直接进入临界区，从而减少了不必要的竞争和上下文切换。\n轻量级锁（自旋锁）：是针对多个线程交替执行临界区代码的情况进行优化的锁机制。当一个线程尝试获取轻量级锁时，会将对象头部的标志位设置为偏向锁或轻量级锁，并将当前线程的ID保存到对象头中。如果其他线程也想获取该锁，则会进行自旋等待，避免进入重量级锁的阻塞状态。\n重量级锁（需要操作系统来组织）：是针对多个线程同时访问临界区代码的情况进行优化的锁机制。当多个线程争用同一个锁时，轻量级锁无法解决冲突，会升级为重量级锁。在重量级锁中，线程会进入阻塞状态，等待锁的释放，从而保证了临界区的互斥访问。\n锁之间的关系\n-XX:UseBiasedLocking : 是否打开偏向锁 -XX:BiasedLockingStartupDelay : 默认是4秒 程序首先检查偏向锁是否启用，然后在等待4秒后启动多个线程来竞争同一个锁对象。如果 -XX:UseBiasedLocking 打开，并且 -XX:BiasedLockingStartupDelay 设置为默认的4秒，则程序会在启动后4秒钟开始使用偏向锁。如果偏向锁启用，则第一个获取锁的线程将会获得偏向锁，并且其他线程会在竞争锁时自旋等待。\n4. 谈谈你对 AQS 对理解。AQS 如何实现可重入锁？ # AQS(AbstractQueuedSynchronizer) 是一个 JAVA 线程同步的框架。是 JDK 中很多锁工具的核心实现框架。\n在 AQS 中，维护了一个信号量 state 和一个线程组成的双向链表队列。其中，这个线程队列，就是用来给线程排队的，而 state 就像一个红绿灯，用来控制线程排队或者放行。在不同的场景下，有不同意义。\n在可重入锁的场景下，state 就用来表示加锁的次数。0 表示无锁，每加一次锁，state 就加1.释放锁 state 就减 1。\nimport java.util.concurrent.locks.ReentrantLock; public class ReentrantLockExample { private static final ReentrantLock lock = new ReentrantLock(); public static void main(String[] args) { new Thread(ReentrantLockExample::outerMethod).start(); new Thread(ReentrantLockExample::outerMethod).start(); } public static void outerMethod() { lock.lock(); try { System.out.println(\u0026#34;Outer method is called by \u0026#34; + Thread.currentThread().getName()); innerMethod(); } finally { lock.unlock(); } } public static void innerMethod() { lock.lock(); try { System.out.println(\u0026#34;Inner method is called by \u0026#34; + Thread.currentThread().getName()); } finally { lock.unlock(); } } } 5. 有 A、B、C 三个线程，如何保证三个线程同时执行？如何在并发情况下保证三个线程依次执行？如何保证三个线程有序交错进行？ # CountDownlatch、CylicBarrier、Semaphore\nCountDownLatch（倒计时门闩）： CountDownLatch 是一种同步辅助工具，它允许一个或多个线程等待其他线程执行完一组操作后再继续执行。它被初始化为一个计数值，每次调用 countDown() 方法都会将计数值减一。等待线程可以调用 await() 方法来阻塞，直到计数值变为零。通常用于某个线程需要等待其他线程执行完特定任务后才能继续执行的情况。\n案例：假设有一个主线程需要等待多个子线程完成某个任务后才能进行后续操作，可以使用 CountDownLatch。以下是一个简单的示例：\nimport java.util.concurrent.CountDownLatch; public class SimultaneousExecutionExample { public static void main(String[] args) throws InterruptedException { int numberOfThreads = 3; CountDownLatch startLatch = new CountDownLatch(1); for (int i = 0; i \u0026lt; numberOfThreads; i++) { new Thread(() -\u0026gt; { try { startLatch.await(); // 等待主线程释放信号 System.out.println(Thread.currentThread().getName() + \u0026#34; is running\u0026#34;); } catch (InterruptedException e) { e.printStackTrace(); } }).start(); } // 主线程释放信号，让所有线程同时执行 startLatch.countDown(); } } Thread-0 is running Thread-2 is running Thread-1 is running CyclicBarrier（循环屏障）： CyclicBarrier 是一种同步屏障，它允许一组线程互相等待，直到所有线程都到达指定的屏障点后再继续执行。它初始化时指定了线程数，每个线程调用 await() 方法来表示自己已经到达屏障，当所有线程都到达时，屏障就会打开，所有线程可以继续执行。与 CountDownLatch 不同，CyclicBarrier 可以重用。\n案例：假设有一个任务需要分成多个子任务并行执行，然后在所有子任务执行完成后合并结果，可以使用 CyclicBarrier。以下是一个示例：\nimport java.util.concurrent.CyclicBarrier; public class SequentialExecutionExample { public static void main(String[] args) { int numberOfThreads = 3; CyclicBarrier barrier = new CyclicBarrier(numberOfThreads); for (int i = 0; i \u0026lt; numberOfThreads; i++) { new Thread(() -\u0026gt; { try { System.out.println(Thread.currentThread().getName() + \u0026#34; is waiting\u0026#34;); barrier.await(); // 等待其他线程到达屏障 System.out.println(Thread.currentThread().getName() + \u0026#34; is running\u0026#34;); } catch (Exception e) { e.printStackTrace(); } }).start(); } } } Thread-0 is waiting Thread-1 is waiting Thread-2 is waiting Thread-0 is running Thread-1 is running Thread-2 is running Semaphore（信号量）： Semaphore 是一种用于控制对共享资源访问的同步辅助工具。它维护一组许可证，线程需要通过 acquire() 方法获取许可证，通过 release() 方法释放许可证。初始化时指定了许可证数量，每次调用 acquire() 时，许可证数量减一，如果许可证数量为零，则调用线程会被阻塞。通常用于控制对有限资源（如连接数、线程数等）的访问。\n案例：假设有一个连接池，最多只能同时有3个线程获取连接，其他线程需要等待连接释放后才能获取，可以使用 Semaphore。以下是一个示例：\nimport java.util.concurrent.Semaphore; public class OrderedInterleavedExecutionExample { public static void main(String[] args) { Semaphore semaphore1 = new Semaphore(1); Semaphore semaphore2 = new Semaphore(0); Semaphore semaphore3 = new Semaphore(0); new Thread(() -\u0026gt; { try { while (true) { semaphore1.acquire(); System.out.println(\u0026#34;Thread A is running\u0026#34;); semaphore2.release(); } } catch (InterruptedException e) { e.printStackTrace(); } }).start(); new Thread(() -\u0026gt; { try { while (true) { semaphore2.acquire(); System.out.println(\u0026#34;Thread B is running\u0026#34;); semaphore3.release(); } } catch (InterruptedException e) { e.printStackTrace(); } }).start(); new Thread(() -\u0026gt; { try { while (true) { semaphore3.acquire(); System.out.println(\u0026#34;Thread C is running\u0026#34;); semaphore1.release(); } } catch (InterruptedException e) { e.printStackTrace(); } }).start(); } } Thread A is running Thread B is running Thread C is running Thread A is running Thread B is running Thread C is running ... 6. 如何对一个字符串快速进行排序？ # Fork/Join 框架：是 Java 中用于并行执行任务的一个框架。它提供了一种将大型任务拆分成更小的子任务，并将这些子任务并行执行的机制。Fork/Join 框架的核心是工作窃取（work-stealing）算法，即当一个线程执行完自己的任务后，会去其他线程的任务队列中窃取任务来执行，从而实现了任务的动态负载均衡。Fork/Join 框架通常用于处理递归分治算法的任务，比如归并排序、快速排序等。在 Java 中，Fork/Join 框架主要通过 ForkJoinPool、ForkJoinTask 和 RecursiveTask 等类来实现。\nimport java.util.Arrays; import java.util.concurrent.ForkJoinPool; import java.util.concurrent.RecursiveAction; public class StringMergeSortExample { public static void main(String[] args) { String[] array = {\u0026#34;banana\u0026#34;, \u0026#34;apple\u0026#34;, \u0026#34;orange\u0026#34;, \u0026#34;grape\u0026#34;, \u0026#34;kiwi\u0026#34;, \u0026#34;peach\u0026#34;}; ForkJoinPool pool = new ForkJoinPool(); pool.invoke(new MergeSortTask(array)); System.out.println(Arrays.toString(array)); } static class MergeSortTask extends RecursiveAction { private final String[] array; private final int low; private final int high; public MergeSortTask(String[] array) { this(array, 0, array.length - 1); } public MergeSortTask(String[] array, int low, int high) { this.array = array; this.low = low; this.high = high; } @Override protected void compute() { if (low \u0026lt; high) { int mid = (low + high) \u0026gt;\u0026gt;\u0026gt; 1; MergeSortTask leftTask = new MergeSortTask(array, low, mid); MergeSortTask rightTask = new MergeSortTask(array, mid + 1, high); invokeAll(leftTask, rightTask); merge(array, low, mid, high); } } private void merge(String[] array, int low, int mid, int high) { String[] temp = new String[high - low + 1]; int i = low, j = mid + 1, k = 0; while (i \u0026lt;= mid \u0026amp;\u0026amp; j \u0026lt;= high) { if (array[i].compareTo(array[j]) \u0026lt;= 0) { temp[k++] = array[i++]; } else { temp[k++] = array[j++]; } } while (i \u0026lt;= mid) { temp[k++] = array[i++]; } while (j \u0026lt;= high) { temp[k++] = array[j++]; } System.arraycopy(temp, 0, array, low, temp.length); } } } 在这个示例中，我们定义了一个 MergeSortTask 类，继承自 RecursiveAction，用于执行归并排序。在 compute() 方法中，我们首先检查待排序的数组范围是否需要分解成子任务，如果需要，则创建两个子任务并调用 invokeAll() 方法并行执行。然后，在子任务执行完成后，再调用 merge() 方法进行合并。最后，我们通过创建 ForkJoinPool 实例并调用 invoke() 方法来启动任务。\n","date":"21 February 2024","permalink":"/posts/language/java/%E9%9D%A2%E7%BB%8F%E7%BC%A9%E5%87%8F%E7%89%88/%E5%B9%B6%E5%8F%91%E7%AF%87/","section":"博客","summary":"Java并发指的是在Java程序中同时执行多个任务的能力。通过多线程、同步机制、锁、线程池等工具，Java提供了丰富的并发编程支持。并发编程可以提高程序性能和资源利用率，但也面临线程安全、死锁等问题。Java并发编程涉及线程创建、管理、通信、同步等方面，包括使用synchronized关键字、Lock接口、线程池、并发集合等技术来实现并发控制和数据共享。","title":"java面经-并发篇"},{"content":"","date":"21 February 2024","permalink":"/tags/spring/","section":"Tags","summary":"","title":"Spring"},{"content":"常用注解 # Controller # 注解一个类表示控制器，Spring MVC会自动扫描标注了这个注解的类。\nRequestMapping # 请求路径映射，可以标注类，也可以是方法，可以指定请求类型，默认不指定为全部接收。\nRequestParam # 放在参数前，表示只能接收参数a=b格式的数据，即 Content-Type 为 application/x-www-form-urlencoded 类型的内容。\nRequestBody # 放在参数前，表示参数从request body中获取，而不是从地址栏获取，所以这肯定是接收一个POST请求的非a=b格式的数据，即 Content-Type 不为 application/x-www-form-urlencoded 类型的内容。\nResponseBody # 放在方法上或者返回类型前，表示此方法返回的数据放在 response body 里面，而不是跳转页面。一般用于ajax请求，返回json数据。\nRestController # 这个是Controller和ResponseBody的组合注解，表示@Controller标识的类里面的所有返回参数都放在response body里面。\nPathVariable # 路径绑定变量，用于绑定restful路径上的变量。\n@RequestHeader # 放在方法参数前，用来获取request header中的参数值。\n@CookieValue # 放在方法参数前，用来获取request header cookie中的参数值。\nGetMapping PostMapping PutMapping.. # *Mapping的是Spring4.3加入的新注解，表示特定的请求类型路径映射，而不需要写RequestMethod来指定请求类型。\nimport org.dom4j.util.UserDataElement; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.PathVariable; import org.springframework.web.bind.annotation.RequestBody; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RequestMethod; import org.springframework.web.bind.annotation.ResponseBody; @Controller @RequestMapping(\u0026#34;/test\u0026#34;) public class TestController { @RequestMapping(value = \u0026#34;/get/{no}\u0026#34;, method = RequestMethod.GET) @ResponseBody public Object get(@PathVariable(\u0026#34;no\u0026#34;) String no) { return new UserDataElement(\u0026#34;\u0026#34;); } @RequestMapping(value = \u0026#34;/save\u0026#34;, method = RequestMethod.POST) public void save(@RequestBody UserDataElement user) { } } ","date":"21 February 2024","permalink":"/posts/language/java/spring/spring-mvc%E5%B8%B8%E7%94%A8%E6%B3%A8%E8%A7%A3/","section":"博客","summary":"Spring MVC 是一个基于 Java 的 Web 应用开发框架，它通过提供模型-视图-控制器（MVC）架构来简化 Web 开发。Spring MVC 充分利用了依赖注入和面向切面编程等特性，使得开发者能够轻松地构建灵活、模块化的 Web 应用。","title":"Spring MVC常用注解"},{"content":" VO：（View Object）视图对象，一般位于Controller层，用于展示视图。 DTO：（Data Transfer Object）数据传输对象， 即RPC 接口请求或传输出去的对象，用于展示层与服务层之间的数据传输对象。 BO：（Business Object）业务层对象，一般位于Service层，它与 DO 会有一定的属性差别。 PO：（Persistent Object）持久化对象，对象属性与数据库字段形成映射关系。 DO：（Domain Object）领域对象，就是从现实世界中抽象出来的有形或无形的业务实体，可以当成BO；（Data Object）数据层对象，对象属性与数据库字段形成映射关系，可以当成PO VO与DTO # VO：展示层需要展示的数据。 DTO：业务逻辑层需要接收的数据和返回的数据。 VO与DTO的属性值基本相同，但是VO是DTO的最终解释，可以对DTO的字段进行适当删减。 举个例子：\n例子1\nDTO中返回的字段为\n{ \u0026#34;sex\u0026#34;: \u0026#34;女\u0026#34;, } 对于大部分需求来说中返回的字段为中返回的字段就已经够用了，但是如果说业务的需求是展示具体的身份，那就需要在VO层进行进一步解释， VO中返回的字段为：\n{ \u0026#34;sex\u0026#34;: \u0026#34;女演员\u0026#34;, } 例子2\nDTO\n{ \u0026#34;sex\u0026#34;: \u0026#34;男\u0026#34;, \u0026#34;age\u0026#34;: \u0026#34;17\u0026#34; } 如果说展示层只需要展示是否成年， VO：\n{ \u0026#34;age\u0026#34;: \u0026#34;未成年\u0026#34; } PO与DO # PO：通常来说是只有get/set方法的POJO，也就是常说的实体类。 DO：一般是PO的组合，也就是多表联查的映射。 例子1\nPO1：\n{ \u0026#34;id\u0026#34;: \u0026#34;999\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;ahzoo\u0026#34; } PO2：\n{ \u0026#34;userId\u0026#34;: \u0026#34;999\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;管理员\u0026#34; } DO：\n{ \u0026#34;id\u0026#34;: \u0026#34;999\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;ahzoo\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;管理员\u0026#34; } BO和PO # BO：与PO的区别是，BO会删掉一些不宜在展示层展示的数据，只对外提供必要的字段属性。 比如用户需要查询用户的信息，通常来说这个返回给用户的用户信息是不应该包含密码之类的一些字段的，那么我们就可以在BO层做一下处理。\n举例， 如果用PO层的话返回的字段为：\n{ \u0026#34;id\u0026#34;: \u0026#34;999\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;ahzoo\u0026#34;, \u0026#34;password\u0026#34;:\u0026#34;123456\u0026#34; } BO层的放回字段：\n{ \u0026#34;id\u0026#34;: \u0026#34;999\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;ahzoo\u0026#34; } ","date":"17 February 2024","permalink":"/posts/language/java/vodtobopodo%E5%8C%BA%E5%88%AB/","section":"博客","summary":"VO（View Object）视图对象；DTO（Data Transfer Object）数据传输对象；BO（Business Object）业务层对象；PO（Persistent Object）持久化对象；DO（Domain Object）领域对象。","title":"VO、DTO、BO、PO、DO区别"},{"content":" 对于普通的Java对象，当new的时候创建对象，当它没有任何引用的时候被垃圾回收机制回收。 而由Spring IoC容器托管的对象，它们的生命周期完全由容器控制。 概括Spring中Bean的生命周期 # Spring中每个Bean的生命周期如下：\n实例化Bean # BeanFactory和ApplicationContext是Spring框架中两个核心接口，它们都用于管理Bean的生命周期和依赖关系。ApplicationContext继承自BeanFactory接口，ApplicationContext提供了BeanFactory的所有功能，并且添加了更多企业级的特性。\n但是，BeanFactory和ApplicationContext在实例化Bean的时机和方式有一定区别。\nBeanFactory # public interface BeanFactory { Object getBean(String name) throws BeansException; } 惰性初始化：BeanFactory容器在默认情况下会延迟初始化Bean，即只有在第一次通过getBean()方法请求获取Bean时，或初始化bean的时候需要注入另一个尚未初始化的依赖时，BeanFactory容器才会调用createBean进行实例化和初始化。这种行为称为惰性初始化或按需初始化。 手动配置：对于某些需要提前初始化的Bean，可以通过配置Bean的lazy-init属性为false来实现非惰性初始化，但这需要手动配置。 // BeanFactory 创建 Bean 实例的伪代码 public Object createBean(String beanName) { BeanDefinition beanDefinition = getBeanDefinition(beanName); Object bean = instantiateBean(beanDefinition); BeanWrapper beanWrapper = new BeanWrapperImpl(bean); return beanWrapper.getWrappedInstance(); } ApplicationContext # ApplicationContext容器，当容器启动结束后，便实例化所有的bean。容器通过获取BeanDefinition对象中的信息进行实例化。并且这一步仅仅是简单的实例化，并未进行依赖注入。实例化对象被包装在BeanWrapper对象中，BeanWrapper提供了设置对象属性的接口，从而避免了使用反射机制设置属性。\npublic interface ApplicationContext extends EnvironmentCapable, ListableBeanFactory, HierarchicalBeanFactory, MessageSource, ApplicationEventPublisher, ResourcePatternResolver { } 预先实例化：与BeanFactory不同，ApplicationContext会在容器启动的过程中预先实例化和初始化所有的单例Bean（singleton scope）。这意味着，一旦ApplicationContext容器被创建并启动完成，所有的单例Bean就已经被创建并准备好了，无需等待getBean()方法的调用。 自动配置：ApplicationContext提供了更多的企业级特性，例如国际化支持、事件发布/订阅机制、注解支持等。这些特性往往需要容器在启动时预先加载和初始化相关的Bean，从而为这些高级特性提供支持。 设置对象属性（依赖注入） # 实例化后的对象被封装在BeanWrapper对象中。此时，对象仍然是一个原生的状态，Spring会根据BeanDefinition中的信息进行依赖注入。\npublic void populateBean(String beanName, BeanDefinition beanDefinition, BeanWrapper beanWrapper) { PropertyValues pvs = beanDefinition.getPropertyValues(); applyPropertyValues(beanName, beanWrapper, pvs); } 注入Aware接口 # Spring会检测Bean是否实现了某些Aware接口（如ApplicationContextAware, BeanNameAware等），并将相应的实例注入给Bean。\nif (bean instanceof ApplicationContextAware) { ((ApplicationContextAware)bean).setApplicationContext(this.applicationContext); } 在Spring框架中，Aware接口允许Bean获得对Spring容器或容器内特定资源的访问权。当一个Bean实现了某个Aware接口后，Spring容器在创建该Bean的过程中将会自动调用相应的Aware接口方法，注入相应的对象或信息。这种机制允许Bean与Spring容器进行交互，获取容器自身的一些信息。\n下面是一些常见的Aware接口及其作用：\nApplicationContextAware 用途：允许Bean获取到ApplicationContext的引用，即Spring容器本身。 方法：void setApplicationContext(ApplicationContext applicationContext) BeanNameAware 用途：允许Bean获取到自己在Spring容器中的名字。 方法：void setBeanName(String name) BeanFactoryAware 用途：允许Bean获取BeanFactory的引用，即Bean的工厂。 方法：void setBeanFactory(BeanFactory beanFactory) EnvironmentAware 用途：允许Bean获取到环境相关的信息，比如配置文件中的属性值。 方法：void setEnvironment(Environment environment) ResourceLoaderAware 用途：允许Bean获取到资源加载器，可以用来加载外部资源。 方法：void setResourceLoader(ResourceLoader resourceLoader) import org.springframework.context.ApplicationContext; import org.springframework.context.ApplicationContextAware; public class MyBean implements ApplicationContextAware { private ApplicationContext applicationContext; @Override public void setApplicationContext(ApplicationContext applicationContext) { this.applicationContext = applicationContext; } public void doSomething() { // 使用applicationContext进行某些操作 } } 在上面的例子中，MyBean 实现了ApplicationContextAware接口。Spring容器在创建MyBean的实例时，会自动调用setApplicationContext方法，并传入当前的ApplicationContext实例。这样，MyBean就能够使用ApplicationContext来执行一些操作，比如获取其他Bean的实例、访问资源文件等。\nBeanPostProcessor # BeanPostProcessor允许对Bean的实例进行额外的处理。这一步发生在Bean的初始化之前和之后。\n// BeanPostProcessor 的前置处理 public Object applyBeanPostProcessorsBeforeInitialization(Object existingBean, String beanName) { Object result = existingBean; for (BeanPostProcessor processor : this.beanPostProcessors) { result = processor.postProcessBeforeInitialization(result, beanName); if (result == null) return existingBean; } return result; } // BeanPostProcessor 的后置处理 public Object applyBeanPostProcessorsAfterInitialization(Object existingBean, String beanName) { Object result = existingBean; for (BeanPostProcessor processor : this.beanPostProcessors) { result = processor.postProcessAfterInitialization(result, beanName); if (result == null) return existingBean; } return result; } InitializingBean与init-method # 当BeanPostProcessor的前置处理完成后，如果Bean实现了InitializingBean接口，Spring将调用afterPropertiesSet方法。同时，如果在Bean定义中指定了init-method，Spring也会在这一阶段调用该方法。\nif (bean instanceof InitializingBean) { ((InitializingBean)bean).afterPropertiesSet(); } // 调用 init-method 指定的初始化方法 invokeInitMethods(String beanName, final Object bean, @Nullable BeanDefinition mbd); DisposableBean和destroy-method # 当Spring容器关闭时，如果Bean实现了DisposableBean接口，Spring将调用destroy方法。同时，如果在Bean定义中指定了destroy-method，Spring也会在这一阶段调用该方法。\nif (bean instanceof DisposableBean) { ((DisposableBean)bean).destroy(); } // 调用 destroy-method 指定的销毁方法 invokeCustomDestroyMethod(String beanName, final Object bean, String destroyMethodName); 示例 # 考虑一个简单的Bean，它实现了InitializingBean和DisposableBean接口，同时在Bean定义中指定了init-method和destroy-method。\npublic class MyBean implements InitializingBean, DisposableBean { private String property; public void setProperty(String property) { this.property = property; } @Override public void afterPropertiesSet() throws Exception { // 初始化逻辑 } @Override public void destroy() throws Exception { // 销毁逻辑 } public void customInit() { // 自定义初始化方法 } public void customDestroy() { // 自定义销毁方法 } } 在applicationContext.xml中配置Bean：\n\u0026lt;bean id=\u0026#34;myBean\u0026#34; class=\u0026#34;com.example.MyBean\u0026#34; init-method=\u0026#34;customInit\u0026#34; destroy-method=\u0026#34;customDestroy\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;property\u0026#34; value=\u0026#34;Some value\u0026#34;/\u0026gt; \u0026lt;/bean\u0026gt; 模拟面试 # 面试官：今天要不来聊聊Spring对Bean的生命周期管理？\n候选者：嗯，没问题的。很早之前我就看过源码，但Spring源码的实现类都太长了。我也记不得很清楚某些实现类的名字，要不我大概来说下流程？\n面试官：没事，你开始吧\n候选者：首先要知道的是普通Java对象和Spring所管理的Bean实例化的过程是有些区别的\n在普通Java环境下创建对象简要的步骤可以分为： java源码被编译为为class文件 等到类需要被初始化时（比如说new、反射等） class文件被虚拟机通过类加载器加载到JVM 初始化对象供我们使用 简单来说，可以理解为它是用Class对象作为「模板」进而创建出具体的实例\n而Spring所管理的Bean不同的是，除了Class对象之外，还会使用BeanDefinition的实例来描述对象的信息。比如说，我们可以在Spring所管理的Bean有一系列的描述：@Scope、@Lazy、@DependsOn等等，可以理解为Class只描述了类的信息，而BeanDefinition描述了对象的信息\n面试官：嗯，这我大致了解你的意思了。你就是想告诉我，Spring有BeanDefinition来存储着我们日常给Spring Bean定义的元数据（@Scope、@Lazy、@DependsOn等等），对吧？\n候选者：不愧是你\n面试官：赶紧的，继续吧\n候选者：Spring在启动的时候需要「扫描」在XML/注解/JavaConfig 中需要被Spring管理的Bean信息。随后，会将这些信息封装成BeanDefinition，最后会把这些信息放到一个beanDefinitionMap中。我记得这个Map的key应该是beanName，value则是BeanDefinition对象，到这里其实就是把定义的元数据加载起来，目前真实对象还没实例化。\n候选者：接着会遍历这个beanDefinitionMap，执行BeanFactoryPostProcessor这个Bean工厂后置处理器的逻辑。比如说，我们平时定义的占位符信息，就是通过BeanFactoryPostProcessor的子类PropertyPlaceholderConfigurer进行注入进去。\n占位符通常以${\u0026hellip;}的形式出现。例如，你可能在Spring的配置文件中有如下的定义：\n\u0026lt;bean id=\u0026#34;myBean\u0026#34; class=\u0026#34;com.example.MyBean\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;someProperty\u0026#34; value=\u0026#34;${my.config.value}\u0026#34; /\u0026gt; \u0026lt;/bean\u0026gt; 这里，${my.config.value}就是一个占位符，它的值将从Spring的配置源（如properties文件、环境变量等）中获取。\nPropertyPlaceholderConfigurer是BeanFactoryPostProcessor的一个子类，用于解析配置文件中的占位符，并用实际的配置值替换这些占位符。BeanFactoryPostProcessor是一个更广泛的接口，它允许对bean的定义（BeanDefinition）进行读取和修改，在容器实例化任何bean之前执行。\n从Spring 3.1开始，PropertyPlaceholderConfigurer已经被PropertySourcesPlaceholderConfigurer所取代，后者支持基于新的Environment和PropertySource抽象来解析占位符。使用PropertySourcesPlaceholderConfigurer可以更灵活地处理配置信息。\n在基于Java的配置中，你可以这样配置PropertySourcesPlaceholderConfigurer：\n@Configuration public class AppConfig { @Bean public static PropertySourcesPlaceholderConfigurer placeholderConfigurer() { PropertySourcesPlaceholderConfigurer configurer = new PropertySourcesPlaceholderConfigurer(); configurer.setLocation(new ClassPathResource(\u0026#34;application.properties\u0026#34;)); return configurer; } } 这段配置代码定义了一个PropertySourcesPlaceholderConfigurer bean，它会加载application.properties文件，并解析文件中定义的占位符。\n候选者：当然了，这里我们也可以自定义BeanFactoryPostProcessor来对我们定义好的Bean元数据进行获取或者修改。只是一般我们不会这样干，实际上也很有少的使用场景。\n面试官：嗯…\n候选者：BeanFactoryPostProcessor后置处理器执行完了以后，就到了实例化对象啦。在Spring里边是通过反射来实现的，一般情况下会通过反射选择合适的构造器来把对象实 例化。但这里把对象实例化，只是把对象给创建出来，而对象具体的属性是还没注入的。比如，我的对象是UserService，而UserService对象依赖着SendService对象，这时候的SendService还是null的\n候选者：所以，下一步就是把对象的相关属性给注入。\n候选者：相关属性注入完之后，往下接着就是初始化的工作了。首先判断该Bean是否实现了Aware相关的接口，如果存在则填充相关的资源。比如我这边在项目用到的：我希望通过代码程序的方式去获取指定的Spring Bean。我们这边会抽取成一个工具类，去实现ApplicationContextAware接口，来获取ApplicationContext对象进而获取Spring Bean。\n候选者：Aware相关的接口处理完之后，就会到BeanPostProcessor后置处理器啦。BeanPostProcessor后置处理器有两个方法，一个是before，一个是after（那肯定是 before先执行、after后执行）\n候选者：这个BeanPostProcessor后置处理器是AOP实现的关键（关键子类AnnotationAwareAspectJAutoProxyCreator）。所以，执行完Aware相关的接口就会执行BeanPostProcessor相关子类的before方法。BeanPostProcessor相关子类的before方法执行完，则执行init相关的方法，比如说@PostConstruct、实现了InitializingBean接口、定义的init-method方法。当时我还去官网去看他们的被调用，「执行顺序」分别是：@PostConstruct、实现了InitializingBean接口以及init-method方法。这些都是Spring给我们的「扩展」，像@PostConstruct我就经常用到。\n候选者：比如，对象实例化后，我要做些初始化的相关工作或者就启个线程去Kafka拉取数据，等到init方法 执行完之后，就会执行BeanPostProcessor的after方法。\n使用InitializingBean接口：通过实现InitializingBean接口的afterPropertiesSet方法，可以在所有必需的属性被设置后添加自定义的初始化逻辑。 import org.springframework.beans.factory.InitializingBean; public class MyBean implements InitializingBean { @Override public void afterPropertiesSet() throws Exception { // 初始化逻辑，例如启动线程去Kafka拉取数据 } } 使用@PostConstruct注解：@PostConstruct注解用于在依赖注入完成后执行初始化逻辑。 import javax.annotation.PostConstruct; public class MyBean { @PostConstruct public void init() { // 初始化逻辑 } } 定义init-method：在Spring的XML配置文件或者使用@Bean注解定义Bean时，可以指定init-method属性，Spring将在Bean的属性设置完成后调用这个方法。 public class MyBean { public void myInitMethod() { // 初始化逻辑 } } \u0026lt;bean id=\u0026#34;myBean\u0026#34; class=\u0026#34;com.example.MyBean\u0026#34; init-method=\u0026#34;myInitMethod\u0026#34;/\u0026gt; 或者使用Java配置：\n@Bean(initMethod = \u0026#34;myInitMethod\u0026#34;) public MyBean myBean() { return new MyBean(); } BeanPostProcessor的使用；在Bean的初始化方法执行完之后，BeanPostProcessor的postProcessAfterInitialization方法会被调用。这可以用来执行一些在Bean完全初始化后才能执行的后处理逻辑。 import org.springframework.beans.factory.config.BeanPostProcessor; public class MyBeanPostProcessor implements BeanPostProcessor { @Override public Object postProcessBeforeInitialization(Object bean, String beanName) { // 初始化之前的处理逻辑 return bean; } @Override public Object postProcessAfterInitialization(Object bean, String beanName) { // 初始化之后的处理逻辑，例如检查Bean是否需要某种代理 return bean; } } 通过上述任一方法，您都可以在对象实例化后进行必要的初始化工作。选择哪种方法取决于具体需求以及个人偏好。@PostConstruct和InitializingBean提供了依赖注入完成后立即执行初始化代码的方式，而BeanPostProcessor则提供了更为灵活的在Bean初始化前后执行自定义逻辑的能力。\n候选者：基本重要的流程已经走完了，我们就可以获取到对象去使用了\n候选者：销毁的时候就看有没有配置相关的destroy方法，执行就完事了\n面试官：嗯，了解，**你看过Spring是怎么解决循环依赖的吗？**如果现在有个A对象，它的属性是B对象，而B对象的属性也是A对象，说白了就是A依赖B，而B又依赖A，Spring是怎么做的？\n候选者：嗯，这块我也是看过的，其实也是在Spring的生命周期里面嘛。从上面我们可以知道，对象属性的注入在对象实例化之后的嘛。它的大致过程是这样的：\n首先A对象实例化，然后对属性进行注入，发现依赖B对象 B对象此时还没创建出来，所以转头去实例化B对象 B对象实例化之后，发现需要依赖A对象，那A对象已经实例化了嘛，所以B对象最终能完成创建 B对象返回到A对象的属性注入的方法上，A对象最终完成创建 上面就是大致的过程；\n面试官：听起来你还会原理哦？\n候选者：Absolutely。至于原理，其实就是用到了三级的缓存。所谓的三级缓存其实就是三个Map。首先明确一点，我对这里的三级缓存定义是这样的：\nsingletonObjects（一级，日常实际获取Bean的地方）； earlySingletonObjects（二级，还没进行属性注入，由三级缓存放进来）； singletonFactories（三级，Value是一个对象工厂）； 候选者：再回到刚才讲述的过程中，A对象实例化之后，属性注入之前，其实会把A对象放入三级缓存中。key是BeanName，Value是ObjectFactory。等到A对象属性注入时，发现依赖B，又去实例化B时，B属性注入需要去获取A对象，这里就是从三级缓存里拿出ObjectFactory，从ObjectFactory得到对应的Bean（就是对象A），把三级缓存的A记录给干掉，然后放到二级缓存中 候选者。显然，二级缓存存储的key是BeanName，value就是Bean（这里的Bean还没做完属性注入 相关的工作）。等到完全初始化之后，就会把二级缓存给remove掉，塞到一级缓存中。我们自己去getBean的时候，实际上拿到的是一级缓存的，大致的过程就是这样。\n面试官：那我想问一下，为什么是三级缓存？\n候选者：首先从第三级缓存说起（就是key是BeanName，Value为ObjectFactory）。我们的对象是单例的，有可能A对象依赖的B对象是有AOP的（B对象需要代理）。假设没有第三级缓存，只有第二级缓存（Value存对象，而不是工厂对象）。那如果有AOP的情况下，岂不是在存入第二级缓存之前都需要先去做AOP代理 ？这不合适嘛。\n候选者：这里肯定是需要考虑代理的情况的，比如A对象是一个被AOP增量的对象，B依赖A时，得到的A肯定是代理对象的。所以，三级缓存的Value是ObjectFactory，可以从里边拿到代理对象。而二级缓存存在的必要就是为了性能，从三级缓存的工厂里创建出对象，再扔到二级缓存（这样就不用每次都要从工厂里拿）。应该很好懂吧？\n面试官：确实。\n总结：首先是Spring Bean的生命周期过程，Spring使用BeanDefinition来装载着我们给Bean定义的元数据。实例化Bean的时候实际上就是遍历BeanDefinitionMap，Spring的Bean实例化和属性赋值是分开两步来做的，在Spring Bean的生命周期，Spring预留了很多的hook给我们去扩展：\nBean实例化之前有BeanFactoryPostProcessor Bean实例化之后，初始化时，有相关的Aware接口供我们去拿到Context相关信息 环绕着初始化阶段，有BeanPostProcessor（AOP的关键） 在初始化阶段，有各种的init方法供我们去自定义 而循环依赖的解决主要通过三级的缓存，在实例化后，会把自己扔到三级缓存（此时的key是BeanName，Value是ObjectFactory），在注入属性时，发现需要依赖B，也会走B的实例化过程，B属性注入依赖A，从三级缓存找到A，删掉三级缓存，放到二级缓存。\n关键源码方法 # 强烈建议自己去撸一遍\norg.springframework .context.support.AbstractApplicationContext#refresh (入口) org.springframework.context.supportAbstractApplicationContext#finishBeanFactoryInitialization (初始化单例对象入口) org.springframework.beans.factory.configConfigurableListableBeanFactory#preInstantiateSingletons (初始化单例对象入口) org.springframework.beans.factory.support.AbstractBeanFactory#getBean(java.lang.String) （万恶之源，获取并创建Bean的入口） org.springframework.beans.factory.support.AbstractBeanFactory#doGetBean （实际的获取并创建Bean的实现） org.springframework.beans.factory.support.DefaultSingletonBeanRegistry#getSingleton(java.lang.String) （从缓存中尝试获取） org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory#createBean(java.lang.String, org.springframework.beans.factory.support.RootBeanDefinition, java.lang.Object[])（实例化Bean） org.springframework.beans.factory.supportAbstractAutowireCapableBeanFactory#doCreateBean（实例化Bean具体实现） org.springframework.beans.factory.supportAbstractAutowireCapableBeanFactory#createBeanInstance（具体实例化过程） org.springframework.beans.factory.support.DefaultSingletonBeanRegistry#addSingletonFactory （将实例化后的Bean添加到三级缓存） org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory#populateBean （实例化后属性注入） org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory#initializeBean(java.lang.String, java.lang.Object, org.springframework.beans.factory.support.RootBeanDefinition) （初始化入口） ","date":"15 February 2024","permalink":"/posts/language/java/spring/spring%E4%B8%ADbean%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/","section":"博客","summary":"对于普通的Java对象，当new的时候创建对象，当它没有任何引用的时候被垃圾回收机制回收。 而由Spring IoC容器托管的对象，它们的生命周期完全由容器控制。 概括Spring中Bean的生命周期 # Spring中每个Bean的生命周期如下：","title":"Spring中Bean的生命周期"},{"content":"滑动窗口技巧 # 滑动窗口算法的思路非常简单，就是维护一个窗口，不断滑动，然后更新答案。\n该算法的大致逻辑如下：\nint left = 0, right = 0; while (right \u0026lt; s.size()) { // 增大窗口 window.add(s[right]); right++; while (window needs shrink) { // 缩小窗口 window.remove(s[left]); left++; } } 这个算法技巧的时间复杂度是 O(N)，比字符串暴力算法要高效得多。\n其实困扰大家的，不是算法的思路，而是各种细节问题。比如说如何向窗口中添加新元素，如何缩小窗口，在窗口滑动的哪个阶段更新结果。即便你明白了这些细节，也容易出 bug，找 bug 还不知道怎么找，真的挺让人心烦的。\n所以我写了一套滑动窗口算法的代码框架，并且给出了 debug 的输出，以后遇到相关的问题，就默写框架然后改三个地方就行：\n/* 滑动窗口算法框架 */ import java.util.HashMap; public class Main { void slidingWindow(String s) { HashMap\u0026lt;Character, Integer\u0026gt; window = new HashMap\u0026lt;\u0026gt;(); int left = 0, right = 0; while (right \u0026lt; s.length()) { // c 是将移入窗口的字符 char c = s.charAt(right); // 增大窗口 right++; // 进行窗口内数据的一系列更新 // 比如：window.put(c, window.getOrDefault(c, 0) + 1); ... /*** debug 输出的位置 ***/ // 注意在最终的解法代码中不要 print // 因为 IO 操作很耗时，可能导致超时 System.out.println(\u0026#34;window: [\u0026#34; + left + \u0026#34;, \u0026#34; + right + \u0026#34;)\u0026#34;); /********************/ // 判断左侧窗口是否要收缩 while (/* window needs shrink */) { // d 是将移出窗口的字符 char d = s.charAt(left); // 缩小窗口 left++; // 进行窗口内数据的一系列更新 // 比如：window.put(d, window.get(d) - 1); ... } } } public static void main(String[] args) { Main m = new Main(); m.slidingWindow(\u0026#34;your_test_string_here\u0026#34;); } } 其中两处 ... 表示的更新窗口数据的地方，到时候你直接往里面填就行了。\n而且，这两个 ... 处的操作分别是扩大和缩小窗口的更新操作，等会你会发现它们操作是完全对称的。\n另外，虽然滑动窗口代码框架中有一个嵌套的 while 循环，但算法的时间复杂度依然是 O(N)，其中 N 是输入字符串/数组的长度。\n为什么呢？简单说，字符串/数组中的每个元素都只会进入窗口一次，然后被移出窗口一次，不会说有某些元素多次进入和离开窗口，所以算法的时间复杂度就和字符串/数组的长度成正比。\n下面就直接上四道力扣原题来套这个框架，其中第一道题会详细说明其原理，后面四道就直接闭眼睛秒杀了。\n简单介绍一下一些用到的数据结构：\nunordered_map 就是哈希表（字典），相当于 Java 的 HashMap，它的一个方法 count(key) 相当于 Java 的 containsKey(key) 可以判断键 key 是否存在。\n可以使用方括号访问键对应的值 map[key]。需要注意的是，如果该 key 不存在，C++ 会自动创建这个 key，并把 map[key] 赋值为 0。所以代码中多次出现的 map[key]++ 相当于 Java 的 map.put(key, map.getOrDefault(key, 0) + 1)。\n另外，Java 中的 Integer 和 String 这种包装类不能直接用 == 进行相等判断，而应该使用类的 equals 方法，这个语言特性坑了不少读者，在代码部分我会给出具体提示。\n一、最小覆盖子串 # 先来看看力扣第 76 题「 最小覆盖子串」难度 Hard：\n就是说要在 S(source) 中找到包含 T(target) 中全部字母的一个子串，且这个子串一定是所有可能子串中最短的。\n如果我们使用暴力解法，代码大概是这样的：\nfor (int i = 0; i \u0026lt; s.size(); i++) for (int j = i + 1; j \u0026lt; s.size(); j++) if s[i:j] 包含 t 的所有字母: 更新答案 思路很直接，但是显然，这个算法的复杂度肯定大于 O(N^2) 了，不好。\n滑动窗口算法的思路是这样：\n我们在字符串 S 中使用双指针中的左右指针技巧，初始化 left = right = 0，把索引左闭右开区间 [left, right) 称为一个「窗口」。 PS：理论上你可以设计两端都开或者两端都闭的区间，但设计为左闭右开区间是最方便处理的。因为这样初始化 left = right = 0 时区间 [0, 0) 中没有元素，但只要让 right 向右移动（扩大）一位，区间 [0, 1) 就包含一个元素 0 了。如果你设置为两端都开的区间，那么让 right 向右移动一位后开区间 (0, 1) 仍然没有元素；如果你设置为两端都闭的区间，那么初始区间 [0, 0] 就包含了一个元素。这两种情况都会给边界处理带来不必要的麻烦。\n我们先不断地增加 right 指针扩大窗口 [left, right)，直到窗口中的字符串符合要求（包含了 T 中的所有字符）。 此时，我们停止增加 right，转而不断增加 left 指针缩小窗口 [left, right)，直到窗口中的字符串不再符合要求（不包含 T 中的所有字符了）。同时，每次增加 left，我们都要更新一轮结果。 重复第 2 和第 3 步，直到 right 到达字符串 S 的尽头。 这个思路其实也不难，第 2 步相当于在寻找一个「可行解」，然后第 3 步在优化这个「可行解」，最终找到最优解，也就是最短的覆盖子串。左右指针轮流前进，窗口大小增增减减，窗口不断向右滑动，这就是「滑动窗口」这个名字的来历。\n下面画图理解一下，needs 和 window 相当于计数器，分别记录 T 中字符出现次数和「窗口」中的相应字符的出现次数。\n初始状态：\n增加 right，直到窗口 [left, right) 包含了 T 中所有字符：\n现在开始增加 left，缩小窗口 [left, right)：\n直到窗口中的字符串不再符合要求，left 不再继续移动：\n之后重复上述过程，先移动 right，再移动 left…… 直到 right 指针到达字符串 S 的末端，算法结束。\n如果你能够理解上述过程，恭喜，你已经完全掌握了滑动窗口算法思想。现在我们来看看这个滑动窗口代码框架怎么用：\n首先，初始化 window 和 need 两个哈希表，记录窗口中的字符和需要凑齐的字符：\nunordered_map\u0026lt;char, int\u0026gt; need, window; for (char c : t) need[c]++; 然后，使用 left 和 right 变量初始化窗口的两端，不要忘了，区间 [left, right) 是左闭右开的，所以初始情况下窗口没有包含任何元素：\nint left = 0, right = 0; int valid = 0; while (right \u0026lt; s.size()) { // 开始滑动 } 其中 valid 变量表示窗口中满足 need 条件的字符个数，如果 valid 和 need.size 的大小相同，则说明窗口已满足条件，已经完全覆盖了串 T。\n现在开始套模板，只需要思考以下几个问题：\n什么时候应该移动 right 扩大窗口？窗口加入字符时，应该更新哪些数据？ 什么时候窗口应该暂停扩大，开始移动 left 缩小窗口？从窗口移出字符时，应该更新哪些数据？ 我们要的结果应该在扩大窗口时还是缩小窗口时进行更新？ 如果一个字符进入窗口，应该增加 window 计数器；如果一个字符将移出窗口的时候，应该减少 window 计数器；当 valid 满足 need 时应该收缩窗口；应该在收缩窗口的时候更新最终结果。\n下面是完整代码：\nstring minWindow(string s, string t) { unordered_map\u0026lt;char, int\u0026gt; need, window; for (char c : t) need[c]++; int left = 0, right = 0; int valid = 0; // 记录最小覆盖子串的起始索引及长度 int start = 0, len = INT_MAX; while (right \u0026lt; s.size()) { // c 是将移入窗口的字符 char c = s[right]; // 扩大窗口 right++; // 进行窗口内数据的一系列更新 if (need.count(c)) { window[c]++; if (window[c] == need[c]) valid++; } // 判断左侧窗口是否要收缩 while (valid == need.size()) { // 在这里更新最小覆盖子串 if (right - left \u0026lt; len) { start = left; len = right - left; } // d 是将移出窗口的字符 char d = s[left]; // 缩小窗口 left++; // 进行窗口内数据的一系列更新 if (need.count(d)) { if (window[d] == need[d]) valid--; window[d]--; } } } // 返回最小覆盖子串 return len == INT_MAX ? \u0026#34;\u0026#34; : s.substr(start, len); } PS：使用 Java 的读者要尤其警惕语言特性的陷阱。Java 的 Integer，String 等类型判定相等应该用 equals 方法而不能直接用等号 ==，这是 Java 包装类的一个隐晦细节。所以在缩小窗口更新数据的时候，不能直接改写为 window.get(d) == need.get(d)，而要用 window.get(d).equals(need.get(d))，之后的题目代码同理。\n需要注意的是，当我们发现某个字符在 window 的数量满足了 need 的需要，就要更新 valid，表示有一个字符已经满足要求。而且，你能发现，两次对窗口内数据的更新操作是完全对称的。\n当 valid == need.size() 时，说明 T 中所有字符已经被覆盖，已经得到一个可行的覆盖子串，现在应该开始收缩窗口了，以便得到「最小覆盖子串」。\n移动 left 收缩窗口时，窗口内的字符都是可行解，所以应该在收缩窗口的阶段进行最小覆盖子串的更新，以便从可行解中找到长度最短的最终结果。\n至此，应该可以完全理解这套框架了，滑动窗口算法又不难，就是细节问题让人烦得很。以后遇到滑动窗口算法，你就按照这框架写代码，保准没有 bug，还省事儿。\n下面就直接利用这套框架秒杀几道题吧，你基本上一眼就能看出思路了。\n二、字符串排列 # 这是力扣第 567 题「 字符串的排列」，难度中等：\n注意哦，输入的 s1 是可以包含重复字符的，所以这个题难度不小。\n这种题目，是明显的滑动窗口算法，相当给你一个 S 和一个 T，请问你 S 中是否存在一个子串，包含 T 中所有字符且不包含其他字符？\n首先，先复制粘贴之前的算法框架代码，然后明确刚才提出的几个问题，即可写出这道题的答案：\n// 判断 s 中是否存在 t 的排列 bool checkInclusion(string t, string s) { unordered_map\u0026lt;char, int\u0026gt; need, window; for (char c : t) need[c]++; int left = 0, right = 0; int valid = 0; while (right \u0026lt; s.size()) { char c = s[right]; right++; // 进行窗口内数据的一系列更新 if (need.count(c)) { window[c]++; if (window[c] == need[c]) valid++; } // 判断左侧窗口是否要收缩 while (right - left \u0026gt;= t.size()) { // 在这里判断是否找到了合法的子串 if (valid == need.size()) return true; char d = s[left]; left++; // 进行窗口内数据的一系列更新 if (need.count(d)) { if (window[d] == need[d]) valid--; window[d]--; } } } // 未找到符合条件的子串 return false; } 对于这道题的解法代码，基本上和最小覆盖子串一模一样，只需要改变几个地方：\n本题移动 left 缩小窗口的时机是窗口大小大于 t.size() 时，因为排列嘛，显然长度应该是一样的。 当发现 valid == need.size() 时，就说明窗口中就是一个合法的排列，所以立即返回 true。 至于如何处理窗口的扩大和缩小，和最小覆盖子串完全相同。\nPS：由于这道题中 [left, right) 其实维护的是一个定长的窗口，窗口大小为 t.size()。因为定长窗口每次向前滑动时只会移出一个字符，所以可以把内层的 while 改成 if，效果是一样的。\n三、找所有字母异位词 # 这是力扣第 438 题「 找到字符串中所有字母异位词」，难度中等：\n呵呵，这个所谓的字母异位词，不就是排列吗，搞个高端的说法就能糊弄人了吗？相当于，输入一个串 S，一个串 T，找到 S 中所有 T 的排列，返回它们的起始索引。\n直接默写一下框架，明确刚才讲的 4 个问题，即可秒杀这道题：\nvector\u0026lt;int\u0026gt; findAnagrams(string s, string t) { unordered_map\u0026lt;char, int\u0026gt; need, window; for (char c : t) need[c]++; int left = 0, right = 0; int valid = 0; vector\u0026lt;int\u0026gt; res; // 记录结果 while (right \u0026lt; s.size()) { char c = s[right]; right++; // 进行窗口内数据的一系列更新 if (need.count(c)) { window[c]++; if (window[c] == need[c]) valid++; } // 判断左侧窗口是否要收缩 while (right - left \u0026gt;= t.size()) { // 当窗口符合条件时，把起始索引加入 res if (valid == need.size()) res.push_back(left); char d = s[left]; left++; // 进行窗口内数据的一系列更新 if (need.count(d)) { if (window[d] == need[d]) valid--; window[d]--; } } } return res; } 跟寻找字符串的排列一样，只是找到一个合法异位词（排列）之后将起始索引加入 res 即可。\n四、最长无重复子串 # 这是力扣第 3 题「 无重复字符的最长子串」，难度中等：\n这个题终于有了点新意，不是一套框架就出答案，不过反而更简单了，稍微改一改框架就行了：\nclass Solution { public int lengthOfLongestSubstring(String s) { HashMap\u0026lt;Character, Integer\u0026gt; win = new HashMap\u0026lt;\u0026gt;(); int left = 0, right = 0; int res = 0; while (right \u0026lt; s.length()) { char c = s.charAt(right); win.put(c, win.getOrDefault(c, 0) + 1); right++; while (win.get(c) \u0026gt; 1) { char d = s.charAt(left); left++; if(win.containsKey(d)) win.put(d, win.getOrDefault(d, 0) - 1); } res = Math.max(res, right - left); } return res; } } 这就是变简单了，连 need 和 valid 都不需要，而且更新窗口内数据也只需要简单的更新计数器 window 即可。\n当 window[c] 值大于 1 时，说明窗口中存在重复字符，不符合条件，就该移动 left 缩小窗口了嘛。\n唯一需要注意的是，在哪里更新结果 res 呢？我们要的是最长无重复子串，哪一个阶段可以保证窗口中的字符串是没有重复的呢？\n这里和之前不一样，要在收缩窗口完成后更新 res，因为窗口收缩的 while 条件是存在重复元素，换句话说收缩完成后一定保证窗口中没有重复嘛。\n好了，滑动窗口算法模板就讲到这里，希望大家能理解其中的思想，记住算法模板并融会贯通。回顾一下，遇到子数组/子串相关的问题，你只要能回答出来以下几个问题，就能运用滑动窗口算法：\n什么时候应该扩大窗口？ 什么时候应该缩小窗口？ 什么时候应该更新答案？ 单调队列结构解决滑动窗口问题 # 单调队列就是一个「队列」，只是使用了一点巧妙的方法，使得队列中的元素全都是单调递增（或递减）的。\n为啥要发明「单调队列」这种结构呢，主要是为了解决下面这个场景：\n给你一个数组 window，已知其最值为 A，如果给 window 中添加一个数 B，那么比较一下 A 和 B 就可以立即算出新的最值；但如果要从 window 数组中减少一个数，就不能直接得到最值了，因为如果减少的这个数恰好是 A，就需要遍历 window 中的所有元素重新寻找新的最值。\n这个场景很常见，但不用单调队列似乎也可以，比如优先级队列也是一种特殊的队列，专门用来动态寻找最值的，我创建一个大（小）顶堆，不就可以很快拿到最大（小）值了吗？\n如果单纯地维护最值的话，优先级队列很专业，队头元素就是最值。但优先级队列无法满足标准队列结构「先进先出」的时间顺序，因为优先级队列底层利用二叉堆对元素进行动态排序，元素的出队顺序是元素的大小顺序，和入队的先后顺序完全没有关系。\n所以，现在需要一种新的队列结构，既能够维护队列元素「先进先出」的时间顺序，又能够正确维护队列中所有元素的最值，这就是「单调队列」结构。\n「单调队列」这个数据结构主要用来辅助解决滑动窗口相关的问题，滑动窗口算法可以作为双指针技巧的一部分，但有些稍微复杂的滑动窗口问题不能只靠两个指针来解决，需要上更先进的数据结构。比如说，判断一个窗口中最值的问题，你就无法单凭移出窗口的那个元素更新窗口的最值，除非重新遍历所有元素，但这样的话时间复杂度就上来了，这是我们不希望看到的。\n我们来看看力扣第 239 题「 滑动窗口最大值」，就是一道标准的滑动窗口问题：\n给你输入一个数组 nums 和一个正整数 k，有一个大小为 k 的窗口在 nums 上从左至右滑动，请你输出每次窗口中 k 个元素的最大值。\n函数签名如下：\nint[] maxSlidingWindow(int[] nums, int k); 比如说力扣给出的一个示例：\n接下来，我们就借助单调队列结构，用 O(1) 时间算出每个滑动窗口中的最大值，使得整个算法在线性时间完成。\n一、搭建解题框架 # 在介绍「单调队列」这种数据结构的 API 之前，先来看看一个普通的队列的标准 API：\nclass Queue { // enqueue 操作，在队尾加入元素 n void push(int n); // dequeue 操作，删除队头元素 void pop(); } 我们要实现的「单调队列」的 API 也差不多：\nclass MonotonicQueue { // 在队尾添加元素 n void push(int n); // 返回当前队列中的最大值 int max(); // 队头元素如果是 n，删除它 void pop(int n); } 当然，这几个 API 的实现方法肯定跟一般的 Queue 不一样，不过我们暂且不管，而且认为这几个操作的时间复杂度都是 O(1)，先把这道「滑动窗口」问题的解答框架搭出来：\nint[] maxSlidingWindow(int[] nums, int k) { MonotonicQueue window = new MonotonicQueue(); List\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; nums.length; i++) { if (i \u0026lt; k - 1) { //先把窗口的前 k - 1 填满 window.push(nums[i]); } else { // 窗口开始向前滑动 // 移入新元素 window.push(nums[i]); // 将当前窗口中的最大元素记入结果 res.add(window.max()); // 移出最后的元素 window.pop(nums[i - k + 1]); } } // 将 List 类型转化成 int[] 数组作为返回值 int[] arr = new int[res.size()]; for (int i = 0; i \u0026lt; res.size(); i++) { arr[i] = res.get(i); } return arr; } 这个思路很简单，能理解吧？下面我们开始重头戏，单调队列的实现。\n二、实现单调队列数据结构 # 观察滑动窗口的过程就能发现，实现「单调队列」必须使用一种数据结构支持在头部和尾部进行插入和删除，很明显双链表是满足这个条件的。\n「单调队列」的核心思路和「单调栈」类似，push 方法依然在队尾添加元素，但是要把前面比自己小的元素都删掉：\nclass MonotonicQueue { // 双链表，支持头部和尾部增删元素 // 维护其中的元素自尾部到头部单调递增 private LinkedList\u0026lt;Integer\u0026gt; maxq = new LinkedList\u0026lt;\u0026gt;(); // 在尾部添加一个元素 n，维护 maxq 的单调性质 public void push(int n) { // 将前面小于自己的元素都删除 while (!maxq.isEmpty() \u0026amp;\u0026amp; maxq.getLast() \u0026lt; n) { maxq.pollLast(); } maxq.addLast(n); } 你可以想象，加入数字的大小代表人的体重，把前面体重不足的都压扁了，直到遇到更大的量级才停住。\n如果每个元素被加入时都这样操作，最终单调队列中的元素大小就会保持一个单调递减的顺序，因此我们的 max 方法可以可以这样写：\npublic int max() { // 队头的元素肯定是最大的 return maxq.getFirst(); } pop 方法在队头删除元素 n，也很好写：\npublic void pop(int n) { if (n == maxq.getFirst()) { maxq.pollFirst(); } } 之所以要判断 data.getFirst() == n，是因为我们想删除的队头元素 n 可能已经被「压扁」了，可能已经不存在了，所以这时候就不用删除了：\n至此，单调队列设计完毕，看下完整的解题代码：\n/* 单调队列的实现 */ class MonotonicQueue { LinkedList\u0026lt;Integer\u0026gt; maxq = new LinkedList\u0026lt;\u0026gt;(); public void push(int n) { // 将小于 n 的元素全部删除 while (!maxq.isEmpty() \u0026amp;\u0026amp; maxq.getLast() \u0026lt; n) { maxq.pollLast(); } // 然后将 n 加入尾部 maxq.addLast(n); } public int max() { return maxq.getFirst(); } public void pop(int n) { if (n == maxq.getFirst()) { maxq.pollFirst(); } } } /* 解题函数的实现 */ int[] maxSlidingWindow(int[] nums, int k) { MonotonicQueue window = new MonotonicQueue(); List\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; nums.length; i++) { if (i \u0026lt; k - 1) { //先填满窗口的前 k - 1 window.push(nums[i]); } else { // 窗口向前滑动，加入新数字 window.push(nums[i]); // 记录当前窗口的最大值 res.add(window.max()); // 移出旧数字 window.pop(nums[i - k + 1]); } } // 需要转成 int[] 数组再返回 int[] arr = new int[res.size()]; for (int i = 0; i \u0026lt; res.size(); i++) { arr[i] = res.get(i); } return arr; } 有一点细节问题不要忽略，在实现 MonotonicQueue 时，我们使用了 Java 的 LinkedList，因为链表结构支持在头部和尾部快速增删元素；而在解法代码中的 res 则使用的 ArrayList 结构，因为后续会按照索引取元素，所以数组结构更合适。\n关于单调队列 API 的时间复杂度，读者可能有疑惑：push 操作中含有 while 循环，时间复杂度应该不是 O(1) 呀，那么本算法的时间复杂度应该不是线性时间吧？\n这里就用到了 算法时空复杂度分析使用手册 中讲到的摊还分析：\n单独看 push 操作的复杂度确实不是 O(1)，但是算法整体的复杂度依然是 O(N) 线性时间。要这样想，nums 中的每个元素最多被 push 和 pop 一次，没有任何多余操作，所以整体的复杂度还是 O(N)。空间复杂度就很简单了，就是窗口的大小 O(k)。\n三、拓展延伸 # 最后，我提出几个问题请大家思考：\n本文给出的 MonotonicQueue 类只实现了 max 方法，你是否能够再额外添加一个 min 方法，在 O(1) 的时间返回队列中所有元素的最小值？ 本文给出的 MonotonicQueue 类的 pop 方法还需要接收一个参数，这显然有悖于标准队列的做法，请你修复这个缺陷。 请你实现 MonotonicQueue 类的 size 方法，返回单调队列中元素的个数（注意，由于每次 push 方法都可能从底层的 q 列表中删除元素，所以 q 中的元素个数并不是单调队列的元素个数）。 也就是说，你是否能够实现单调队列的通用实现：\n/* 单调队列的通用实现，可以高效维护最大值和最小值 */ class MonotonicQueue\u0026lt;E extends Comparable\u0026lt;E\u0026gt;\u0026gt; { // 标准队列 API，向队尾加入元素 public void push(E elem); // 标准队列 API，从队头弹出元素，符合先进先出的顺序 public E pop(); // 标准队列 API，返回队列中的元素个数 public int size(); // 单调队列特有 API，O(1) 时间计算队列中元素的最大值 public E max(); // 单调队列特有 API，O(1) 时间计算队列中元素的最小值 public E min(); } Java LinkedList详解 # LinkedList 是 List 接口的实现类，这意味着它可以根据索引来随机访问集合中的元素。除此之外，LinkedList 还实现了 Deque 接口，所以可以被当成“双端队列”来使用，还可以被当成 栈 来使用。\nLinkedList 不是线程安全的，如果想使用线程安全的 LinkedList，可以通过如下方式实现：\n1. 构造器 # LinkedList(): 空参构造器，创建一个空的列表 LinkedList(Collection\u0026lt;? extends E\u0026gt; c)：构造一个包含指定Collection中元素的列表，列表中元素的顺序跟Collection的迭代器返回元素的顺序一样。 2. 常用方法 # LinkedList里的方法挺多的，有些方法虽然名字不一样，但其实作用完全一样。这里仅挑几个有代表性的方法，并且按照使用场景归类。\n2.1 作为List集合 # boolean add(E e)：将指定的元素追加到列表的结尾； void add(int index, E element)：向列表的指定位置插入一个元素。将当前位于该位置的元素（如果有的话）和它的后续元素向右移动，并且将这些向右移动的元素的索引+1; E get(int index)：返回列表中指定位置的元素; E set(int index, E element)：替换列表中指定位置的元素，返回被替换的元素； E remove(int index)：移除列表中指定位置的元素，将该位置元素的后续元素整体向左移动，并将这些元素的索引-1，最后返回被移除的元素； int size()：返回列表中元素的个数。 2.2 作为队列 # boolean offer(E e)：将指定元素添加到队列尾部； boolean offerLast(E e)：同上； boolean offerFirst(E e)：将指定元素添加到队列头部； E peek()：获取但不删除队列中第一个元素； E peekFirst()：同上； E peekLast()：获取但不删除队列最后一个元素； E poll()：获取并删除队列中第一个元素； E pollFirst()：同上； E pollLast()：获取并删除队列最后一个元素。 2.3 作为栈 # push(E e)：将一个元素压入栈顶，也就是向列表头部插入一个元素； E pop()：弹出栈顶元素，即移除列表第一个元素。 3. 源码解析 # 核心的就是操作链表的方法，对外提供的一些 public 方法其实最终调用的都是这些操作链表的方法。\n/** *用于记录链表的大小 */ transient int size = 0; /** * 链表第一个节点的引用 */ transient Node\u0026lt;E\u0026gt; first; /** * 链表最后一个节点的引用 */ transient Node\u0026lt;E\u0026gt; last; /** * 私有的静态内部类，用于描述双向链表的节点 */ private static class Node\u0026lt;E\u0026gt; { E item; // 节点的值，也就是List中的元素 Node\u0026lt;E\u0026gt; next; // 直接前驱节点 Node\u0026lt;E\u0026gt; prev; // 直接后继节点 Node(Node\u0026lt;E\u0026gt; prev, E element, Node\u0026lt;E\u0026gt; next) { this.item = element; this.next = next; this.prev = prev; } } /** * 向指定索引处插入元素 */ public void add(int index, E element) { // 检查index是否合法，index \u0026gt;= 0 \u0026amp;\u0026amp; index \u0026lt;= size checkPositionIndex(index); if (index == size) linkLast(element); // 追加到链表尾部 else linkBefore(element, node(index)); } /** * 将元素添加到链表尾部。 */ void linkLast(E e) { final Node\u0026lt;E\u0026gt; l = last; // 记录链表的最后一个节点 // 创建新节点，设置其前驱节点为l，值为e，后继节点为null final Node\u0026lt;E\u0026gt; newNode = new Node\u0026lt;\u0026gt;(l, e, null); last = newNode; // last指向新创建的这个节点 /*如果l指向的是null（注意，此时l和last指向的已经不是同一个对象了）， 说明newNode既是第一个节点，又是最后一个节点，也就是说newNode是链表中唯一的一个节点*/ if (l == null) first = newNode; else l.next = newNode; // l的后继节点设置为newNode size++; modCount++; } /** * 返回指定索引处的节点 */ Node\u0026lt;E\u0026gt; node(int index) { // assert isElementIndex(index); // 可以看出这里是通过遍历的方式查找index处的节点。 // 为了减少遍历的节点个数，提高查找效率，这里判断了index是在链表的前半段还是后半段， // 从而实现从半个链表中查找index处的节点 if (index \u0026lt; (size \u0026gt;\u0026gt; 1)) { Node\u0026lt;E\u0026gt; x = first; for (int i = 0; i \u0026lt; index; i++) x = x.next; return x; } else { Node\u0026lt;E\u0026gt; x = last; for (int i = size - 1; i \u0026gt; index; i--) x = x.prev; return x; } } /** * 将元素包装成链表节点插入到指定节点的前面 */ void linkBefore(E e, Node\u0026lt;E\u0026gt; succ) { // assert succ != null; final Node\u0026lt;E\u0026gt; pred = succ.prev; // 记录succ的前驱节点 final Node\u0026lt;E\u0026gt; newNode = new Node\u0026lt;\u0026gt;(pred, e, succ); // 创建新节点newNode，值为e succ.prev = newNode; // 将succ的前驱节点设置为newNode if (pred == null) // 此时链表的第一个节点是newNode first = newNode; else pred.next = newNode; size++; modCount++; } /** * 获取指定索引处的元素 */ public E get(int index) { // 索引合法性检查。index \u0026gt;= 0 \u0026amp;\u0026amp; index \u0026lt; size checkElementIndex(index); return node(index).item; // 通过遍历查找节点并获取元素 } /** * 替换列表中指定位置的元素，返回被替换的元素。 * 核心代码依旧是通过遍历查找节点 */ public E set(int index, E element) { checkElementIndex(index); Node\u0026lt;E\u0026gt; x = node(index); E oldVal = x.item; x.item = element; return oldVal; } /** * 移除列表中指定位置的元素 */ public E remove(int index) { checkElementIndex(index); // 先遍历查找index处的节点，再调用unlink方法 return unlink(node(index)); } /** * 从链表中删除一个节点 */ E unlink(Node\u0026lt;E\u0026gt; x) { // assert x != null; final E element = x.item; // 记录被删除节点的元素 final Node\u0026lt;E\u0026gt; next = x.next; // 记录被删除节点的后继节点 final Node\u0026lt;E\u0026gt; prev = x.prev; // 记录被删除节点的前驱节点 if (prev == null) { // 被删除节点的前驱节点为null，说明被删除节点是链表的第一个节点 first = next; // 此时链表的第一个节点是被删除节点的后继节点 } else { prev.next = next; x.prev = null; } if (next == null) { // 被删除节点的后继节点为null，说明被删除节点是链表的最后一个节点 last = prev; // 此时链表的最后一个节点是被删除节点的前驱节点 } else { next.prev = prev; x.next = null; } x.item = null; // 彻底释放被删除节点 size--; // 表长减一 modCount++; return element; } ","date":"14 February 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E7%AE%97%E6%B3%95/","section":"博客","summary":"滑动窗口技巧的思路非常简单，就是维护一个窗口，不断滑动，然后更新答案。LeetCode 上有起码 10 道运用滑动窗口算法的题目，难度都是中等和困难。","title":"滑动窗口技巧"},{"content":"六款二分模版 # https://leetcode.cn/circle/discuss/ObmjbJ/ 1、整数上的二分搜索 # 模板一 # 最经典的二分查找模板，教科书指定二分查找模板。\n区分语法\n初始条件：left = 0, right = n - 1 循环条件：left \u0026lt;= right 向左查找：right = mid-1 向右查找：left = mid+1 mid指针计算：左右指针和折半向下取整 模板口诀\n左闭右闭，向下取整，左加右减，相错终止。\n使用说明\n二分查找的最基础和最基本的形式。 适合查找在所有元素均不相同的数组中查找某特定元素。 代码实现\nint binarySearch(int[] nums, int target){ if(nums == null || nums.length == 0) return -1; int left = 0, right = nums.length - 1; while(left \u0026lt;= right){ // Prevent (left + right) overflow int mid = left + (right - left) / 2; if(nums[mid] == target) { return mid; } else if (nums[mid] \u0026lt; target) { left = mid + 1; } else { right = mid - 1; } } // End Condition: left \u0026gt; right return -1; } 查找等于 target 的第一个元素下标，即查找 target 左边界。\nint binarySearch(int[] nums, int target){ if(nums == null || nums.length == 0) return -1; int left = 0, right = nums.length - 1; while(left \u0026lt;= right){ // Prevent (left + right) overflow int mid = left + (right - left) / 2; if (nums[mid] \u0026lt; target) { left = mid + 1; } else { right = mid - 1; } } // End Condition: left \u0026gt; right if (left != nums.length \u0026amp;\u0026amp; nums[left] == target) return left; return -1; } 查找等于 target 的最后一个元素下标，即查找 target 右边界。\nint binarySearch(int[] nums, int target){ if(nums == null || nums.length == 0) return -1; int left = 0, right = nums.length - 1; while(left \u0026lt;= right){ // Prevent (left + right) overflow int mid = left + (right - left) / 2; if (nums[mid] \u0026lt;= target) { left = mid + 1; } else { right = mid - 1; } } // End Condition: left \u0026gt; right if (right != -1 \u0026amp;\u0026amp; nums[right] == target) return right; return -1; } 模板二 # Python中bisect源码实现模板，右边界可对应C++中end迭代器。\n左闭右开的区间分割方式在数学和计算机中都极为普遍。\n区分语法\n初始条件：left = 0, right = n 循环条件：left \u0026lt; right 向左查找：right = mid 向右查找：left = mid+1 mid指针计算：左右指针和折半向下取整 模板口诀\n左闭右开，向下取整，左加右定，相等终止。\n使用说明\n二分查找的高级方法。 查找条件需要访问元素的直接右邻居。 需要进行后处理。 代码实现\nint binarySearch(int[] nums, int target){ if(nums == null || nums.length == 0) return -1; int left = 0, right = nums.length; while(left \u0026lt; right) { // Prevent (left + right) overflow int mid = left + (right - left) / 2; if(nums[mid] == target) { return mid; } else if (nums[mid] \u0026lt; target) { left = mid + 1; } else { right = mid; } } // Post-processing: // End Condition: left == right if(left != nums.length \u0026amp;\u0026amp; nums[left] == target) return left; return -1; } 查找等于 target 的第一个元素下标，即查找 target 左边界。\nint binarySearch(int[] nums, int target){ if(nums == null || nums.length == 0) return -1; int left = 0, right = nums.length; while(left \u0026lt; right) { // Prevent (left + right) overflow int mid = left + (right - left) / 2; if (nums[mid] \u0026lt; target) { left = mid + 1; } else { right = mid; } } // Post-processing: // End Condition: left == right if(left != nums.length \u0026amp;\u0026amp; nums[left] == target) return left; return -1; } 查找等于 target 的最后一个元素下标，即查找 target 右边界。\nint binarySearch(int[] nums, int target){ if(nums == null || nums.length == 0) return -1; int left = 0, right = nums.length; while(left \u0026lt; right) { // Prevent (left + right) overflow int mid = left + (right - left) / 2; if (nums[mid] \u0026lt;= target) { left = mid + 1; } else { right = mid; } } // Post-processing: // End Condition: left == right if(left != nums.length \u0026amp;\u0026amp; nums[left] == target) return left; if(left \u0026gt; 0 \u0026amp;\u0026amp; nums[left - 1] == target) return left - 1; return -1; } 模板三 # 用于搜索需要访问当前索引及其在数组中的直接左右邻居索引的元素或条件。\n区分语法\n初始条件：left = 0, right = n - 1 循环条件：left + 1 \u0026lt; right 向左查找：right = mid 向右查找：left = mid mid指针计算：左右指针和折半向下取整 模板口诀\n左闭右闭，向下取整，左定右定，相邻终止。\n使用说明\n二分查找的的高级方法。 用于搜索需要访问当前索引及其在数组中的直接左右邻居索引的元素或条件。 需要进行后处理。 代码实现\nint binarySearch(int[] nums, int target) { if (nums == null || nums.length == 0) return -1; int left = 0, right = nums.length - 1; while (left + 1 \u0026lt; right){ // Prevent (left + right) overflow int mid = left + (right - left) / 2; if (nums[mid] == target) { return mid; } else if (nums[mid] \u0026lt; target) { left = mid; } else { right = mid; } } // Post-processing: // End Condition: left + 1 == right if(nums[left] == target) return left; if(nums[right] == target) return right; return -1; } 查找等于 target 的第一个元素下标，即查找 target 左边界。\nint binarySearch(int[] nums, int target) { if (nums == null || nums.length == 0) return -1; int left = 0, right = nums.length - 1; while (left + 1 \u0026lt; right){ // Prevent (left + right) overflow int mid = left + (right - left) / 2; if (nums[mid] \u0026lt; target) { left = mid; } else { right = mid; } } // Post-processing: // End Condition: left + 1 == right if(nums[left] == target) return left; if(nums[right] == target) return right; return -1; } 查找等于 target 的最后一个元素下标，即查找 target 右边界。\nint binarySearch(int[] nums, int target) { if (nums == null || nums.length == 0) return -1; int left = 0, right = nums.length - 1; while (left + 1 \u0026lt;= right){ // Prevent (left + right) overflow int mid = left + (right - left) / 2; if (nums[mid] \u0026lt; target) { left = mid; } else { right = mid; } } // Post-processing: // End Condition: left + 1 == right if(nums[right] == target) return right; if(nums[left] == target) return left; return -1; } 模板四 # ACM/ICPC大佬推荐查询目标元素左边界模板。\n区分语法\n初始条件：left = 0, right = n - 1 循环条件：left \u0026lt; right 向左查找：right = mid 向右查找：left = mid + 1 mid指针计算：左右指针和折半向下取整 模板口诀\n左闭右闭，向下取整，左加右定，相等终止。\n使用说明\n二分查找的的高级方法。 需要进行后处理。 注：在处理上下边界问题时，一般用模板四模板计算左边界。 代码实现\nint binarySearch(int[] nums, int target) { if (nums == null || nums.length == 0) return -1; int left = 0, right = nums.length - 1; while (left \u0026lt; right) { // Prevent (left + right) overflow int mid = left + (right - left) / 2; if (nums[mid] == target) { return mid; } else if (nums[mid] \u0026lt; target) { left = mid + 1; } else { right = mid; } } // Post-processing: // End Condition: left == right if(nums[left] == target) return left; return -1; } 查找等于 target 的第一个元素下标，即查找 target 左边界。\nint binarySearch(int[] nums, int target) { if (nums == null || nums.length == 0) return -1; int left = 0, right = nums.length - 1; while (left \u0026lt; right) { // Prevent (left + right) overflow int mid = left + (right - left) / 2; if (nums[mid] \u0026lt; target) { left = mid + 1; } else { right = mid; } } // Post-processing: // End Condition: left == right if(nums[left] == target) return left; return -1; } 查找等于 target 的最后一个元素下标，即查找 target 右边界。\nint binarySearch(int[] nums, int target) { if (nums == null || nums.length == 0) return -1; int left = 0, right = nums.length - 1; while (left \u0026lt; right) { // Prevent (left + right) overflow int mid = left + (right - left) / 2; if (nums[mid] \u0026lt;= target) { left = mid + 1; } else { right = mid; } } // Post-processing: // End Condition: left == right if(nums[left] == target) return left; if(left \u0026gt; 0 \u0026amp;\u0026amp; nums[left - 1] == target) return left - 1; return -1; } 模板五 # ACM/ICPC大佬推荐查询目标元素右边界模板。\n区分语法\n初始条件：left = 0, right = n - 1 循环条件：left \u0026lt; right 向左查找：right = mid - 1 向右查找：left = mid mid指针计算：左右指针和折半向上取整 模板口诀\n左闭右闭，向上取整，左定右减，相等终止。\n使用说明\n二分查找的的高级方法。 需要进行后处理。 注：在处理上下边界问题时，一般用模板五模板计算右边界。 代码实现\nint binarySearch(int[] nums, int target) { if (nums == null || nums.length == 0) return -1; int left = 0, right = nums.length - 1; while (left \u0026lt; right) { // Prevent (left + right) overflow int mid = left + (right - left + 1) / 2; if (nums[mid] == target) { return mid; } else if (nums[mid] \u0026lt; target) { left = mid; } else { right = mid - 1; } } // Post-processing: // End Condition: left == right if(nums[left] == target) return left; if(left + 1 \u0026lt; nums.length \u0026amp;\u0026amp; nums[left + 1] == target) return left + 1; return -1; } 查找等于 target 的第一个元素下标，即查找 target 左边界。\nint binarySearch(int[] nums, int target) { if (nums == null || nums.length == 0) return -1; int left = 0, right = nums.length - 1; while (left \u0026lt; right) { // Prevent (left + right) overflow int mid = left + (right - left + 1) / 2; if (nums[mid] \u0026lt; target) { left = mid; } else { right = mid - 1; } } // Post-processing: // End Condition: left == right if(nums[left] == target) return left; if(left + 1 \u0026lt; nums.length \u0026amp;\u0026amp; nums[left + 1] == target) return left + 1; return -1; } 查找等于 target 的最后一个元素下标，即查找 target 右边界。\nint binarySearch(int[] nums, int target) { if (nums == null || nums.length == 0) return -1; int left = 0, right = nums.length - 1; while (left \u0026lt; right) { // Prevent (left + right) overflow int mid = left + (right - left + 1) / 2; if (nums[mid] \u0026lt;= target) { left = mid; } else { right = mid - 1; } } // Post-processing: // End Condition: left == right if(nums[left] == target) return left; return -1; } 2、实数上的二分搜索 # 实数上的二分搜索不可以直接比较大小，可以采用 $r-l\u0026gt;eps$ 作为循环条件，$eps$ 为一个较小的数，如 $1e-7$ 等。为避免丢失可能解，缩小范围时 $r=mid$ 或 $l=mid$ ，循环结束时返回最后一个可行解。\nl=a; r=b; //初始搜索范围 while(r-l\u0026gt;eps){//判断差值 double mid=(l+r)/2; if(judge(mid)) l=mid; //l记录了可行解，循环结束后返回答案l else r=mid; } // l 是答案 还可以运行固定的次数，如运行100次，可达 $10^{-30}$ 精度，一般情况都可以解决问题。\nl=a; r=b; for(int i=0; i\u0026lt;100; i++) { //运行100次 double mid=(l+r)/2; if(judge(mid)) l=mid; else r=mid; } // l 是答案 3、二分搜索详解 # 例如，给定n个元素序列，这些元素是有序的（假定为升序），从序列中查找元素x。\n用一维数组S[]存储该有序序列，设变量low和high表示查找范围的下界和上界，middle表示查找范围的中间位置，x为特定的查找元素。\n3.1 算法步骤 # （1）初始化。令low=0，即指向有序数组S[]的第一个元素；high=n−1，即指向有序数组S[]的最后一个元素。\n（2）判定low≤high是否成立，如果成立，转向第3步，否则，算法结束。\n（3）middle=(low+high)/2，即指向查找范围的中间元素。如果数量较大，为避免low+high溢出，可以采用middle=low+(high-low)/2。\n（4）判断x与S[middle]的关系。如果x=S[middle]，则搜索成功，算法结束；如果x\u0026gt;S[middle]，则令low=middle+1；否则令high=middle−1，转向第2步。\n3.2 算法实现 # 用 BinarySearch(int n, int s[], int x)函数实现折半查找算法，其中 $n$ 为元素个数，$s[]$ 为有序数组，$x$ 为待查找元素。$low$ 指向数组的第一个元素，$high$ 指向数组的最后一个元素。如果 $low≤high$，$middle=(low+high)/2$，即指向查找范围的中间元素。如果$x=S[middle]$ ，搜索成功，算法结束；如果 $x\u0026gt;S[middle]$，则令 $low=middle+1$ ，去后半部分搜索；否则令 $high=middle−1$ ，去前半部分搜索。\n3.2.1 非递归算法 # int BinarySearch(int s[],int n,int x) { //二分查找非递归算法 int low=0,high=n-1; //low指向数组的第一个元素，high指向数组的最后一个元素 while(low\u0026lt;=high) { int middle=(low+high)/2; //middle为查找范围的中间值 if(x==s[middle]) //x等于查找范围的中间值，算法结束 return middle; else if(x\u0026gt;s[middle]) //x大于查找范围的中间元素，则从左半部分查找 low=middle+1; else //x小于查找范围的中间元素，则从右半部分查找 high=middle-1; } return -1; } 3.2.2 递归算法 # 递归有自调用问题，增加两个参数 low 和 high来标记搜索范围的开始和结束。\nint recursionBS(int s[],int x,int low,int high) { //二分查找递归算法 //low指向搜索区间的第一个元素，high指向搜索区间的最后一个元素 if(low\u0026gt;high) //递归结束条件 return -1; int middle=(low+high)/2; //计算middle值(查找范围的中间值) if(x==s[middle]) //x等于s[middle]，查找成功，算法结束 return middle; else if(x\u0026lt;s[middle]) //x小于s[middle]，则从前半部分查找 return recursionBS(s,x,low,middle-1); else //x大于s[middle]，则从后半部分查找 return recursionBS(s,x,middle+1,high); } 3.2.3 算法分析 # 时间复杂度\n二分查找算法，时间复杂度怎么计算呢？如果用T(n)来表示n个有序元素的二分查找算法的时间复杂度，那么：\n当n=1时，需要一次比较，T(n)=O(1)。 当n\u0026gt;1时，待查找元素和中间位置元素比较，需要O(1)时间，如果比较不成功，那么需要在前半部分或后半部分搜索，问题的规模缩小了一半，时间复杂度变为 $T(\\frac{n}{2})$ 。 二分查找的非递归算法和递归算法查找的方法是一样的，时间复杂度相同，均为 $O(logn)$。\n空间复杂度\n二分查找的非递归算法中，变量占用了一些辅助空间，这些辅助空间都是常数阶的，因此空间复杂度为 $O(1)$。\n二分查找的递归算法，除了使用一些变量外，递归调用还需要使用栈来实现。在递归算法中，每一次递归调用都需要一个栈空间存储，那么我们只需要看看有多少次调用。假设原问题的规模为n，那么第一次递归就分为两个规模为 $\\frac{n}{2}$ 的子问题，这两个子问题并不是每个都执行，只会执行其中之一。因为和中间值比较后，要么去前半部分查找，要么去后半部分查找；然后再把规模为 $\\frac{n}{2}$ 的子问题继续划分为两个规模为 $\\frac{n}{4}$ 的子问题，选择其一；继续分治下去，最坏的情况会分治到只剩下一个数值，那么算法执行的节点数就是从树根到叶子所经过的节点，每一层执行一个，直到最后一层，如图所示。\n递归调用最终的规模为1，即 $\\frac{n}{2x}=1$，则 $x=logn$ 。假设阴影部分是搜索经过的路径，一共经过了 $logn$ 个节点，也就是说递归调用了 $logn$ 次。递归算法使用的栈空间为递归树的深度，因此二分查找递归算法的空间复杂度为 $O(logn)$ 。\n4、二分搜索需要注意的几个问题 # 必须满足有序性。 搜索范围。初始时，需要指定搜索范围，如果不知道具体范围，正数可以采用范围 $[0,inf]$ ，有可能为负数可以采用范围[-inf,inf]，inf为无穷大，通常设定为 $0x3f3f3f3f$。 二分搜索。一般情况下，$mid=(l+r)/2$ 或 $mid=(l+r)\u0026raquo;1$ 。如果l和r特别大，为避免 $l+r$ 溢出，可以采用 $mid=l+(r-l)/2$ 。判断二分搜索结束的条件，以及当判断 $mid$ 可行时到前半部分搜索，还是到后半部分搜索，需要具体问题具体分析。 答案是什么。特别小心搜索范围减少时，是否丢失在mid点上的答案。 ","date":"13 February 2024","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/%E4%BA%8C%E5%88%86%E6%A8%A1%E7%89%88/","section":"博客","summary":"二分查找是一种高效的搜索算法，适用于有序数组。它通过每次将搜索范围缩小一半来查找目标元素，直到找到或者确定不存在。时间复杂度为O(log n)，效率高于线性搜索。","title":"二分查找技巧"},{"content":"Spring Boot 的常用注解 # 1、@SpringBootApplication # 这是 Spring Boot 最最最核心的注解，用在 Spring Boot 主类上，标识这是一个 Spring Boot 应用，用来开启 Spring Boot 的各项能力。\n其实这个注解就是 @SpringBootConfiguration、@EnableAutoConfiguration、@ComponentScan 这三个注解的组合，也可以用这三个注解来代替 @SpringBootApplication 注解。看下 @SpringBootApplication 注解的源码，一切明了！\n@Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) }) public @interface SpringBootApplication { ... } 示例代码\npackage com.example.myproject; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplication // same as @Configuration @EnableAutoConfiguration @ComponentScan public class Application { public static void main(String[] args) { SpringApplication.run(Application.class, args); } } 2、@EnableAutoConfiguration # 允许 Spring Boot 自动配置注解，开启这个注解之后，Spring Boot 就能根据当前类路径下的包或者类来配置 Spring Bean，即尝试根据你添加的jar依赖自动配置你的Spring应用。\n例如：当前类路径下有 Mybatis 这个 JAR 包，MybatisAutoConfiguration 注解就能根据相关参数来配置 Mybatis 的各个 Spring Bean；如果你的classpath下存在HSQLDB，并且你没有手动配置任何数据库连接beans，那么我们将自动配置一个内存型（in-memory）数据库。\n你可以将 @EnableAutoConfiguration 或者 @SpringBootApplication 注解添加到一个 @Configuration 类上来选择自动配置。如果发现应用了你不想要的特定自动配置类，你可以使用 @EnableAutoConfiguration 注解的排除属性来禁用它们。\n3、@Configuration # 这是 Spring 3.0 添加的一个注解，用来代替 applicationContext.xml 配置文件，所有这个配置文件里面能做到的事情都可以通过这个注解所在类来进行注册。\n如果有些第三方库需要用到xml文件，建议仍然通过 @Configuration 类作为项目的配置主类，对于第三方库可以使用 @ImportResource 注解加载xml配置文件。 可以用 @Import 注解用来引入额外的一个或者多个 @Configuration 修饰的配置文件类。 4、@SpringBootConfiguration # 这个注解就是 @Configuration 注解的变体，只是用来修饰是 Spring Boot 配置而已，或者可利于 Spring Boot 后续的扩展。源码如下。\n@Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Configuration public @interface SpringBootConfiguration { } 5、@ComponentScan # 这是 Spring 3.1 添加的一个注解，用来代替配置文件中的 component-scan 配置，开启组件扫描，即自动扫描包路径下的 @Component 注解进行注册 bean 实例到 context 中。\n另外，@ComponentScans 是可重复注解，即可以配置多个，用来配置注册不同的子包。\n@Retention (RetentionPolicy.RUNTIME) @Target (ElementType.TYPE) @Documented @Repeatable(ComponentScans.class) public @interface ComponentScan { } @Retention (RetentionPolicy.RUNTIME) @Target (ElementType.TYPE) @Documented public @interface ComponentScans { } 6、@Conditional # 这是 Spring 4.0 添加的新注解，用来标识一个 Spring Bean 或者 Configuration 配置文件，当满足指定的条件才开启配置。\n7、@ConditionalOnBean # 组合 @Conditional 注解，当容器中有指定的 Bean 才开启配置。\n8、@ConditionalOnMissingBean # 组合 @Conditional 注解，和 @ConditionalOnBean 注解相反，当容器中没有指定的 Bean 才开启配置。\n9、@ConditionalOnClass # 组合 @Conditional 注解，当容器中有指定的 Class 才开启配置。\n10、@ConditionalOnMissingClass # 组合 @Conditional 注解，和 @ConditionalOnMissingClass 注解相反，当容器中没有指定的 Class 才开启配置。\n11、@ConditionalOnWebApplication # 组合 @Conditional 注解，当前项目类型是 WEB 项目才开启配置。\n当前项目有以下 3 种类型。\nenum Type { /** * Any web application will match. */ ANY, /** * Only servlet-based web application will match. */ SERVLET, /** * Only reactive-based web application will match. */ REACTIVE } 12、@ConditionalOnNotWebApplication # 组合 @Conditional 注解，和 @ConditionalOnWebApplication 注解相反，当前项目类型不是 WEB 项目才开启配置。\n13、@ConditionalOnProperty # 组合 @Conditional 注解，当指定的属性有指定的值时才开启配置。\n14、@ConditionalOnExpression # 组合 @Conditional 注解，当 SpEL 表达式为 true 时才开启配置。\n15、@ConditionalOnJava # 组合 @Conditional 注解，当运行的 Java JVM 在指定的版本范围时才开启配置。\n16、@ConditionalOnResource # 组合 @Conditional 注解，当类路径下有指定的资源才开启配置。\n17、@ConditionalOnJndi # 组合 @Conditional 注解，当指定的 JNDI 存在时才开启配置。\n18、@ConditionalOnCloudPlatform # 组合 @Conditional 注解，当指定的云平台激活时才开启配置。\n19、@ConditionalOnSingleCandidate # 组合 @Conditional 注解，当指定的 class 在容器中只有一个 Bean，或者同时有多个但为首选时才开启配置。\n20、@ConfigurationProperties # 用来加载额外的配置（如 .properties 文件），可用在 @Configuration 注解类，或者 @Bean 注解方法上面。\n21、@EnableConfigurationProperties # 一般要配合 @ConfigurationProperties 注解使用，用来开启对 @ConfigurationProperties 注解配置 Bean 的支持。\n22、@AutoConfigureAfter # 用在自动配置类上面，表示该自动配置类需要在另外指定的自动配置类配置完之后。\n如 Mybatis 的自动配置类，需要在数据源自动配置类之后。\n@AutoConfigureAfter(DataSourceAutoConfiguration.class) public class MybatisAutoConfiguration { } 23、@AutoConfigureBefore # 这个和 @AutoConfigureAfter 注解使用相反，表示该自动配置类需要在另外指定的自动配置类配置之前。\n24、@Import # 这是 Spring 3.0 添加的新注解，用来导入一个或者多个 @Configuration 注解修饰的类，这在 Spring Boot 里面应用很多。\n25、@ImportResource # 这是 Spring 3.0 添加的新注解，用来导入一个或者多个 Spring 配置文件，这对 Spring Boot 兼容老项目非常有用，因为有些配置无法通过 Java Config 的形式来配置就只能用这个注解来导入。\n26、@Service # 一般用于修饰service层的组件\n27、@Repository # 使用 @Repository 注解可以确保DAO或者repositories提供异常转译，这个注解修饰的 DAO 或者 repositories 类会被 ComponetScan 发现并配置，同时也不需要为它们提供XML配置项。\n28、@Bean # 用@Bean标注方法等价于XML中配置的bean。相当于XML中的，放在方法的上面，而不是类，意思是产生一个bean，并交给spring管理。\n29、@Value # 注入Spring boot application.properties 配置的属性的值。示例代码：\n@Value(value = \u0026#34;#{message}\u0026#34;) private String message; 30、@Inject # 等价于默认的 @Autowired ，只是没有required属性\n31、@AutoWired # 自动导入依赖的bean。byType方式。把配置好的Bean拿来用，完成属性、方法的组装，它可以对类成员变量、方法及构造函数进行标注，完成自动装配的工作。当加上（required=false）时，就算找不到bean也不报错。\n32、@Qualifier # 当有多个同一类型的Bean时，可以用 @Qualifier(\u0026quot;name\u0026quot;) 来指定。\n与 @Autowired 配合使用。\n@Qualifier 限定描述符除了能根据名字进行注入，但能进行更细粒度的控制如何选择候选者，具体使用方式如下：\n@Autowired @Qualifier(value = \u0026#34;demoInfoService\u0026#34;) private DemoInfoService demoInfoService; Spring MVC常用注解 # 1、@Controller # 用于定义控制器类，在spring 项目中由控制器负责将用户发来的URL请求转发到对应的服务接口（service层），一般这个注解在类中，通常方法需要配合注解@RequestMapping。示例代码：\n@Controller @RequestMapping(\u0026#34;/demoInfo\u0026#34;) publicclass DemoController { @Autowired private DemoInfoService demoInfoService; @RequestMapping(\u0026#34;/hello\u0026#34;) public String hello(Map\u0026lt;String,Object\u0026gt; map){ System.out.println(\u0026#34;DemoController.hello()\u0026#34;); map.put(\u0026#34;hello\u0026#34;,\u0026#34;from TemplateController.helloHtml\u0026#34;); //会使用hello.html或者hello.ftl模板进行渲染显示. return\u0026#34;/hello\u0026#34;; } } 2、@RequestMapping # @RequestMapping(\u0026quot;/path\u0026quot;) 表示该控制器处理所有 \u0026ldquo;/path\u0026rdquo; 的 URL请求。RequestMapping是一个用来处理请求地址映射的注解，可用于类或方法上。\n用于类上，表示类中的所有响应请求的方法都是以该地址作为父路径。该注解有六个属性：\nparams：指定request中必须包含某些参数值是，才让该方法处理。 headers：指定request中必须包含某些指定的header值，才能让该方法处理请求。 value：指定请求的实际地址，指定的地址可以是URI Template 模式 method：指定请求的method类型， GET、POST、PUT、DELETE等 consumes：指定处理请求的提交内容类型（Content-Type），如application/json,text/html; produces：指定返回的内容类型，仅当request请求头中的(Accept)类型中包含该指定类型才返回 3、@RequestParam # 放在参数前，表示只能接收参数a=b格式的数据，即 Content-Type 为 application/x-www-form-urlencoded 类型的内容。\nname的别名是value，value的别名是name。二者皆可，并且开发中两个都能获得参数，获得一样的结果。源码：\npublic @interface RequestParam { @AliasFor(\u0026#34;name\u0026#34;) String value() default \u0026#34;\u0026#34;; @AliasFor(\u0026#34;value\u0026#34;) String name() default \u0026#34;\u0026#34;; boolean required() default true; String defaultValue() default \u0026#34;\\n\\t\\t\\n\\t\\t\\n\\ue000\\ue001\\ue002\\n\\t\\t\\t\\t\\n\u0026#34;; } 示例：\n// \u0026lt;a href=\u0026#34;anno/testRequestParam?name=哈哈\u0026#34;\u0026gt;点击测试RequestParam\u0026lt;/a\u0026gt; @Controller @RequestMapping(\u0026#34;/anno\u0026#34;) public class AnnoController { @RequestMapping(\u0026#34;/testRequestParam\u0026#34;) public String testRequestParam(@RequestParam(value=\u0026#34;name\u0026#34;) String username){ System.out.println(\u0026#34;执行了...\u0026#34;); System.out.println(username); return \u0026#34;success\u0026#34;; } } 4、@RequestBody # 放在参数前，表示参数从 request body 中获取，而不是从地址栏获取，所以这肯定是接收一个POST请求的非a=b格式的数据，即 Content-Type 不为 application/x-www-form-urlencoded 类型的内容。\n5、@ResponseBody # 表示该方法的返回结果直接写入HTTP response body中，一般在异步获取数据时使用，用于构建RESTful的api。在使用 @RequestMapping 后，返回值通常解析为跳转路径，加上 @Responsebody 后返回结果不会被解析为跳转路径，而是直接写入 HTTP response body 中。\n比如异步获取json数据，加上@responsebody后，会直接返回json数据。该注解一般会配合@RequestMapping一起使用。示例代码：\n@RequestMapping(\u0026#34;/test\u0026#34;) @ResponseBody public String test(){ return\u0026#34;ok\u0026#34;; } 6、@RestController # 用于标注控制层组件(如struts中的action)，@ResponseBody 和 @Controller 的合集，表示@Controller标识的类里面的所有返回参数都放在response body里面。\n示例代码：\npackage com.kfit.demo.web; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; @RestController @RequestMapping(\u0026#34;/demoInfo2\u0026#34;) publicclass DemoController2 { @RequestMapping(\u0026#34;/test\u0026#34;) public String test(){ return\u0026#34;ok\u0026#34;; } } 7、@PathVariable # 路径绑定变量，用于绑定restful路径上的变量。\nRequestMapping(\u0026#34;user/get/mac/{macAddress}\u0026#34;) public String getByMacAddress(@PathVariable String macAddress){ //do something; } 8、@RequestHeader # 放在方法参数前，用来获取request header中的参数值。\n9、@CookieValue # 放在方法参数前，用来获取request header cookie中的参数值。\n10、GetMapping PostMapping PutMapping.. # *Mapping 的是 Spring4.3 加入的新注解，表示特定的请求类型路径映射，而不需要写 RequestMethod 来指定请求类型。\nimport org.dom4j.util.UserDataElement; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.PathVariable; import org.springframework.web.bind.annotation.RequestBody; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RequestMethod; import org.springframework.web.bind.annotation.ResponseBody; @Controller @RequestMapping(\u0026#34;/test\u0026#34;) public class TestController { @RequestMapping(value = \u0026#34;/get/{no}\u0026#34;, method = RequestMethod.GET) @ResponseBody public Objectget(@PathVariable(\u0026#34;no\u0026#34;) String no) { return new UserDataElement(\u0026#34;\u0026#34;); } @RequestMapping(value = \u0026#34;/save\u0026#34;, method = RequestMethod.POST) public void save(@RequestBody UserDataElement user) { } } JPA注解 # 1、@Entity # //表明这是一个实体类。 @Entity @Table(name=\u0026#34;\u0026#34;) 一般用于jpa这两个注解一般一块使用，但是如果表名和实体类名相同的话，@Table 可以省略。\n2、@MappedSuperClass # @MappedSuperClass：用在确定是父类的entity上。父类的属性子类可以继承。\n3、@NoRepositoryBean # @NoRepositoryBean：一般用作父类的repository，有这个注解，spring不会去实例化该repository。\n4、@Column # @Column：如果字段名与列名相同，则可以省略。\n5、@Id # @Id：表示该属性为主键。\n6、@GeneratedValue # @GeneratedValue(strategy = GenerationType.SEQUENCE, generator = \u0026#34;repair_seq\u0026#34;) @GeneratedValue(strategy = GenerationType.AUTO, generator = \u0026#34;uuid-string\u0026#34;) 使用 @GeneratedValue 注释自动生成的实体标识可以是数值类型字段如 byte、short、int、long等，或者它们对应的包装器类型 Byte、Short、Integer、Long等，也可以是字符串类型。\nGenerationType有四种生成策略\nTABLE：使用一个特定的数据库表格来保存主键。 SEQUENCE：根据底层数据库的序列来生成主键，条件是数据库支持序列。 IDENTITY：主键由数据库自动生成（主要是自动增长型） AUTO：主键由程序控制。 表示主键生成策略是sequence（可以为Auto、IDENTITY、native等，Auto表示可在多个数据库间切换），指定sequence的名字是repair_seq。\n7、@SequenceGeneretor # @SequenceGeneretor(name = \u0026quot;repair_seq\u0026quot;, sequenceName = \u0026quot;seq_repair\u0026quot;, allocationSize = 1)：name为sequence的名称，以便使用，sequenceName为数据库的sequence名称，两个名称可以一致。\n8、@Transient # @Transient：表示该属性并非一个到数据库表的字段的映射，ORM框架将忽略该属性。如果一个属性并非数据库表的字段映射，就务必将其标示为 @Transient，否则，ORM框架默认其注解为@Basic。@Basic(fetch=FetchType.LAZY)：标记可以指定实体属性的加载方式。\n9、@JsonIgnore # @JsonIgnore：作用是json序列化时将Java bean中的一些属性忽略掉,序列化和反序列化都受影响。\n10、@JoinColumn # @JoinColumn(name=\u0026quot;loginId\u0026quot;)：一对一：本表中指向另一个表的外键。一对多：另一个表指向本表的外键。\n11、@OneToOne、@OneToMany、@ManyToOne # @OneToOne、@OneToMany、@ManyToOne：对应hibernate配置文件中的一对一，一对多，多对一。\nResources # https://javaguide.cn/system-design/framework/spring/spring-common-annotations.html#_1-springbootapplication ","date":"7 February 2024","permalink":"/posts/language/java/spring/spring-boot-%E7%9A%84%E6%A0%B8%E5%BF%83%E6%B3%A8%E8%A7%A3/","section":"博客","summary":"Spring Boot 是一种快速开发应用程序的框架，提供了一系列的注解来简化开发过程。其中，最重要的就是核心注解，它能帮助开发者快速配置应用程序。本文将深入探讨 Spring Boot 的核心注解，介绍它的作用以及常见的组成注解。","title":"Spring Boot 的核心注解"},{"content":"","date":"7 February 2024","permalink":"/tags/@interface/","section":"Tags","summary":"","title":"@Interface"},{"content":"jdk1.5 # jdk1.5起开始提供了4个元注解，用来定义自定义注解的注解，它们分别是：\n@Target # 指定注解使用的目标范围（类、方法、字段等），其参考值见类的定义：java.lang.annotation.ElementType\n@Documented # 指定被标注的注解会包含在javadoc中。\n@Retention # 指定注解的生命周期（源码、class文件、运行时），其参考值见类的定义：java.lang.annotation.RetentionPolicy\n@Inherited # 指定子类可以继承父类的注解，只能是类上的注解，方法和字段的注解不能继承。即如果父类上的注解是 @Inherited 修饰的就能被子类继承。\njdk1.8 # jdk1.8又提供了以下两个元注解\n@Native # 指定字段是一个常量，其值引用native code。\n@Repeatable # 注解上可以使用重复注解，即可以在一个地方可以重复使用同一个注解，像 spring 中的包扫描注解就使用了这个。\n@Retention (RetentionPolicy.RUNTIME) @Target (ElementType.TYPE) @Documented @Repeatable(ComponentScans.class) public @interface ComponentScan { } @Retention (RetentionPolicy.RUNTIME) @Target (ElementType.TYPE) @Documented public @interface ComponentScans { } 所有元注解定义在 java.lang.annotation 包下面\n其中 Annotation 是注解的基本接口，所有的注解都继承这个接口。\n看下 @Autowired 注解的实现\n其实就是继承了Annotation接口。\n","date":"7 February 2024","permalink":"/posts/language/java/java%E5%85%83%E6%B3%A8%E8%A7%A3@interface/","section":"博客","summary":"注解（Annontion）是Java5开始引入的新特征。它提供了一种安全的类似注释的机制，用来将任何的信息或元数据（metadata）与程序元素（类、方法、成员变量等）进行关联。元注解的作用就是负责注解其他注解。","title":"java元注解@interface"},{"content":"","date":"31 January 2024","permalink":"/tags/http/","section":"Tags","summary":"","title":"Http"},{"content":"","date":"31 January 2024","permalink":"/tags/rpc/","section":"Tags","summary":"","title":"Rpc"},{"content":"RPC 的全称是 Remote Procedure Call Protocol，中文名是远程过程调用协议。\n通俗点讲就是：客户端在不知道调用细节的情况下，调用存在于远程计算机上的某个对象，就像调用本地应用程序中的对象一样。举个例子：两个不同的服务 A、B 部署在两台不同的机器上，服务 A 如果想要调用服务 B 中的某个方法的话就可以通过 RPC 来做。\n官方的描述是：一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。\nRPC 的特点\nRPC 是一种协议。RPC实现包括：Dubbo、Thrift、GRPC、Netty等。 网络协议和网络 IO 模型对其透明。RPC 的客户端认为自己是在调用本地对象，因此其对使用的网络协议（ HTTP 协议等）以及网络 IO 模型，是不关心的。 信息格式对其透明。调用方法是需要传递参数的，对于远程调用来说，传递过程中参数的信息格式是怎样构成，以及提供者如何使用这些参数，都是不用关心的。 有跨语言能力。因为调用方实际上也不清楚远程服务器的应用程序是使用什么语言运行的。那么对于调用方来说，无论服务器方使用的是什么语言，本次调用都应该成功，并且返回值也应该按照调用方程序语言所能理解的形式进行描述。 RPC 原理 # 客户端（服务消费端）：调用远程方法的一端。 客户端 Stub（桩）：这其实就是一代理类。代理类主要做的事情很简单，就是把你调用方法、类、方法参数等信息传递到服务端。 网络传输：网络传输就是你要把你调用的方法的信息比如说参数啊这些东西传输到服务端，然后服务端执行完之后再把返回结果通过网络传输给你传输回来。网络传输的实现方式有很多种比如最基本的 Socket 或者性能以及封装更加优秀的 Netty（推荐）。 服务端 Stub（桩）：这个桩就不是代理类了。我觉得理解为桩实际不太好，大家注意一下就好。这里的服务端 Stub 实际指的就是接收到客户端执行方法的请求后，去执行对应的方法然后返回结果给客户端的类。 服务端（服务提供端）：提供远程方法的一端。 服务消费端（client）以本地调用的方式调用远程服务； 客户端 Stub（client stub） 接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体（序列化）：RpcRequest； 客户端 Stub（client stub） 找到远程服务的地址，并将消息发送到服务提供端； 服务端 Stub（桩）收到消息将消息反序列化为 Java 对象: RpcRequest； 服务端 Stub（桩）根据RpcRequest中的类、方法、方法参数等信息调用本地的方法； 服务端 Stub（桩）得到方法执行结果并将组装成能够进行网络传输的消息体：RpcResponse（序列化）发送至消费方； 客户端 Stub（client stub）接收到消息并将消息反序列化为 Java 对象: RpcResponse ，这样也就得到了最终结果。over! 哪些框架支持 RPC？ # Dubbo # Apache Dubbo 是一款微服务框架，为大规模微服务实践提供高性能 RPC 通信、流量治理、可观测性等解决方案， 涵盖 Java、Golang 等多种语言 SDK 实现。\nDubbo 提供了从服务定义、服务发现、服务通信到流量管控等几乎所有的服务治理能力，支持 Triple 协议（基于 HTTP/2 之上定义的下一代 RPC 通信协议）、应用级服务发现、Dubbo Mesh （Dubbo3 赋予了很多云原生友好的新特性）等特性。\nGitHub：https://github.com/apache/incubator-dubbo 官网：https://dubbo.apache.org/zh/ Motan\nMotan 是新浪微博开源的一款 RPC 框架，据说在新浪微博正支撑着千亿次调用。不过笔者倒是很少看到有公司使用，而且网上的资料也比较少。\n很多人喜欢拿 Motan 和 Dubbo 作比较，毕竟都是国内大公司开源的。笔者在查阅了很多资料，以及简单查看了其源码之后发现：Motan 更像是一个精简版的 Dubbo，可能是借鉴了 Dubbo 的思想，Motan 的设计更加精简，功能更加纯粹。\n不过，我不推荐你在实际项目中使用 Motan。如果你要是公司实际使用的话，还是推荐 Dubbo ，其社区活跃度以及生态都要好很多。\n从 Motan 看 RPC 框架设计：http://kriszhang.com/motan-rpc-impl/ Motan 中文文档：https://github.com/weibocom/motan/wiki/zh_overview gRPC\ngRPC 是 Google 开源的一个高性能、通用的开源 RPC 框架。其由主要面向移动应用开发并基于 HTTP/2 协议标准而设计（支持双向流、消息头压缩等功能，更加节省带宽），基于 ProtoBuf 序列化协议开发，并且支持众多开发语言。\nProtoBuf 是一种更加灵活、高效的数据格式，可用于通讯协议、数据存储等领域，基本支持所有主流编程语言且与平台无关。不过，通过 ProtoBuf 定义接口和数据类型还挺繁琐的，这是一个小问题。\nThrift\nApache Thrift 是 Facebook 开源的跨语言的 RPC 通信框架，目前已经捐献给 Apache 基金会管理，由于其跨语言特性和出色的性能，在很多互联网公司得到应用，有能力的公司甚至会基于 thrift 研发一套分布式服务框架，增加诸如服务注册、服务发现等功能。\nThrift支持多种不同的编程语言，包括C++、Java、Python、PHP、Ruby等（相比于 gRPC 支持的语言更多 ）。\n官网：https://thrift.apache.org/ Thrift 简单介绍：https://www.jianshu.com/p/8f25d057a5a9 有了 HTTP 协议，为什么还要有 RPC ？ # 作为一个程序员，假设我们需要在 A 电脑的进程发一段数据到 B 电脑的进程，我们一般会在代码里使用 socket 进行编程。\n这时候，我们可选项一般也就TCP 和 UDP 二选一。TCP 可靠，UDP 不可靠。 除非是马总这种神级程序员（早期 QQ 大量使用 UDP），否则，只要稍微对可靠性有些要求，普通人一般无脑选 TCP 就对了。\n类似下面这样。\nfd = socket(AF_INET,SOCK_STREAM,0); 其中SOCK_STREAM，是指使用字节流传输数据，说白了就是TCP 协议。\n在定义了 socket 之后，我们就可以愉快的对这个 socket 进行操作，比如用bind()绑定 IP 端口，用connect()发起建连。\n在连接建立之后，我们就可以使用send()发送数据，recv()接收数据。\n光这样一个纯裸的 TCP 连接，就可以做到收发数据了，那是不是就够了？\n不行，这么用会有问题。\n八股文常背，TCP 是有三个特点，面向连接、可靠、基于字节流。\n这三个特点真的概括的非常精辟，这个八股文我们没白背。\n每个特点展开都能聊一篇文章，而今天我们需要关注的是基于字节流这一点。\n字节流可以理解为一个双向的通道里流淌的二进制数据，也就是 01 串 。纯裸 TCP 收发的这些 01 串之间是没有任何边界的，你根本不知道到哪个地方才算一条完整消息。\n正因为这个没有任何边界的特点，所以当我们选择使用 TCP 发送 \u0026ldquo;夏洛\u0026quot;和\u0026quot;特烦恼\u0026rdquo; 的时候，接收端收到的就是 \u0026ldquo;夏洛特烦恼\u0026rdquo; ，这时候接收端没发区分你是想要表达 \u0026ldquo;夏洛\u0026rdquo;+\u0026ldquo;特烦恼\u0026rdquo; 还是 \u0026ldquo;夏洛特\u0026rdquo;+\u0026ldquo;烦恼\u0026rdquo;\n这就是所谓的粘包问题。说这个的目的是为了告诉大家，纯裸 TCP 是不能直接拿来用的，你需要在这个基础上加入一些自定义的规则，用于区分消息边界。\n于是我们会把每条要发送的数据都包装一下，比如加入消息头，消息头里写清楚一个完整的包长度是多少，根据这个长度可以继续接收数据，截取出来后它们就是我们真正要传输的消息体。\n而这里头提到的消息头，还可以放各种东西，比如消息体是否被压缩过和消息体格式之类的，只要上下游都约定好了，互相都认就可以了，这就是所谓的协议。\n每个使用 TCP 的项目都可能会定义一套类似这样的协议解析标准，他们可能 有区别，但原理都类似。\n于是基于TCP，就衍生了非常多的协议，比如 HTTP 和 RPC。\nHTTP 和 RPC # RPC 其实是一种调用方式\n我们回过头来看网络的分层图。\nTCP 是传输层的协议 ，而基于 TCP 造出来的 HTTP 和各类 RPC 协议，它们都只是定义了不同消息格式的 应用层协议 而已。\nHTTP（Hyper Text Transfer Protocol）协议又叫做 超文本传输协议 。我们用的比较多，平时上网在浏览器上敲个网址就能访问网页，这里用到的就是 HTTP 协议。\n而 RPC（Remote Procedure Call）又叫做 远程过程调用，它本身并不是一个具体的协议，而是一种 调用方式 。\n举个例子，我们平时调用一个 本地方法 就像下面这样。\nres = localFunc(req) 如果现在这不是个本地方法，而是个远端服务器暴露出来的一个方法remoteFunc，如果我们还能像调用本地方法那样去调用它，这样就可以屏蔽掉一些网络细节，用起来更方便，岂不美哉？\nres = remoteFunc(req) 基于这个思路，大佬们造出了非常多款式的 RPC 协议，比如比较有名的gRPC，thrift。\n值得注意的是，虽然大部分 RPC 协议底层使用 TCP，但实际上它们不一定非得使用 TCP，改用 UDP 或者 HTTP，其实也可以做到类似的功能。\n那既然有 RPC 了，为什么还要有 HTTP 呢？\n其实，TCP 是 70 年 代出来的协议，而 HTTP 是 90 年代 才开始流行的。而直接使用裸 TCP 会有问题，可想而知，这中间这么多年有多少自定义的协议，而这里面就有 80 年代 出来的RPC。\n所以我们该问的不是 既然有 HTTP 协议为什么要有 RPC ，而是 为什么有 RPC 还要有 HTTP 协议?\n现在电脑上装的各种联网软件，比如 xx 管家，xx 卫士，它们都作为客户端（Client） 需要跟服务端（Server） 建立连接收发消息，此时都会用到应用层协议，在这种 Client/Server (C/S) 架构下，它们可以使用自家造的 RPC 协议，因为它只管连自己公司的服务器就 ok 了。\n但有个软件不同，浏览器（Browser） ，不管是 Chrome 还是 IE，它们不仅要能访问自家公司的服务器（Server） ，还需要访问其他公司的网站服务器，因此它们需要有个统一的标准，不然大家没法交流。于是，HTTP 就是那个时代用于统一 Browser/Server (B/S) 的协议。\n也就是说在多年以前，HTTP 主要用于 B/S 架构，而 RPC 更多用于 C/S 架构。但现在其实已经没分那么清了，B/S 和 C/S 在慢慢融合。 很多软件同时支持多端，比如某度云盘，既要支持网页版，还要支持手机端和 PC 端，如果通信协议都用 HTTP 的话，那服务器只用同一套就够了。而 RPC 就开始退居幕后，一般用于公司内部集群里，各个微服务之间的通讯。\n那这么说的话，都用 HTTP 得了，还用什么 RPC？\n仿佛又回到了文章开头的样子，那这就要从它们之间的区别开始说起。\nHTTP 和 RPC 有什么区别 # 服务发现 # 首先要向某个服务器发起请求，你得先建立连接，而建立连接的前提是，你得知道IP 地址和端口。这个找到服务对应的 IP 端口的过程，其实就是服务发现。\n在 HTTP 中，你知道服务的域名，就可以通过 DNS 服务 去解析得到它背后的 IP 地址，默认 80 端口。\n而 RPC 的话，就有些区别，一般会有专门的中间服务去保存服务名和 IP 信息，比如 Consul、Etcd、Nacos、ZooKeeper，甚至是 Redis。想要访问某个服务，就去这些中间服务去获得 IP 和端口信息。由于 DNS 也是服务发现的一种，所以也有基于 DNS 去做服务发现的组件，比如 CoreDNS。\n可以看出服务发现这一块，两者是有些区别，但不太能分高低。\n底层连接形式 # 以主流的 HTTP1.1 协议为例，其默认在建立底层 TCP 连接之后会一直保持这个连接（keep alive），之后的请求和响应都会复用这条连接。\n而 RPC 协议，也跟 HTTP 类似，也是通过建立 TCP 长链接进行数据交互，但不同的地方在于，RPC 协议一般还会再建个 连接池，在请求量大的时候，建立多条连接放在池内，要发数据的时候就从池里取一条连接出来，用完放回去，下次再复用，可以说非常环保。\n由于连接池有利于提升网络请求性能，所以不少编程语言的网络库里都会给 HTTP 加个连接池，比如 Go 就是这么干的。\n可以看出这一块两者也没太大区别，所以也不是关键。\n传输的内容 # 基于 TCP 传输的消息，说到底，无非都是 消息头 Header 和消息体 Body。\nHeader 是用于标记一些特殊信息，其中最重要的是 消息体长度。\nBody 则是放我们真正需要传输的内容，而这些内容只能是二进制 01 串，毕竟计算机只认识这玩意。所以 TCP 传字符串和数字都问题不大，因为字符串可以转成编码再变成 01 串，而数字本身也能直接转为二进制。但结构体呢，我们得想个办法将它也转为二进制 01 串，这样的方案现在也有很多现成的，比如 JSON，Protocol Buffers (Protobuf) 。\n这个将结构体转为二进制数组的过程就叫 序列化 ，反过来将二进制数组复原成结构体的过程叫 反序列化。\n对于主流的 HTTP1.1，虽然它现在叫超文本协议，支持音频视频，但 HTTP 设计 初是用于做网页文本展示的，所以它传的内容以字符串为主。Header 和 Body 都是如此。在 Body 这块，它使用 JSON 来 序列化 结构体数据。\n我们可以随便截个图直观看下。\n可以看到这里面的内容非常多的冗余，显得非常啰嗦。最明显的，像 Header 里的那些信息，其实如果我们约定好头部的第几位是 Content-Type，就不需要每次都真的把 Content-Type 这个字段都传过来，类似的情况其实在 Body 的 JSON 结构里也特别明显。\n而 RPC，因为它定制化程度更高，可以采用体积更小的 Protobuf 或其他序列化协议去保存结构体数据，同时也不需要像 HTTP 那样考虑各种浏览器行为，比如 302 重定向跳转啥的。因此性能也会更好一些，这也是在公司内部微服务中抛弃 HTTP，选择使用 RPC 的最主要原因。\n当然上面说的 HTTP，其实 特指的是现在主流使用的 HTTP1.1，HTTP2在前者的基础上做了很多改进，所以 性能可能比很多 RPC 协议还要好，甚至连gRPC底层都直接用的HTTP2。\n为什么既然有了 HTTP2，还要有 RPC 协议？\n这个是由于 HTTP2 是 2015 年出来的。那时候很多公司内部的 RPC 协议都已经跑了好些年了，基于历史原因，一般也没必要去换了。\n","date":"31 January 2024","permalink":"/posts/architecture/distributed/rpc/rpc-%E6%A6%82%E8%BF%B0/","section":"博客","summary":"RPC 的全称是 Remote Procedure Call Protocol，中文名是远程过程调用协议。官方的描述是：一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。","title":"RPC-概述"},{"content":"在生活、特别是工作当中，我们每个人都会碰到需要向他人以书面形式或口头形式阐述自己的观点、想法的情况。可现实中，我们说话做事经常出现逻辑混乱、想到哪说到哪，不能很好地突出重点，层次分明的把一件事情说清楚，这往往会限制、阻碍我们的发展。面对这些问题，如何才能突出重点、逻辑清晰、主次分明的思考、表达和解决问题呢？学会“金字塔原理”就够了！\n下面我就以巴巴拉·明托的《金字塔原理》一书，和大家具体的讲一讲金字塔原理（结构化思维）。\n金字塔通常可以分为“塔尖、塔身、塔基”三个部分。金字塔原理，就是根据金字塔的结构，把金字塔分成纵向结构和横向结构。纵向结构就是从塔尖到塔身，再到塔基，横向结构就是塔身和塔身、塔基和塔基之间的逻辑结构。\n运用金字塔原理，简单来说就是：我们在写作、思考、表达、解决问题的时候，要像金字塔结构一样，整体上把握纵向结构，突出重点；具体内容上把握横向结构，突出层次性和逻辑性。也就是：从金字塔顶端的一个核心观点开始，自上而下的表达；然后从左往右，沿着各个分支，把你的内容一层一层呈现给别人。这样不仅能让你的思路清晰，同时能让别人更好的理解你的内容。\n我们先来说一下金字塔的纵向结构策略。\n纵向结构策略，简单来说就是要：先说结论，然后以上统下。\n比如在给领导汇报工作、或写文案、文章时，应该把你想表达的观点、结论先说出来，同时上一层的内容必须是对下一层内容的总结概括。而不是先说一大堆事实，然后等着听众或读者来猜你要说什么。工作生活中的多数时候，大家既没那个时间，也没那个精力，所以要先说结论并以上统下。\n我我之前分享过的“PREP模式”就是一种符合金字塔纵向结构的策略：\n先表明你的观点、结论或主张P；然后再说出支持你结论的“依据”R；接着再用具体的案例，或者实际的资料、数据来提高你结论或观点的说服力E；最后再重申一下结论P，确保自己想传达的信息，已确实传递。\n比如你希望老板给你加薪，首先，你要提出你要加薪的想法（塔尖/P），然后列举说出你为什么想加薪，也就是理由（塔身/R），最后用事实和数据支撑你加薪的理由（塔基/E）。\n到这里，还没完，因为先说结论也是要讲究技巧的。你不能一来就没头没脑地就说一个结论，这样会很突兀，所以还要学会用序言来打磨塔尖。就像书籍有序言一样，作报告、写文章、阐述观点时也需要一个序言。也就是做一下包装、找一个由头、用一些引导的句子，这样才能引起对方的兴趣，让对方能够听进去或看进去。\n要做好序言，要搞清楚四个关键：背景、冲突、疑问和回答。\n背景指的是跟你主题有关，不需要深入论述，别人也不会产生异议的内容。比如我开篇的第一句话，就是一个简单的背景。\n冲突指的是背景信息里面存在的矛盾，或推动故事情节的转折点。比如我开篇的第二句话。\n疑问说的是用较为清楚的语言把矛盾揭露出来。比如我开篇的第三句话。\n回答指的是答案/结论，就是回答上面的问题，同时引出你的中心思想。比如我开篇的第四句话。\n这个“背景、冲突、疑问、答案”序言四段论不是完全固化的，它可以根据具体的语境、对象、需要，打乱次序、自由调换。比如：开门见山的方式，先说答案，然后说背景，再说冲突和疑问；或者突出忧虑的方式，先说冲突，再说背景，再说答案；或者先给出一个问题，然后说背景，冲突，最后给出答案。\n说完了纵向结构策略，我们再来看金字塔的横向结构策略。 横向结构策略就是表达过程中的“横向逻辑、并列逻辑”。\n首先要做的是，对你要表达的内容进行归类分组，并保证每一组的内容属于同一个范畴，同时给每个范畴起个名字。这个很好理解，比如你整理文件的时候，会把类似的一些内容整理到一个文件夹中，然后再给这个文件夹贴个标签，命个名。把具体的、杂乱的一些内容，进行归纳分组后，既方便我们自身理解和表达，也能让他人更好的理解和接受。\n其次要遵循“逻辑递进”的原则，对每组的具体内容按照一定的逻辑顺序进行分解、拆分、排列。逻辑顺序的具体分类比较多，常见的有三种：一是时间顺序（比如过去、现在、未来，第一步、第二步、第三步等）；二是结构顺序（比如从整体到部分的分解）；三是程度顺序（比如上期讲到过的“时间四象限法”中处理事情的顺序：先紧急且重要，接着重要但不紧急，再到紧急但不重要，最后不紧急不重要）。\n需要注意的是，在做归类分组和排列顺序的时候，要符合MECE法则。MECE法则的意思是“相互独立，完全穷尽”。相互独立说的是拆分的子问题要没有交叉（比如把人分为男人和已婚的人，就有了重叠交叉），完全穷尽说的是拆分的子问题要尽可能周密和面面俱到，不能有遗漏。\n最后我们来简单总结一下：金字塔纵向结构策略的“结论先行和以上统下”让我们突出重点、层次分明；金字塔横向结构策略的“归类分组和顺序排列”，让我们的内容既有广度、也有深度。如此，我们就可以做到逻辑清晰的思考、写作和表达了，更重要的是让我们的内容更容易被别人看懂、听懂。\n","date":"18 January 2024","permalink":"/read/%E9%87%91%E5%AD%97%E5%A1%94%E5%8E%9F%E7%90%86/","section":"阅读","summary":"在生活、特别是工作当中，我们每个人都会碰到需要向他人以书面形式或口头形式阐述自己的观点、想法的情况。可现实中，我们说话做事经常出现逻辑混乱、想到哪说到哪，不能很好地突出重点，层次分明的把一件事情说清楚，这往往会限制、阻碍我们的发展。面对这些问题，如何才能突出重点、逻辑清晰、主次分明的思考、表达和解决问题呢？学会“金字塔原理”就够了！","title":"《金字塔原理》读书笔记"},{"content":"23年秋季课上讲的模式有：状态、观察、单例、装饰者、工厂方法、抽象工厂、适配器、外观模式、模板方法、命令、迭代器、组合、策略、代理模式\n比较代理模式和适配器模式\n适配器模式是因为新旧接口不一致导致出现了客户端无法得到满足的问题，但是，由于旧的接口是不能被完全重构掉的，因为我们还想使用实现了这个接口的一些服务。那么为了使用以前实现旧接口的服务，我们就应该把新的接口转换成旧接口；实现这个转换的类就是抽象意义的转换器； 代理就不一样了，虽然代理也同样是增加了一层，但是，代理提供的接口和原本的接口是一样的，代理模式的作用是不把实现直接暴露给client，而是通过代理这个层，代理能够做一些处理； 比较工厂方法和抽象工厂方法\n工厂方法模式可以看成是抽象方法模式的一种特例，工厂方法模式是创建一个产品结构的，而抽象工厂模式是用来创建多个产品结构的。 工厂方法只有一个产品抽象类，而抽象工厂有多个产品抽象类。 工厂方法模式中的工厂具体类只能创建一类产品类对象，而抽象工厂模式的具体工厂可以创建多个产品类的实例。 比较透明组合和安全组合：\n安全组合\n透明组合\n透明组合模式的缺点是不够安全，因为叶子对象和容器对象在本质是有区别的。叶子对象不可能有下一个层次的对象，即不可能包含成员对象，因此为其提供add，remove，getChild等方法是没有意义的，这在编译阶段不会出错，但在运行阶段如果调用这些方法可能会出错。（如果没有提供相应的错误处理代码）。 抽象构件中没有声明任何用于管理成员对象的方法，对于叶子对象，客户端不可能调用到这些方法，所以这种做法是安全的。而安全组合模式的缺点就是不够透明，因为叶子构件和容器构件具有不同的方法，且容器构件中那些用于管理成员对象的方法没有在抽象构件类中定义，因此客户端不能完全针对抽象编程，必须有区别地对待叶子构件和容器构件。 比较 strict 观察者模式和 relaxed 观察者模式（这个是我们论文里的）\nMVC如何体现设计模式\n策略模式：View和controller使用了策略模式，controller提供策略 观察者模式：状态发生变化的时候让相关的部分进行更新，使用观察者模式让model独立于viewandcontroller，可以在同样的model下使用不同的view 组合模式：展示界面包括了一系列的组件，这些组件或者是leaf，或者是composite，所以当controller告诉view更新的时候，只需要告诉顶层容器即可 考点 # 题型：简答、应⽤用\n题⽬ # 设计⼀一个新模式 Get your ideas down on the paper in a way others can understand. 结合⼀个已有的模式定义⼀个新的模式。 Thinking in pattern kiss(Keep it simple) 对不需要的部分就不使⽤ 复习 # ⼯厂\n⼯厂对产品有么不同的要求? 设计原则 对开闭原则的支持程度(哪些修改⽀持，哪些不⽀持) 设计细节 ⼯厂之间的关系 ⼯厂⽅法的优缺点(见PPT) 简单⼯厂对开闭支持最差，⼯厂⽅法符合“开闭原则” ⼯厂⽅法，⼯厂父类负责定义产品对象的公共接口，⼯厂⼦类负责⽣成具体的产品对象，将产品类的实例化操作延迟到⼦类。 简单⼯厂简化了产品的生产过程，⼯厂⽅法通过继承来实现灵活性。 三种⼯厂⽅法的优缺点\n抽象⼯厂的退化\n当抽象⼯厂模式中每一个具体⼯厂类只创建⼀个产品对象，也就是只存在一个产品等级结构时，抽象⼯厂模式退化成⼯厂⽅法模式； 当⼯厂⽅法模式中抽象⼯厂与具体⼯厂合并，提供⼀个统⼀的⼯厂来创建产品对象，并将创建对象的⼯厂⽅法设计为静态⽅法时，⼯厂⽅法模式退化成简单⼯厂模式。 单例模式\n主要掌握代码实现，有三个注意点：\n单例例类的构造函数为私有; 提供⼀个⾃自身的静态私有成员变量; 提供⼀个公有的静态⼯厂⽅法。 优缺点\n适配器模式\n(画类图注意实线虚线) 适配器的扩展，如何对适配器模版进⾏双向\n双向适配器P32\n适配器器别名Wrapper，装饰器别名也为Wrapper 适配器，同时实现Target接口还有Adaptee的方法\n对象适配器器 关联关系 类适配器器\n模式透明性(透明性和安全性的考量)\n客户代码希望变化是透明的 更更注重容器器内组合结构的时候，可以牺牲透明，实现安全组合模式\n享元模式Flyweight Pool\n⼯厂模式和享元模式可以结合\n享元的内部状态和外部状态 享元⼯厂\n2019年 # 简答题 # 1. 请写出与工厂方法模式有关的OO原则 # 工厂方法模式是一种创建型设计模式，其主要目的是定义一个用于创建对象的接口，但将对象的实际实例化延迟到子类中。在使用工厂方法模式时，可以遵循以下面向对象的设计原则：\n开闭原则（Open/Closed Principle）： 定义： 软件实体（类、模块、函数等）应该对扩展开放，对修改关闭。 关联工厂方法模式： 工厂方法模式支持新产品的添加，只需创建新的具体产品类和相应的具体工厂类，而无需修改已有代码。 实现方式： 定义产品接口和工厂接口，通过子类扩展新产品和工厂，而不是修改已有的类。 单一职责原则（Single Responsibility Principle）： 定义： 一个类应该只有一个引起变化的原因，即一个类应该只有一个职责。 关联工厂方法模式： 工厂方法模式的每个具体工厂类负责创建一种具体产品，确保每个类都有清晰的职责，即产品的创建。 实现方式： 一个具体工厂类对应一个具体产品类，每个类都有自己的责任领域。 依赖倒置原则（Dependency Inversion Principle）： 定义： 高层模块不应该依赖于低层模块，二者都应该依赖于抽象。抽象不应该依赖于细节，细节应该依赖于抽象。 关联工厂方法模式： 客户端代码（高层模块）通过工厂接口（抽象）与具体工厂类进行交互，而不直接依赖于具体产品类。 实现方式： 客户端依赖于工厂接口，而具体工厂类和具体产品类都实现这个接口，实现了抽象和细节的分离。 2. 什么是设计模式？什么是design pattern catalog？（原题就是这么写的） # 记录设计面向对象软件的经验；\n每个设计模式在面向对象系统中系统地命名，解释和评估一个重要而且重复的设计； 目标是以人们可以有效使用的形式捕捉设计经验。\n设计模式目录包括创建型（单例、工厂模式）、行为型（命令、迭代器、状态、策略、模版方法）、结构型（适配器、组合、装饰、外观、代理），每一个设计模式包括下面四个元素：\npattern name模式名称，一个助记名，它用两个词来描述模式的问题、解决方案和效果。 problem问题，描述了应该在何时使用模式。 solution解决方案，描述了设计的组成成分，它们之间的相互关系及各自的职责和协作方式。 consequence效果，描述了模式应用的效果及使用模式应权衡的问题。 3. 观察者模式中的update方法是否必须？请说明理由。 # 在观察者模式中，update 方法并不是必须的，因为具体的观察者类可以根据需要选择实现这个方法或不实现。观察者模式的核心思想是定义对象间的一对多的依赖关系，当一个对象（主题或被观察者）的状态发生变化时，所有依赖于它的对象（观察者）都得到通知并被自动更新。\n在典型的观察者模式中，通常有两种方式来通知观察者：\n推模型： 主题对象推送详细信息给观察者，观察者在接收到通知后，调用相应的更新方法进行处理。 拉模型： 主题对象只通知观察者发生了变化，观察者根据需要自行从主题对象中获取详细信息。 如果使用推模型，那么观察者类通常需要实现 update 方法，因为主题对象需要调用这个方法来传递详细信息。但如果使用拉模型，观察者类可以选择性地实现 update 方法，因为观察者可以通过其他方式获取信息。\n4. 比较策略模式和状态模式。 # 策略模式（Strategy Pattern）和状态模式（State Pattern）都属于行为型设计模式，它们有一些相似之处，但在用途和实现上存在一些关键的区别。\n相似之处：\n目标： 两者的目标都是定义一系列算法或行为，并且使得这些算法或行为可以相互替换，从而使得系统更加灵活、可扩展和可维护。 结构： 在两者中，都通过定义一组类来实现这些算法或行为，使得它们可以独立变化而不影响客户端。 不同之处：\n关注点： 策略模式： 关注的是算法的不同实现，客户端可以选择使用不同的策略来达到相同的目标。 状态模式： 关注的是对象在不同状态下的不同行为，对象在不同状态下有不同的行为响应。 切换时机： 策略模式： 切换策略是由客户端控制的，客户端根据需要决定何时切换不同的策略。 状态模式： 切换状态通常是由对象自身内部状态改变触发的，状态的改变会导致对象的行为发生变化。 关系复杂度： 策略模式： 各个策略之间通常是独立的，彼此之间没有太多关联。 状态模式： 不同的状态之间通常存在复杂的转换关系，状态之间的切换可能受到一些条件的限制。 上下文对象的角色： 策略模式： 上下文对象了解各个策略，负责选择和切换策略。 状态模式： 上下文对象不仅了解各个状态，还负责管理状态之间的切换。 5. 比较适配器模式、外观模式、装饰者模式的意图。 # 见前\n6. 比较透明组合和安全组合。 # 见前\n7. 圣诞节快到了，我们需要生产礼物，要确保礼物是能适合所有年龄段的，并且礼物中给出了建议的年龄段，请问要用什么设计模式来实现，并解释。 # 为了实现适合所有年龄段的礼物，并且为每个礼物给出了建议的年龄段，可以考虑使用工厂方法模式和抽象工厂模式。这两种设计模式可以帮助你灵活地创建不同类型的礼物，并为每个礼物定义适合的年龄段。\n8. 比较strict观察者模式和relaxed观察者模式（这个是我们论文里的） # 设计题 # 1、第一题是书上装饰者模式的例子，要求画出类图，写出代码，把书上例子看懂没问题。 # 2、写出一个多例模式 # 是有个数限制的单例\npublic class Multiton { private static Multiton multi1 = new Multiton(); private static Multiton multi2 = new Multiton(); private Multiton() {} public static Multiton getInstance(int key) { if(key == 1) { return multi1; } else { return multi2; } } } 3、写出猫狗的双向适配器，猫有捉老鼠catchMouse的方法，狗有bark的方法，写一个适配器，让猫能bark，狗能catchMouse。（2018年的原题） # package com; public interface Icat { public void catLooks(); public void zhuoshu(); } package com; public interface Idog { public void dogLooks(); public void goujiao(); } package com; public class IcatImpl implements Icat { @Override public void catLooks() { System.out.println(\u0026#34;我有猫的外表\u0026#34;); } @Override public void zhuoshu() { System.out.println(\u0026#34;我能抓老鼠\u0026#34;); } } package com; public class IdogImpl implements Idog { @Override public void dogLooks() { System.out.println(\u0026#34;我有狗的外表\u0026#34;); } @Override public void goujiao() { System.out.println(\u0026#34;我会狗叫\u0026#34;); } } package com; public class Adapter implements Icat, Idog { Icat cat=null; Idog dog=null; public Adapter(Icat cat) { this.cat=cat; } public Adapter(Idog dog) { this.dog=dog; } @Override public void dogLooks() { System.out.println(\u0026#34;我是一只狗\u0026#34;); } @Override public void goujiao() { cat.zhuoshu(); } @Override public void catLooks() { System.out.println(\u0026#34;我是一只猫\u0026#34;); } @Override public void zhuoshu() { dog.goujiao(); } } package com; public class Main { public static void main(String[] args) { //把狗设配成猫 Icat fakercat=new Adapter(new IdogImpl()); fakercat.catLooks(); fakercat.zhuoshu(); //把猫设配成狗 Idog fakerdog=new Adapter(new IcatImpl()); fakerdog.dogLooks(); fakerdog.goujiao(); } } 4、背景：公司中有许多部门，呈现树状结构，现在公司的OA系统要有通知功能，它能够通知一个部门的所有人，也能够通知single individual，请你设计这个系统的结构，使用Gama（伽马）表示法，画出示系统示意图。（设计模式可视化论文中的） # 2018年 # 简答题 # 1.\t请写出至少三个 OO 设计原则，并说明在状态模式中是如何体现的。 # 开闭原则（Open Close Principle） 开闭原则的意思是：对扩展开放，对修改关闭。在程序需要进行拓展的时候，不能去修改原有的代码，实现一个热插拔的效果。简言之，是为了使程序的扩展性好，易于维护和升级。想要达到这样的效果，我们需要使用接口和抽象类，后面的具体设计中我们会提到这点。(别写,状态不符合开闭) 依赖倒转原则（Dependence Inversion Principle） 这个原则是开闭原则的基础，具体内容：针对接口编程，依赖于抽象而不依赖于具体。 State是一个接口 迪米特法则，又称最少知道原则（Demeter Principle） 最少知道原则是指：一个实体应当尽量少地与其他实体之间发生相互作用，使得系统功能模块相对独立。 Context 调用State 合成复用原则（Composite Reuse Principle） 合成复用原则是指：尽量使用合成/聚合的方式，而不是使用继承 Context 里组合了State 2.\t求写出宏命令的概念，并用代码实现宏命令 # 是命令模式和组合模式联用的产物\n// 数字接受者： package DesignPatterns.CommandMode2; import java.util.Random; public class PrintNumber { public void printNumber() { Random random=new Random(10); int a=random.nextInt(); System.out.println(\u0026#34;number:\u0026#34;+a); } } // 语言接受者： package DesignPatterns.CommandMode2; public class PrintLetter { public void printEnglish() { String eng=\u0026#34;English\u0026#34;; System.out.println(eng); } public void printRussian() { String russian=\u0026#34;Ж жЗ з Z 兹И и I 伊\u0026#34;; System.out.println(russian); } } // 命令接口： package DesignPatterns.CommandMode2; public interface Command { void execute(); } // 英文具体命令： package DesignPatterns.CommandMode2; public class PrintEnglishCommand implements Command{ private PrintLetter printLetter; public PrintEnglishCommand(PrintLetter printLetter) { this.printLetter=printLetter; } @Override public void execute() { printLetter.printEnglish(); } } // 俄文具体命令： package DesignPatterns.CommandMode2; public class PrintRussianCommand implements Command { private PrintLetter printLetter; public PrintRussianCommand(PrintLetter printLetter) { this.printLetter=printLetter; } @Override public void execute() { printLetter.printRussian(); } } // 数字具体命令： package DesignPatterns.CommandMode2; public class PrintEvenNumberCommand implements Command { private PrintNumber printNumber; public PrintEvenNumberCommand(PrintNumber printNumber) { this.printNumber=printNumber; } @Override public void execute() { printNumber.printNumber(); } } 宏命令（包含英文命令，俄文命令，数字命令的集合） package DesignPatterns.CommandMode2; import java.util.ArrayList; public class MacroCommand implements Command { private ArrayList\u0026lt;Command\u0026gt; arrayList; public MacroCommand(ArrayList\u0026lt;Command\u0026gt; arrayList) { this.arrayList=arrayList; } @Override public void execute() { for (int i=0;i\u0026lt;arrayList.size();i++) { Command command=arrayList.get(i); command.execute(); } } } // 请求者： package DesignPatterns.CommandMode2; public class RequestMakedir { private Command command; public void setCommand(Command command) { this.command=command; } public void executeCommand() { command.execute(); } } // 测试类： package DesignPatterns.CommandMode2; import java.util.ArrayList; public class Application { public static void main(String[] args) { ArrayList\u0026lt;Command\u0026gt; arrayList=new ArrayList\u0026lt;\u0026gt;(); Command command1=new PrintEnglishCommand(new PrintLetter()); Command command2=new PrintRussianCommand(new PrintLetter()); Command command3=new PrintEvenNumberCommand(new PrintNumber()); arrayList.add(command1); arrayList.add(command2); arrayList.add(command3); Command macroCommand=new MacroCommand(arrayList); RequestMakedir requestMakedir=new RequestMakedir(); requestMakedir.setCommand(macroCommand); requestMakedir.executeCommand(); } } 3.\t比较策略模式和状态模式。 # 见前\n4. 比较透明组合和安全组合。 # 见前\n5. 说出建造者模式的适用场景。 # 意图：将一个复杂的构建与其表示相分离，使得同样的构建过程可以创建不同的表示。 主要解决：主要解决在软件系统中，有时候面临着\u0026quot;一个复杂对象\u0026quot;的创建工作，其通常由各个部分的子对象用一定的算法构成；由于需求的变化，这个复杂对象的各个部分经常面临着剧烈的变化，但是将它们组合在一起的算法却相对稳定。 何时使用：一些基本部件不会变，而其组合经常变化的时候。 6.\t什么是设计模式？如何使用设计模式目录？ # 见前\n7. 在桥接模式中，如何实现从实现中抽象解耦？ # 在一个软件系统中的抽象化和实现化之间使用组合关系，而不是继承关系，从而使两者可以相对对立的变化。\n让抽象部分和实现部分独立出来，分别定义接口\n8. MVC 如何体现设计模式？ # Erich Gamma等人归纳总结了23个经典的设计模式，每一个设计模式都给出了一个完美的解决方 案。MVC是一个比较高层的模式，它由多个更基本的设计模式组合而成，其中与 MVC最为密切相关的有 三种设计模式：观察者模式 (Observer)、策略模式 (Strategy)和合成模式 (Composite) 。\nObserver模式 Observer模式定义对象间的一对多的依赖关系，当一个对象的值或状态发生改变时，所有与它有依赖 关系的对象都得到通知并自动更新。某一数据可能有多种显示方式，并且可能同时以不同的方式显示，通过某一种方式改变了数据，那么其他的显示都应该能立即知道数据的改变和做相应的调整 MVC通过使用 push和 pull的技术分离了Model和 View，使得端界面代码与数据和逻辑分离开来。 当View中的操作导致 Model数据发生变化的时候，它通过 Controller将变化通知给 Model(push技术)； 当Model中的数据被其他客户端改变的时候，Model将数据更新通知给View(pull技术)，从而保证 View 能始终正确地反映出Model的内容和状态 。模型的状态和视图的显示相互响应，Model_View的关系实际 上就是被描述为 Observer的设计模式， Strategy模式 Strategy模式定义一系列的算法，并且把它们封装起来，使它们可以互相替换，使得算法可以独立于使 用它的客户端的变化。 MVC可以在不改变 View的情况下改变 View对用户输入的响应方式，这对于一个经常需要变更响应 逻辑的系统来说是非常重要的。MVC把响应逻辑封装在Controller中。有一个 Controller的类层次结构，可 以方便地对原有 Controller做适当改变，创建新的Controller。View用一个特定的Controller子类的实例来 实现一个特定的响应策略，更换不同的Controller实例，可以改变View对用户输入的响应。还可以在运行 时通过改变 View的Controller来改变View对用户输入的响应策略。这种 View—Controller的关系就是被 描述为 Strategy 的设计模式。 Composite模式 Composite模式将对象组合成树形结构以表示\u0026quot;部分 一整体\u0026quot;层次结构。Composite使组合对象的使用 和单个对象的使用具有一致性。 MVC的一个重要特征就是View可以嵌套。嵌套的组合视图可用于任何视图可用的地方，而且可以管 理嵌套视图。这种思想反映出将组合的视图与其组件平等对待的设计。这种设计思想在面向对象领域内被 描述为 Composite的设计模式。 当然，MVC还使用了其他的设计模式，如：用来指定视图缺省控制器的FactoryMethod、实现滚动视图 的Decorator、单文档的Singleton等等。MVC可以看作是一些设计模式进行组合之后的结果。 9. 享元模式如何实现对象复用且不是完全相同？ # 享元模式尝试重用现有的同类对象，如果未找到匹配的对象，则创建新对象。 内部状态相同的对象复用,不是完全相同是因为外部状态不同\n享元模式以共享的方式高效地支持大量的细粒度对象，享元对象能做到共享的关键是区分内部状态(Internal State)和外部状态(External State)。其中：\n内部状态 是存储在享元对象内部并且不会随环境改变而改变的状态，因此内部状态可以共享。 外部状态 是随环境改变而改变的、不可以共享的状态。享元对象的外部状态必须由客户端保存，并在享元对象被创建之后，在需要使用的时候再传入到享元对象内部。一个外部状态与另一个外部状态之间是相互独立的。 设计题 # 用代码实现一个多例模式。 # 见前\n写一个双向适配器的代码，并画出类图。以 Cat 会捉老鼠，Dog 会 bark 为例，实现 Cat bark，Dog catch # 见前\n设计一个 OA 系统。公司需要向部门全体员工或个人发送消息。 # https://www.runoob.com/design-pattern/observer-pattern.html\n观察者\n","date":"31 December 2023","permalink":"/posts/reviews/%E7%A0%94%E7%A9%B6%E7%94%9F%E5%A4%8D%E4%B9%A0/%E9%AB%98%E7%BA%A7%E8%BD%AF%E4%BB%B6%E8%AE%BE%E8%AE%A1/","section":"博客","summary":"南京大学软件学院高级软件设计简答题部分复习","title":"高级软件设计复习"},{"content":"","date":"31 December 2023","permalink":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","section":"Tags","summary":"","title":"设计模式"},{"content":"","date":"31 December 2023","permalink":"/posts/design-pattern/","section":"博客","summary":"设计模式是解决问题的方案，学习现有的设计模式可以做到经验复用。拥有设计模式词汇，在沟通时就能用更少的词汇来讨论，并且不需要了解底层细节。","title":"设计模式"},{"content":"一、前言 # 设计模式是解决问题的方案，学习现有的设计模式可以做到经验复用。拥有设计模式词汇，在沟通时就能用更少的词汇来讨论，并且不需要了解底层细节。\n二、创建型 # 单例 简单工厂 工厂方法 抽象工厂 生成器 原型模式 三、行为型 # 责任链 命令 解释器 迭代器 中介者 备忘录 观察者 状态 策略 模板方法 访问者 空对象 四、结构型 # 适配器 桥接 组合 装饰 外观 享元 代理 参考资料 # 弗里曼. Head First 设计模式 [M]. 中国电力出版社, 2007. Gamma E. 设计模式: 可复用面向对象软件的基础 [M]. 机械工业出版社, 2007. Bloch J. Effective java[M]. Addison-Wesley Professional, 2017. Design Patterns Design patterns implemented in Java The breakdown of design patterns in JDK ","date":"31 December 2023","permalink":"/posts/design-pattern/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%9B%AE%E5%BD%95/","section":"博客","summary":"设计模式是解决问题的方案，学习现有的设计模式可以做到经验复用。拥有设计模式词汇，在沟通时就能用更少的词汇来讨论，并且不需要了解底层细节。","title":"设计模式-目录"},{"content":"5. 中介者（Mediator） # Intent # 集中相关对象之间复杂的沟通和控制方式。\nClass Diagram # Mediator：中介者，定义一个接口用于与各同事（Colleague）对象通信。 Colleague：同事，相关对象 Implementation # Alarm（闹钟）、CoffeePot（咖啡壶）、Calendar（日历）、Sprinkler（喷头）是一组相关的对象，在某个对象的事件产生时需要去操作其它对象，形成了下面这种依赖结构：\n使用中介者模式可以将复杂的依赖结构变成星形结构：\npublic abstract class Colleague { public abstract void onEvent(Mediator mediator); } public class Alarm extends Colleague { @Override public void onEvent(Mediator mediator) { mediator.doEvent(\u0026#34;alarm\u0026#34;); } public void doAlarm() { System.out.println(\u0026#34;doAlarm()\u0026#34;); } } public class CoffeePot extends Colleague { @Override public void onEvent(Mediator mediator) { mediator.doEvent(\u0026#34;coffeePot\u0026#34;); } public void doCoffeePot() { System.out.println(\u0026#34;doCoffeePot()\u0026#34;); } } public class Calender extends Colleague { @Override public void onEvent(Mediator mediator) { mediator.doEvent(\u0026#34;calender\u0026#34;); } public void doCalender() { System.out.println(\u0026#34;doCalender()\u0026#34;); } } public class Sprinkler extends Colleague { @Override public void onEvent(Mediator mediator) { mediator.doEvent(\u0026#34;sprinkler\u0026#34;); } public void doSprinkler() { System.out.println(\u0026#34;doSprinkler()\u0026#34;); } } public abstract class Mediator { public abstract void doEvent(String eventType); } public class ConcreteMediator extends Mediator { private Alarm alarm; private CoffeePot coffeePot; private Calender calender; private Sprinkler sprinkler; public ConcreteMediator(Alarm alarm, CoffeePot coffeePot, Calender calender, Sprinkler sprinkler) { this.alarm = alarm; this.coffeePot = coffeePot; this.calender = calender; this.sprinkler = sprinkler; } @Override public void doEvent(String eventType) { switch (eventType) { case \u0026#34;alarm\u0026#34;: doAlarmEvent(); break; case \u0026#34;coffeePot\u0026#34;: doCoffeePotEvent(); break; case \u0026#34;calender\u0026#34;: doCalenderEvent(); break; default: doSprinklerEvent(); } } public void doAlarmEvent() { alarm.doAlarm(); coffeePot.doCoffeePot(); calender.doCalender(); sprinkler.doSprinkler(); } public void doCoffeePotEvent() { // ... } public void doCalenderEvent() { // ... } public void doSprinklerEvent() { // ... } } public class Client { public static void main(String[] args) { Alarm alarm = new Alarm(); CoffeePot coffeePot = new CoffeePot(); Calender calender = new Calender(); Sprinkler sprinkler = new Sprinkler(); Mediator mediator = new ConcreteMediator(alarm, coffeePot, calender, sprinkler); // 闹钟事件到达，调用中介者就可以操作相关对象 alarm.onEvent(mediator); } } doAlarm() doCoffeePot() doCalender() doSprinkler() JDK # All scheduleXXX() methods of java.util.Timer java.util.concurrent.Executor#execute() submit() and invokeXXX() methods of java.util.concurrent.ExecutorService scheduleXXX() methods of java.util.concurrent.ScheduledExecutorService java.lang.reflect.Method#invoke() ","date":"31 December 2023","permalink":"/posts/design-pattern/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E4%B8%AD%E4%BB%8B%E8%80%85/","section":"博客","summary":"集中相关对象之间复杂的沟通和控制方式。","title":"设计模式 - 中介者"},{"content":"6. 原型模式（Prototype） # Intent # 使用原型实例指定要创建对象的类型，通过复制这个原型来创建新对象。\nClass Diagram # Implementation # public abstract class Prototype { abstract Prototype myClone(); } public class ConcretePrototype extends Prototype { private String filed; public ConcretePrototype(String filed) { this.filed = filed; } @Override Prototype myClone() { return new ConcretePrototype(filed); } @Override public String toString() { return filed; } } public class Client { public static void main(String[] args) { Prototype prototype = new ConcretePrototype(\u0026#34;abc\u0026#34;); Prototype clone = prototype.myClone(); System.out.println(clone.toString()); } } abc JDK # java.lang.Object#clone() ","date":"31 December 2023","permalink":"/posts/design-pattern/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F/","section":"博客","summary":"使用原型实例指定要创建对象的类型，通过复制这个原型来创建新对象。","title":"设计模式 - 原型模式"},{"content":"备忘录（Memento） # Intent # 在不违反封装的情况下获得对象的内部状态，从而在需要时可以将对象恢复到最初状态。\nClass Diagram # Originator：原始对象 Caretaker：负责保存好备忘录 Memento：备忘录，存储原始对象的状态。备忘录实际上有两个接口，一个是提供给 Caretaker 的窄接口：它只能将备忘录传递给其它对象；一个是提供给 Originator 的宽接口，允许它访问到先前状态所需的所有数据。理想情况是只允许 Originator 访问本备忘录的内部状态。 Implementation # 以下实现了一个简单计算器程序，可以输入两个值，然后计算这两个值的和。备忘录模式允许将这两个值存储起来，然后在某个时刻用存储的状态进行恢复。\n实现参考： Memento Pattern - Calculator Example - Java Sourcecode\n/** * Originator Interface */ public interface Calculator { // Create Memento PreviousCalculationToCareTaker backupLastCalculation(); // setMemento void restorePreviousCalculation(PreviousCalculationToCareTaker memento); int getCalculationResult(); void setFirstNumber(int firstNumber); void setSecondNumber(int secondNumber); } /** * Originator Implementation */ public class CalculatorImp implements Calculator { private int firstNumber; private int secondNumber; @Override public PreviousCalculationToCareTaker backupLastCalculation() { // create a memento object used for restoring two numbers return new PreviousCalculationImp(firstNumber, secondNumber); } @Override public void restorePreviousCalculation(PreviousCalculationToCareTaker memento) { this.firstNumber = ((PreviousCalculationToOriginator) memento).getFirstNumber(); this.secondNumber = ((PreviousCalculationToOriginator) memento).getSecondNumber(); } @Override public int getCalculationResult() { // result is adding two numbers return firstNumber + secondNumber; } @Override public void setFirstNumber(int firstNumber) { this.firstNumber = firstNumber; } @Override public void setSecondNumber(int secondNumber) { this.secondNumber = secondNumber; } } /** * Memento Interface to Originator * * This interface allows the originator to restore its state */ public interface PreviousCalculationToOriginator { int getFirstNumber(); int getSecondNumber(); } /** * Memento interface to CalculatorOperator (Caretaker) */ public interface PreviousCalculationToCareTaker { // no operations permitted for the caretaker } /** * Memento Object Implementation * \u0026lt;p\u0026gt; * Note that this object implements both interfaces to Originator and CareTaker */ public class PreviousCalculationImp implements PreviousCalculationToCareTaker, PreviousCalculationToOriginator { private int firstNumber; private int secondNumber; public PreviousCalculationImp(int firstNumber, int secondNumber) { this.firstNumber = firstNumber; this.secondNumber = secondNumber; } @Override public int getFirstNumber() { return firstNumber; } @Override public int getSecondNumber() { return secondNumber; } } /** * CareTaker object */ public class Client { public static void main(String[] args) { // program starts Calculator calculator = new CalculatorImp(); // assume user enters two numbers calculator.setFirstNumber(10); calculator.setSecondNumber(100); // find result System.out.println(calculator.getCalculationResult()); // Store result of this calculation in case of error PreviousCalculationToCareTaker memento = calculator.backupLastCalculation(); // user enters a number calculator.setFirstNumber(17); // user enters a wrong second number and calculates result calculator.setSecondNumber(-290); // calculate result System.out.println(calculator.getCalculationResult()); // user hits CTRL + Z to undo last operation and see last result calculator.restorePreviousCalculation(memento); // result restored System.out.println(calculator.getCalculationResult()); } } 110 -273 110 JDK # java.io.Serializable ","date":"31 December 2023","permalink":"/posts/design-pattern/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%A4%87%E5%BF%98%E5%BD%95/","section":"博客","summary":"在不违反封装的情况下获得对象的内部状态，从而在需要时可以将对象恢复到最初状态。","title":"设计模式 - 备忘录"},{"content":"工厂方法（Factory Method） # Intent # 定义了一个创建对象的接口，但由子类决定要实例化哪个类。工厂方法把实例化操作推迟到子类。\nClass Diagram # 在简单工厂中，创建对象的是另一个类，而在工厂方法中，是由子类来创建对象。\n下图中，Factory 有一个 doSomething() 方法，这个方法需要用到一个产品对象，这个产品对象由 factoryMethod() 方法创建。该方法是抽象的，需要由子类去实现。\nImplementation # public abstract class Factory { abstract public Product factoryMethod(); public void doSomething() { Product product = factoryMethod(); // do something with the product } } public class ConcreteFactory extends Factory { public Product factoryMethod() { return new ConcreteProduct(); } } public class ConcreteFactory1 extends Factory { public Product factoryMethod() { return new ConcreteProduct1(); } } public class ConcreteFactory2 extends Factory { public Product factoryMethod() { return new ConcreteProduct2(); } } JDK # java.util.Calendar java.util.ResourceBundle java.text.NumberFormat java.nio.charset.Charset java.net.URLStreamHandlerFactory java.util.EnumSet javax.xml.bind.JAXBContext ","date":"31 December 2023","permalink":"/posts/design-pattern/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%B7%A5%E5%8E%82%E6%96%B9%E6%B3%95/","section":"博客","summary":"定义了一个创建对象的接口，但由子类决定要实例化哪个类。工厂方法把实例化操作推迟到子类。","title":"设计模式 - 工厂方法"},{"content":"4. 抽象工厂（Abstract Factory） # Intent # 提供一个接口，用于创建 相关的对象家族 。\nClass Diagram # 抽象工厂模式创建的是对象家族，也就是很多对象而不是一个对象，并且这些对象是相关的，也就是说必须一起创建出来。而工厂方法模式只是用于创建一个对象，这和抽象工厂模式有很大不同。\n抽象工厂模式用到了工厂方法模式来创建单一对象，AbstractFactory 中的 createProductA() 和 createProductB() 方法都是让子类来实现，这两个方法单独来看就是在创建一个对象，这符合工厂方法模式的定义。\n至于创建对象的家族这一概念是在 Client 体现，Client 要通过 AbstractFactory 同时调用两个方法来创建出两个对象，在这里这两个对象就有很大的相关性，Client 需要同时创建出这两个对象。\n从高层次来看，抽象工厂使用了组合，即 Cilent 组合了 AbstractFactory，而工厂方法模式使用了继承。\nImplementation # public class AbstractProductA { } public class AbstractProductB { } public class ProductA1 extends AbstractProductA { } public class ProductA2 extends AbstractProductA { } public class ProductB1 extends AbstractProductB { } public class ProductB2 extends AbstractProductB { } public abstract class AbstractFactory { abstract AbstractProductA createProductA(); abstract AbstractProductB createProductB(); } public class ConcreteFactory1 extends AbstractFactory { AbstractProductA createProductA() { return new ProductA1(); } AbstractProductB createProductB() { return new ProductB1(); } } public class ConcreteFactory2 extends AbstractFactory { AbstractProductA createProductA() { return new ProductA2(); } AbstractProductB createProductB() { return new ProductB2(); } } public class Client { public static void main(String[] args) { AbstractFactory abstractFactory = new ConcreteFactory1(); AbstractProductA productA = abstractFactory.createProductA(); AbstractProductB productB = abstractFactory.createProductB(); // do something with productA and productB } } JDK # javax.xml.parsers.DocumentBuilderFactory javax.xml.transform.TransformerFactory javax.xml.xpath.XPathFactory ","date":"31 December 2023","permalink":"/posts/design-pattern/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E6%8A%BD%E8%B1%A1%E5%B7%A5%E5%8E%82/","section":"博客","summary":"提供一个接口，用于创建相关的对象家族。","title":"设计模式 - 抽象工厂"},{"content":"模板方法（Template Method） # Intent # 定义算法框架，并将一些步骤的实现延迟到子类。\n通过模板方法，子类可以重新定义算法的某些步骤，而不用改变算法的结构。\nClass Diagram # Implementation # 冲咖啡和冲茶都有类似的流程，但是某些步骤会有点不一样，要求复用那些相同步骤的代码。\npublic abstract class CaffeineBeverage { final void prepareRecipe() { boilWater(); brew(); pourInCup(); addCondiments(); } abstract void brew(); abstract void addCondiments(); void boilWater() { System.out.println(\u0026#34;boilWater\u0026#34;); } void pourInCup() { System.out.println(\u0026#34;pourInCup\u0026#34;); } } public class Coffee extends CaffeineBeverage { @Override void brew() { System.out.println(\u0026#34;Coffee.brew\u0026#34;); } @Override void addCondiments() { System.out.println(\u0026#34;Coffee.addCondiments\u0026#34;); } } public class Tea extends CaffeineBeverage { @Override void brew() { System.out.println(\u0026#34;Tea.brew\u0026#34;); } @Override void addCondiments() { System.out.println(\u0026#34;Tea.addCondiments\u0026#34;); } } public class Client { public static void main(String[] args) { CaffeineBeverage caffeineBeverage = new Coffee(); caffeineBeverage.prepareRecipe(); System.out.println(\u0026#34;-----------\u0026#34;); caffeineBeverage = new Tea(); caffeineBeverage.prepareRecipe(); } } boilWater Coffee.brew pourInCup Coffee.addCondiments ----------- boilWater Tea.brew pourInCup Tea.addCondiments JDK # java.util.Collections#sort() java.io.InputStream#skip() java.io.InputStream#read() java.util.AbstractList#indexOf() ","date":"31 December 2023","permalink":"/posts/design-pattern/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E6%A8%A1%E6%9D%BF%E6%96%B9%E6%B3%95/","section":"博客","summary":"定义算法框架，并将一些步骤的实现延迟到子类。通过模板方法，子类可以重新定义算法的某些步骤，而不用改变算法的结构。","title":"设计模式 - 模板方法"},{"content":"5. 生成器（Builder） # Intent # 封装一个对象的构造过程，并允许按步骤构造。\nClass Diagram # Implementation # 以下是一个简易的 StringBuilder 实现，参考了 JDK 1.8 源码。\npublic class AbstractStringBuilder { protected char[] value; protected int count; public AbstractStringBuilder(int capacity) { count = 0; value = new char[capacity]; } public AbstractStringBuilder append(char c) { ensureCapacityInternal(count + 1); value[count++] = c; return this; } private void ensureCapacityInternal(int minimumCapacity) { // overflow-conscious code if (minimumCapacity - value.length \u0026gt; 0) expandCapacity(minimumCapacity); } void expandCapacity(int minimumCapacity) { int newCapacity = value.length * 2 + 2; if (newCapacity - minimumCapacity \u0026lt; 0) newCapacity = minimumCapacity; if (newCapacity \u0026lt; 0) { if (minimumCapacity \u0026lt; 0) // overflow throw new OutOfMemoryError(); newCapacity = Integer.MAX_VALUE; } value = Arrays.copyOf(value, newCapacity); } } public class StringBuilder extends AbstractStringBuilder { public StringBuilder() { super(16); } @Override public String toString() { // Create a copy, don\u0026#39;t share the array return new String(value, 0, count); } } public class Client { public static void main(String[] args) { StringBuilder sb = new StringBuilder(); final int count = 26; for (int i = 0; i \u0026lt; count; i++) { sb.append((char) (\u0026#39;a\u0026#39; + i)); } System.out.println(sb.toString()); } } abcdefghijklmnopqrstuvwxyz JDK # java.lang.StringBuilder java.nio.ByteBuffer java.lang.StringBuffer java.lang.Appendable Apache Camel builders ","date":"31 December 2023","permalink":"/posts/design-pattern/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%94%9F%E6%88%90%E5%99%A8/","section":"博客","summary":"封装一个对象的构造过程，并允许按步骤构造。","title":"设计模式 - 生成器"},{"content":"空对象（Null） # Intent # 使用什么都不做的空对象来代替 NULL。\n一个方法返回 NULL，意味着方法的调用端需要去检查返回值是否是 NULL，这么做会导致非常多的冗余的检查代码。并且如果某一个调用端忘记了做这个检查返回值，而直接使用返回的对象，那么就有可能抛出空指针异常。\nClass Diagram # Implementation # public abstract class AbstractOperation { abstract void request(); } public class RealOperation extends AbstractOperation { @Override void request() { System.out.println(\u0026#34;do something\u0026#34;); } } public class NullOperation extends AbstractOperation{ @Override void request() { // do nothing } } public class Client { public static void main(String[] args) { AbstractOperation abstractOperation = func(-1); abstractOperation.request(); } public static AbstractOperation func(int para) { if (para \u0026lt; 0) { return new NullOperation(); } return new RealOperation(); } } ","date":"31 December 2023","permalink":"/posts/design-pattern/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%A9%BA%E5%AF%B9%E8%B1%A1/","section":"博客","summary":"使用什么都不做的空对象来代替 NULL。","title":"设计模式 - 空对象"},{"content":"9. 策略（Strategy） # Intent # 定义一系列算法，封装每个算法，并使它们可以互换。\n策略模式可以让算法独立于使用它的客户端。\nClass Diagram # Strategy 接口定义了一个算法族，它们都实现了 behavior() 方法。 Context 是使用到该算法族的类，其中的 doSomething() 方法会调用 behavior()，setStrategy(Strategy) 方法可以动态地改变 strategy 对象，也就是说能动态地改变 Context 所使用的算法。 与状态模式的比较 # 状态模式的类图和策略模式类似，并且都是能够动态改变对象的行为。但是状态模式是通过状态转移来改变 Context 所组合的 State 对象，而策略模式是通过 Context 本身的决策来改变组合的 Strategy 对象。所谓的状态转移，是指 Context 在运行过程中由于一些条件发生改变而使得 State 对象发生改变，注意必须要是在运行过程中。\n状态模式主要是用来解决状态转移的问题，当状态发生转移了，那么 Context 对象就会改变它的行为；而策略模式主要是用来封装一组可以互相替代的算法族，并且可以根据需要动态地去替换 Context 使用的算法。\nImplementation # 设计一个鸭子，它可以动态地改变叫声。这里的算法族是鸭子的叫声行为。\npublic interface QuackBehavior { void quack(); } public class Quack implements QuackBehavior { @Override public void quack() { System.out.println(\u0026#34;quack!\u0026#34;); } } public class Squeak implements QuackBehavior{ @Override public void quack() { System.out.println(\u0026#34;squeak!\u0026#34;); } } public class Duck { private QuackBehavior quackBehavior; public void performQuack() { if (quackBehavior != null) { quackBehavior.quack(); } } public void setQuackBehavior(QuackBehavior quackBehavior) { this.quackBehavior = quackBehavior; } } public class Client { public static void main(String[] args) { Duck duck = new Duck(); duck.setQuackBehavior(new Squeak()); duck.performQuack(); duck.setQuackBehavior(new Quack()); duck.performQuack(); } } squeak! quack! JDK # java.util.Comparator#compare() javax.servlet.http.HttpServlet javax.servlet.Filter#doFilter() ","date":"31 December 2023","permalink":"/posts/design-pattern/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%AD%96%E7%95%A5/","section":"博客","summary":"定义一系列算法，封装每个算法，并使它们可以互换。","title":"设计模式 - 策略"},{"content":"简单工厂（Simple Factory） # Intent # 在创建一个对象时不向客户暴露内部细节，并提供一个创建对象的通用接口。\nClass Diagram # 简单工厂把实例化的操作单独放到一个类中，这个类就成为简单工厂类，让简单工厂类来决定应该用哪个具体子类来实例化。\n这样做能把客户类和具体子类的实现解耦，客户类不再需要知道有哪些子类以及应当实例化哪个子类。客户类往往有多个，如果不使用简单工厂，那么所有的客户类都要知道所有子类的细节。而且一旦子类发生改变，例如增加子类，那么所有的客户类都要进行修改。\nImplementation # public interface Product { } public class ConcreteProduct implements Product { } public class ConcreteProduct1 implements Product { } public class ConcreteProduct2 implements Product { } 以下的 Client 类包含了实例化的代码，这是一种错误的实现。如果在客户类中存在这种实例化代码，就需要考虑将代码放到简单工厂中。\npublic class Client { public static void main(String[] args) { int type = 1; Product product; if (type == 1) { product = new ConcreteProduct1(); } else if (type == 2) { product = new ConcreteProduct2(); } else { product = new ConcreteProduct(); } // do something with the product } } 以下的 SimpleFactory 是简单工厂实现，它被所有需要进行实例化的客户类调用。\npublic class SimpleFactory { public Product createProduct(int type) { if (type == 1) { return new ConcreteProduct1(); } else if (type == 2) { return new ConcreteProduct2(); } return new ConcreteProduct(); } } public class Client { public static void main(String[] args) { SimpleFactory simpleFactory = new SimpleFactory(); Product product = simpleFactory.createProduct(1); // do something with the product } } ","date":"31 December 2023","permalink":"/posts/design-pattern/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%AE%80%E5%8D%95%E5%B7%A5%E5%8E%82/","section":"博客","summary":"在创建一个对象时不向客户暴露内部细节，并提供一个创建对象的通用接口。","title":"设计模式 - 简单工厂"},{"content":"组合（Composite） # Intent # 将对象组合成树形结构来表示“整体/部分”层次关系，允许用户以相同的方式处理单独对象和组合对象。\nClass Diagram # 组件（Component）类是组合类（Composite）和叶子类（Leaf）的父类，可以把组合类看成是树的中间节点。\n组合对象拥有一个或者多个组件对象，因此组合对象的操作可以委托给组件对象去处理，而组件对象可以是另一个组合对象或者叶子对象。\nImplementation # public abstract class Component { protected String name; public Component(String name) { this.name = name; } public void print() { print(0); } abstract void print(int level); abstract public void add(Component component); abstract public void remove(Component component); } public class Composite extends Component { private List\u0026lt;Component\u0026gt; child; public Composite(String name) { super(name); child = new ArrayList\u0026lt;\u0026gt;(); } @Override void print(int level) { for (int i = 0; i \u0026lt; level; i++) { System.out.print(\u0026#34;--\u0026#34;); } System.out.println(\u0026#34;Composite:\u0026#34; + name); for (Component component : child) { component.print(level + 1); } } @Override public void add(Component component) { child.add(component); } @Override public void remove(Component component) { child.remove(component); } } public class Leaf extends Component { public Leaf(String name) { super(name); } @Override void print(int level) { for (int i = 0; i \u0026lt; level; i++) { System.out.print(\u0026#34;--\u0026#34;); } System.out.println(\u0026#34;left:\u0026#34; + name); } @Override public void add(Component component) { throw new UnsupportedOperationException(); // 牺牲透明性换取单一职责原则，这样就不用考虑是叶子节点还是组合节点 } @Override public void remove(Component component) { throw new UnsupportedOperationException(); } } public class Client { public static void main(String[] args) { Composite root = new Composite(\u0026#34;root\u0026#34;); Component node1 = new Leaf(\u0026#34;1\u0026#34;); Component node2 = new Composite(\u0026#34;2\u0026#34;); Component node3 = new Leaf(\u0026#34;3\u0026#34;); root.add(node1); root.add(node2); root.add(node3); Component node21 = new Leaf(\u0026#34;21\u0026#34;); Component node22 = new Composite(\u0026#34;22\u0026#34;); node2.add(node21); node2.add(node22); Component node221 = new Leaf(\u0026#34;221\u0026#34;); node22.add(node221); root.print(); } } Composite:root --left:1 --Composite:2 ----left:21 ----Composite:22 ------left:221 --left:3 JDK # javax.swing.JComponent#add(Component) java.awt.Container#add(Component) java.util.Map#putAll(Map) java.util.List#addAll(Collection) java.util.Set#addAll(Collection) ","date":"31 December 2023","permalink":"/posts/design-pattern/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%BB%84%E5%90%88/","section":"博客","summary":"将对象组合成树形结构来表示“整体/部分”层次关系，允许用户以相同的方式处理单独对象和组合对象。","title":"设计模式 - 组合"},{"content":"装饰（Decorator） # Intent # 为对象动态添加功能。\nClass Diagram # 装饰者（Decorator）和具体组件（ConcreteComponent）都继承自组件（Component），具体组件的方法实现不需要依赖于其它对象，而装饰者组合了一个组件，这样它可以装饰其它装饰者或者具体组件。所谓装饰，就是把这个装饰者套在被装饰者之上，从而动态扩展被装饰者的功能。装饰者的方法有一部分是自己的，这属于它的功能，然后调用被装饰者的方法实现，从而也保留了被装饰者的功能。可以看到，具体组件应当是装饰层次的最低层，因为只有具体组件的方法实现不需要依赖于其它对象。\nImplementation # 设计不同种类的饮料，饮料可以添加配料，比如可以添加牛奶，并且支持动态添加新配料。每增加一种配料，该饮料的价格就会增加，要求计算一种饮料的价格。\n下图表示在 DarkRoast 饮料上新增新添加 Mocha 配料，之后又添加了 Whip 配料。DarkRoast 被 Mocha 包裹，Mocha 又被 Whip 包裹。它们都继承自相同父类，都有 cost() 方法，外层类的 cost() 方法调用了内层类的 cost() 方法。\npublic interface Beverage { double cost(); } public class DarkRoast implements Beverage { @Override public double cost() { return 1; } } public class HouseBlend implements Beverage { @Override public double cost() { return 1; } } public abstract class CondimentDecorator implements Beverage { protected Beverage beverage; } public class Milk extends CondimentDecorator { public Milk(Beverage beverage) { this.beverage = beverage; } @Override public double cost() { return 1 + beverage.cost(); } } public class Mocha extends CondimentDecorator { public Mocha(Beverage beverage) { this.beverage = beverage; } @Override public double cost() { return 1 + beverage.cost(); } } public class Client { public static void main(String[] args) { Beverage beverage = new HouseBlend(); beverage = new Mocha(beverage); beverage = new Milk(beverage); System.out.println(beverage.cost()); } } 3.0 设计原则 # 类应该对扩展开放，对修改关闭：也就是添加新功能时不需要修改代码。饮料可以动态添加新的配料，而不需要去修改饮料的代码。\n不可能把所有的类设计成都满足这一原则，应当把该原则应用于最有可能发生改变的地方。\nJDK # java.io.BufferedInputStream(InputStream) java.io.DataInputStream(InputStream) java.io.BufferedOutputStream(OutputStream) java.util.zip.ZipOutputStream(OutputStream) java.util.Collections#checked List|Map|Set|SortedSet|SortedMap ","date":"31 December 2023","permalink":"/posts/design-pattern/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%A3%85%E9%A5%B0/","section":"博客","summary":"为对象动态添加功能。","title":"设计模式 - 装饰"},{"content":"7. 观察者（Observer） # Intent # 定义对象之间的一对多依赖，当一个对象状态改变时，它的所有依赖都会收到通知并且自动更新状态。\n主题（Subject）是被观察的对象，而其所有依赖者（Observer）称为观察者。\nClass Diagram # 主题（Subject）具有注册和移除观察者、并通知所有观察者的功能，主题是通过维护一张观察者列表来实现这些操作的。\n观察者（Observer）的注册功能需要调用主题的 registerObserver() 方法。\nImplementation # 天气数据布告板会在天气信息发生改变时更新其内容，布告板有多个，并且在将来会继续增加。\npublic interface Subject { void registerObserver(Observer o); void removeObserver(Observer o); void notifyObserver(); } public class WeatherData implements Subject { private List\u0026lt;Observer\u0026gt; observers; private float temperature; private float humidity; private float pressure; public WeatherData() { observers = new ArrayList\u0026lt;\u0026gt;(); } public void setMeasurements(float temperature, float humidity, float pressure) { this.temperature = temperature; this.humidity = humidity; this.pressure = pressure; notifyObserver(); } @Override public void registerObserver(Observer o) { observers.add(o); } @Override public void removeObserver(Observer o) { int i = observers.indexOf(o); if (i \u0026gt;= 0) { observers.remove(i); } } @Override public void notifyObserver() { for (Observer o : observers) { o.update(temperature, humidity, pressure); } } } public interface Observer { void update(float temp, float humidity, float pressure); } public class StatisticsDisplay implements Observer { public StatisticsDisplay(Subject weatherData) { weatherData.registerObserver(this); } @Override public void update(float temp, float humidity, float pressure) { System.out.println(\u0026#34;StatisticsDisplay.update: \u0026#34; + temp + \u0026#34; \u0026#34; + humidity + \u0026#34; \u0026#34; + pressure); } } public class CurrentConditionsDisplay implements Observer { public CurrentConditionsDisplay(Subject weatherData) { weatherData.registerObserver(this); } @Override public void update(float temp, float humidity, float pressure) { System.out.println(\u0026#34;CurrentConditionsDisplay.update: \u0026#34; + temp + \u0026#34; \u0026#34; + humidity + \u0026#34; \u0026#34; + pressure); } } public class WeatherStation { public static void main(String[] args) { WeatherData weatherData = new WeatherData(); CurrentConditionsDisplay currentConditionsDisplay = new CurrentConditionsDisplay(weatherData); StatisticsDisplay statisticsDisplay = new StatisticsDisplay(weatherData); weatherData.setMeasurements(0, 0, 0); weatherData.setMeasurements(1, 1, 1); } } CurrentConditionsDisplay.update: 0.0 0.0 0.0 StatisticsDisplay.update: 0.0 0.0 0.0 CurrentConditionsDisplay.update: 1.0 1.0 1.0 StatisticsDisplay.update: 1.0 1.0 1.0 JDK # java.util.Observer java.util.EventListener javax.servlet.http.HttpSessionBindingListener RxJava ","date":"31 December 2023","permalink":"/posts/design-pattern/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%A7%82%E5%AF%9F%E8%80%85/","section":"博客","summary":"定义对象之间的一对多依赖，当一个对象状态改变时，它的所有依赖都会收到通知并且自动更新状态。","title":"设计模式 - 观察者"},{"content":"解释器（Interpreter） # Intent # 为语言创建解释器，通常由语言的语法和语法分析来定义。\nClass Diagram # TerminalExpression：终结符表达式，每个终结符都需要一个 TerminalExpression。 Context：上下文，包含解释器之外的一些全局信息。 Implementation # 以下是一个规则检验器实现，具有 and 和 or 规则，通过规则可以构建一颗解析树，用来检验一个文本是否满足解析树定义的规则。\n例如一颗解析树为 D And (A Or (B C))，文本 \u0026ldquo;D A\u0026rdquo; 满足该解析树定义的规则。\n这里的 Context 指的是 String。\npublic abstract class Expression { public abstract boolean interpret(String str); } public class TerminalExpression extends Expression { private String literal = null; public TerminalExpression(String str) { literal = str; } public boolean interpret(String str) { StringTokenizer st = new StringTokenizer(str); while (st.hasMoreTokens()) { String test = st.nextToken(); if (test.equals(literal)) { return true; } } return false; } } public class AndExpression extends Expression { private Expression expression1 = null; private Expression expression2 = null; public AndExpression(Expression expression1, Expression expression2) { this.expression1 = expression1; this.expression2 = expression2; } public boolean interpret(String str) { return expression1.interpret(str) \u0026amp;\u0026amp; expression2.interpret(str); } } public class OrExpression extends Expression { private Expression expression1 = null; private Expression expression2 = null; public OrExpression(Expression expression1, Expression expression2) { this.expression1 = expression1; this.expression2 = expression2; } public boolean interpret(String str) { return expression1.interpret(str) || expression2.interpret(str); } } public class Client { /** * 构建解析树 */ public static Expression buildInterpreterTree() { // Literal Expression terminal1 = new TerminalExpression(\u0026#34;A\u0026#34;); Expression terminal2 = new TerminalExpression(\u0026#34;B\u0026#34;); Expression terminal3 = new TerminalExpression(\u0026#34;C\u0026#34;); Expression terminal4 = new TerminalExpression(\u0026#34;D\u0026#34;); // B C Expression alternation1 = new OrExpression(terminal2, terminal3); // A Or (B C) Expression alternation2 = new OrExpression(terminal1, alternation1); // D And (A Or (B C)) return new AndExpression(terminal4, alternation2); } public static void main(String[] args) { Expression define = buildInterpreterTree(); String context1 = \u0026#34;D A\u0026#34;; String context2 = \u0026#34;A B\u0026#34;; System.out.println(define.interpret(context1)); System.out.println(define.interpret(context2)); } } true false JDK # java.util.Pattern java.text.Normalizer All subclasses of java.text.Format javax.el.ELResolver ","date":"31 December 2023","permalink":"/posts/design-pattern/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%A7%A3%E9%87%8A%E5%99%A8/","section":"博客","summary":"为语言创建解释器，通常由语言的语法和语法分析来定义。","title":"设计模式 - 解释器"},{"content":"访问者（Visitor） # Intent # 为一个对象结构（比如组合结构）增加新能力。\nClass Diagram # Visitor：访问者，为每一个 ConcreteElement 声明一个 visit 操作 ConcreteVisitor：具体访问者，存储遍历过程中的累计结果 ObjectStructure：对象结构，可以是组合结构，或者是一个集合。 Implementation # public interface Element { void accept(Visitor visitor); } class CustomerGroup { private List\u0026lt;Customer\u0026gt; customers = new ArrayList\u0026lt;\u0026gt;(); void accept(Visitor visitor) { for (Customer customer : customers) { customer.accept(visitor); } } void addCustomer(Customer customer) { customers.add(customer); } } public class Customer implements Element { private String name; private List\u0026lt;Order\u0026gt; orders = new ArrayList\u0026lt;\u0026gt;(); Customer(String name) { this.name = name; } String getName() { return name; } void addOrder(Order order) { orders.add(order); } public void accept(Visitor visitor) { visitor.visit(this); for (Order order : orders) { order.accept(visitor); } } } public class Order implements Element { private String name; private List\u0026lt;Item\u0026gt; items = new ArrayList(); Order(String name) { this.name = name; } Order(String name, String itemName) { this.name = name; this.addItem(new Item(itemName)); } String getName() { return name; } void addItem(Item item) { items.add(item); } public void accept(Visitor visitor) { visitor.visit(this); for (Item item : items) { item.accept(visitor); } } } public class Item implements Element { private String name; Item(String name) { this.name = name; } String getName() { return name; } public void accept(Visitor visitor) { visitor.visit(this); } } public interface Visitor { void visit(Customer customer); void visit(Order order); void visit(Item item); } public class GeneralReport implements Visitor { private int customersNo; private int ordersNo; private int itemsNo; public void visit(Customer customer) { System.out.println(customer.getName()); customersNo++; } public void visit(Order order) { System.out.println(order.getName()); ordersNo++; } public void visit(Item item) { System.out.println(item.getName()); itemsNo++; } public void displayResults() { System.out.println(\u0026#34;Number of customers: \u0026#34; + customersNo); System.out.println(\u0026#34;Number of orders: \u0026#34; + ordersNo); System.out.println(\u0026#34;Number of items: \u0026#34; + itemsNo); } } public class Client { public static void main(String[] args) { Customer customer1 = new Customer(\u0026#34;customer1\u0026#34;); customer1.addOrder(new Order(\u0026#34;order1\u0026#34;, \u0026#34;item1\u0026#34;)); customer1.addOrder(new Order(\u0026#34;order2\u0026#34;, \u0026#34;item1\u0026#34;)); customer1.addOrder(new Order(\u0026#34;order3\u0026#34;, \u0026#34;item1\u0026#34;)); Order order = new Order(\u0026#34;order_a\u0026#34;); order.addItem(new Item(\u0026#34;item_a1\u0026#34;)); order.addItem(new Item(\u0026#34;item_a2\u0026#34;)); order.addItem(new Item(\u0026#34;item_a3\u0026#34;)); Customer customer2 = new Customer(\u0026#34;customer2\u0026#34;); customer2.addOrder(order); CustomerGroup customers = new CustomerGroup(); customers.addCustomer(customer1); customers.addCustomer(customer2); GeneralReport visitor = new GeneralReport(); customers.accept(visitor); visitor.displayResults(); } } customer1 order1 item1 order2 item1 order3 item1 customer2 order_a item_a1 item_a2 item_a3 Number of customers: 2 Number of orders: 4 Number of items: 6 JDK # javax.lang.model.element.Element and javax.lang.model.element.ElementVisitor javax.lang.model.type.TypeMirror and javax.lang.model.type.TypeVisitor ","date":"31 December 2023","permalink":"/posts/design-pattern/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%AE%BF%E9%97%AE%E8%80%85/","section":"博客","summary":"为一个对象结构（比如组合结构）增加新能力。","title":"设计模式 - 访问者"},{"content":"责任链（Chain Of Responsibility） # Intent # 使多个对象都有机会处理请求，从而避免请求的发送者和接收者之间的耦合关系。将这些对象连成一条链，并沿着这条链发送该请求，直到有一个对象处理它为止。\nClass Diagram # Handler：定义处理请求的接口，并且实现后继链（successor） Implementation # public abstract class Handler { protected Handler successor; public Handler(Handler successor) { this.successor = successor; } protected abstract void handleRequest(Request request); } public class ConcreteHandler1 extends Handler { public ConcreteHandler1(Handler successor) { super(successor); } @Override protected void handleRequest(Request request) { if (request.getType() == RequestType.TYPE1) { System.out.println(request.getName() + \u0026#34; is handle by ConcreteHandler1\u0026#34;); return; } if (successor != null) { successor.handleRequest(request); } } } public class ConcreteHandler2 extends Handler { public ConcreteHandler2(Handler successor) { super(successor); } @Override protected void handleRequest(Request request) { if (request.getType() == RequestType.TYPE2) { System.out.println(request.getName() + \u0026#34; is handle by ConcreteHandler2\u0026#34;); return; } if (successor != null) { successor.handleRequest(request); } } } public class Request { private RequestType type; private String name; public Request(RequestType type, String name) { this.type = type; this.name = name; } public RequestType getType() { return type; } public String getName() { return name; } } public enum RequestType { TYPE1, TYPE2 } public class Client { public static void main(String[] args) { Handler handler1 = new ConcreteHandler1(null); Handler handler2 = new ConcreteHandler2(handler1); Request request1 = new Request(RequestType.TYPE1, \u0026#34;request1\u0026#34;); handler2.handleRequest(request1); Request request2 = new Request(RequestType.TYPE2, \u0026#34;request2\u0026#34;); handler2.handleRequest(request2); } } request1 is handle by ConcreteHandler1 request2 is handle by ConcreteHandler2 JDK # java.util.logging.Logger#log() Apache Commons Chain javax.servlet.Filter#doFilter() ","date":"31 December 2023","permalink":"/posts/design-pattern/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%B4%A3%E4%BB%BB%E9%93%BE/","section":"博客","summary":"使多个对象都有机会处理请求，从而避免请求的发送者和接收者之间的耦合关系。将这些对象连成一条链，并沿着这条链发送该请求，直到有一个对象处理它为止。","title":"设计模式 - 责任链"},{"content":"迭代器（Iterator） # Intent # 提供一种顺序访问聚合对象元素的方法，并且不暴露聚合对象的内部表示。\nClass Diagram # Aggregate 是聚合类，其中 createIterator() 方法可以产生一个 Iterator； Iterator 主要定义了 hasNext() 和 next() 方法； Client 组合了 Aggregate，为了迭代遍历 Aggregate，也需要组合 Iterator。 Implementation # public interface Aggregate { Iterator createIterator(); } public class ConcreteAggregate implements Aggregate { private Integer[] items; public ConcreteAggregate() { items = new Integer[10]; for (int i = 0; i \u0026lt; items.length; i++) { items[i] = i; } } @Override public Iterator createIterator() { return new ConcreteIterator\u0026lt;Integer\u0026gt;(items); } } public interface Iterator\u0026lt;Item\u0026gt; { Item next(); boolean hasNext(); } public class ConcreteIterator\u0026lt;Item\u0026gt; implements Iterator { private Item[] items; private int position = 0; public ConcreteIterator(Item[] items) { this.items = items; } @Override public Object next() { return items[position++]; } @Override public boolean hasNext() { return position \u0026lt; items.length; } } public class Client { public static void main(String[] args) { Aggregate aggregate = new ConcreteAggregate(); Iterator\u0026lt;Integer\u0026gt; iterator = aggregate.createIterator(); while (iterator.hasNext()) { System.out.println(iterator.next()); } } } JDK # java.util.Iterator java.util.Enumeration ","date":"31 December 2023","permalink":"/posts/design-pattern/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%BF%AD%E4%BB%A3%E5%99%A8/","section":"博客","summary":"提供一种顺序访问聚合对象元素的方法，并且不暴露聚合对象的内部表示。","title":"设计模式 - 迭代器"},{"content":"1. 适配器（Adapter） # Intent # 把一个类接口转换成另一个用户需要的接口。\nClass Diagram # Implementation # 鸭子（Duck）和火鸡（Turkey）拥有不同的叫声，Duck 的叫声调用 quack() 方法，而 Turkey 调用 gobble() 方法。\n要求将 Turkey 的 gobble() 方法适配成 Duck 的 quack() 方法，从而让火鸡冒充鸭子！\npublic interface Duck { void quack(); } public interface Turkey { void gobble(); } public class WildTurkey implements Turkey { @Override public void gobble() { System.out.println(\u0026#34;gobble!\u0026#34;); } } public class TurkeyAdapter implements Duck { Turkey turkey; public TurkeyAdapter(Turkey turkey) { this.turkey = turkey; } @Override public void quack() { turkey.gobble(); } } public class Client { public static void main(String[] args) { Turkey turkey = new WildTurkey(); Duck duck = new TurkeyAdapter(turkey); duck.quack(); } } JDK # java.util.Arrays#asList() java.util.Collections#list() java.util.Collections#enumeration() javax.xml.bind.annotation.adapters.XMLAdapter ","date":"31 December 2023","permalink":"/posts/design-pattern/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E9%80%82%E9%85%8D%E5%99%A8/","section":"博客","summary":"把一个类接口转换成另一个用户需要的接口。","title":"设计模式 - 适配器"},{"content":"享元（Flyweight） # Intent # 利用共享的方式来支持大量细粒度的对象，这些对象一部分内部状态是相同的。\nClass Diagram # Flyweight：享元对象 IntrinsicState：内部状态，享元对象共享内部状态 ExtrinsicState：外部状态，每个享元对象的外部状态不同 Implementation # public interface Flyweight { void doOperation(String extrinsicState); } public class ConcreteFlyweight implements Flyweight { private String intrinsicState; public ConcreteFlyweight(String intrinsicState) { this.intrinsicState = intrinsicState; } @Override public void doOperation(String extrinsicState) { System.out.println(\u0026#34;Object address: \u0026#34; + System.identityHashCode(this)); System.out.println(\u0026#34;IntrinsicState: \u0026#34; + intrinsicState); System.out.println(\u0026#34;ExtrinsicState: \u0026#34; + extrinsicState); } } public class FlyweightFactory { private HashMap\u0026lt;String, Flyweight\u0026gt; flyweights = new HashMap\u0026lt;\u0026gt;(); Flyweight getFlyweight(String intrinsicState) { if (!flyweights.containsKey(intrinsicState)) { Flyweight flyweight = new ConcreteFlyweight(intrinsicState); flyweights.put(intrinsicState, flyweight); } return flyweights.get(intrinsicState); } } public class Client { public static void main(String[] args) { FlyweightFactory factory = new FlyweightFactory(); Flyweight flyweight1 = factory.getFlyweight(\u0026#34;aa\u0026#34;); Flyweight flyweight2 = factory.getFlyweight(\u0026#34;aa\u0026#34;); flyweight1.doOperation(\u0026#34;x\u0026#34;); flyweight2.doOperation(\u0026#34;y\u0026#34;); } } Object address: 1163157884 IntrinsicState: aa ExtrinsicState: x Object address: 1163157884 IntrinsicState: aa ExtrinsicState: y JDK # Java 利用缓存来加速大量小对象的访问时间。\njava.lang.Integer#valueOf(int) java.lang.Boolean#valueOf(boolean) java.lang.Byte#valueOf(byte) java.lang.Character#valueOf(char) ","date":"31 December 2023","permalink":"/posts/design-pattern/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E4%BA%AB%E5%85%83/","section":"博客","summary":"利用共享的方式来支持大量细粒度的对象，这些对象一部分内部状态是相同的。","title":"设计模式 - 享元"},{"content":"代理（Proxy） # Intent # 控制对其它对象的访问。\nClass Diagram # 代理有以下四类：\n远程代理（Remote Proxy）：控制对远程对象（不同地址空间）的访问，它负责将请求及其参数进行编码，并向不同地址空间中的对象发送已经编码的请求。 虚拟代理（Virtual Proxy）：根据需要创建开销很大的对象，它可以缓存实体的附加信息，以便延迟对它的访问，例如在网站加载一个很大图片时，不能马上完成，可以用虚拟代理缓存图片的大小信息，然后生成一张临时图片代替原始图片。 保护代理（Protection Proxy）：按权限控制对象的访问，它负责检查调用者是否具有实现一个请求所必须的访问权限。 智能代理（Smart Reference）：取代了简单的指针，它在访问对象时执行一些附加操作：记录对象的引用次数；当第一次引用一个对象时，将它装入内存；在访问一个实际对象前，检查是否已经锁定了它，以确保其它对象不能改变它。 Implementation # 以下是一个虚拟代理的实现，模拟了图片延迟加载的情况下使用与图片大小相等的临时内容去替换原始图片，直到图片加载完成才将图片显示出来。\npublic interface Image { void showImage(); } public class HighResolutionImage implements Image { private URL imageURL; private long startTime; private int height; private int width; public int getHeight() { return height; } public int getWidth() { return width; } public HighResolutionImage(URL imageURL) { this.imageURL = imageURL; this.startTime = System.currentTimeMillis(); this.width = 600; this.height = 600; } public boolean isLoad() { // 模拟图片加载，延迟 3s 加载完成 long endTime = System.currentTimeMillis(); return endTime - startTime \u0026gt; 3000; } @Override public void showImage() { System.out.println(\u0026#34;Real Image: \u0026#34; + imageURL); } } public class ImageProxy implements Image { private HighResolutionImage highResolutionImage; public ImageProxy(HighResolutionImage highResolutionImage) { this.highResolutionImage = highResolutionImage; } @Override public void showImage() { while (!highResolutionImage.isLoad()) { try { System.out.println(\u0026#34;Temp Image: \u0026#34; + highResolutionImage.getWidth() + \u0026#34; \u0026#34; + highResolutionImage.getHeight()); Thread.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } } highResolutionImage.showImage(); } } public class ImageViewer { public static void main(String[] args) throws Exception { String image = \u0026#34;http://image.jpg\u0026#34;; URL url = new URL(image); HighResolutionImage highResolutionImage = new HighResolutionImage(url); ImageProxy imageProxy = new ImageProxy(highResolutionImage); imageProxy.showImage(); } } JDK # java.lang.reflect.Proxy RMI ","date":"31 December 2023","permalink":"/posts/design-pattern/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E4%BB%A3%E7%90%86/","section":"博客","summary":"控制对其它对象的访问。","title":"设计模式 - 代理"},{"content":"单例（Singleton） # Intent # 确保一个类只有一个实例，并提供该实例的全局访问点。\nClass Diagram # 使用一个私有构造函数、一个私有静态变量以及一个公有静态函数来实现。\n私有构造函数保证了不能通过构造函数来创建对象实例，只能通过公有静态函数返回唯一的私有静态变量。\nImplementation # Ⅰ 懒汉式-线程不安全 # 以下实现中，私有静态变量 uniqueInstance 被延迟实例化，这样做的好处是，如果没有用到该类，那么就不会实例化 uniqueInstance，从而节约资源。\n这个实现在多线程环境下是不安全的，如果多个线程能够同时进入 if (uniqueInstance == null) ，并且此时 uniqueInstance 为 null，那么会有多个线程执行 uniqueInstance = new Singleton(); 语句，这将导致实例化多次 uniqueInstance。\npublic class Singleton { private static Singleton uniqueInstance; private Singleton() { } public static Singleton getUniqueInstance() { if (uniqueInstance == null) { uniqueInstance = new Singleton(); } return uniqueInstance; } } Ⅱ 饿汉式-线程安全 # 线程不安全问题主要是由于 uniqueInstance 被实例化多次，采取直接实例化 uniqueInstance 的方式就不会产生线程不安全问题。\n但是直接实例化的方式也丢失了延迟实例化带来的节约资源的好处。\nprivate static Singleton uniqueInstance = new Singleton(); Ⅲ 懒汉式-线程安全 # 只需要对 getUniqueInstance() 方法加锁，那么在一个时间点只能有一个线程能够进入该方法，从而避免了实例化多次 uniqueInstance。\n但是当一个线程进入该方法之后，其它试图进入该方法的线程都必须等待，即使 uniqueInstance 已经被实例化了。这会让线程阻塞时间过长，因此该方法有性能问题，不推荐使用。\npublic static synchronized Singleton getUniqueInstance() { if (uniqueInstance == null) { uniqueInstance = new Singleton(); } return uniqueInstance; } Ⅳ 双重校验锁-线程安全 # uniqueInstance 只需要被实例化一次，之后就可以直接使用了。加锁操作只需要对实例化那部分的代码进行，只有当 uniqueInstance 没有被实例化时，才需要进行加锁。\n双重校验锁先判断 uniqueInstance 是否已经被实例化，如果没有被实例化，那么才对实例化语句进行加锁。\npublic class Singleton { private volatile static Singleton uniqueInstance; private Singleton() { } public static Singleton getUniqueInstance() { if (uniqueInstance == null) { synchronized (Singleton.class) { if (uniqueInstance == null) { uniqueInstance = new Singleton(); } } } return uniqueInstance; } } 考虑下面的实现，也就是只使用了一个 if 语句。在 uniqueInstance == null 的情况下，如果两个线程都执行了 if 语句，那么两个线程都会进入 if 语句块内。虽然在 if 语句块内有加锁操作，但是两个线程都会执行 uniqueInstance = new Singleton(); 这条语句，只是先后的问题，那么就会进行两次实例化。因此必须使用双重校验锁，也就是需要使用两个 if 语句：第一个 if 语句用来避免 uniqueInstance 已经被实例化之后的加锁操作，而第二个 if 语句进行了加锁，所以只能有一个线程进入，就不会出现 uniqueInstance == null 时两个线程同时进行实例化操作。\nif (uniqueInstance == null) { synchronized (Singleton.class) { uniqueInstance = new Singleton(); } } uniqueInstance 采用 volatile 关键字修饰也是很有必要的， uniqueInstance = new Singleton(); 这段代码其实是分为三步执行：\n为 uniqueInstance 分配内存空间 初始化 uniqueInstance 将 uniqueInstance 指向分配的内存地址 但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1\u0026gt;3\u0026gt;2。指令重排在单线程环境下不会出现问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用 getUniqueInstance() 后发现 uniqueInstance 不为空，因此返回 uniqueInstance，但此时 uniqueInstance 还未被初始化。\n使用 volatile 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。\nⅤ 静态内部类实现 # 当 Singleton 类被加载时，静态内部类 SingletonHolder 没有被加载进内存。只有当调用 getUniqueInstance() 方法从而触发 SingletonHolder.INSTANCE 时 SingletonHolder 才会被加载，此时初始化 INSTANCE 实例，并且 JVM 能确保 INSTANCE 只被实例化一次。\n这种方式不仅具有延迟初始化的好处，而且由 JVM 提供了对线程安全的支持。\npublic class Singleton { private Singleton() { } private static class SingletonHolder { private static final Singleton INSTANCE = new Singleton(); } public static Singleton getUniqueInstance() { return SingletonHolder.INSTANCE; } } Ⅵ 枚举实现 # public enum Singleton { INSTANCE; private String objName; public String getObjName() { return objName; } public void setObjName(String objName) { this.objName = objName; } public static void main(String[] args) { // 单例测试 Singleton firstSingleton = Singleton.INSTANCE; firstSingleton.setObjName(\u0026#34;firstName\u0026#34;); System.out.println(firstSingleton.getObjName()); Singleton secondSingleton = Singleton.INSTANCE; secondSingleton.setObjName(\u0026#34;secondName\u0026#34;); System.out.println(firstSingleton.getObjName()); System.out.println(secondSingleton.getObjName()); // 反射获取实例测试 try { Singleton[] enumConstants = Singleton.class.getEnumConstants(); for (Singleton enumConstant : enumConstants) { System.out.println(enumConstant.getObjName()); } } catch (Exception e) { e.printStackTrace(); } } } firstName secondName secondName secondName 该实现可以防止反射攻击。在其它实现中，通过 setAccessible() 方法可以将私有构造函数的访问级别设置为 public，然后调用构造函数从而实例化对象，如果要防止这种攻击，需要在构造函数中添加防止多次实例化的代码。该实现是由 JVM 保证只会实例化一次，因此不会出现上述的反射攻击。\n该实现在多次序列化和序列化之后，不会得到多个实例。而其它实现需要使用 transient 修饰所有字段，并且实现序列化和反序列化的方法。\nExamples # Logger Classes Configuration Classes Accesing resources in shared mode Factories implemented as Singletons JDK # java.lang.Runtime#getRuntime() java.awt.Desktop#getDesktop() java.lang.System#getSecurityManager() ","date":"31 December 2023","permalink":"/posts/design-pattern/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8D%95%E4%BE%8B/","section":"博客","summary":"使用一个私有构造函数、一个私有静态变量以及一个公有静态函数来实现。私有构造函数保证了不能通过构造函数来创建对象实例，只能通过公有静态函数返回唯一的私有静态变量。","title":"设计模式 - 单例"},{"content":"2. 命令（Command） # Intent # 将命令封装成对象中，具有以下作用：\n使用命令来参数化其它对象 将命令放入队列中进行排队 将命令的操作记录到日志中 支持可撤销的操作 Class Diagram # Command：命令 Receiver：命令接收者，也就是命令真正的执行者 Invoker：通过它来调用命令 Client：可以设置命令与命令的接收者 Implementation # 设计一个遥控器，可以控制电灯开关。\npublic interface Command { void execute(); } public class LightOnCommand implements Command { Light light; public LightOnCommand(Light light) { this.light = light; } @Override public void execute() { light.on(); } } public class LightOffCommand implements Command { Light light; public LightOffCommand(Light light) { this.light = light; } @Override public void execute() { light.off(); } } public class Light { public void on() { System.out.println(\u0026#34;Light is on!\u0026#34;); } public void off() { System.out.println(\u0026#34;Light is off!\u0026#34;); } } /** * 遥控器 */ public class Invoker { private Command[] onCommands; private Command[] offCommands; private final int slotNum = 7; public Invoker() { this.onCommands = new Command[slotNum]; this.offCommands = new Command[slotNum]; } public void setOnCommand(Command command, int slot) { onCommands[slot] = command; } public void setOffCommand(Command command, int slot) { offCommands[slot] = command; } public void onButtonWasPushed(int slot) { onCommands[slot].execute(); } public void offButtonWasPushed(int slot) { offCommands[slot].execute(); } } public class Client { public static void main(String[] args) { Invoker invoker = new Invoker(); Light light = new Light(); Command lightOnCommand = new LightOnCommand(light); Command lightOffCommand = new LightOffCommand(light); invoker.setOnCommand(lightOnCommand, 0); invoker.setOffCommand(lightOffCommand, 0); invoker.onButtonWasPushed(0); invoker.offButtonWasPushed(0); } } JDK # java.lang.Runnable Netflix Hystrix javax.swing.Action ","date":"31 December 2023","permalink":"/posts/design-pattern/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%91%BD%E4%BB%A4/","section":"博客","summary":"将命令封装成对象中","title":"设计模式 - 命令"},{"content":"外观（Facade） # Intent # 提供了一个统一的接口，用来访问子系统中的一群接口，从而让子系统更容易使用。\nClass Diagram # Implementation # 观看电影需要操作很多电器，使用外观模式实现一键看电影功能。\npublic class SubSystem { public void turnOnTV() { System.out.println(\u0026#34;turnOnTV()\u0026#34;); } public void setCD(String cd) { System.out.println(\u0026#34;setCD( \u0026#34; + cd + \u0026#34; )\u0026#34;); } public void startWatching(){ System.out.println(\u0026#34;startWatching()\u0026#34;); } } public class Facade { private SubSystem subSystem = new SubSystem(); public void watchMovie() { subSystem.turnOnTV(); subSystem.setCD(\u0026#34;a movie\u0026#34;); subSystem.startWatching(); } } public class Client { public static void main(String[] args) { Facade facade = new Facade(); facade.watchMovie(); } } 设计原则 # 最少知识原则：只和你的密友谈话。也就是说客户对象所需要交互的对象应当尽可能少。\n","date":"31 December 2023","permalink":"/posts/design-pattern/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%A4%96%E8%A7%82/","section":"博客","summary":"提供了一个统一的接口，用来访问子系统中的一群接口，从而让子系统更容易使用。","title":"设计模式 - 外观"},{"content":"桥接（Bridge） # Intent # 将抽象与实现分离开来，使它们可以独立变化。\nClass Diagram # Abstraction：定义抽象类的接口 Implementor：定义实现类接口 Implementation # RemoteControl 表示遥控器，指代 Abstraction。\nTV 表示电视，指代 Implementor。\n桥接模式将遥控器和电视分离开来，从而可以独立改变遥控器或者电视的实现。\npublic abstract class TV { public abstract void on(); public abstract void off(); public abstract void tuneChannel(); } public class Sony extends TV { @Override public void on() { System.out.println(\u0026#34;Sony.on()\u0026#34;); } @Override public void off() { System.out.println(\u0026#34;Sony.off()\u0026#34;); } @Override public void tuneChannel() { System.out.println(\u0026#34;Sony.tuneChannel()\u0026#34;); } } public class RCA extends TV { @Override public void on() { System.out.println(\u0026#34;RCA.on()\u0026#34;); } @Override public void off() { System.out.println(\u0026#34;RCA.off()\u0026#34;); } @Override public void tuneChannel() { System.out.println(\u0026#34;RCA.tuneChannel()\u0026#34;); } } public abstract class RemoteControl { protected TV tv; public RemoteControl(TV tv) { this.tv = tv; } public abstract void on(); public abstract void off(); public abstract void tuneChannel(); } public class ConcreteRemoteControl1 extends RemoteControl { public ConcreteRemoteControl1(TV tv) { super(tv); } @Override public void on() { System.out.println(\u0026#34;ConcreteRemoteControl1.on()\u0026#34;); tv.on(); } @Override public void off() { System.out.println(\u0026#34;ConcreteRemoteControl1.off()\u0026#34;); tv.off(); } @Override public void tuneChannel() { System.out.println(\u0026#34;ConcreteRemoteControl1.tuneChannel()\u0026#34;); tv.tuneChannel(); } } public class ConcreteRemoteControl2 extends RemoteControl { public ConcreteRemoteControl2(TV tv) { super(tv); } @Override public void on() { System.out.println(\u0026#34;ConcreteRemoteControl2.on()\u0026#34;); tv.on(); } @Override public void off() { System.out.println(\u0026#34;ConcreteRemoteControl2.off()\u0026#34;); tv.off(); } @Override public void tuneChannel() { System.out.println(\u0026#34;ConcreteRemoteControl2.tuneChannel()\u0026#34;); tv.tuneChannel(); } } public class Client { public static void main(String[] args) { RemoteControl remoteControl1 = new ConcreteRemoteControl1(new RCA()); remoteControl1.on(); remoteControl1.off(); remoteControl1.tuneChannel(); RemoteControl remoteControl2 = new ConcreteRemoteControl2(new Sony()); remoteControl2.on(); remoteControl2.off(); remoteControl2.tuneChannel(); } } JDK # AWT (It provides an abstraction layer which maps onto the native OS the windowing support.) JDBC ","date":"31 December 2023","permalink":"/posts/design-pattern/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E6%A1%A5%E6%8E%A5/","section":"博客","summary":"将抽象与实现分离开来，使它们可以独立变化。","title":"设计模式 - 桥接"},{"content":"8. 状态（State） # Intent # 允许对象在内部状态改变时改变它的行为，对象看起来好像修改了它所属的类。\nClass Diagram # Implementation # 糖果销售机有多种状态，每种状态下销售机有不同的行为，状态可以发生转移，使得销售机的行为也发生改变。\npublic interface State { /** * 投入 25 分钱 */ void insertQuarter(); /** * 退回 25 分钱 */ void ejectQuarter(); /** * 转动曲柄 */ void turnCrank(); /** * 发放糖果 */ void dispense(); } public class HasQuarterState implements State { private GumballMachine gumballMachine; public HasQuarterState(GumballMachine gumballMachine) { this.gumballMachine = gumballMachine; } @Override public void insertQuarter() { System.out.println(\u0026#34;You can\u0026#39;t insert another quarter\u0026#34;); } @Override public void ejectQuarter() { System.out.println(\u0026#34;Quarter returned\u0026#34;); gumballMachine.setState(gumballMachine.getNoQuarterState()); } @Override public void turnCrank() { System.out.println(\u0026#34;You turned...\u0026#34;); gumballMachine.setState(gumballMachine.getSoldState()); } @Override public void dispense() { System.out.println(\u0026#34;No gumball dispensed\u0026#34;); } } public class NoQuarterState implements State { GumballMachine gumballMachine; public NoQuarterState(GumballMachine gumballMachine) { this.gumballMachine = gumballMachine; } @Override public void insertQuarter() { System.out.println(\u0026#34;You insert a quarter\u0026#34;); gumballMachine.setState(gumballMachine.getHasQuarterState()); } @Override public void ejectQuarter() { System.out.println(\u0026#34;You haven\u0026#39;t insert a quarter\u0026#34;); } @Override public void turnCrank() { System.out.println(\u0026#34;You turned, but there\u0026#39;s no quarter\u0026#34;); } @Override public void dispense() { System.out.println(\u0026#34;You need to pay first\u0026#34;); } } public class SoldOutState implements State { GumballMachine gumballMachine; public SoldOutState(GumballMachine gumballMachine) { this.gumballMachine = gumballMachine; } @Override public void insertQuarter() { System.out.println(\u0026#34;You can\u0026#39;t insert a quarter, the machine is sold out\u0026#34;); } @Override public void ejectQuarter() { System.out.println(\u0026#34;You can\u0026#39;t eject, you haven\u0026#39;t inserted a quarter yet\u0026#34;); } @Override public void turnCrank() { System.out.println(\u0026#34;You turned, but there are no gumballs\u0026#34;); } @Override public void dispense() { System.out.println(\u0026#34;No gumball dispensed\u0026#34;); } } public class SoldState implements State { GumballMachine gumballMachine; public SoldState(GumballMachine gumballMachine) { this.gumballMachine = gumballMachine; } @Override public void insertQuarter() { System.out.println(\u0026#34;Please wait, we\u0026#39;re already giving you a gumball\u0026#34;); } @Override public void ejectQuarter() { System.out.println(\u0026#34;Sorry, you already turned the crank\u0026#34;); } @Override public void turnCrank() { System.out.println(\u0026#34;Turning twice doesn\u0026#39;t get you another gumball!\u0026#34;); } @Override public void dispense() { gumballMachine.releaseBall(); if (gumballMachine.getCount() \u0026gt; 0) { gumballMachine.setState(gumballMachine.getNoQuarterState()); } else { System.out.println(\u0026#34;Oops, out of gumballs\u0026#34;); gumballMachine.setState(gumballMachine.getSoldOutState()); } } } public class GumballMachine { private State soldOutState; private State noQuarterState; private State hasQuarterState; private State soldState; private State state; private int count = 0; public GumballMachine(int numberGumballs) { count = numberGumballs; soldOutState = new SoldOutState(this); noQuarterState = new NoQuarterState(this); hasQuarterState = new HasQuarterState(this); soldState = new SoldState(this); if (numberGumballs \u0026gt; 0) { state = noQuarterState; } else { state = soldOutState; } } public void insertQuarter() { state.insertQuarter(); } public void ejectQuarter() { state.ejectQuarter(); } public void turnCrank() { state.turnCrank(); state.dispense(); } public void setState(State state) { this.state = state; } public void releaseBall() { System.out.println(\u0026#34;A gumball comes rolling out the slot...\u0026#34;); if (count != 0) { count -= 1; } } public State getSoldOutState() { return soldOutState; } public State getNoQuarterState() { return noQuarterState; } public State getHasQuarterState() { return hasQuarterState; } public State getSoldState() { return soldState; } public int getCount() { return count; } } public class Client { public static void main(String[] args) { GumballMachine gumballMachine = new GumballMachine(5); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); gumballMachine.insertQuarter(); gumballMachine.ejectQuarter(); gumballMachine.turnCrank(); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); gumballMachine.ejectQuarter(); gumballMachine.insertQuarter(); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); } } You insert a quarter You turned... A gumball comes rolling out the slot... You insert a quarter Quarter returned You turned, but there\u0026#39;s no quarter You need to pay first You insert a quarter You turned... A gumball comes rolling out the slot... You insert a quarter You turned... A gumball comes rolling out the slot... You haven\u0026#39;t insert a quarter You insert a quarter You can\u0026#39;t insert another quarter You turned... A gumball comes rolling out the slot... You insert a quarter You turned... A gumball comes rolling out the slot... Oops, out of gumballs You can\u0026#39;t insert a quarter, the machine is sold out You turned, but there are no gumballs No gumball dispensed ","date":"31 December 2023","permalink":"/posts/design-pattern/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%8A%B6%E6%80%81/","section":"博客","summary":"允许对象在内部状态改变时改变它的行为，对象看起来好像修改了它所属的类。","title":"设计模式 - 状态"},{"content":"童年 # 1944年的10月25日，在新中国还没有成立的动荡年代，一个叫做任正非的孩子降生在了一个穷苦的贵州山区家庭。任正非的父亲叫做任木生。任木生年轻时因为家道中落，所以选择去了国民党412军工厂担任一位会计员，私下经常参与爱国抗日运动。但因同意共产党观点，遭国民党特务追捕，为逃避追捕，任木生逃往贵州山区。\n当时的贵州还非常贫穷，在这个环境恶劣的穷乡僻壤，任木生找了份工作，也就是当地的山区教师。也正是因为这个身份，他结识了任正非的母亲陈远昭，两人相恋结婚后共育了7个孩子。在那个年代，靠着两人微薄的教师工资养育9口之家非常困难，夫妻俩只能日夜辛勤工作，勉强维持生计。\n任正非自己的回忆中，最多的是母亲没日没夜的辛勤劳作。任正非的母亲陈远昭是标准的中国女性，勤劳耐劳，慈爱坚韧。他记得母亲怀孕时如何辛苦地采摘野菜，然后勉强填饱家人的肚子。在这种艰苦环境下长大的任正非心中有两个梦想，努力读书以带领家人脱离贫困，以及吃上一个纯白的白面馒头。他的童年，与他相伴的只有饥饿。\n高考\u0026amp;大学 # 在任正非高考之前，在家复习功课的时候，他曾经因为实在饿得受不了，用米糠和野菜捏了个烙饼吃。正巧被他父亲发现了。父亲看着儿子任正非手中的饼，既心疼又无奈。心疼的是自己的儿子马上要参加高考，但是却被这样的饥饿折磨。无奈的是，任正非手上的这块饼，又会让家里的其他孩子少吃一口饭。\n虽然任木生那个时候只是叹了口气走了，但是从那天起，之后的3个月内，家里面都为任正非多准备了一份早饭。每天任正非都能从母亲手上多得到一块小小的玉米饼。任正非知道，这块玉米饼就已经是家人竭尽所能，能给予任正非最好的帮助了。\n多年之后，任正非谈起自己的过去，仍然能清楚地记得这块玉米饼。他说：“如果没有那块玉米饼，我就进不了华为这个公司。我也许会是一个喂猪高手，或者是路边摆摊的能工巧匠。”正是因为有那一片小小的玉米饼，才让他走到了今天。\n新中国成立之后，任正非参加了高考，而且他也以优异的成绩考进了重庆建筑工程学院。大学那个山区里面的穷小子摇身一变变成了大学生了。在那个年代，大学生是很值钱的。原本只要任正非毕业之后，他就能得到一份体面的工作，而这份体面的工作就能支持他和他的家人脱离贫困，走向温饱。\n但是命运却在这个时候跟任正非开了一个玩笑。正当任正非即将毕业的时候，那十年到来了，那个被称为十年浩劫的年代。任正非的父亲因为他特殊的身份，我们之前说过了，任正非的父亲曾经给国民党当过会计，然后现在是当老师，他的职业和他的背景都有着巨大的问题，所以被当成了典型来批斗。而这个批斗也为任正非的家庭带来了无穷的折磨。\n但是远在重庆的任正非，他并不知道这一切。直到有一次，任正非偶然从一位贵州老乡的口中得知了家中的情况。他这才知道自己的父亲已经被批斗了。于是他想买火车回自己的老家，但是却买不着回家的车票，只能连夜扒火车，想要回到贵州老家。但是任正非的运气却不太好，他想要去扒火车，却被乘务人员给发现了，被痛打一顿之后扔下了火车。任正非只能从重庆徒步走回家。\n他心中挂念着自己的父母和自己的弟妹，于是他日夜不歇地赶回了自己的家中。当他回到家的时候，看到的只有一片狼藉的房子和饱受折磨的父亲。在他的父亲被抓去折磨的时候，他的母亲扛起了整个家庭的重担，我们之前说过，两个人要支撑这个9口之家尚且很困难，现在变成了一个人，那基本上就是扛不动，但是总不能让自己的孩子们饿死吧，顶不住也得顶。于是在夜以继日的劳作当中，听力受损了，还患上了肺结核。而任正非的弟弟妹妹们为了让自己的母亲少一点压力，都选择外出打工。他们的年纪很小，工作非常的辛苦，但又挣不到钱，但是他们承担的这一切，付出的这一切，他们都没有告诉过任正非，他们不想告诉自己好不容易有前程的儿子，家里变成了这样，他们不想让自己的哥哥知道，自己现在已经走到了这个地步。他们所做的这一切，这些默默忍受，都是在为任正非换来一张平稳的书桌。\n任正非回到家乡，知道了这一切之后，巨大的无力感如海啸一般朝着任正非席卷而来。他陷入了迷茫。任正非当时想放弃自己的学业，立刻回到家中，用自己的肩膀同家人们一起分担重担。他把这个想法告诉了自己的父亲，但是他的父亲却捏着他的肩膀告诉他：“如果你想当一个孝子，那就现在滚回你的学校去读书。记住，知识就是力量。别人不学，你也要学。千万不要随大流。”如果任正非在这个时候放弃了学业，那么这个家庭之前的所有努力都会白白浪费，一家人的血和汗就白流了。\n在那个时候，任正非亲眼目睹了自己的父亲在这样巨大的压力和困境下表达出来的强大意志。这个强大的意志深深地撼动了任正非的内心。父亲就如同一棵受尽风吹雨打的巨树，屹立在任正非的面前，帮任正非撑起了一片天，挡住了一切的风霜雨雪。感受到了自己父亲的伟大，任正非放弃了辍学的念头，乖乖地坐回了那张由全家人为他换来的平稳书桌前。\n从家乡回到自己的大学的那一天，任正非哭得泣不成声。也许任正非的父母并没有为他带来实的家庭，但是他们的父母却以身作则地告诉了任正非真正人生中最宝贵的财富是什么。根据任正非回忆，当他回到重庆的时候，学校已经是一片枪林弹雨的环境了。动乱已经蔓延到了校园里。在其他同学都聚在一起激烈地讨论政治的时候，任正非却成了一名独行侠。他远离人群，背道而驰，独自坐在图书馆，把电子计算机数字技术、自动控制等课程全部自学完了。\n除此之外，任正非还结交了一些西安交大的老师。那个时候，知识分子是一个非常危险的身份，但是比起这些危险，对于任正非来说，最重要的还是获取知识。他没有能力为自己的家庭做其他的了，唯有学习，也只有学习。他还把樊映川的高等数学习题集从头到尾做了两遍。除此之外，他还自学了三门外语。\n在那个年代，想要保持独立的人格，拥有自己的追求是一件极其困难的事。然而在枪林弹雨的十年动乱当中，任正非依旧能保持自己的独立追求，并且默默地为之努力。在僻静之处打开毛泽东选集，并不像那些同学那样狂热地追捧和崇拜，而是从伟人的思想当中汲取着人生中永恒的力量，以此来丰富自己的基础思想，打牢了自己的专业知识。\n正如任正非所说的，这个年代对国家来说是一场灾难，但是对他的人生来说是一次洗礼，使得他在政治上成熟了起来，不再是一个单纯的书呆子。也许大多数成功的人为了歌颂自己的来之不易都会强调自己的痛苦，但是对于任正非来说，他的的确确是从淤泥中爬上岸的人，所以说，他格外珍惜现在自己拥有的一切。在那个年代里，磨练出了自己一身功夫的任正非。\n入伍（1968-1982） # 在1968年，穿着父亲送的翻毛皮鞋，任正非踏上了求职的道路。那个时候的政策一般会把大学生先分配到基层，毕业生们要先去当一段时间的农民，或者是工人，或者说先应征入伍去当兵。任正非当时就坚定地选择了当兵这个最光荣的职业。那个时候新中国刚刚建立，工程兵兵种急缺这方面的人才，正好任正非又是专业对口。\n任正非当时并没有意识到，成为一名军人会给自己以后的人生带来巨大的影响。当时他的想法也很简单，他只是想选择一条社会主流认可的道路，以及锻炼自己。而且比较幸运的是，在当时的军人的政治审查当中，对工程基建兵的政治要求没有那么严苛，军方并没有因为任正非的父亲特殊的身份作出什么奇怪的评价。毕竟在那个时代，有着任正非父亲这种身份嫌疑的人多如牛毛。\n侥幸跨过这道坎之后，一个让任正非展现自己优秀个人能力的舞台正在逐渐地拉开帷幕。关于攻城基建兵，这个兵种是在1966年8月1日在陆军中成立的新兵种，而这个兵种在1984年的年底完成了撤销和改编，如今已经是消失在历史长河当中的一个兵种了。\n但是在这个兵种存在的18年的过程当中，这支将近49万人的部队跑遍了大江南北，先后在全国的28个省、包括自治区和直辖市，为全国各地建设基础工程，像是冶金、煤炭、石化、水电、交通、航空、兵器等种种方面都是由他们来负责的，以及北京等城市地铁和市政的建设这些任务也是由他们去参与的。可以说工程基建兵是块砖，哪里需要往哪里搬。\n然而，军人这个职业的荣光几乎都是用艰辛的磨练换来的。刚进入军营的任正非跟着部队回到了自己的老家，贵州安顺地区，参加了代号是011的航空军事建设工程。任正非和他的战友们在这里要做的就是建立起成规模的军用飞机和航空发动机的制造工厂。\n这里的工作环境几乎就是一种煎熬。当部队在这片荒地上开始施工的时候，周围连个房子都没有。工作结束之后，随便往地上一躺，那就是床。想要睡得舒服一点，那就去旁边的草地上躺着，那当真是来自大自然的席梦思了。那时候正值盛夏，这些荒地上有一群快要饿疯了的蚊子，只要看到有人经过，这些蚊子眼睛都冒绿光，携家带口的过来吸血咬人。\n刚开始，任正非他们还不习惯这样的工作，但是随着这些蚊子天天都来咬，这些工程兵们也挡也挡不住，最后干脆就放弃抵抗了。你要咬就让你去咬。整个夏天就在这样喂蚊子的过程中度过了。虽然到了冬天没有蚊子了，但是冬天更加要命。\n到了冬天，虽然说已经盖起了几间土坯房，可是那些房子基本上没有什么质量可言，毕竟只是临时搭建的。主要就有两个缺点，第一个是透光，透光透到什么程度呢，能第一时间看见升起的太阳，而第二个缺点就是透风。冬天的风一吹，整个小屋里冷的像冰窖一样，又没有厚被子盖，就只能咬牙挨冻。\n日常的饮食那就更别提了。在老舍的《四世同堂》当中，曾经描写过，北京城被入侵之后，日本军人曾经在北京售卖过一种叫做共和面的食品，而这个共和面就是用糠和麸，还有磨碎的豆饼混着沙子装成一兜的粮食，日本军人就把这个卖给北京的老百姓吃。而在任正非当时的部队当中，因为我们的国家那个时候很穷，一穷二白，所以吃的东西是一种叫做枪杀的杂交高粱。\n虽然这种东西比那个共和面要好很多了，他们至少是粮食，但是吃起来的口感跟吃沙子没有什么区别。然而纵使是过着这么艰难的生活，任正非在多年之后再次回忆起这段时间的时候，他就像他的父亲一样，并没有过多的来介绍自己的痛苦遭遇。\n任正非只是说：“我在这里接触到了最先进的科技，而且在这里我可以放心地学习了。”在苦难中挣扎过的人总会对这个世界抱有极大的善意。在部队的生活就算再苦，也不可苦得过任正非小时候的生活。所以这一切对于任正非来说都不是一个巨大的困难。\n在部队里，任正非曾经做过的那些努力并没有白费。他大学时的技能点就已经拉满了的专业知识，以及实践积累下来的经验都被他熟练地运用到了军队的工作上。别人弄不懂的问题，任正非知道答案；别人已经解决的问题，任正非研究得更加透彻。不仅如此，任正非在工作上还曾经两次为国家科技空白的部分做了及时的填补和规划，以及他和他的各位战友在共同努力之下，航空工程的总装厂防空洞、飞机洞库以及实验场地等数十个项目基地在限定的时间内完工了。工作的表现不能说十分出色，只能说是天衣无缝。如今011基地这个地方仍然在贵州被命名为贵航飞机研究所。如果有贵州的小伙伴感兴趣的话，可以远远的去观摩一下。\n任正非从1968年应征入伍，到1982年以副团长的职位成功实现了干部转业。在这个中间也经历了许多的坎坷，原因是因为任正非他的家庭背景或多或少的还是对他的军旅生涯有了一定的影响。\n但是，任正非依旧能靠他自己的努力和坚持，打破那些谣言和风言风语。1976年的十月动荡十年宣布结束，任正非的父亲任木生的冤案也得到平反。之前，任正非在军营里做出了很多的贡献，但是这些贡献呢都因为他的家庭背景，所以最终没能算成他的功绩。\n任正非在部队当中工作了将近10年，仅仅获得了学毛柱标兵的口头嘉奖。而最渴望认可的青年时代就在这个过程中消磨殆尽了。任正非无论做出怎样的贡献，都无人问津。正当任正非学着淡泊名利的时候，他的父亲却一下子平反了，而且十年动乱也结束了。这一下，任正非又一下子成了奖状的暴发户，那十年当中没能给他的奖状又一次性全部的补给了他，他为国家填补了技术空缺又一直有技术发明，这些本来就是他该得到的荣耀，而如今这些东西对于任正非来说却让他受宠若惊。\n但是不仅仅于此，随后更大的荣誉接踵而至。任正非获得了出席全国科学大会的机会，那一年任正非才33岁，在一共有6,000人参加的大会当中，35岁以下的仅仅有150多人，他看似步入了中年实际上却是年轻有为。后来他又作为代表出席了党的十二次全国代表大会，会议上任正非和党中央领导还一起合照了，得知了这个消息的任正非的父亲任木生非常高兴的去买了一个大大的相框把这张照片洗出来然后挂在了家里。\n任正非终于能做一些对他父亲来说可以算是回报的事情了，但是对于任正非本人来说最让他感觉到高兴的是因为他成功的入党了。\n人生当中的每一段经历都会试图教会我们一些道理，而任正非同样也从他这段军旅生涯当中悟出了一个道理：打铁还需自身硬，想要在竞争当中立于不败之地必须苦练技能并且紧随社会发展不断的进步，当然还需要一点点运气。\n随后时间来到了上个世纪末，中国的社会正在处于高速发展时期。中国用30年走完了西方世界100年的发展道路，从那个时候起，时代的发展速度就开始逐渐的变快了。当时的邓老先生的和平宣言打开了中国大规模裁军的先河。那个时候的任正非早已习惯了朴素且安逸的部队生活，然而这突如其来的裁军却让他感觉到不知所措，并且已经到了这个年纪的他已经有了自己的家庭。他的妻子叫孟军，他还有了两个孩子，大女儿叫做孟晚舟，小儿子叫做任平。\n任正非常年都待在军营当中，也如同他自己的父亲对他一样。他也很少出现在自己孩子的成长当中。在任正非的女儿孟晚舟上初中的时候，因为本来可以作为老师的父亲没有出现的原因，他的成绩一度很差。而他的儿子任平也是一个晚熟的孩子，非常的任性。学习成绩那就更不用多提了。这两个孩子一直是任正非的牵挂，他深刻的铭记着自己小时候，父母是如何为自己和弟弟、妹妹献出一切的。\n而任正非现在对比一下自己，发现自己并不是一个称职的父亲。不称职到什么程度呢？他的女儿孟晚舟曾经愤怒的对他说：“爸爸，如果我考不上大学，那一定有您的责任。您要为我的前途负责。”如果当一个女儿都开始对父亲反唇相击的话，那就只能说明孩子已经把家长放到了自己的对立面。但是这个问题随后就得到了解决。\n在军队裁军之后，身为技术骨干的任正非原本可以转去军事科研基地，在那里他可以走入更光明的前途。但是，很有可能是为了自己的孩子的原因，他没有选择去军事科研基地，反而是激流勇退，从部队里退伍了。任正非放弃了他所喜爱的军旅生活，部队当中养出来的硬汉，面对着自己的家庭，终于露出了柔软的一面。为了儿女，任正非选择了去转业。\n1982年的时候，任正非离开了曾经人生中最困难的时候救赎了他的军队。离开军队之后的任正非会面临怎么样的社会环境呢？\n初来深圳 # 在上个世纪的80年代，似乎一切都在为经济发展蓄势待发。就连邓老先生都曾经说：“中央没钱，你们自己去搞，争取杀出一条血路来。”而眼尖敏锐的任正非，自然选择了那个在南海当中的那个圈，作为自己往后生存奋斗的地方。而那个地方的名字叫做深圳。\n而那个时候的深圳，来自全国各地的建设者，推平了深圳沉睡的山丘，填平了所有阻碍发展的沟壑，渔村变作高楼大厦，良田变作花园都市，中国速度从那里开始被传唱。人人都在说深圳，那里充满了生机，深圳遍地黄金。\n而对于任正非，这样一个在军队当中混出来的老兵，来说这一切是很陌生的，因为军队当中是相对淳朴的，部队里到处都是赤子之心，每个人都能推心置腹而来。到了社会上这里每个人都尔虞我诈，勾心斗角。任正非刚来到深圳的时候，这里的一切都让他感觉到无所适从，就连平时在街头买东西都觉得小商贩在试图多赚他几毛钱。\n好在他的专业知识和经验过硬，刚来到深圳后不久，就进入了南油集团工作。这个集团在当时的深圳，算得上是数一数二了。然而任正非耿直的性格，却让他在这个工作环境当中没能少吃亏。军人的性格我们都知道，耿直热情又吃苦耐劳又不爱抱怨，加上军人的服从性很强，在这个充满了耍小聪明的集团当中，善良忠厚的任正非，只有被别人算计的份。\n而这些细枝末节任正非他可以不计较，他唯一看不惯的就是那些部门的领导们天天得过且过的行为。在这两年的时间当中，任正非一直没有什么机会做出一番大的成就，但是任正非又是一个坚定相信自己能力的人，而且他从小培养的责任感和在部队当中的生活也让他觉得自己一定能成就一番事业。\n任正非想要让这个集团能够大力一点，拉动深圳的经济发展，所以他就直接跑到了领导的办公室，拍着胸脯的向领导立军令状。任正非对领导说：“您要相信我的能力，您可以试着把集团下一个公司交给我管理，我可以保证这个季度的收益可以翻倍。”\n这个领导也没想到这个才年仅30岁的毛头小子竟然能如此的冲动，竟然敢跑到自己的面前来立军令状。但是这个领导他也是做了一番思考的，给任正非做吧，万一他做不好盈亏了怎么办呢，但是你说不给他吧，他也是一个有能力的人，而且是个当兵的人，看起来不太好惹。思来想去，领导把任正非安排到了下属公司的一个电子公司去当副经理。\n正当任正非怀揣着自己的理想，想要大干一番的时候，却遇到了自己这前半生最大的危机。在这场危机当中，任正非失去了自己所有的一切，甚至包括他的家庭。\n在前往电子公司上任之后的不久，任正非就谈成了一笔200多万的大生意。这让任正非非常的高兴，毕竟，这是自己第一次接触这么大的订单。任正非严谨的性格，也让他对这件事情非常的慎重。从生产到发货，他都亲自到现场把关，并且按照合同规定的如期把货物发给了对方。\n可当对方收到了货物之后，却迟迟不肯掏钱。一开始，对方还找一些理由来敷衍任正非，后来干脆连电话都不接了。再到后来人都找不到了，任正非这下傻了，自己不会被骗了吧。任正非用尽了自己的所有手段，想去找回这笔货款，但是都于事无补，他根本联系不上欠自己钱的那个人。\n200万在现在就不是一个小数目，大家想一下在上个世纪200万是一个什么样的数字。那个时候，一线城市的平均工资还不到100块。前不久才刚跟领导立下军令状，说要收益翻倍，现在直接亏了200万。任正非得到的后果就是，被一脚踢出了南油集团，并且因为他常年工作不回家的原因，妻子积累下了很多的不和，加上这一次严重亏损，给家庭带来了不小的经济压力。\n任正非的妻子孟军直接向任正非提出了离婚，同时还带走了一儿一女。原本就是为了家庭才离开军队的，结果现在却落得个妻离子散的结局。这可能是任正非此生的至暗时刻，但是俗话说：“真的猛士敢于直面惨淡的人生”。任正非他不会因为这点事情就停下自己的脚步，只要自己还没死，自己就有翻身的机会。\n任正非当时突然有了一个想法，既然自己工作处处不顺利，为什么我不自己创业呢？任正非刚刚来到深圳的时候因为没有经验不敢创业，而现在的任正非已经明白了该如何创业了。虽然自己亏了一笔，但是自己既然已经一无所有了，那还有什么不敢拼的呢？\n创业 # 1987年的10月，在深圳的一间破烂不堪的房子里，一个刚刚迈入中年的男人正在眉飞色舞地向自己面前的5个好朋友讲述自己未来的计划。这个一笑起来，眼睛就眯成一条缝的男人大声地告诉他的朋友们：“我要去创办一家公司，去研究数学式程控电话交换机。”\n旁边的人听完他这么说都沉默了。毕竟在这个叫做深圳的城市里，每天都会有无数的人说自己要创立公司，然后如何如何一步一步做大做强。他们每个人都有自己的一片宏伟蓝图。在20世纪末的深圳，这座城市从来不缺梦想。\n所以当这个男人喊出自己要创立公司的时候，他的朋友们都沉默了，直到他的朋友当中有一个人打破了这个沉默。他回应了那个中年男人，他问：“你要做的那是个什么东西啊？”男人简单地告诉了他：“那是控制人与人之间打电话用的东西。”\n另一个人也疑惑地说道：“那这个东西他怎么挣钱呢？”那个怀揣着梦想的男人向他解释道：“这个东西我们可以先研究出来，然后把它作为商品卖出去。”紧接着他的另外一位朋友也问他：“那你打算筹集多少资金来开发这个项目呢？”男人默默地伸出了两根手指。\n男人的朋友们都很惊讶，200万？男人摇了摇头。他的朋友们又继续问道：“难道是20万吗？”这个男人还是继续摇了摇头，紧接着，他对他的朋友们说：“2万多一点也可以，两万零一千也行。”\n听到这个男人自信的说完，屋子里的其他五个人无一不发出了叹息的声音。“2万块钱，你开个什么公司啊，而且还要研究技术，这简直比格林童话还要天方夜谭！”在众人的质疑声当中，这个男人有些激动了。他打断了面前这些人对他的质疑，告诉他们：“你们要相信，数字式乘控电话交换机的前景是无限广阔的。你们要知道，我们国内对这方面还是一片空白，这是蓝海市场啊。只要我们去做，我们就一定能够抢占到市场。而且，这样也可以让我们国家的通讯技术得到发展和突破。”\n看到这个男人那么激动，其他五个人也不知道该说什么了。为了维护自己朋友的面子，这5个人就开始问这个男人：“那你准备把你的公司取名叫什么名字呢？”这个男人自信的笑了一笑，他为自己这个还没有诞生的公司取了一个名字，这个名字叫做华为。\n华为的前期收入是从哪来的呢？全都是靠着任正非和他的朋友们东奔西走，卖减肥药和火灾报警器赚来的。任正非这段时间干的事情可以说是非常的励志了。他走街串巷，一家一家的推销自己的减肥药。堂堂一个华为公司的创始人，竟然在外面卖减肥药。现在听来，我们感觉是非常有意思的一件事，但是在当时，像任正非这样创业的人还有很多。\n自研程控交换机 # 经过很长一段时间的饥不择食，终于公司的经济压力得到了缓解，华为终于可以发展正常的业务了。得益于早年间的经验，任正非敏锐的目光盯上了国内几乎是空白的通讯行业。在刚刚改革开放之后的80年代，我国的电话业务还远远没有普及。但是呢这个时候，我国的工商业已经开始蓬勃的发展了，电话的需求呈指数级增加。但是在中国市场上，竟然没有一家中国企业能够稳定的生产电话。\n那个时候，中国电话接通和计费的程控交换机几乎全都是从外国进口的，全靠着进口。那就有两个问题，第一个是从国外买来的交换机完全填不上中国市场的缺口，第二个就是从国外买来的交换机经过关税之后，它的价格往往是非常高的。\n所以在当时，有很多人专门做这方面的走私，但是改革开放之后，中国的市场向全世界敞开了。这个时候无数的外国资本惊讶的发现，中国人竟然没有制造交换机和电话的企业。因为中国当时有着好几亿的人口，却没有一家企业来吃下这个市场，于是国外的企业就开始纷纷的进入中国的通讯市场。\n他们贪婪的瓜分着这一块大蛋糕，在那个时候，中国流传着一种叫七国八制的说法，说的就是当时我们国内的通讯市场有着8种制式通讯设备，分别来自于7个国家，比如日本的NEC和富士通，美国的朗讯，还有德国的西门子等等。\n这7个国家的厂商疯了一样的抢占中国市场，并且不同的厂商的交换机之间连接还要额外收取费用，这就给中国的通信行业带来了巨大的冲击。这些厂商仗着中国人无法与他们竞争，于是在自己的通讯业务上加了很多不合理的收费，并且将这些收费的价格定得奇高无比，像出装费长话费漫游费这几个费加在一起，让早期中国的通讯费高得简直是离谱。\n在那个年代，普通的家庭要装一个电话就得花好几千，那可是90年代呀。大家知道，90年代的好几千是什么概念吗？90年代你一个月能拿100块的工资都算是高薪人群了。本就是通讯工程兵出身的任正非，看到七国八制肆虐中国的通讯市场，立刻就有了自己的想法，为什么我们国家不能生产出优质的程控交换机呢？如果是因为没有人去研究的话，那研究出中国的程控交换机的人为什么不能是我任正非呢？\n有了这样的想法之后的任正非，出于对自己技术的自信，他马上为自己这个还在卖减肥药和报警器的公司定了一个目标。他想要自主研发生产优质的远程交换机。当时在国内，当时在中国的国内也不是没有交换机，只是当时中国自己生产的交换机质量非常的差，而且价格也只比国外优质的交换机低一点点。没有人愿意去选择国内的交换机。中国优质交换机的这个市场是一片蓝海。\n代理程控交换机维持生存 # 于是，任正非就走上了他的交换机研究之路。但是梦想有了，不能不顾及现实，没有任何背景的任正非无法搞到贷款。没有钱研究怎么办？那就自己去搞。因此，在华为刚刚诞生的前几年，任正非只能靠代理香港某公司的程控交换机以此来维持生存。就这么点事情，还是他拍着胸口保证利益翻倍才从港商那里争取来的。最终翻没翻倍我不知道，但是任正非就是从这里赚到了自己的第一桶金。\n第一款产品BH01 # 资金暂且富裕了之后，任正非果断的抛弃了代理权，开始研究制造交换机。在没有技术没有人才的情况下，任正非想了一个好法子，那就是当一个缝合怪，他从不同的单位购买散件，那真是东市买芯片，西市买主机，南市买电线，北市买储存器。买齐之后再自己组装。就这样，华为缝出了自己的第一款产品BH01。大家请记住这个名字，这个名字代表了华为，代表了中国通讯业走出了自己的第一步。\n尽管这个BH01只是低端机中的低端机，最多只能承载24个用户，并且只能在矿山和卫生院当中使用，但是这无疑是一个良好的开端。华为有了自己的交换机之后，任正非有他的销售渠道，这都是他早年间走南闯北积累下来的人脉，于是他就开始销售自己的产品。任正非觉得，自己的产品可能不会卖的特别好，结果没想到，华为的第一个产品竟然供不应求，好评如潮，BH01也算是给任正非打了一针强心剂。\n原来就是这样水平的产品，在中国竟然能卖得风生水起。任正非他明白了，这条路是可以走下去的。已经不惑之年的他，终于感受到了一次生活的温暖。但是这个温暖却是短暂的。正当任正非和他的团队为第一次胜利庆祝的时候，华为买的散件被人断了货源，不再供货了，收了客户的钱却无法制造发货。这就是华为遇到的第一次危机。这次危机让任正非明白了，必须要自主研发所有的零件要实现自己控制生产，才能够把企业做大做强。这个教训任正非一直记得，这也是华为一直能走到今天的一个重要原因。我们缺什么就要自己造。\nBH01的续作BH03 # 于是在1990年，华为生产了BH01的续作BH03。从性能上来说，BH01和BH03没有任何的区别，但是二者唯一的不同就是，BH03的每一块电路板都是华为自己制作的，连芯片都是。虽然产品做出来了，但是这个时候的任正非更加寝食难安了，因为他们的研发资金有限，所以在测试可靠性的时候，没有用很多的专业设备。BH03到底市场反响会怎么样，任正非也不知道，会不会使用着使用着就短路死机，任正非他也不知道，这一切都是未知数。\n不过最后用户的最终反馈还是让任正非放下了心。用户们告诉任正非，产品性能稳定，没有任何问题，甚至可以量产。一直在七国八制的乌云笼罩下的中国通讯业此时此刻升起了一颗新星。正因为华为的发展和突破，这片乌云当中透出了一丝光。\nBH03广泛的投入市场，华为的资金充裕了起来。但是就在这时，中国的房地产产业突然蓬勃发展起来，到处都有工地在开工。当时的房地产行业恐怖到什么程度，一个楼的图纸都能转让，一手二手三手价格不菲。当时人人都在炒房，开发房子的人，甚至还没有炒作的人赚的多。开发商可能每平米赚500块，而投机倒把的贩子可能每平米就赚1,000块。那个时候可是90年代，当时华为所在的深圳更是炒作最激烈的地方。\n当时有很多人劝任正非也要跟这个风去做房地产，但是任正非义正言辞的拒绝了。他说：“我们赚的所有钱都要拿来研究技术，房地产投资等个三五年依旧能赚一笔，但是我可以等，但是通讯技术等不得。你们要明白，我们现在已经处于民族通信工业生死存亡的关头了。” 所以我们每个人都要竭尽全力。\n挖掘人才 # 军人性格的任正非可以说是说到做到。在华为实现真正盈利之后，任正非决定用这笔钱解决人才问题，也没有投到房地产里面去。他找到了华中科技大学，清华大学等名校邀请教授带着学生到华为参观访问，名义上来说是深化了解，但是实际上就是任正非悄咪咪的在挖掘人才。\n就在那一年，任正非靠着自己的口才和热情，在参观华为的这一大批学生当中，找到了两个对日后的华为举足轻重的人物。第一位是郭平。在郭平来到华为之后，他马上就成为了华为第二款自主产品的项目经理。他成功的实现了产品的升级，他做到了让华为的第三代机器能够同时承载48个用户。虽然我们现在看来48这个数字很小，没什么厉害的，但是只要我换一个词，你就会明白他有多厉害了。在他的领导下，华为的第三代产品较第二代性能提升100%。\n任正非挖到的第二位牛人是郑宝用。在华为的第三个产品BH03U项目结束之后，他成为了华为的副总经理兼总工程师。但凡是没做出来的产品都归他负责。郑宝用的到来，让华为的技术实现了一次飞跃。他来到华为之后，做的第一台交换机就是一个500门交换机，一台机器现在可以带500个用户了，效率一下子就提升了25倍，并且连接稳定。他的交换机被评为国产同类产品当中质量最可靠的用户机，没有之一。\n这还不是最牛逼的，他还带领人员成功的研究出了100门、200门和400门的交换机，成功让华为填补了市场的空缺。就这样时间来到了1992年，在这一年，华为的总产值第一次超过了1亿元，总利润超过了1,000万元的销售业绩。大家想一想，在那个年代，1,000万的销售业绩，钱已经赚到了，大家总该享受享受了吧？挣来的钱，那就给大家多发点奖金啊。\n1992年终总结大会——洗牌 # 于是在1992年，华为的年终总结大会在任正非的主持下，更像是一场忆苦思甜的整封大会，也可以说是洗牌大会。他坐在简陋的桌子面前，第一个发言说我们终于活下来了。明明没有硝烟，但这却是一场战争。任正非不知道为什么，在这种应该庆祝的时候，他竟然哭了。然后任正非抹着眼泪对台下的人说，我们的钱分可以，但是我们只能分一点点。我们其他所有的钱都得投资到开发局。用交换机里面，我们要进军公用电话电信领域。革命尚未成功，同志仍需努力。\n我们想想，假如我们是任正非的员工，我们已经做了很多的事情了，但是老板没有给我们分钱。你想想如果是你们的话，你们会不会生气？反正我觉得如果是我是会生气的。但是任正非的员工也很奇怪，他们竟然没有抱怨这个事情，并且他们当中的技术人员几乎都赞成任正非的这个选择，说我们应该研究局用交换机，这是一个值得研究的项目。\n当然，在那一年，也有选择离开华为的人，毕竟钱都不发，我跟你玩命干嘛呀。但是所有要离开的人都拿到了他本来应该拿到的那份奖金和鼓励。随后，任正非就选择将自己公司的股份全部分给了员工，自己只留了1%。\n任正非告诉大家说，如果公司要继续发展，我们的所有钱都必须投资到局部交换机里面去，我实在没有钱分给大家，我只能把股份给大家。以后赚了钱，公司做的越大，大家手上的股份就越值钱。\n至此，华为终于完成了一次洗牌。选择留下的，可以说是最坚定最坚定的统治了。在洗牌完成之后的华为里，我们所熟悉的狼性文化正在萌芽。如何让员工成为狼，那就是让员工明白，这个公司能帮助他们实现自己的价值，而不是让员工为了老板的利益，拼死拼活。要扔出肉才能吸引狼。现在这个公司，不再是属于任正非一个人的了，是属于我们所有人的。我们把蛋糕做大，我们自己才能拿到更多。\n在这之后，华为在技术上投资了巨额的开发费，用尽了全公司上下所有的力量，继续朝着科技的完善和进步去全进，终于实现了技术层面的进步。模拟局用交换机开发成功了。然而就在任正非和他的华为等待着进一步的发展的时候，市场和经济的进步也愚弄了任正非一番。\n如今的市场，已经不再需要任正非刚刚开发出来的模拟局用交换机了。高速发展的一二线城市，更想要的是可以直接运用光缆配合数字程控机进行传输信号的工具。也就是说，让任正非弄走了那么多员工，投入了那么多钱，开发出来的局用交换机是没有用的，已经是一个过时的产品了。\n面对这个几乎无解的局面，任正非冷静的分析了当时的局面，并且采用了一个经典的办法：农村包围城市。铺设光缆需要极高的成本，暂时只能在大城市当中做到，而偏远的乡村县城这些地方，是那些外资企业根本不在考虑范围内的地方。他们所忽略的地方就是任正非的战场。\n于是华为的地推人员们不辞辛苦，走上了万里长征之路。无论是我国的塞外高原，还是边防海岛，还是山区小镇，还是革命老区，只要有人需要我们的模拟局用交换机，我们就把战火燃到那里。这些工作人员，他们用最耐心的话语和最周到的服务，给所有用了华为交换机的用户都留下了良好的印象，并且为了应对偏僻地区会出现的各种技术难题，华为员工都练就了过硬的专业技术。\n甚至有的交换机坏了都不用送回华为的总部，华为的地推人员都会修。这取决于任正非对装机团队的严格要求。这个严格要求很能体现任正非，他曾经是军人的那一面。任正非告诉装机队，你们在外面工作就是华为公司的代表，你们一定要让用户对华为公司留下良好的印象，言行举止都要注意，因为这代表了华为的风范。\n局用交换机和数字交换机 # 在93年94年这两年的时间里，凭借着良好的口碑、过硬的技术，华为赢得了大量的用户信赖。正所谓塞翁失马焉知非福。JK1000这个所谓的局用交换机，虽然在商业上不怎么成功，但是却为华为带来了宝贵的经验和服务口碑，并且这个农村包围城市的办法在不久的将来将会成为华为纵横世界的法宝。\n有了模拟局用交换机的技术经验，那么研究数字交换机也就成为了可能。只是此时此刻，华为的资金又不多了，常常是上个月发了钱，下个月就没了着落。很多员工私下讨论最多的就是公司什么时候破产。\n然而就在这样艰苦的条件下，任正非带领华为坚持了下来，省吃俭用的，把所有的钱都拿去研究技术。终于是熬到了数字交换机研究出来的那一天。凭借着之前积累下来的良好口碑，新生产的交换机未测试完成就被强行拉到了浙江义乌开机了。果不其然，一开机bug满天飞。\n但是华为不愧是华为，他的售后精神是很好的。白天有很多人在使用这个交换机，所以没办法进行维修和调试。但是一到了晚上，华为所有的工程师都会开始加班。这些工程师们，他们纷纷进入电信局开始调试，一直忙碌到第二天白天，然后回到任正非专门为他们安排的旅馆去休息修改了两个多月，终于才把调试给勉强完成了。\n多年以后，这台交换机被送入了华为博物馆永久展。随后，任正非又一次力排众议。他告诉大家说，我们应该研究万门交换机。当时华为内部有人认为2,000门的交换机在中国市场就已经够用了，万门根本就没有必要，中国不会有那么多人用电话的，就算研究出来了也卖不出去。\n面对这种论调，任正非无论如何安抚自己员工的情绪，都有员工表示激烈的反对。直到郑宝佑站了出来，站在了任正非这一边。他说，大家尽管放心，研究开发出来，我保证可以帮大家卖出10台。如果卖不掉，我就自己买回家。\n后来的事实告诉了我们，万门交换机哪里才卖了10台，在中国就卖了几十万台。这意想不到的胜利，让华为终于摆脱了生存之忧，并且中国通讯行业终于是站稳了脚跟。在当时大半个中国的公用电话用的都是华为的万门机。华为正式成为了当时中国通巨大中华四大巨头之一。\n“巨大中华”四巨头 # 当时的四巨头分别是巨龙、大唐、中兴和华为。他们的头一个字连起来就正好是巨大中华。四家企业都用自己的方式在自己的范围里将外国势力清理的干干净净。外国的势力是扫清楚了，但是此时此刻的中国通讯业正好要迎来4家争霸的新局面。\n这四大巨头并立的时间并没有太久。巨龙就因为股东的原因，竟在自己的内部打起了价格战，被华为趁虚而入占领了市场。不得不感叹啊，教员的学生终究是要屠龙的。\n然而大唐由于成立的比较晚，而且缺乏长期的规划，虽然一直缓慢增长，但是终究体量太小了，无力抵抗衰落的局面，最终还是难逃陨落的下场。而此时此刻，四大企业当中就只剩下中华了，中兴了，华为在通讯业最后的对手。所谓卧榻之策，岂容他人酣睡。\n中兴 华为 商战 # 所以在1988年，华为率先向中兴发起了进攻。华为搞了一个比较书，列举了华为的优势和中兴的缺点，并且广泛的投送给了目标用户中，得知之后非常的气愤：“这不是欺负老实人吗？我又没招你，又没惹你，你凭啥呀。”\n本着以牙还牙的精神，忠心也搞了一封比较书，但是似乎华为预料到了这一切，先发制人，分别在河南和长沙把中兴告上了法庭。理由是，中兴进行了一次引人误解的对比，并且要求赔偿1,200万和600万。看得出来确实是资本雄厚了，现在张口就敢要600万。\n中兴也不甘示弱，在同样的地方，以同样的理由起诉了华为，分别要求赔偿1,500万和750万。法官人都傻了，你们两个人在干什么呀。于是呢给这两个企业各打50大板，互相赔偿了，但是华为赔的要多一点，毕竟中兴要的要多一点。早知道华为就应该多要一点，自己就不会亏了。\n这一次对垒呢是被中兴找回了一些颜面，自此之后，两家公司都很默契的朝着不同的方向发展了，毕竟都是中国企业，要打的头破血流也不至于。那么为什么华为要挑起这次商战呢？很多人总结，是因为两家企业的领导者性格不同。中兴的老总侯为贵是标准的东方企业家，而任正非呢，他反而更像是一个西方的企业家。不同的领导者造就了不同的文化。\n关于这场商战的最终结果，看起来好像是华为赔了很多钱，但是实际上，这场战争却为华为上了商场上的很重要的一刻。相较于华为之后就要经历的战场，这场和中兴之间的决战更像是小打小闹。\n香港市场 # 在这场小打小闹结束之后，华为迈出了自己的新的一步。任正非决定他们要开拓海外市场，毕竟内卷是绝对没有前途的。如果一个企业老是惦记着在国内扯头皮，互相抢市场的话，那么这个企业永远也不能真正的走向世界。\n任正非要走向世界的第一步，首先是香港市场。这个时候的香港市场还是香港电信一家独大，而给香港电信提供交换机的正是大名鼎鼎的西门子。别说西门子这个品牌了，单说德国工艺，在上个90年代末的国际上，那都是备受欢迎的。\n然而刚刚踏出大陆，名不见经传的华为竟然做出了一个惊人的决定。他们决定要在香港市场上和老牌豪强西门子一决雌雄。这样的行为放在当年，如果告诉一个路人的话，那个路人肯定会笑的牙都要笑掉了。在那个年代，无论是谁听到这个计划，都会觉得任正非这个计划简直是自不量力。\n这个时候，不光是旁观者想看华为碰一鼻子灰，就连香港的很多企业也担心内地的交换机质量不过关，都不想和华为进行合作。但是就在这个时候，华为和任正非遇上了一个非常好的时机，那就是1997年香港宣布回归大陆。\n香港回归大陆之后，大陆方面给了非常多的优惠政策，其中对于华为来说最重要的一点，曾经名义上香港还是归英国管的，所以要收关税，而现在香港回归了，那自然关税也就无从谈起了。\n不加关税的华为的价格，比起加了关税的西门子或者是其他的外资企业来说，那个价格简直是太香了。于是在香港回归之后，香港的很多运营商都想选择国内的企业，而这个国内的企业就是指的华为。\n由于华为可以提供优秀的政策加上价格的优势，香港通信市场的第二名合计公司最终决定与华为合作。但是这个时候却出了一个问题。任正非，虽然早早的就和香港有生意往来。我们之前说过，任正非在创业的时候曾经代理过香港的产品，但是呢，这毕竟是华为第一次走出大陆市场。华为从前的交换机都是在一套体系下建立的，也就是大陆体系下建立的。而现在要拿到香港去用，香港当时的基础建设都是由英国进行建设，他们连电服都跟我们不一样。\n那在这种情况下，华为的商品会不会有问题？到底能不能用？用不用的好？会不会有 bug？这些事情谁也不知道。在和合计公司签订了合同之后的不久，华为终于去香港安装了第一台华为的交换机。但是正如我们刚才所说的，果不其然，这个交换机出问题了。从他开机的那一刻开始就开始 bug 满天飞，这让合计公司很不满，甚至让他们对华为这个品牌的质量都产生了怀疑。\n他们当时直接给华为下了最后通牒，说我们在香港屈居第二，想要生存下去就必须要拿出最优质的质量和服务。如果这些问题不能得到解决，不仅是我们合计要被香港电信打压下去，你们华为之后在香港也绝对没有站稳脚跟的机会。\n在这样的窘境之下，华为的员工们又一次拿出了当年在浙江义乌的精神。工程师们这一次直接搬进了机房，上一次在义乌出了问题是工程师们昼伏夜出，加班加点的改，而这一次呢，比上一次还要紧急。\n那怎么办呢？那就直接分成两批人，24 小时不间断的调试机器，甚至其中还有很大一部分的技术员每天白天要工作，晚上还得通宵。华为员工的辛勤付出最终打动了合计公司的人员们。他们最终向华为伸出了援手，双方努力之下终于赶在规定日期之前完成了调试，完成了验收。\n华为因为这一次的效率和服务终于得到了应有的回报，在香港站稳了自己的脚跟，并且顺利的拿到了电信业务的经营权，成功打消了其他香港企业对国产交换机的偏见。但是这只是华为的第一战。\n俄罗斯市场 # 华为在吃下了香港的市场之后，转头将自己的目光盯上了俄罗斯。当时的俄罗斯刚刚经历完苏联解体，整个国家都处于元气大伤、全民低迷的一个状态。俄罗斯国家内部更是在运行一种由西方建议他们运行的休克疗法，导致各行各业一片惨淡。最奇妙的是，他们自己都烂成这样了，他们还歧视中国。在当时很多俄罗斯人的心中，中国制造四个字就等同于假冒伪劣。反正总而言之，俄罗斯非常看不起中国的企业。\n但是俄罗斯作为一个幅员辽阔的国家，他的市场潜力是非常庞大的。俄罗斯的电信普及率极低，国土面积比中国还大。于是任正非亲自到了俄罗斯做宣传，举办了一场展览，希望可以把在中国的经验复制到俄罗斯。结果这一场宣讲之后，根本没有人买账。最让任正非感到挫败的是，他在俄罗斯的这段时间经常会出去逛街，但是他每次逛街都能发现，在每一家俄罗斯商店的门口都会有一块小牌子，这个牌子上用大字写着：“本店不出售中国货，大家可以放心购买。”这就是赤裸裸的歧视。任正非作为一个中国人，看到这样的东西，也只能尴尬的笑一笑。\n但是时间来到1998年，华为的转机来了。俄罗斯又爆发了金融危机，整个电信业也无法幸免，几乎处于一个全部停滞的状态，俄罗斯人连日常打电话都成了问题。看到俄罗斯如此衰败的景象，任正非当即决定把最优秀的人才调往俄罗斯开拓市场。并且任正非义正言辞的对这次带队的负责人说：“如果有一天，俄罗斯市场复苏了，而我们华为却被挡在门外的话，那你就从楼上跳下去吧。”负责人轻描淡写地回应：“好的，如果我们被挡在外面，我就从楼上跳下去。”为了这个承诺，华为的俄罗斯团队像蜜蜂一样开始了辛勤的工作。\n在几乎无利润的情况下，华为的工作人员们仍是在一日复一日的进行产品推广，不为了卖钱，只为了告诉俄罗斯人，华为还在俄罗斯，中国产品还在俄罗斯。\n但是与此同时，由于苏联的解体，大量的高端人才没有了工作。任正非这个时候也发现了自己的机会，于是他也亲自去到俄罗斯，把大量的人才挖掘到中国。而这批被挖掘来的人才当中的其中一些人，就成为了日后华为突破2G、3G难题的中流砥柱。\n在衰败的俄罗斯市场，华为一直坚持不懈的坚守了两年，终于等来了转机。俄罗斯总统叶立钦下台，普京成为了俄罗斯总统，俄罗斯的政治局面在普京的铁腕强权之下逐渐的稳定了下来，俄罗斯终于能够像一个正常国家那样稳步发展了。\n而华为的俄罗斯团队终于也等到了他们的春天。经过之前的不懈努力，华为与俄罗斯政府之间还是建立了一些信任的，并且最终啊，华为与俄罗斯当地的企业合作，组成了拜托华为，并且在2001年获得了俄罗斯邮电部门的认证。经过这一番操作之后，华为可以说是守得云开见月明了。在他们拿到认证之后，短短一年，华为就在俄罗斯的收入就超过了3亿美元。\n欧洲市场——英国 # 但是这是华为在俄罗斯的故事。华为永远不会将自己的目光只放在亚洲，而是争霸世界。于是在征服了俄罗斯之后，任正非将自己的目光放在了欧洲。要在欧洲打出华为的一片天下的话，那么华为最好的选择是找一个最容易被人攻破的国家。于是任正非选择了法国，于是就在同年，华为开始进攻法兰西的电信市场。华为以非常优惠的价格为法国本土的运营商提供网络，并且他们负责运营3个月。3个月之后，如果你用的觉得合适，那你就和我们华为签约。靠着这样的优惠政策，免费试用3个月，这谁听谁都觉得香。就凭借这样的优惠政策，华为顺利的杀入了法国，在法国的市场纵横驰骋，很快就将竞争对手们全部打垮，成功拿下法国市场的顺利让任正非深受鼓舞。\n于是就在几个月之后，他马上带着团队进军英国市场，但是英国人并不像法国人那样老是喜欢投降。任正非在这里遇到了一系列的问题。首先英国只会给政府认可的企业提供工作的机会，所以任正非的华为连参加招标的资格都没有。而且英国政府的认证是特别繁琐的，涵盖了12个方面的内容。为了应对英国政府严苛的考察，华为甚至为此专门成立了英国电信认证小组，这个小组几乎囊括了华为所有部门的人员，他们存在的意义就是要保证华为绝不会在英国政府的考察上出任何的疏漏。\n时间来到2003年的11月，英国电信采购认证团终于来到了华为，对华为进行了为期四天的严格体检，说是体检，更像是一场刁难，因为他们考虑的根本就不是华为的技术，而是华为所在的国家。他们当时问了很多关于中国人权之类一类莫名其妙的问题，搞得现场的华为员工一脸懵逼。在最后的评分上，基础设施硬件所有的东西都达标了，甚至拿到了一个非常高的分数，但是所有的关于人权之类的软性指标全部都不合格。\n临走的时候，这些所谓的英国专家还说哎呀，你们华为要继续努力哦。华为的员工都傻了，那我们这个怎么努力吗？难道加入英国国籍吗？随后这个英国官员给了华为的员工一个眼神暗示，就好像在说你懂的。这次考核的那个记录书上每一页都写着要努力。华为的员工横竖睡不着，醒了半夜，才从那个眼神当中看出两个字来。原来那本审查书的每一页都写着两个字：要钱。\n于是华为心领神会，在接下来的两年时间，华为交了数亿元到英国政府官员手里。当时在英国本土的一些通信企业很多次向这些官员表示千万不要放华为进来，因为他们和华为打价格战自己是必死的。但是经过华为的不懈努力，终于让这些官员看到了华为的诚意，最终批准了华为进入英国市场。英国通信业的企业人都傻了，都不知道官员在干什么，属于是自己扼杀自己国家的通信行业了。\n关于这件事，华为欧洲图标部的主管在很多年之后曾经说过：几年来全球各地一共投了多少次标，他自己都记不清了。唯独英国电信这个标是以后怎么都忘记不了的，因为这一群人，他们自诩身世，干的却是一些强盗都不如的勾当。\n也就是说，他们这个强行找华为所绘的这个行为，到今天为止，华为的这些领导层们都是记得的。这很难让人忘记。虽然我们说的比较好玩，但是实际上这是一个巨大的屈辱，但是这并没有让华为暂缓他们的脚步。\n欧洲市场——德国 # 接下来华为乘胜追击。多年之前，华为还是在香港和德国的西门子抢市场的。西门子在香港输给了华为，而华为呢，如今要闪击柏林了。\n这里我们要先科普一个小知识。大家要知道，德国的科技公司，平均每年会把将近7%的销售额投到研发里面。中国的公司有很多都不足5%。所以在当时的世界上，基本上没有人看得起中国企业，因为中国企业是不创新的。\n而对于德国的西门子来说，他们在香港失败了，败给了华为。他们从来没有深究过华为这个公司到底是个什么样的公司。他们只是觉得，这次失败是来自于中国人的民族主义，都是不愿意买外国人的，只愿意买自己人的，所以才会失败。但是他们却不了解，中国人最是讲实际的。如果他的产品不比你好的话，你怎么可能会输给他呢？\n这里我们终于可以揭晓谜底了。华为每年会把自己的百分之15的销售额投到研发里，是其他公司的两倍还要多百分之一。当然西门子不知道这个事情，他们的幻想当中从来没有想过会有一家公司打败他们，甚至有一家中国公司会打败他们，这是不敢想的。\n于是在那一年，一年一度的德国汉诺威计算机博览会上，西门子带着自己的最新科研成果也参会了。但是他们的展台面前却没有多少人，大多数的人都集中在另外一个展台上。而这个展台竟然不是一个德国企业，在他们的LOGO上，用大大的中文写着“华为”。\n华为第一次参加一年一度的德国汉诺威计算机博览大会，就成为了全场最大的焦点。他们经过4个月不断测试的新产品，在这次发布会上力压所有的德国企业，并且还在随后的竞标当中，得到了德国最大电信运营商QSC的青睐。\n在这次博览会上的其他德国企业，人都傻了。“华为是谁？是德国公司吗？他为什么来这里啊？”敌人是谁？我是谁？我在哪？我在干嘛？我怎么输了？反正是一问三不知，最后就团灭了。\n欧洲市场——荷兰 # 拿下了德国的市场之后，华为又将目光放到了荷兰。\n荷兰这个国家，虽然只有区区1,600万的人口，但是却拥有五家世界级的运营商。在这一个小小的国家里面疯狂的竞争，他们都想完全吃下荷兰这块蛋糕。\n面对这样的情况下，华为选择了五家当中最弱小，叫做太福的那家企业合作。合作成立了一个移动创新中兴，专门研究荷兰市场适合推出哪些移动服务项目。\n但是这个时候，华为的合作伙伴泰福却担心了一个问题，那就是环保。因为荷兰政府对环保有着近乎苛刻的要求，所以要在荷兰开起一家公司，就需要做到极致的环保，而这需要成本，需要钱。泰福不确定自己能不能支付起这么大一笔巨大的投资。\n华为看到自己的合作伙伴拉了库，于是赶紧的提出了一个叫做分布式基站的解决方案。华为将基站分为两个分离的部分，这样一来，太福就省出了一笔本来应该去拆装交换机的钱，而这个钱就可以用去做环保。这样一来，一举两得，完美的解决了问题。\n得知了华为插了一脚之后，泰孚的2G设备供应商爱立信急了。华为这是想干嘛？是要抢我的饭碗吗？于是他赶紧约见了泰福公司的CEO、CFO等高管人员，并且义正言辞的强烈抗议。在此之后，华为在荷兰继续为泰福公司提供服务，而爱立信则被边缘化。\n泰福公司的CTO对华为解决他们的站点的解决方案非常满意。在随后的一些项目竞标中，华为都是全力以赴的拿下项目，并且屡次和爱立信交手，均占据上风。\n华为这一系列的成功，有人说华为是靠着极低的价格取胜的，但是其实并非如此。欧洲国家的福利非常的好，所以他们的员工有的时候找人是找不到的，可能在放假。而华为的技术人员随叫随到，有问题必解决。\n泰孚就是看中了华为对合同执行的承诺和快速的反应能力，所以才选择了华为。至于价格，华为其实是不那么低的。华为为泰孚解决了这个基站的问题之后，又引起了荷兰龙头通信行业KPN的注意。经过多轮谈判，KPN最终决定收购泰服，并且华为成为了KPN在荷兰全国骨干传输网当中的唯一供应商。华为此时已经成了荷兰最大的全业务运营商。\n中东 # 与此同时，华为还同时在南亚、非洲、南美等多个地方以城市包围农村的战略开始了自己的战役，低价抢夺市场路子，占领大片的市场份额。当然，也不止这几个地方，还有一个特别有钱的大市场，而这个市场的名字叫做中东。\n中东的各国普遍都拥有石油资源，他们的经济非常富裕，但是他们的科技却非常落后。由于这样的特性，导致他们的通讯业全都被国外给瓜分干净了，没有国内企业。这个市场如果中国想要挤进去，是一件很困难的事情。但是由于911事件的爆发，中东各国普遍愿意与中国合作，华为就这样有惊无险的冲进了中东市场。\n华为秉承着支持多元化文化的态度，于是任正非和他的华为横扫亚、非、拉，在欧洲、非洲、南美、南亚站稳了自己的脚跟。而现在当我说了那么多地名之后，大家是不是发现我漏掉了一个地方，而这个地方也就是华为的最后一站，这个地方就是大洋彼岸的美国。\n美国 # 大家都知道，美国是世界上最成熟的市场，同时也是最难进入的市场。最可怕的是，如果你成功进入了他，那么你想保住自我是很难的，要么你成为他的一部分，要么被他们打跑。强大如同索尼，本来是个日本企业吧，去了美国之后，现在也变成了彻头彻尾的美国资本了。\n而任正非是谁啊，他偏偏不信这个邪，老子就是要打进美国市场，精锐老子打的就是精锐。于是他开始了长线的规划。早在1993年，华为就在硅谷成立了芯片研究所。1999年，华为在达拉斯开设了研究所。2002年，华为在德克萨斯成立了子公司。同年的6月，华为产品在北美市场第一次亮相，性能与同类产品相比，基本上是没有什么差距的，但是价格却便宜了一半。亮相之后，华为快速的打进了美国市场，销量猛增七成。\n但是，就是因为华为的市场份额增加的太快，却惹恼了当时的业界大佬思科。思科一开始还在思考，华为究竟能不能作为思科的对手，但是此刻，华为已经开始和思科抢夺市场份额了，一个蛋糕就那么大，你多吃一口我就少吃一口，那这是不能忍的。于是思科这边紧急也下调了价格。\n就在此时，华为发了一条广告，这条广告可以说是骑着思科的脸输，这个广告上只写了一行字，这一行字是唯一的不同就是价格，而这个广告的背景上就是思科这波啊，这波属于是贴脸输出了。业内有人戏称，思科之前的宣传口号思科在你身边，世界由此改变，现在因为华为来到了美国，市场变成了华为在你身边，思科由此改变。\n到这个时候，思科就彻底忍不了了，开始煽动所谓的华为威胁论，并在公司的内部成立了打击华为小组。大家觉得一个美国公司会用什么样的手段打击华为呢，投入更大的资金，去研究更物美价廉的产品吗？不可能的，美国资本从不搞这一套，美国的科技公司如果打不过你，他就去法院告你。\n思科当时认为华为侵犯了知识产权，要求华为侵权赔偿，并且停止销售产品，最好能滚出美国市场。而华为在任正非的授意之下，表示可以停售一些有争议的产品，但是绝对不能承认我们侵权。然而他的好意却没有迎来回报，任正非的让步被认为是侵权的铁证，于是思科在2003年的1月24日德州联邦法院当中提起了诉讼，这次诉讼的材料就多达77页，指控华为21项罪名。\n斯科的高管们以为任正非肯定会很慌，毕竟被起诉了，结果没想到任正非一点都不慌，反而幽默地对斯科说，这是你们斯科送给我们华为的一个春节礼物，然后就开始了自己的部署。首先先组建了应诉团队，你不是爱打官司吗，老子就陪你打到底。然后华为在公司内部展开了自检，检查自身到底还有没有什么把柄会被别人抓住。\n第三就是舆论战。众所周知，美国是陪审团制度，舆论的走向很有可能会决定官司最终的胜负，而且舆论才是名副其实的主战场。然而华为他们一直都以低调著称吗，平时并不在意舆论的走向，所以在一开始，华为在前期非常被动，你不说话，你说可以下架，就是你心虚。在当时，甚至连华为的代理律师团队都认为华为真的有侵权行为，而且美国人一贯傲慢，根本不认为中国企业可以做出这种高科技产品。\n而且此时华为是在美国打的官司，我们的祖国也无法帮助华为，所以此时此刻的华为真的是陷入了孤立无援的地步。面对这样的情况，任正非首先邀请了代理律师来到华为的总部，带他们参观了中国的工厂研发基地，告诉他们华为的实力，告诉他们什么是真正的华为。律师团的成员也是在看到了这些设备之后才化解了误会，原来华为是真的没抄，并且为华为提供了意见。\n此次华为也学到了很多，东西必须要打破自我封闭，更准确的说，要让别人相信其。因为打开自我封闭就容易被别人入侵，所以真正完全打开自我封闭的，那是傻子才会干的事。\n此时此刻，华为前后多次参加听证会，树立企业形象，并且加大投入，舆论的天平逐渐地扳了回来，变成了一个军事。但是胜负仍然不算乐观，任正非是绝对不能容许侥幸存在的，于是他使出了杀手锏。\n随后，华为宣布和仅次于思科的第二大通讯公司3Com成立合资公司，并且华为和3Com进行了深度合作。3Com的CEO甚至公开为华为背书，这个CEO对所有人说，华为的工程师是很有天赋的，他们坐在宽大的工作室里面，操纵着最新的设备和软件，他们拥有我所见过的最先进的机器人设备。反正是华为爱说什么3Com就说什么。\n这个消息被思科的副总裁知道之后，气得那个人三天三夜都睡不着觉。我们正在抗华，你竟然捅我一刀。思科是万万想不到会有这种事发生的，但是事到如今，就已经没有再打下去的必要了。这场官司最终以华为停止使用有争议的路由器软件的原代码而结束。实际上，华为什么都没亏，原代码而已，双方都对外宣称是自己胜利了。\n在这场商战结束之后，华为正式地在北美站稳了脚跟，没有被赶出去，也没有被肢解。谁说进入美国就一定会被美国人吞并？在此时此刻，华为屹立于美洲大地之上。\n在随后，对美国的司法体系有了一定了解之后，华为又做了很多的准备。这就是我们之所以在前几年发现美国政府要搞华为的时候，华为能够应对的原因。\n在美国取得了阶段性的胜利之后，华为该做什么样的事呢？不知道大家还记不记得我之前讲过的，华为在1992年的那次年会。当时华为没有钱，只能将钱优先地研究产品。而现在今时不同往日了，华为已经是一个地跨亚非欧美的跨国大公司了。\n那这个时候该奖励员工了，2003年，华为动用30亿元的内部股权，再次给予了80%以上的员工股票的购买权。这是一个什么概念呢？早在华为的成立之初，就已经有了员工分红等福利了，而到了90年代，任正非把薪资分为了基本工资、福利待遇、加班费用、绩效奖金和岗位补助，以及公司股票等，其中基本工资、股票、绩效占到了员工收入的80%。也就是说，在这一次分配之后，华为的员工能拿到更多华为的股权，而之前没有股的，现在也能分到股。这让员工们真真切切地感受到了，我是为自己在工作。\n任正非自然也明白，只有在待遇上让员工无后顾之忧，才能让他们在工作上全力以赴。所以他经常为员工们涨工资，经常是给自己的员工一涨薪，就涨人家一个月的工资，或者是涨人家小半个月的工资。\n听到这里可能有小伙伴要问了，那么去华为工作能挣多少钱呢？我这里给大家举一个例子。1992年，有一个叫做李一男的高材生，从大学里毕业，听说华为骨干级工程师特别挣钱，收入能达到西方国家小康水平，于是他入职了华为。很快凭借着他的工作能力，得到了丰厚的报酬，在那里工作了几年。当他在2001年以叛徒的身份离开华为的时候，就已经拥有了几千万的存款。划重点在2001年有几千万的存款。\n人才 # 一个企业是很需要人才的，任正非选择了最快、最便捷的招揽人才的办法，那就是给钱。从1996年开始，任正非不仅在国内的高校开始疯狂地挖掘人才，并且他也想让很多在海外归来的技术精英来到华为。所以他就直接说了一句话：你们来华为，年薪不会低于10万美元。这些海归高材生们哪里会想到，20世纪末的中国通讯业竟然能拿到如此高的薪酬。\n在这样高调的高薪招聘下，任正非很顺利地招募到了不少海归人士以及高材生。他们把他们从自己学校和从国外学来的先进技术全身心地投入到了华为里。在工作上，任正非一贯是论功欣赏，研发人员和工程师们工作越出色，他们得到的钱就越多。一位专门研发芯片的工程师，因为业绩突出，刚进入华为的时候，他的年薪。而仅仅工作了不到半年，因为他个人对公司的贡献增大了，所以他的年薪就涨到了50万美元。而对于普通的员工来说，2002年华为新入职的实习生基本工资就在5,000块钱左右，同时还能拿到绩效和补贴。\n在华为工作加班吗？那肯定是要加班的。我们会发现，在其他公司一说加班，很多人都不愿意加班，但是在华为，越是技术骨干，越是在拼命加班。他们是在为华为挣钱吗？不是，他们是在为他自己挣钱。\n在薪酬方面，华为可以说是不少年轻人渴望进入的企业。可能有小伙伴要问我了，我拿的都是一些历史上的数据，那么现在的华为员工可以挣多少钱呢？我找不到2021年的，但是我找到了一份2018年的这个报道。\n截止到2018年，华为员工持有华为股数的人数已经接近10万人了，而这10万人一共持有了华为所有股票当中的98.99%，而任正非只持有了1.01%。那么问题来了，华为的总股份有多少呢？我告诉大家，222亿股。大家可以去自行查一下，华为的市值是多少，然后除以这个数，你就知道他们每个员工手上有多少钱。\n只是大部分的人，有华为股的人，都不会选择将华为的股给变现，因为这些股会随着华为这个公司越来越好不断地增值，这是固定资产。我在上一期节目当中的弹幕当中看到了有一个小伙伴说，他的叔叔就是华为的老一批员工，到现在为止房子都不知道分了几套了。\n这里我们不禁要思考一个问题：这些员工，他们真的是在为任正非赚钱吗？有一说一，任正非只拿了1%，剩下的99%都是自己给自己赚钱了。华为这样的分配方式，比起那些所谓的什么88的那种什么什么创始人说年轻人都该996又不给年轻人钱的那种人，我就想问一个问题，你给我翻译翻译：“狼行千里吃肉，狗行千里吃什么？”因为华为的狼性文化非常成功，所以他们就鼓吹狼性文化，像华为这样，但是他们又不愿意把公司的股权分给员工，想让员工变成狼，又不给狼吃肉，最后就学成了狗性文化，让员工们互相抽嘴巴子。\n任正非在带领着华为的狼，寻找更多的肉，而不是把一块肉扔出来让一群人去抢。一块肉让100匹狼去抢，那个不叫狼性文化，那个叫驯化，叫压榨。在经过了2003年的股权重新分配之后，华为便投入到了3G的研发当中。而我们不得不说的是，这又是一次梭哈。\n任正非认为，2G时代过去了之后，下一个时代必然是3G。而所谓的3G技术，就是第三代移动通讯技术的简称，是指支持高速数据传输的蜂窝移动通讯技术。3G服务能够同时传输声音以及数据信息。在一定的意义上来说，谁掌握了3G的先机，谁就有可能掌握未来通信市场竞争的制高点，取得优越的竞争优势。\n华为南方研究院的一位员工说，华为对3G的热忱已经到了从芯片做起的程度了。每次投片啊，也就是将设计好的芯片设计图交给IC厂投产的时候，所花的这个费用都得在几百万美元以上，累计已达50亿人民币了。为了三季，任正非付出了很大的代价，但是呢，花了那么多的代价却没弄出来。而最后三季是怎么来的呢？\n这里有一个很戏剧性的故事。我们之前不是说了吗，任正非从俄罗斯不是挖了一批高材生吗？这其中就有一个小伙子，当时正隶属于三季标准研发部门。十几年来默默无闻，突然有一天，任正非收到了报告。报告上写道：“您好，任总，我们已经把2G到3G的算法给突破了，俄罗斯一个科学家。”这小伙子不会谈恋爱，就是只会做数学，他到我们公司来十几年，天天在玩电脑，不知道在干啥，突然要告诉我，我们把2G到3G就算法突破，我们纠正了一下就领先全世界。无线电台女性汉语系大陆占领欧洲有很厉害对小孩子啊就挺突然的。\n而这一切的功劳就归功于那个小伙子的灵光一闪。大家还记得，资本发展到一定程度会阻碍科技的发展，这一点在通讯行业当中真的是展现的淋漓尽致。领先了多年的公司一下子被超越了。不过呢，当时未必没有欧洲公司已经突破了算法，比如爱立信。我们看他之后的表现，很明显就是这些公司已经早就突破了算法上的突破。哎，我早就弄好了，但我就是不出，我就是要拖着2G时代把2G时代用到尽头，再用3G来赚你们的钱。既然2G能躺着赚钱，他们绝不会向前走一步。这也算是一种资本的特性吧。不过，由于华为一心的研发3G，忽视了小灵通，让同城的兄弟衷心着实赚了一把。\n但是即使是这样，任正非也不认输。对于他们没有吃上小灵通的红利，任正非说：“小灵通技术不是什么先进技术，并不是能连续多年持续建设的那种技术，他的火爆是政策造成的。我们没有建设小灵通，这到底是不是一个错误，我们过几年再来总结。”\n3G # 时间一转，来到了2005年。国内三汽产业的环境已经形成了系统解决方案日渐成熟。华为呼吁信息产业部借此机会启动3G。信息产业部高级顾问许土木认为，随着3G移动通信技术的发展成熟和商用，信息产业部将根据中国电信业务市场的需要来发放3G经营许可证。\n时间来到2008年，中国电信发布移动业务品牌“天翼”，天翼3G登上历史舞台。随后部分省市3G投入商用，再往后全面转型为全业务经营商。在12月31日，国务院常务委员会议上通过了决议，同意了3G牌照的发放工作。这标志着我国进入了3G时代。\niphone 4 出现 # 在诺基亚的时代，手机的标准应该是结实和耐用，甚至可以用来开核桃。而在当时，诺基亚的功能机有那么几种类型：直板手机、翻盖手机、滑盖手机以及全键盘手机。所有人都以为未来一定会是诺基亚的天下，直到有一个年轻的团队将诺基亚挑落下马。而这个年轻的团队，他的名字叫做苹果，带领着这个团队的人叫做乔布斯。\n在2010年，乔布斯带着iPhone 4震撼了全世界，他重新定义了什么叫做手机。用近乎完美的设计，把手机从一个工业实用品变成了一件艺术奢侈品。全触摸屏的设计、简洁的系统、美观的UI和图标都在不断地挑战着人们的神经。于是渐渐地，大家开始将苹果手机和像苹果手机的手机称为智能机，而不像苹果手机的手机呢，我们将它称之为功能机。\n苹果如此完美的设计，加上APP store里面数以万计的APP，iPhone 4风靡全世界。世界各地的苹果专卖店门口都排起了长队，数以万计的人来到这里，他们只为了购买自己心目中那个仿佛从未来来的产品。随着iPad和iPhone的连续推进，诺基亚的手机帝国轰然倒塌了。渐渐地，人们开始把苹果店称之为教堂，把乔布斯尊为布道者，甚至有更过分的人将他称之为手机行业的上帝。\n面对着这样的局面，无数的手机从业者们都对乔布斯这个人感到害怕。但是坐在华为办公楼的办公室里面的任正非，却看到了希望，因为他明白，未来一定是全面屏智能机的天下，而在这样的大趋势之下，华为要出手了。\n其实说到做手机，任正非一开始是拒绝的，因为早在90年代的时候，华为就尝试过做过电话，但是因为当时的技术还不太成熟，坑了不少的客户。正因为一朝被蛇咬，十年怕井绳，所以说到了3G时代的华为，最多也就是偷偷摸摸的帮别人代工做手机。华为是从来也没有正大光明的做过手机，毕竟我们在上一期节目中说到了，任正非和他的华为在全国各地的通信业都做得丰盛，通信业这么赚钱的情况下，谁会去做手机呢？\n但是现在情况变了，因为华为已经将全世界几乎所有国家的通信业能打入进去的都已经打入进去了，就相当于种子已经种进去了，接下来就等着收获了。等着收获可以，但是像华为这样的一个大企业不能一直等着收获吧。因此华为要开发新的业务，就在这个时候，华为内部一个叫做张立华的人向任正非提议：既然我们都是做通信业起家的，而且我们也做过手机，那我们为什么不把我们已经放弃了的手机行业重新捡起来再继续做呢？\n任正非听完之后觉得很有道理。从前华为做手机失败主要是因为那个时候华为的体量还太小了，没有那么多钱去搞代工厂，也没有那么多精力去管手机上的业务，毕竟通信业都做不过来。而现在华为闲下来了，正好是进军手机市场的好时机。\n当然华为想要做手机行业也不仅仅是因为张立华的一份建议，更重要的是，华为在刚刚开发3G的时候就发现3G系统根本就没有可以配套的。任正非是一个充满自信的人，他并不认为华为的3G系统有问题。如果说系统没有问题，那就只有一个情况了，那就是手机有问题。他坚信自己走在时代的前端，如果没有适配的手机，那就自己来做。\n幸运的任正非全力一搏，终于为他换来了一个还不错的结果。在终端公司成立之后的第二年，华为就在戛纳的国际移动大会上秀出了自己的首款3G手机。所以，华为的3G手机其实出道的非常早。但是，华为的3G手机一般都是卖给手机厂商的客户，卖给这些客户的产品不会打华为自己的标志，而是打上那些厂商们的标志。当时，华为大多做的都是欧洲和中国运营商的定制版，很多上面都没有华为标志，所以很多小伙伴们都不知道其实华为做手机做得很早，而且华为手机的销量一直都很好，至少可以说是做出了风采，做出了成绩。但是这样的好景却不长，直到那一年，乔布斯带着他的苹果手机杀入了手机市场。乔布斯的到来彻底打乱了整个行业的市场，因为乔布斯重新定义了手机是什么，将原本不会那么快到来的互联网时代快速推到了人们的面前。\n十年间，iPhone系列总共卖出了12亿部。在乔布斯的推动下，智能手机消灭功能机的趋势越来越明显。\n而在国内，中兴、华为、酷派、联想，虽然都还抱着运营商定制机的大腿，抢占着本身就已经很落后的非智能机市场，但是这样却有着一个巨大的壁。那就是，运营商虽然采购的数量很多，但是运营商他们压价压得很厉害。这就导致压价下做出的定制机，质量和功能都是非常非常落后的，消费者的不满就落在了手机厂商的身上。当时的中国消费者普遍都认为，中国的手机是远远不如外国的手机的。其实我们不是做不出来，而是那个时候，我们没有人去做这件事情。\n但是对于这些企业们来说，只要有钱赚，做高端机也是做，做低端机也是做，根本就没什么区别。那是谁首先打破了这个诡异的平衡呢？这就要说到2008年，任正非起了念头，打算卖掉华为手机公司49%的股份。然后，华为的启发部就找到了全球很多的基金来谈判，想要卖掉这些股份。结果没想到9月14号，美国的雷曼兄弟破产了，美国的次贷危机开始了，买方的出价一下子就降了很多，就是本来他可能要两个亿才能买这49%的股份，现在美国次贷危机开始了，他可能5000万都拿不出来。这样的价格，看的任正非那是怒从心中起，恶向胆边生。这么便宜的价格你咋不去抢呢？\n于是任正非一气之下，他就不卖了，司不卖了，就有一个非常诡异的局面摆在任正非面前，那就是要么继续做手机，但是如果一直做低端机的话，那基本上是赚不了什么钱，但是如果要做高端机的话，就要投入很多的研发费用。当时，任正非身边有很多的人都劝任正非，根本没有必要做那么好的手机，因为中国的消费能力实在不如美国。但是经历了公司卖不出去的任正非，他又想明白了，他告诉其他人说，中国总有一天是会强大起来的，中国的经济总有一天是会站起来的，我们可以提前为了那个时代去做布局。于是任正非就秉承着既然我们能把手机做出来，那我们就一定能把手机做好这样的概念，开始对华为手机投入大量的资金去研发功能，研发硬件。\n就在任正非决定跟着时代的潮流向前走的时候，有人却因为固步自封而导致自己破产了。曾经世界第一的手机企业诺基亚，在这个时候宣布破产了。\n诺基亚的前CEO奥利拉说：“我们并不知道自己做错了什么，但是不知道为什么我们输了。”这就好像是柯达在破产之前，他们生产的胶卷依旧是质量最好的一样。失败者并没有做错什么，只是他们被时代给抛弃了。如果不想被时代抛弃，就得向前走。任正非明白这个道理，但是很多人却不明白。任正非的军人性格在这个时候又显现出来了。任正非就是很倔强，他无论做什么生意都是想要做成世界一流的，就像是乔布斯一样。乔布斯说的那句话大家都知道，只有相信自己能够改变世界的人，才能改变世界。为了不被时代淘汰，任正非马上带着徐直军、郭平等一众高管跑到华为终端业务部找人，开了一个座谈会，决定放弃幕后工作，不再跟随运营商做定制手机，而是坚定地走向开放市场，建立自己的品牌。而这场会议后来又被大家戏称为“华为终端的遵义会议”。\n这次会议确定了华为之后要发展的方向。但是我们要说，时代在发展，人们的思想也会发展。不只有任正非一个人明白了这个道理。就在华为决定自己要转型的同时，三星、小米、甚至是曾经的对手中兴，都选择了跟随苹果的道路去做智能手机。2011年，华为的手机正式进入手机市场，但是这个时候，我们之前提到的那几家公司已经霸占了国内的江山。想要在这片土地上挖出一个萝卜坑，华为就必须挤掉这些品牌的范围和市场。要做这样的事情，就只有几个选择，要么靠手机的硬实力，要么就是靠品牌的软输出，但是这些能力，华为这个时候是暂且都没有的，因为华为是做通信业的，而在那个年代，知道通信业的人就很少。很多人是用着华为的交换机，但不知道华为这个品牌。\n时间来到2012年，华为的第一款手机投入市场。尽管华为的终端公司的负责人余承东表示，他们已经把能用的所有的最领先、最紧凑最激进的方案都给用上了，但是这一款华为手机在全球的销量只有50万台，和同行相比简直是不值一提。华为这个老牌通信业公司就这样跌跌撞撞地闯入了智能手机的领域，就像是交学费的小孩一样，把领域中所有能踩坑的地方全部踩了一遍。就连最新系列的第一款手机，任正非自己在试用的时候，竟然都死机了。气得任正非直接把余承东叫来，劈头盖脸地大骂了一顿。华为手机的质量不行，谁最高兴呢？媒体最高兴。在当时的报纸或者是为数不多的网络媒体上，天花乱坠都是这样的新闻：试图挤入手机市场的华为自不量力，国产手机质量堪忧，智能手机市场从不给新人留机会。这样的言论不绝于耳。\n面对这样的指责，余承东他要承担绝大部分的责任。当时不仅是社会上，甚至连华为公司内部有很多人要求余承东下台。但是在一片反对的浪潮声中，只有一个人竭尽全力地保下了余承东，而那个人就是任正非。\n我们要知道，任正非是一个对自己和他人都过分严格的人，但是他对失败的余承东却显得格外宽容。也许任正非心里清楚，华为暂时还是智能手机的门外汉。这次失败的责任不仅仅是因为人的问题，也可能是因为他们对这个行业的确不了解。不过，正是因为有了任正非的力保成功，他后面才有真正可以展示自己才华的机会。\n高端机 # 华为在经过失败之后，真真切切地开始进行自我突破，动用了一切力量，想要做出一款良好的产品。任正非当时想的是要做出市场当中最好的产品，但是大家都明白，人不可能一口吃成一个胖子，步子迈大了容易扯着蛋。本来任正非对下面下达的命令是要做一款市场上最好的手机，到了余承东这里，余承东经过自己的思考，把这个目标给削去了一半，变成了做一款市场认可的产品，并且将这个目标传达给了员工。这无疑是一次大好，因为这相当于是对自己的领导阳奉阴违了，但是只要能做出好产品，一切都是值得的。\n紧接着第二年，华为Mate 7上市了，定价标配版2,999元，高配版3,699元。这个手机一经发售，就让很多人非常的惊讶。当时国产手机的定价就没有把价格定到这么高的。你华为一个国产手机，哪来的勇气卖3,000多块？我买一个上一代的苹果，难道不比你这个好吗？很多人都抱有这样的想法。但是真正当华为的手机亮相市场之后，很多的消费者都动心了，因为这个手机搭载的芯片是中国第一次自主研发生产的，名为麒麟的芯片，并且在处理器方面，这款手机是真正的做到了领先自己的竞争对手。此外，华为这款手机还采用了一种叫做窄边框点胶技术，把屏幕的黑边缩小到了最小，拥有了业界最大的宽屏比。最亮眼的功能就是背部的指纹，1秒解锁，那可以说是轰动性的首创。之前乔布斯做出了滑动解锁，而现在华为做出了背面指纹解锁。之前有好几家公司都一直想把这个指纹解锁，但是最终这些公司都因为自己的资金投入不足或者是员工能力不足都没能成功地实现这个功能。但是华为实现，最终Mate 7超出了所有人的预期，全国销售700万部，成为了当年的爆款。很多人他们总是说，华为捆绑了爱国的情怀，才有了如此高的销售量和市场占比度。\n但是实际情况真的是这样吗？我们如果仔细的看一看，这些年的手机市场，我们就能很明显的发现，在华为崛起的过程当中，有另外一个品牌正在因为它们的质量问题一步一步退出市场。而这个品牌就是三星。这几年三星手机在中国的销量可以说是断崖式的下跌。2018年第二季度，三星手机在中国的市场份额只有0.8%。要知道五年前的三星手机在中国的手机市场当中占有率还能达到惊人的20%。仅仅过了5年就混成这个样子了啊，几天不见这么拉了。\n在拉跨这件事上啊，三星绝对没有只拉一点点，那是直接拉在了自己的库里。三星手机退出中国市场除了受苹果和华为、小米的市场挤压之外，更重要的是傲慢自大，不尊重中国消费者。三星note 7在美国爆炸的新闻传到韩国之后，三星在美国纽约时报、华尔街日报和华盛顿邮报三家主流媒体上都投放了他们的全版广告向美国消费者公开致歉，并且迅速召回了全球200万部手机，所涉国家包括美国、韩国、澳大利亚等等，但是唯独就是不包括中国。那当真是端起碗吃饭，放下碗骂娘。这下不仅三星手机炸了，连消费者的情绪也炸了。\n同样是智能手机，一个是傲慢令人生厌的三星，另外一个是质量和设计都不逊于三星的华为。市场已经向我们所有人证明了大家更愿意买哪个。华为的市场份额可以说是从三星手上抢过来的市场份额。所以在前面这么大一段，我才会给大家科普一下三星到底是怎么死的。\n销量上被华为彻底甩开的三星，随后就起诉了华为。三星给出的理由是华为用了三星的专利，但是呢这一次起诉却失败了。三星手机在专利权上的诉讼不止一次，但是最终呢都被华为给化解了。多次败在华为手中，三星就靠着这样者仍厌烦的操作一步一步把自己的市场全都作没了，而他作没的市场全都归到了华为的包包里面。\n但是这个时候华为还不值得高兴，因为在海的另一边是敌人。苹果手机在乔布斯去世之后仿佛失去了灵魂一样，创新方面已经远远不如从前。在2015年以后，华为和苹果公司就开始了业务层面的深度合作，也就是说，两个公司进行了一波专利的互换。华为向苹果公司发放的专利总共是769件，同时也得到了苹果公司98件专利的许可项目。苹果公司在随后的发展当中对华为的专利依赖程度越来越深，这也让华为可以从苹果那里每年拿到1亿美元的专利使用费，并且这一个举动让华为在美国市场的地位空前的提高了。\n很多美国人都开始使用华为的手机了，但是他们却是通过像伊贝雅之类的就是从海外买过来的。既然华为在美国的市场地位提高了，那么自然就可以进军美国市场。随后华为就开始进军美国市场售卖自己的产品。从进军美国市场到2015年的年末，华为在美国授权的专利已经超过了5,000件。华为在美国的生意越做越大，可以说是做得风生水起。当时就开始有一些言论说华为想要打败苹果。毕竟在专利攻势下整个苹果公司已经快被华为渗透烂了。但是任正非却反驳这个观点，他从来不去宣称自己要打败谁谁谁。在一次受采访的过程中，任正非表达了自己的态度，任正非说为什么我们要推翻他们呢？我们能称霸这个世界吗？想要称霸世界的只有两个人，一个是成吉思汗，另外一个是希特勒，他们都死死无葬身之地。这也是任正非一个非常可贵的品质，超越他人的目的不是为了打败他人，而是为了成为更好的自己。华为从不会做手机到走到业界的巅峰，他们的进步与发展只用了不到10年的时间，但是华为也不是一蹴而就的，他是用自己自身能拿得出手的实力一点一点挤进市场的。而现在华为终于走到了市场的中央。随着华为手机的崛起，一些媒体和自媒体他们也坐不住了，开始对华为进行过度的追捧。比如说啊，华为公司3年干掉苹果，5年干掉三星啊，什么举国沸腾，华为碾压高通，击溃欧美这样的新闻标题满天飞，严重影响了华为的形象。这也让任正非开始讨厌并且远离媒体，为了不让外界的杂音影响到华为的内部，也为了打消外界对华为狼性文化的反感，任正非在公司内部做了规定，他严令禁止大家说什么灭了三星，灭了苹果之类的话，无论是在公开场合还是私下场合一次都不能讲谁，要是讲了就罚100。\n4G 时代 # 时至今日啊，华为一路坎坷和发展，终于走到了现在，能代表中国品牌的这样一个地步了。正如任正非是从淤里面爬出来的人一样，华为也从逆境走向了高台。如果真的是固步自封的话，任正非绝不可能走到今天这一步。\n既然我们把四G时代已经讲完了，接下来我们就要讲一讲5G时代了。在10年前，一个土耳其教授发表了一篇数学论文。在两个月之后，华为工程师就跟进了。从这篇论文当中，提取出了一个关键的概念，并且将这个定成了5G的标准。而这个从论文当中提取出来的东西叫做波尔极化码。这个所谓的Polar极化码其实是一种5G数据传输的一种新型的编程方式。\n华为以这个论文为中兴，研究了很多的专利，一步一步的开始将这个东西拆开研究。十年的时间当中，华为让几千个工程师、全世界的几万专家科学家夜以继日的研究这个技术。虽然在2018年，Polar码还是一个不成熟的东西，但是它确实是人类目前已知的一种已经被证明能够达到极限的信道编码方式。\n我们今天知道，5G技术是被成功的开发出来了，但是早在十年前，谁也不敢说这个form码就一定能够研究出更好的通信方式。但是任正非他就是有这个胆子，他觉得5G一定会是未来。所以根据任正非自己所说的，华为将全世界26个研发中心、在职的数学家700多人、物理学家800多人、化学家120多人全部投入到了这个项目当中。\n华为当时还有一个战略研究院，这个研究院具体干什么事呢？就是拿着钱，找全世界著名大学的著名教授，塞钱给他们，让他们帮我们解决问题。用任正非自己的话说，就是那段时间华为撒钱如撒胡椒面。对于这些钱，任正非从来没有想过能回本这种说法，因为这种东西相当于是一次豪赌，赌赌赢了赚钱，赌输了倾家荡产，但任正非相信自己一定能够胜利，并且任正非对于这些科学家真的是非常的尊敬的，给足了人家面子。\n比如当时，任正非就曾经去日本找过一位大学教授，经过一番交涉之后，这个大学教授和他的4个博士生全都来到了华为上班工作，上班的地点就在华为的办公室。而且华为向这个博士许诺，他可以再招4个博士生，等同于是8个博士帮这个教授做研究，而研究出来的研究结果全部都归这个大学教授。华为出钱给你做那些惊为天人的研究，研究成果全归你自己，不属于华为，华为仅仅是需要这个东西的商用资格。\n大家想想是不是把面子给足了，就像是《天下无贼》那部电影里面葛优的那句台词一样，21世纪最重要的是什么？是人才。早就明白了这个道理的任正非，在对待人才方面可以说是投入一切，不求回报。\n人们常说，三流的企业卖产品，二流的企业卖品牌，一流的企业卖标准。谁制定了标准，谁就掌控了话语权，等同于是站在行业的最顶部。而华为提出的这个Polar极化码，其中的基本专利数量就占到了全世界的27%左右，排在第一位。也就是说，华为这一波十年研究，直接就掌握了5G的标准。这是一个非常大的突破，是中国通信史上第一次达到这样的高度。\n5G技术 # 但是还没有等华为施展拳脚，他就受到了很大的阻碍。似乎在任正非的人生道路上，每当他以为自己可以顺利的走完前路的时候，总会有巨大的困难会找上。当华为的5G技术正在风光无限的继续发展的时候，美国的某位董帝突然以保障国家信息安全为由开始全面的抵制华为，并且表示不会采用华为的5G技术和设备，还明令禁止美国本土的企业和华为合作。韩国、日本、印度这些国家纷纷追随美国老大哥的脚步，接二连三的举起抵制大旗。这让华为感到猝不及防，因为这个时候，华为的5G技术还没有成熟，连华为自己都不知道自己的5G有没有问题的时候，美国他就知道了，这完全是不合理的嘛。美国人或者是欧洲人为全世界制定标准可以，中国人为全世界制定标准不行。谁给我翻译翻译，这是他什么操作。\n这些国家抵制5G的原因真的是因为信息安全吗？当然不是的，那是因为5G技术领先的不是他们国家，不能打败那就抵是老套路了，不足为奇啊。这一套大清当年也用过。但是对于华为来说，他一直以来都是打算以欧美的部分国家为重要的战略核心的。这一张牌一打下来，直接就卡住了华为的脖子。美国总统下令抵制中国的一个民营品牌，听着的确是有点可笑，但是这也让我们看清了华为和任正非的实力。以一个品牌的实力让美国人急得抓耳挠腮，还宣布进入紧急状态。当一个品牌走到如此的高度，输赢还重要吗？\n虽然华为屡次遭拒，但是他们也没有气馁。在经历了四处碰壁之后，他们终于找到了同意与华为达成合作的意大利。我们不得不说啊，意大利这个国家真的很神奇，他们在历史上每一次战队永远都是战队了的，这就很离谱。好的我们不开玩笑啊，我们回到这个话题，这应该也算是华为拿下的首个5G。而这一次意大利选择与华为合作，无疑是为华为提供了一次证明自己实力的机会。对于屡次遭受挫折的华为来说，反复横跳的意大利，这一次算是花钱来跟自己雪中送炭了。\n2018年华为的寒冬还没有结束。自从华为5G被打压以来，华为在5G市场上的发展就变得异常低，低不仅不公布自己的5G订单数据，还低调的与运营商进行合作，这也让网上出现了很多批评的声音，说华为的5G就是一场空谈。不信你们看，现在的华为5G真的有在发展。截止到2020年，我们用数据来回答大家这个问题啊。当华为在国际市场上被排除在外之后的这两年当中，华为却获得了印度、尼西亚、突尼斯、马尔代夫等国家的5G大订单。目前为止，华为对外公布已经在全球范围内获得了91份5G商用合同订单。兄弟们，全球一共才197个国家，这里就有96份商用订单了，谁才是5G时代的新霸主就不用我跟大家多说了吧，大家懂的，都懂啊。\n也许对于我们大多数人来说，华为是我们可选可不选的智能手机品牌，然而在科技层面上来说，华为的确确是中国品牌当中的巨人。然而如今只是过了短短的两年，华为再次将惊喜送到了我们的面前。我给大家翻译翻译，什么叫做惊喜啊？2021年6月2号，鸿蒙操作系统发布会正式开幕了，这一次我们赶上了华为跨时代的进步和突破。我们中国终于有了自己的手机系统，而鸿蒙系统是为了应对美国的封杀才这么做的吗？这一切要从任正非最早进军手机行业开始说起。那个时候，目光一向敏锐的任正非发现了一个事情，那也就是啊，这个手机最核心的硬件芯片几乎都被国外企业给垄断了。想要做我们中国的手机也得从外国买芯，就像几十年前任正非发誓一定要做中国优质的交换机一样。在这一次，任正非同样想做中国自己的芯片，于是这一次一款叫做麒麟的高端芯片出现了，从无到有这中间经历的是华为数以亿计的投资以及众多的研发人员日以继日的工作。\n当然，我相信很多小伙伴们其实不明白啊，这个麒麟芯片和之前已经推出的骁龙芯片有什么区别。我们当然不用了解其中的复杂技术，我们只要知道骁龙是美国高通的产品，而麒麟是我们华为的芯片，一定要比骁龙好吗？这是不一定的，就像我们的鸿蒙系统一样，鸿蒙系统一定比iOS和安卓系统好吗？也说不一定。我们目前看来，鸿蒙系统还有很多不完善的地方，但是鸿蒙存在的意义，那就是我们国家终于有了属于我们的系统。在这之后，我们可能会有其他的企业推出其他的系统。鸿蒙诞生的意义是证明我们中国有这个能力，仅此而已。并且啊，这个华为的鸿蒙系统还能为我们带来什么好处呢？我们举个例子，苹果系的商品无论是手机还是手表以及电脑全部采用同样的系统，自己的产品互动性做的非常的好。对比来看，安卓系统除了系统的碎片化以外，不同厂商还有自己的规则，加上杂七杂八的第三方APP导致安卓的互通性很差，互通之间想要达到完美的契合程度就需要比苹果付出更多的人力和时间。鸿蒙系统真的是单纯的要把自己区别于安卓和iOS系统吗？任正非的野心就是这样吗？鸿蒙系统要做的必不可能仅此而已。\n问大家一个问题，假如，你要给自己家的冰箱安一个安卓系统，是不是听上去不太可能？毕竟冰箱又不是手机，它能做到调节温度就已经很不容易了。\n但是现在更高级点，冰箱能连蓝牙了，对吧？手机也能控制冰箱，那干嘛要装安卓系统呢？难道有人抱着冰箱下载b站刷视频吗？\n对的，你猜对了，真就是抱着冰箱刷b站。鸿蒙系统的内核就在于实现这件事儿，也就是所谓的物联网鸿蒙OS。\n在构架设计的那一天，就将系统拆成了很小的颗粒度，甚至可以根据软件进行拼装，甚至可以达到KB的级别。也就是说，一个门锁微波炉都能用鸿蒙系统。\n鸿蒙不像是iOS或者是安卓那样，像是一道成品的菜端到你面前，而是更像一套乐高积木。它可以拆成无数的碎片，想怎样去组合全靠你自己。\n未来我们的物联网系统不再需要去下载各种各样的APP，而是只需要用统一的鸿蒙系统就可以了。我们可以轻易的做到对每一个装了鸿蒙系统的小产品的控制。这就是鸿蒙系统真正想要做的。\n而任正非驱动着华为做这一切，都是为了迎接一个万物联网的时代。一个古稀之年的老人，他不仅没有落后于时代，反而还要带领着他的企业去迎接更崭新科技潮流。\n在那个时代到来的时候，我们必不可能忘记华为这个工程，还有任正非这位老人。华为走过的是一条充满争议的道路。如今的鸿蒙系统仍然在无数人的口诛笔伐当中挣扎。\n我们回望华为发展的这一路，我们不仅可以看到一个企业的崛起和中国科技力量的日新月异，我们同时也可以逐渐的看出未来即将到来一个科技的新时代。\n","date":"30 December 2023","permalink":"/read/%E5%8D%8E%E4%B8%BA/","section":"阅读","summary":"童年 # 1944年的10月25日，在新中国还没有成立的动荡年代，一个叫做任正非的孩子降生在了一个穷苦的贵州山区家庭。任正非的父亲叫做任木生。任木生年轻时因为家道中落，所以选择去了国民党412军工厂担任一位会计员，私下经常参与爱国抗日运动。但因同意共产党观点，遭国民党特务追捕，为逃避追捕，任木生逃往贵州山区。","title":"华为发展历程"},{"content":"","date":"24 December 2023","permalink":"/posts/reviews/%E7%A0%94%E7%A9%B6%E7%94%9F%E5%A4%8D%E4%B9%A0/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%8E%E7%9F%A5%E8%AF%86%E5%8F%91%E7%8E%B0/","section":"博客","summary":"数据仓库是集成、存储企业数据的系统，旨在支持决策制定。它整合来自不同源头的数据，提供一致、可信的信息。知识发现则是通过技术手段从大量数据中挖掘潜在信息。数据仓库为知识发现提供了丰富的数据源，帮助发现隐藏的模式和关系，为业务智能和预测性分析提供基础。这两者相辅相成，共同助力组织更深入地理解数据，做出更明智的决策。","title":"数据仓库与知识发现"},{"content":"总论 # 数据挖掘：是从大量数据中发现有趣模式，这些数据可以存放在数据库，数据仓库或其他信息存储中。\n概念/类描述：一种数据泛化形式，用汇总的、简洁的和精确的方法描述各个类和概念，通过\n数据特征化：目标类数据的一般特性或特征的汇总； 数据区分：将目标类数据的一般特性与一个或多个对比类进行比较； 数据特征化和比较来得到。 关联分析：发现关联规则，这些规则展示属性-值频繁地在给定数据集中一起出现的条件，通常要满足最小支持度阈值和最小置信度阈值。\n分类：找出能够描述和区分数据类或概念的模型，以便能够使用模型预测类标号未知的对象类，导出的模型是基于训练集的分析。导出模型的算法：决策树、神经网络、贝叶斯、（遗传、粗糙集、模糊集）。\n预测：建立连续值函数模型，预测空缺的或不知道的数值数据集。\n孤立点：与数据的一般行为或模型不一致的数据对象。\n聚类：分析数据对象，而不考虑已知的类标记。训练数据中不提供类标记，对象根据最大化类内的相似性和最小化类间的原则进行聚类或分组，从而产生类标号。\n第二章数据仓库 # 数据仓库：是一个面向主题的、集成的、时变的、非易失的数据集合，支持管理部门的决策过程。从一个或多个数据源收集信息，存放在一个一致的模式下。数据仓库通过数据清理、变换、继承、装入和定期刷新过程来构造。\n联机事务处理OLTP：主要任务是执行联机事务和查询处理。\n联系分析处理OLAP：数据仓库系统在数据分析和决策方面为用户或\u0026rsquo;知识工人\u0026rsquo;提供服务。这种系统可以用不同的格式和组织提供数据。\n最流行的数据仓库数据模型是多维数据模型，这种模型可以是星形模式、雪花模式、事实星座模式。\n上卷：上卷操作通过一个维的概念分层向上攀升或者通过维规约，在数据立方体上进行聚集。\n下钻：下钻是上卷的逆操作，它由不太详细的数据到更详细的数据。\n数据仓库模型的不同类型：\n企业仓库：收集了关于整个组织关于主题的所有信息，跨越整个组织，因此是企业范围的。 数据集市：是企业仓库的一个部门子集，它针对选定的主题，对于特定的用户是有用的，因此是部门范围的，其数据通常是汇总的。 虚拟仓库：虚拟仓库是操作数据库上视图的集合，易于建立，但需要操作数据库服务器具有剩余能力。 数据仓库的三层结构：\n仓库数据服务器：使用后端工具（抽取、清晰、转换、装载、刷新）和实用程序由操作数据库和其他外部数据源提取数据，进行数据清理和变换并放入仓库底层 OLAP 服务器：直接实现对多维数据的操作,直接为商务用户提供来自数据仓库或数据集市的多维数据。 前端客户层：包括查询和报表工具、分析工具或数据挖掘工具。 数据仓库的设计：\n分析建立企业模型并映射到数据仓库概念模型； 逻辑模型的设计 物理模型的设计 逻辑模型设计：\n系统数据量估算； 数据粒度的选择； 数据的分割（到各自的物理单元单独处理） 表的合理划分（字段的更新频率和访问频率不一样——稳定性） 删除纯操作数据（\u0026ldquo;收款人\u0026rdquo;），增加导出字段（\u0026ldquo;销售总量\u0026rdquo;） 元数据：描述数据的数据，数据仓库的结构、操作元数据（数据血统、流通）、用于汇总的算法、从操作环境到数据仓库的映射；关于系统性能的数据、商务元数据。\n记录系统：指明数据仓库中关系表的各个字段来源于业务数据库何处\n物理模型的设计：\n确定数据的存储结构（并行RAID） 索引策略（位图索引、连接索引） 位图索引：属性的每一个值 $v$ 都有一个位向量，长度为记录的总数，如果数据表中给定行上该属性的值为 $v$, 则在位图索引的对应行上标注该值的位为 $1$，其余为 $0$，不适用于基数很大的属性。 连接索引：传统的索引将给定列上的值映射到具有该值的行的列表上，连接索引登记来自关系数据库的两个关系的可连接行，对于维护来自可连接的关系的外码和与之匹配的主码的联系特别有用（事实表——维表）。 数据存储策略与性能优化（多路聚集优化、表的归并、分割表的存放、按列存储、存储分配优化） 数据装载接口 并行优化设计 物化：预计算方体。数据立方体计算中多路数组聚集，多路计算\nBUC：bottom-up computation\n数据立方体允许以多维数据建模和观察，它由维和事实定义。维是关于一个组织想要记录的透视或实体，事实是数值度量的。\n数据预处理 # 数据预处理：不完整的、含噪音的、不一致的\n数据清洗（缺失值、噪声、非一致） 、 数据集成（模式集成、发现冗余、数据值冲突检测和处理） 、 数据变换（光滑、聚集、泛化、规范化、属性构造） 、 数据规约（数据立方体聚集、属性子集选择、维度规约、数值规约、离散化和概念分层产生） 、 数据离散化（数值数据：分箱、直方图、聚类、基于熵的离散化、基于直观划分离散化 3-4-5 规则（区间的最高有效位的取值个数） ； 分类数据：用户或专家在模式级显示说明属性偏序、通过显示数据分组说明分层结构的一部分、说明属性集但不说明偏序（层次高，属性值个数越少）、只说明部分属性集（嵌入数据语义，是语义相关的属性集捆绑在一起） ） 。\n噪声：被测量的变量的随机误差或方差。\n噪音数据处理：分箱（按箱平均值平滑、按箱中值平滑、按箱边界平滑）、回归、聚类。\n规范化：\n最小-最大规范化； Z-score 规范化； 小数定标规范化 属性子集选择：删除不相关或冗余的属性，找到最小属性集，使数据类的概率分布尽可能地接近使用所有属性得到的原分布。\n决策树归纳：用于从一组数据中归纳或学习出一个决策树模型。这种方法主要用于分类和回归问题。\n维规约：使用编码机制减小数据集的规模,如压缩。\n数值规约：用替代的、较小的数据表示替换或估计数据，如参数模型 or 非参方法（聚类、抽样、直方图（Equi-depth、equi-width、v-optimal、maxdiff）\n概念分层：对一个属性递归地进行离散化，产生属性值的分层或多分辨率划分。\n离散化：用少数区间标记替换连续属性的数值，从而减少和简化原来的数据。\n特征化和区分 # 描述性数据挖掘：以简洁概要的方式描述概念或数据集，并提供数据的有趣的一般性质。\n预测性数据挖掘：分析数据，建立一个或一组模型，并试图预测新数据集的行为或趋势。\nOLAP（基于数据立方体的多维数组） VS 概念描述：处理类型、自动化方面比较各自优缺点。\nOLAP（联机分析处理）： 处理类型：ROLAP（关系型联机分析处理），MOLAP（多维联机分析处理），HOLAP（混合联机分析处理） 自动化方面：提供高性能查询和交互式分析，但是复杂性和存储开销较大。 概念描述 处理类型：概念建模，将现实世界的概念和关系转化为模型，通常涉及使用概念图或实体-关系图进行描述。 自动化方面：提供清晰的概念表达和抽象层次，但是不直接用于实际数据存储和查询。 数据泛化：\n数据泛化是一种隐私保护的技术手段，旨在通过对敏感信息进行模糊或扰动，以便在保持数据可用性的同时，减少对个人隐私的潜在威胁。在数据泛化中，原始数据中的具体值被替换为更一般或抽象的值，从而在一定程度上隐藏了数据的真实含义。 途径： 数据立方体（OLAP 途径） 面向属性的归纳 面向属性的归纳：\n使用数据库查询收集任务相关的数据 考察相关任务集中的各个属性并进行泛化：通过属性删除（两种情况）或者属性泛化 通过合并相等的广义元组（每个广义元组代表一个规则析取）并累计对应的计数值进行聚集；表现形式：广义关系（表）、交叉表、图、量化特征规则。 属性泛化控制：属性泛化阈值控制；广义关系阈值控制；P-131\n特征化 VS OLAP：\n相同点：在不同层次汇总；迭代的上卷、下钻、旋转、切片/块 不同点：特征化：自动产生层次的分配；多个相关维时进行维的相关分析和排序；维度量的类型可以很复杂 量化规则：带有量化信息的逻辑规则\n解析特征化：\n收集任务相关数据 根据属性分析阈值分析泛化（对目标类和对比类的候选关系） ：属性删除、属性泛化、候选关系 属性的相关性分析（信息增益） （去除不/弱相关，对比类的候选关系）形成目标类的初始工作关系 在初始工作关系上根据属性泛化阈值使用面向属性的归纳 类对比：\n通过查询处理收集数据库中的相关数据集，并分别划分成目标类和一个或多个对比类。 维相关分析（仅选择高度相关的维进一步分析，相关度量/熵的度量） 同步泛化 通过对目标类和对比类使用下钻、上卷和其他 OLAP 操作调整比较描述的抽象层次。 导出比较的表示 关联规则挖掘 # 关联规则挖掘：从操作型数据库、关联数据库或者其他信息库中的项集、对象中，发现频繁模式、关联、相关或者因果结构\n两步过程：\n找出所有的频繁项集 有频繁项集产生强关联规则 Apriori 算法：使用候选产生发现频繁项集（1）连接步（2）剪枝步（子集测试）使用逐层搜索的迭代方法，k 项集用于探索（k+1）项集。首先，通过扫描数据库，累计每个项的计数，并收集满足最小支持度的项，找出频繁 1 项集的集合。该集合记做 L1。然后，L1 用于找到频繁 2 项集的集合 L2，L2 用于找L3，如此下去，直到不能再找到频繁 k 项集。找每个 Lk 需要一次数据库全扫描。\nApriori 性质：频繁项集的所有非空子集也必须是频繁\nApriori 核心：用 k 项集生成k+1 项集；使用数据库扫描和模式匹配收集候选项集计数\nApriori 瓶颈：候选项集计算量大尤其是 1 频繁项集自交叉生成 2 候选项集时；数据库多次扫描，每次抽取都要扫描\n由Apriori 产生频繁项集产生关联规则： 由频繁项集直接产生强关联规则 $s \\rightarrow (l-s)$，$s$ 为 $l$ 的非空子集\n提高Apriori 算法的效率：\n基于散列的技术：一种基于散列的技术可以用于压缩候选 k 项集 Ck（eg：在 C1 中产生 L1 的过程中，可对每个事务产生所有的 2 项集，并将它们散列到散列表结构的不同桶中，并增加对应的桶计数，计数低于最小支持桶中的 2 项集应从2 候选项集中删除） 事务压缩：不包含任何K 频繁项集的事务不可能产生\u0026gt;K 的FI 应在后继的扫描中删除 划分：任何频繁项集必须作为局部频繁项集至少出现在一个划分中。 抽样：在样本上降低阈值 动态项集计数：只有子项集都频繁才将其加入候选项集 FP 树：发现频繁项集而不产生候选；分治策略；首先将提供频繁项的数据库压缩到一棵 FP 树上，仍然保留项集相关信息。然后将压缩后的数据库划分为一组条件数据库，每个关联一个频繁项或模式段，并分别挖掘每个条件数据库。\nFP 挖掘过程：由每个长度为 1 的频繁模式（初始后缀模式）开始，构造它的条件模式基（一个\u0026quot;子数据库\u0026quot;，由 FP 树中与后缀模式一起出现的前缀路径集组成），然后构造它的（条件）FP 树，并递归地对该树进行挖掘直到 FP 树为空或者只有单个频繁模式路径。模式增长通过后缀模式与条件 FP 树产生的频繁模式连接实现（abcdef 频繁当且仅当 abcde 频繁，且 f 在包含 abcde 的事务中也是频繁的，\u0026ldquo;cm\u0026quot;例子） 。\nFP 核心：利用FP 树递归地增长频繁模式路径（分治）\nFP 优点：去除了不相关的信息；出去节点连接和计数规模比原数据库小；快速；将发现长频繁模式的问题转换成递归地搜索一些较短的模式。\nFP 性能优于Apriori 的原因：\n没有候选的产生 采用紧凑的数据结构 消除了对数据库的重复扫描 基本的操作既是对 FP 的构建和计数 提升度（lift）：\n与1 比较（相关分析）\n多层关联规则挖掘\n多层关联规则：在多个抽象层上挖掘数据产生的关联规则。 一致支持度、递减支持度、基于分组的支持度 递减支持度策略 分层独立策略：检查所有的节点而不考虑其父节点是否频繁 K 项集交叉层次过滤： 分类和预测 # 分类：找出描述并区分数据类或概念的模型，以便能够使用模型预测未知对象类的类标记，模型的构建依赖于训练集和分类属性的类标号的使用。\n预测：建立连续值函数模型，预测某些空缺的或不知道的数据值而不是类标记。\n从数据分析的角度来看\n监督学习（分类）：提供了每个训练元组的类标号，未知元组通过由训练元组构造的模型来定性类标号的预测\n非监督学习（聚类）：每个训练元组的类标号是未知的，并且要学习的类的个数或集合也可能事先不知道，力求寻找类或聚类的存在。\n测试集来评估模型的正确性\n决策树：P-188，构造；剪枝（反映噪声和离群点）；不能预测隐藏在训练元组中的不可见样本\nBasic algorithm (a greedy algorithm)自顶向下、递归、分治的贪心策略：\nTree is constructed in a top-down recursive divide-and-conquer manner At start, all the training examples are at the root Attributes are categorical (if continuous-valued, they are discretized in advance) Examples are partitioned recursively based on selected attributes Test attributes are selected on the basis of a heuristic or statistical measure (e.g.,information gain) 结束条件：\n所有的样本都属于同一个类 没有剩余的样本可用 没有剩余的属性（投票 避免过度拟合：前剪枝（在构造过程中，预定义阈值） VS 后剪枝（构造完成，用其子树中最频繁的类标记）：\n贝叶斯：概率学习、增量、概率预测、标准，可以解决不可见样本问题 sample X ，class label C 寻找使 P(C|X)最大的X\n朴素假设：类条件独立 $P(x_1,…,x_k|C) = P(x_1|C)·…·P(x_k|C)$，当出现新的独立类时可在原基础上直接计算，即增量\n神经网络：一组连接的输入输出单元，每个连接都有一个权重与之相关联，在学习阶段通过调整这些权重能够预测输入元组的正确类标号。\n后向传播(图)：初始化权重——向前传播输入——向后传播误差——调整权值——终止条件终止：超过预先指定的周期数；前一周期的权值调整小于预定值/误分的百分比小于预定值。\nStep1: feed forward input from input layer to hidden layer to output layer Step2: compute the error of the output layer Step3: back propagate the error from output layer to hidden layer Step4: adjust weights and biases. Step5: if termination criterion is satisfied , stop. Otherwise go to step1. 后向传播： 通过迭代地处理一组训练样本，将每个样本的网络预测与实际知道的类标号比较，进行学习。对于每个训练样本，修改权，使得网络预测和实际类之间的均方误差最小，这种修改\u0026quot;后向\u0026quot;进行。\n向前传播输入：计算隐藏层和输出层每个单元的净输入和输出。\n向后传播误差：通过更新权和偏置以反映网络预测的误差，向后传播误差。\n急切学习法：在接收待分类的新元组之前构造分类模型。\n懒惰学习法：给定训练元组时，只是简单存储，并一直等到待检验元组出现才进行泛化，以便根据存储的训练元组的相似性对元组进行分类。\nK-近邻 找到最接近未知元组的 K 个训练元组 基于案例推理 粗糙集：基于等价类的建立，给定类的粗糙集定义用两个集合近似：上近似，不能认为不属于C 的集合；下近似：必定属于 C 的集合。分类精度高，处理离散属性\n模糊集：对每个类定义\u0026quot;模糊\u0026quot;的阈值和边界，模糊逻辑 0-0.1 之间的真值表示一个特定的值是一个给定类成员的隶属程度，而不是用精确的截断，每个类表示一个模糊集。分类正确性的验证：划分（独立的训练集和测试集，大规模） ；交叉验证（K 个子样本集，中等规模）\n分类和预测的组装方法：\n装袋：有训练集有放回随即抽样产生 N 个训练子集，导出 N 个模型，对未知数据，给出对应的N 个结果，分类-多数表决；预测-均值 提升：有训练集有放回随即抽样产生 N 个训练子集，导出 N 个模型，每个训练元组都赋予一个权重，对每个训练元组从 1-N 模型迭代地进行，重整每个元组的权重，使得在下一轮更关注上一轮误分的元组，并计算每个模型的投票权重，分类返回具有最大权重的类 聚类挖掘 # 聚类：要划分的类是未知的，将数据对象分组成为多个类或簇，在同一个簇中的对象之间具有较高的相似度，而不同簇中的对象差别较大。\n划分方法：\nK-均值：以K 为输入参数，将对象分为 K 个簇，是簇内~，簇外~\n随即选择 K 个对象作为K 个簇的中心 选择离 K 最近的点形成簇 更具簇中的点计算新的均值，这个均值可以看做簇的中心OR 质心 以新的中心更新簇，从步骤 2 开始重复直到簇不在变化 优点：相对可伸缩，有效率；往往终止局部最优解； 缺点：用户给出 K；对分类属性的数据均值无定义；对噪声和离群点敏感，不适合凹形 K-中心点算法：簇的中心必须落在某个是在的点上，对噪声不敏感\n层次方法：凝聚的；分裂的；优点：在运行中可随时停止，不要K 参数；缺点：不可回溯\n基于密度的方法：\n优点： 发现任意形状的簇； 处理噪声； 一次扫描； 需要密度参数作为终止条件； DBSCAN：密度可达和密度相连\n邻域 核心对象 直接密度可达： 密度可达：对象链 Pi+1 是从Pi 关于 E 和 MINPTS 直接密度可达的，则对象 P1 是从 Pn 关于~密度可达的。 密度相连：p,q 都是从o 关于~密度可达的，则 p 到q 是关于~密度相连的。基于密度的簇是基于密度可达性的最大密度相连对象的集合，不包含在簇中的认为是噪声（MINPTS 的限制不可能成为核心对象）。 离群点：与数据的一般行为或模型不一致。\n基于统计分布：例如正态分布的3σ 以外的区域 基于距离： 阈值1：D; 阈值2：水平eg:95% 到其他点的距离有大于95%的大于D，则认为是离群点 基于偏差：首先假设一个模型，根据假设模型检查筛选（发现驱动的探查） ","date":"24 December 2023","permalink":"/posts/reviews/%E7%A0%94%E7%A9%B6%E7%94%9F%E5%A4%8D%E4%B9%A0/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%8E%E7%9F%A5%E8%AF%86%E5%8F%91%E7%8E%B0/concept/","section":"博客","summary":"《数据仓库与知识发现》名词解释部分复习","title":"数据仓库与知识发现名词解释"},{"content":"Apriori 是一种用于关联规则挖掘的经典算法。关联规则挖掘是数据挖掘领域的一项重要任务，其目标是从大规模数据集中发现项集之间的关联关系。Apriori 算法是由Rakesh Agrawal 和 Ramakrishnan Srikant 在1994年提出的，它主要用于发现数据集中频繁出现的项集。\n关联规则的强度度量：支持度和置信度\n","date":"24 December 2023","permalink":"/posts/reviews/%E7%A0%94%E7%A9%B6%E7%94%9F%E5%A4%8D%E4%B9%A0/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%8E%E7%9F%A5%E8%AF%86%E5%8F%91%E7%8E%B0/apriori/","section":"博客","summary":"Apriori 是一种用于关联规则挖掘的经典算法。关联规则挖掘是数据挖掘领域的一项重要任务，其目标是从大规模数据集中发现项集之间的关联关系。Apriori 算法是由Rakesh Agrawal 和 Ramakrishnan Srikant 在1994年提出的，它主要用于发现数据集中频繁出现的项集。","title":"Apriori"},{"content":"第一章（12） # 1.什么是零日(0day)漏洞？什么是零日(0day)攻击？\n零日漏洞是指未被公开披露的软件漏洞，没有给软件的作者或厂商以时间去为漏洞打补丁或是给出解决方案建议，从而使攻击者能够利用这种漏洞破坏计算机程序、数据及设备。注意，零日漏洞并不是指软件发布后被立刻发现的漏洞。\n利用零日漏洞开发攻击工具进行的攻击称为零日攻击。零日攻击所针对的漏洞由于软件厂商还没有发现或是还未提供相应的补丁，所以零日攻击的成功率高，造成的破坏大。\n2.为什么说面对当前的全球网络空间安全威胁，必须对软件安全给予强烈关注？\n当前，软件已融入人们日常生活的方方面面，已经成为国家和社会关键基础设施的重要组成部分，因此，软件的安全关乎信息系统的安全，关乎关键基础设施的安全，关乎个人安全乃至社会和国家的安全。\n软件已经渗透到社会、经济与国防建设的方方面面，是信息时代所依赖的重要技术与手段，其安全直接关系到国计民生与国家安全，因此，软件安全关乎国家竞争力。\n3.当前，黑客为了能够有效达到窃取数据、破坏系统的目的，常常通过挖掘或是购买零日漏洞，开发针对零日漏洞的攻击工具，零日漏洞威胁实际上反映了软件系统存在的一个什么问题？\n软件漏洞普遍存在，零日漏洞成为主要安全威胁。\n现在大多数的网络攻击利用了软件（尤其是应用软件）的漏洞。根据统计分析，绝大多数成功的攻击都是针对和利用已知的、未打补丁的软件漏洞和不安全的软件配置，而这些软件安全问题都是在软件设计和开发过程中产生的。\n4.根据本书的介绍，软件安全威胁可以分为哪几类？\n软件自身的安全（软件漏洞）、恶意代码及软件侵权\n5.试谈谈对软件漏洞的认识，举出软件漏洞造成危害的事件例子。\n软件漏洞通常被认为是软件生命周期中与安全相关的设计错误、编码缺陷及运行故障等。\n一方面，软件漏洞可能会造成软件在运行过程中出现错误结果或运行不稳定、崩遗等现象，甚至引起死机等情况，举例如下：\n操作系统启动时发现未能驱动的硬件而导致蓝屏。 应用软件由于存在内存泄露，运行时系统内存消耗越来越大，直至最后崩溃。 网络软件由于对用户并发数考虑不周，导致用户数量超出预计，程序运行错误。 多线程软件对线程同步考虑不周，导致系统因资源死锁而死机。 另一方面，软件漏洞会被黑客发现和利用，进而实施窃取隐私信息、甚至破坏系统等攻击行为，举例如下：\n软件使用明文存储用户口令，黑客通过数据库漏洞直接获取口令明文。 软件存在缓冲区溢出漏洞，黑客利用溢出攻击而获得远程用户权限。 软件对用户登录的安全验证强度太低，黑客假冒合法用户登录。 软件对用户的输入没有严限制，被黑客利用后执行系统删除命令，从而导致系统被破坏。 6.什么是恶意代码？除了传统的计算机病毒，还有哪些恶意代码类型？\n恶意代码（Malicious Sofiware， Malware）是在未被授权的情况下，以破坏软硬件设备、窃取用户信息、干扰用户正常使用、扰乱用户心理为目的而编制的软件或代码片段。\n恶意代码包括计算机病毒（Computer Vinus）、蠕虫（Worm）、特洛伊木马（TrojanHorse）、后门 （Back Door）、内核套件（Rootkit）、间谍软件（Spyware）、恶意广告（Dishonest Adware）、流软件（Crimeware）、逻辑炸弹（Logic Bomb）、僵尸网络（Botnet）、网络钓鱼（Phishing）、恶意脚本（Malice Script）及垃圾信息（Spam）等恶意的或令人讨厌的软件及代码片段。近几年危害甚广的勒索软件（Ransomware）也屈于恶意代码范畴。\n7.针对软件的版权，有哪些侵权行为？\n未经软件著作权人许可，发表、登记、修改或翻译其软件。 将他人软件作为自己的软件发表或者登记，在他人软件上署名或者更改他人软件上的署名。 未经合作者许可，将与他人合作开发的软件作为自己单独完成的软件发表或者登记。 复制或者部分复制著作权人的软件。 向公众发行、出租或通过信息网络传播著作权人的软件。 故意避开或者破坏著作权人为保护其软件著作权而采取的技术措施。 故意删除或者改变软件权利管理电子信息。 转让或者许可他人行使著作权人的软件著作权。 本题类似14章第2、3题，在其中选几条记住即可\n8.谈谈对软件安全概念的理解。\n（软件安全是）软件工程与软件保障的一个方面，它提供一种系统的方法来标识、分析和追踪对危害以及具有危害性的功能（例如数据和命令）的软件缓解措施与控制。\n软件的安全性是软件产品在指定使用环境下达到对人类、业务、软件、财产或环境造成损害的可接受的风险级别的能力。这里的风险常常由软件内部和外部质量组成中的功能性、可靠性、易用性或维护性中的缺陷所致。\n9.简述软件和软件工程的概念。\n计算机程序、规则和可能相关的文档。 软件工程是指，采用工程的概念、原理、技术和方法来开发和维护软件，把经过时间考验而证明正确的管理技术和当前能够得到的最好的技术方法结合起来，从而经济地开发出高质量的软件并有效地进行维护。概括地说，软件工程是指导计算机软件开发和维护的一门工程学科，是技术与管理紧密结合形成的工程学科。 10.对照一般软件工程的概念，软件安全工程主要增添了哪些任务？\n一般软件工程中的软件生命周期包括需求分析、可行性分析、总体描述、系统设计、编码、调试和测试、验收与运行、维护升级、废弃等多个阶段，每个阶段都要定义、审查并形成文档以供交流或备查。 软件安全工程增添了软件安全开发、软件安全检测及软件版权保护等任务，关注的是如何运用系统安全工程的思想，以软件的安全性为核心，将安全要素嵌入软件开发生命周期的全过程，软件安全开发方法抛弃了传统的先构建系统，再将安全手段应用于系统的构建模式，而是保留了采用风险管理、身份认证、访问控制、数据加密保护和入侵检测等传统安全方法，将安全作为功能需求的必要组成部分，在系统开发的需求阶段就引入安全要素，同时对软件开发全过程的每一个阶段实施风险管理。 根据软件开发生命周期的阶段划分，软件安全开发涉及以下几个方面的内容：\n软件安全需求分析 软件安全设计 软件安全编码 软件安全测试 软件安全部署 11.谈谈软件安全与软件危机、软件质量和软件质量保证、软件保障、软件可靠性、应用软件系统安全、可信软件和软件定义安全等概念的区别和联系\n软件危机，也称为软件萧条或软件困扰，是指在计算机软件的开发和维护过程中所遇到的一系列严重问题。软件存在安全漏洞、恶意软件泛滥及软件版权保护等安全问题还只是软件危机的冰山一角。\n概括地说，软件质量就是软件与明确的和隐含的定义的需求相一致的程度。具体地说，软件质量是软件符合明确叙述的功能和性能需求、文档中明确描述的开发标准，以及所有专业开发的软件都应具有的和隐含特征相一致的程度。使用质量的属性分类为4个特性：有效性、生产率、安全性和满意度。由此可见，安全性是软件质量的一个重要属性。\n软件质量保证是建立一套有计划、有系统的方法，向管理层保证拟定出的标准、步骤、实践和方法能够正确地被所有项目所采用。 通常软件保障包括软件质量、软件安全性、软件可靠性、软件验证与确认，以及独立验证与确认等学科领域。软件保障，也有译为软件确保，是用于提高软件质量的实践、技术和工具。软件保障目前包括4个核心服务，即软件的安全性、保险性、可靠性和生存性。\n软件可靠性定义如下：\n在规定条件下，在规定的时间内软件不引起系统失效的概率。该概率是系统输入和系统使用的函数，也是软件中存在的缺陷的函数。系统输入将确定是否会遇到已存在的缺陷(如果缺陷存在的话)。 在规定的时间周期内所述条件下程序执行所要求的功能的能力。 由上述定义可知，软件可靠性不但与软件存在的缺陷和/或差错有关，而且与系统输入和系统使用有关。提高软件可靠性就是要减少软件中的缺陷或错误，提高软件系统的健壮性。因此，软件可靠性通常涉及软件安全性的要求，但是软件可靠性要求不能完全取代软件安全性的要求。 应用软件系统位于信息系统的上层，是在信息系统的硬件系统、操作系统、网络系统和数据库管理系统的支持下运行的，是构成信息系统的最重要部分，是信息系统中直接为用户提供服务的部分。\n可信性是在正确性、可靠性、安全性、时效性、完整性、可用性、可预测性、生存性及可控性等众多概念的基础上发展起来的一个新概念，是客观对象的诸多属性在人们心目中的一个综合反映。\nSDS（软件定义安全）是适应SDN复杂网络的安全防护新思想，基本原理是将物理及虚拟的网络安全设备预期接入模式、部署方式和实现功能进行解耦，底层抽象为安全资源池里的资源，顶层统一通过软件编程的方式进行智能化、自动化的业务编排和管理，以完成相应的安全功能，从而实现一种灵活的安全防护。SDS可以分解为软件定义流量、软件定义资源和软件定义威胁模型，三个举措环环相扣，形成一个动态、闭环的工作模型。\n确保软件安全的基本思路是什么？软件安全涉及的技术主要有哪些方面？\n①软件安全开发关注的是如何运用系统安全工程的思想，以软件的安全性为核心，将安全要素嵌入软件开发生命周期的全过程，有效减少软件产品潜在的漏洞数量或控制在一个风险可接受的水平内，提高软件系统的整体安全性。\n②软件安全开发方法抛弃了传统的先构建系统，再将安全手段应用于系统的构建模式，而是保留了采用风险管理、身份认证、访问控制、数据加密保护和入侵检测等传统安全方法，将安全作为功能需求的必要组成部分，在系统开发的需求阶段就引入安全要素，同时对软件开发全过程的每一个阶段实施风险管理，以期减少每一个开发步骤中可能出现的安全问题，最终提高软件产品的本质安全性。\n第二章（7） # 1.试述软件漏洞的概念，谈谈软件漏洞与软件错误、软件缺陷、软件Bug的区别与联系\n软件错误（Eror）是指在软件开发过程中出现的不符合期望或不可接受的人为差错，其结果将可能导致软件敏陷的产生。在软件开发过程中，人是主体，难免会犯错误。软件错误主要是一种人为错误，相对于软件本身而言，是一种外部行为。\n软件缺陷（Bug/Defect）是指由于人为差错或其他客观原因，导致软件隐含能导致其在运行过程中出现不希望或不可接受的偏差，例如软件需求定义，以及设计、实现等错误。在这种意义下，软件缺陷和较件错误有着相近的含义。当软件运行于某一特定的环境条件时出现故障，这时称软件缺陷被激活。软件缺陷存在于软件内部，是一种静态形式\n软件漏洞通常被认为是软件生命周期中与安全相关的设计错误、编码缺陷及运行故障等。漏洞是贯穿软件生命周期各环节的。\n软件错误：在软件开发过程中 人为差错 可能会导致软件缺陷的产生。人为错误 外部行为\n软件缺陷：由于人为差错和各种其他客观原因，导致软件隐含（存在）在运行过程中出现…的偏差。存在于软件内部 静态形式\n软件Bug=软件缺陷\n2.为什么说安全缺陷或者说Bug是一个需要考虑具体环境、具体对象的概念？\n需要说明的是，安全缺陷或者说 Bug 是一个需要考虑具体环境、具体对象的概念。举例来说，一般的 Web 应用程序没有使用 HTTPS 协议（超文本传输安全协议）来加密传输的状态并不能算作是 Bug，而对于网上银行或电子商务等应用，不采用HTTPS协议进行加密传输就应当算作一个 Bug。如同使用 HTTPS 来对传输内容进行加密那样，积极主动地加强安全性的措施，也就是增加安全性功能，可以尽可能地消除Bug。安全性功能实际为软件系统的一种需求，所以也被称为安全性需求。是否将安全性功能加入到项目需求中，还需要根据项目的具体情况考虑，如项目经费等。\n3.试分析软件漏洞的成因。\n计算机系统结构决定了漏洞的必然性 软件趋向大型化，第三方扩展增多 新技术、新应用产生之初即缺乏安全性考虑 软件使用场景更具威胁 对软件安全开发重视不够，软件开发者缺乏安全知识 4.软件漏洞如何分类分级管理？\n1）软件漏洞分类\n通常可以从漏洞利用的成因、利用的位置、和对系统造成的直接威胁进行分类。\n基于漏洞成因的分类：内存破坏类、逻辑错误类、输入验证类、设计错误类和配置错误类。 基于漏洞利用位置的分类 本地漏洞。即需要操作系统级的有效帐号登录到本地才能利用的漏洞，主要构成为权限提升类漏洞，即把自身的执行权限从普通用户级别提升到管理员级别。 远程漏洞。即无需系统级的帐号验证即可通过网络访问目标进行利用的漏洞。 基于威胁类型的分类 获取控制。即可以导致劫持程序执行流程，转向执行攻击者指定的任意指令或命令，控制应用系统或操作系统。威胁最大，同时影响系统的机密性、完整性，甚至在需要的时候可以影响可用性。主要来源：内存破坏类。 获取信息。即可以导致劫持程序访问预期外的资源并泄露给攻击者，影响系统的机密性。主要来源：输入验证类、配置错误类漏洞。 拒绝服务。即可以导致目标应用或系统暂时或永远性地失去响应正常服务的能力，影响系统的可用性。主要来源：内存破坏类、意外处理错误类漏洞。 2）软件漏洞分级\n对漏洞进行分级有助于人们对数目众多的安全漏洞给予不同程度的关注并采取不同级别的措施。\n按照漏洞严重等级进行分级\n利用通用漏洞评分系统（CVSS）进行分级，使用三种度量评价标准对一个已知的安全漏洞危害程度进行打分。\n基本度量：用于描述漏洞的固有基本特性，这些特性不随时间和用户环境的变化而改变。 时间度量：用于描述漏洞随时间而改变的特性，这些特性不随用户环境的变化而改变。 环境度量：用于描述漏洞与特殊用户环境相关的特性。 6.软件漏洞买卖合法吗？软件漏洞应当如何管控？\n不合法\n不管是讲道义还是讲法律，对漏洞的有效管控已经是势在必行。漏洞的发现和报告机制、潺洞的交易、漏洞的利用都应该有着法律的界限，相应的管理也带要与时俱进。\n国外政府高度重视对漏洞资源的管控，通过建立完善的国象漏洞管理体系，将漏洞资源纳人国家管控机制。我国政府也高度重视对信息安全漏洞的管控，通过政策法规和专业机构，形成了一套管控体系。\n为了应对日益增加的漏洞，增加自身产品和服务的安全性，许多厂商纷纷成立安全应急响应部门（SRC），向社会收录旗下相关产品及业务的安全漏洞和威胁信息，并在第一时间进行处置，及时消除安全隐患。各厂商应急响应部门的迅速建立和发展，打通了厂商与\u0026quot;白帽\u0026quot;之间的正规渠道，相应的奖励也使得更多的\u0026quot;白帽\u0026quot;关注并协助厂商发现漏洞与风险，很大程度上提高了厂商的信息安全程度。\n7.厂商发布漏洞信息的标准过程是怎样的？\n根据安全漏洞生命周期中漏洞所处的发现、利用、修复和公开4个阶段，该标准将漏洞管理行为分为预防、收集、消减和发布等实施活动。\n在漏洞预防阶段，厂商应采取相应手段来提高产品安全水平；用户应对使用的计算机系统进行安全加固、安装安全防护产品和开启相应的安全配置。 在漏洞收集阶段，漏洞管理组织与漏洞管理中涉及的各方进行沟通与协调，广泛收集并及时处置漏洞；厂商应提供接收漏洞信息的渠道，确认所提交漏洞的真实存在性，并回复报告方。 在漏洞消减阶段，厂商依据消减处理策略在规定时间内修复漏洞，依据漏洞类型和危害程度，优先开发高危漏洞的修复措施。同时，厂商应保证补丁的有效性和安全性，并进行兼容性测试；用户应及时跟踪公布的漏洞信息和相关厂商的安全公告，进行及时修复。 在漏洞发布阶段，漏洞管理组织应在规定时间内发布漏洞及修复措施等信息（參见《信息安全技术 安全漏洞标识与描述规范（CB/T 28458—2012）》）；厂商应建立发布渠道，发布漏洞信息及修复措施，并迪知用户。 第三章（6） # 1.程序运行时的内存布局是怎样的？\n2.在程序运行时，用来动态申请分配数据和对象的内存区域形式称为什么？\n堆区\n3.什么是缓冲区溢出漏洞？\n缓冲区溢出漏洞就是在向缓冲区写入数据时，由于没有做边界检查，导致写入缓冲区的数据超过预先分配的边界，从而使溢出数据覆盖在合法数据上而引起系统异常的一种现象。\n4.简述Windows安全漏洞保护的基本技术及其存在的问题。\n（1）栈溢出检测选项/GS\n调用函数时将一个随机生成的秘密值（安全Cookie）存放在栈上，当函数返回时，检查这个堆栈检测仪的值是否被修改，以此判断是否发生了栈溢出。\n对抗/GS保护：围绕Cookie值展开的。猜测Cookie值、通过同时替换栈中的Cookie和Cookie副本、覆盖SEH绕过Cookie检查、覆盖父函数的栈数据绕过Cookie检查。\n（2）数据执行保护DEP（Data Execution Prevention）\n通过使可写内存不可执行或使可执行内存不可写来消除类似的威胁。\n对抗方法：\n利用ret-to-libc执行命令或进行API调用，如调用WinExec实现执行程序。 将包含Shellcode的内存页面标记为可执行，然后再跳过去执行。 通过分配可执行内存，再将Shellcode复制到内存区域，然后跳过去执行。 先尝试关闭当前进程的DEP保护，然后再运行Shellcode。 （3）地址空间布局随机化ASLR\n通过对堆、栈、共享库映射等线性区域布局的随机化，增加攻击者预测目的地址的难度，防止攻击者直接定位攻击代码位置，达到阻止漏洞利用的目的。\nASLR机制的缺陷和绕过方法：\n对本地攻击者无能为力 造成内存碎片的增多` 利用没有采用/DYNAMICBASE选项保护的模块做跳板 （4）安全结构化异常处理SafeSEH\nSafeSEH保护机制的作用是防止覆盖和使用存储栈上的SEH结构。\n其实现原理是，编译器在链接生成二进制IMAGE时，把所有合法的异常处理函数的地址解析出来制成一张安全的SEH表，保存在程序的IMAGE数据块里面，当程序调用异常处理函数时会将函数地址与安全SEH表中的地址进行匹配，检查调用的异常处理函数是否位于该表中。\n对抗SafeSEH机制的方法：\n利用未启用SafeSEH的模块作为跳板进行绕过；\n利用加载模块之外的地址进行绕过。\n（5）增强缓解体验工具包EMET\nEMET的基本保护功能介绍如下。\n增强型DEP；SafeSEH的升级版——SEHOP；强制性ASLR；HeapSpray防护。\n5.本章介绍了Windows的5种典型保护机制，但是每一种保护机制仍然面临着缺陷和许多对抗的方法，这说明了什么问题？应当如何应对这一问题？\n这些问题表明，安全是一个动态的过程，而不是一次性的解决方案。应对这些问题的方法包括：\n定期更新系统： 及时应用操作系统和应用程序的安全更新，以修补已知漏洞。 使用最新的安全技术： 持续关注和采用最新的安全技术，以提高系统的抵御能力。 网络安全意识培训： 提高用户和管理员的网络安全意识，以减少社会工程学攻击的成功率。 监控和响应： 实施有效的监控和响应机制，及时检测异常活动并采取措施应对。 多层次的安全防御： 组合使用多个安全解决方案，如防火墙、入侵检测系统、终端保护软件等，以提高整体安全性。 6.可以将内存访问错误大致分成以下几类：数组越界读或写、访问未初始化内存、访问已经释放的内存和重复释放内存或释放非法内存。下面的代码集中显示了上述问题的典型例子。这个包含许多错误的程序可以编译连接，而且可以在很多平台上运行。但是这些错误就像定时炸弹，会在特殊配置下触发，造成不可预见的错误。这就是内存错误难以发现的一个主要原因。试分析以下代码中存在的安全问题\n#include \u0026lt;iostream\u0026gt; using namescpace std; int main() { char* str1 = \u0026#34;four\u0026#34;; char* str2 = new char[4]; //not enough space char* str3 = str2; cout\u0026lt;\u0026lt;str2\u0026lt;\u0026lt;endl; // UMR strcpy(str2,str1); //ABW cout\u0026lt;\u0026lt;str2\u0026lt;\u0026lt;endl; //ABR delete str2; str2[0] += 2; //FMR and FMW delete str3; //FFM } 未初始化内存读取 (UMR): cout \u0026lt;\u0026lt; str2 \u0026lt;\u0026lt; endl; // UMR str2 是一个指向字符数组的指针，但在使用之前并没有被正确地初始化。因此，cout \u0026lt;\u0026lt; str2 将导致未定义行为，可能输出一些未知的值或者导致程序崩溃。\n越界写 (ABW - Array Bounds Write): strcpy(str2, str1); // ABW str2 数组的长度只有 4 个字符，而 str1 的长度是 5 个字符（包括字符串结束符 \u0026lsquo;\\0\u0026rsquo;），这将导致越界写入 str2，破坏了内存。\n数组越界读取 (ABR - Array Bounds Read): cout \u0026lt;\u0026lt; str2 \u0026lt;\u0026lt; endl; // ABR 在越界写入后，再次输出 str2 可能导致数组越界读取，因为 str2 的内存已经被破坏。\n释放后访问 (FFM - Free Memory): delete str2; // FFM str2[0] += 2; // FMR (Free Memory Read) and FMW (Free Memory Write) delete str2; 释放了 str2 指向的内存，但之后仍然尝试通过 str2[0] += 2; 来访问该已释放内存。这可能导致未定义行为，甚至程序崩溃。\n重复释放 (Double Free): delete str3; // Double Free str3 指向的内存在之前已经被 delete str2; 释放，再次释放同一块内存可能导致内存错误。\n为了修复这些问题，需要进行以下修改：\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;cstring\u0026gt; // Include for strcpy int main() { const char* str1 = \u0026#34;four\u0026#34;; // Use const char* for string literals char* str2 = new char[5]; // Allocate enough space, including \u0026#39;\\0\u0026#39; char* str3 = str2; std::cout \u0026lt;\u0026lt; str2 \u0026lt;\u0026lt; std::endl; // Corrected // Use strcpy to copy the string, and ensure proper null-termination strcpy(str2, str1); std::cout \u0026lt;\u0026lt; str2 \u0026lt;\u0026lt; std::endl; // Corrected delete[] str2; // Corrected // Avoid accessing the deleted memory // str2[0] += 2; // Commented out to prevent Free Memory Read/Write // No need to delete str3, as it points to the same memory as str2 return 0; } 这样修改后，代码中的常见内存错误就得到了修复。不过，值得注意的是，C++ 中可以使用更安全的字符串处理函数和智能指针等工具来避免这些问题。\n第四章（13） # 1. 常用的Web三层架构是怎样的？\n2.当在浏览器的地址栏中输入一个完整的URL，再按Enter键直至页面加载完成，整个过程发生了什么？\n域名解析：浏览器会依次查询浏览器的DNS缓存、系统缓存、路由器缓存，如果没有找到，则一直查询到根域名服务器缓存，找到域名所对应的的IP地址。 TCP连接：通过IP地址找到IP对应的服务器后，要求建立TCP连接。 HTTP连接：TCP连接成功后，浏览器开始向这个服务器发送一个HTTP请求。服务器接收到请求后开始进行处理，处理结束，返回一个响应包 浏览器接收和处理：浏览器接收到来自服务器的响应后，开始解析和渲染接收到的内容并呈现给用户。 TCP断开连接：最后客户端断开与服务器的TCP连接。 3.根据OWASP在2013发布的Web安全十大威胁报告，Web漏洞分为哪几大类型？请将该报告与2017年发布的Web安全十大威胁进行对比分析，了解这几年来Web安全威胁有哪些新的变化和发展\n删除了A8和A10项。由于更多的平台添加了CSRF防御，所以发现CSRF漏洞的应用程序不到5％，同时只在8％左右的应用程序中发现未验证的重定向和转发漏洞，因此A8和A10这两项排除在了top10之外，但仍然是需要关注的重要风险。 A3降位。2013年排名第三的XSS在2017年只排名第七名，其中除了开发者对XSS的防范意识加强之外另一個关键的原因在于，目前有很多自动化的扫描工具，都已经内建XSS扫描，可以加快漏洞修补速度，使得整体XSS漏洞数量看起来比以往少，但XSS风险却没有因此减少。 新添加3项。包括XXE、不安全的反序列化和不足的日志记录和监控，而后者对于许多组织来说是个严重的问题。 4.试将Web典型漏洞根据客户端和服务器端来划分，并根据漏洞原理阐述这样划分的理由。\nWeb典型漏洞主要可以根据漏洞的产生和利用方式，分为客户端漏洞和服务器端漏洞。下面对这两类漏洞进行划分并阐述其原理：\n客户端漏洞：\n跨站脚本攻击 (XSS): 原理：攻击者通过在网页中注入恶意脚本，使用户的浏览器执行这些脚本。这可能导致窃取用户信息、会话劫持等问题。 划分理由：XSS是一种利用客户端浏览器执行脚本的攻击，因此属于客户端漏洞。 跨站请求伪造 (CSRF): 原理：攻击者通过诱使用户在已登录的网站上执行恶意请求，以实现未经授权的操作。 划分理由：CSRF的攻击原理是利用用户已经在某个站点上建立的身份验证状态，因此它也主要涉及客户端。 点击劫持: 原理：攻击者通过将网页嵌套在透明的图层下面，诱使用户在不知情的情况下点击隐藏的可信任页面上的链接或按钮。 划分理由：点击劫持是通过伪造用户的交互行为来实现攻击的，因此它也属于客户端漏洞。 服务器端漏洞：\nSQL注入: 原理：攻击者通过在输入参数中注入SQL代码，利用不当的输入验证或过滤，执行恶意的SQL查询。 划分理由：SQL注入涉及对服务器端数据库的直接攻击，因此它是一种服务器端漏洞。 跨站请求伪造 (CSRF): 原理：在某些情况下，CSRF也可以涉及到服务器端，尤其是在攻击者需要在用户受害者的帐户上执行某些操作时。 划分理由：尽管CSRF的一部分涉及到用户浏览器，但服务器端也需要实施适当的防护措施，以防止未经授权的操作。 文件上传漏洞: 原理：攻击者通过在文件上传功能中上传包含恶意代码的文件，然后执行该代码。 划分理由：文件上传漏洞是通过利用服务器端的文件处理功能来实现攻击的，因此它属于服务器端漏洞。 路径遍历漏洞: 原理：攻击者试图通过输入特殊字符或构造恶意请求，访问文件系统中的敏感文件。 划分理由：路径遍历漏洞是直接涉及服务器文件系统的漏洞，因此属于服务器端。 5.简述SQL注入漏洞的原理？为什么SQL注入漏洞多年来一直名列Web安全漏洞的榜首？\n原理：攻击者能够利用现有Web应用程序，将恶意的数据插入SQL查询中，提交到后台数据库引擎执行非授权操作。\nSQL注入攻击具有广泛性。 相较其他漏洞对于SQL注入漏洞的防范要困难。 6.防范SQL注入漏洞的基本方法有哪些？重点谈谈在代码开发层面的安全措施。\n采用强类型语言，如Java、C#。 尽可能避免使用拼接的动态SQL语句，所有的查询语句都使用数据库提供的参数化查询接口。参数化的语句使用参数而不是将用户输入变量嵌入到SQL语句中。 在服务端验证用户输入的值和类型是否符合程序的预期要求。 在服务器端对用户输入进行过滤。 避免网站显示SQL错误信息，比如类型错误、字段不匹配等，防止攻击者利用这些错误信息进行一些判断。 加固应用程序服务器和数据库，利用最低权限账户与数据库连接。 Web安全开发中应当遵循安全规范，例如OWASP企业安全应用程序接口（The OWASP Enterprise Security API，OWASP ESAPI）。 7.什么是SQL盲注？它与一般的SQL注入有什么区别？\n盲注就是在sql注入过程中，sql语句执行的选择后，选择的数据不能回显到前端页面。此时，我们需要利用一些方法进行判断或者尝试，这个过程称之为盲注。\n普通注入是可以根据报错提示，进行sql语句注入，从而直接显示我们想要的信息，比如数据库版本、数据库名、用户名、操作系统版本等；而盲注只能通过多次猜测，从而猜解出有用信息。相对来说sql盲注更加考验安全人员的手注能力。\n9.简述XSS跨站脚本漏洞的原理。\nXSS漏洞是指应用程序没有对接收到的不可信数据经过适当的验证或转义就直接发给客户端浏览器。XSS是脚本代码注入的一种。因为Web浏览器可以执行HTML页面中嵌入的脚本命令，支持多种脚本语言类型，其中最主要的是JavaScript。攻击者利用XSS漏洞将恶意脚本代码注入到网页中，当用户浏览该网页时，便会触发执行恶意代码。\n10.简述CSRF跨站请求伪造漏洞的原理，并比较其与XSS漏洞的不同。\nCSRF跨站请求伪造攻击迫使登录用户的浏览器将伪造的HTTP请求，包括该用户的会话cookie和其他认证信息，发送给一个存在漏洞的Web应用程序，这些请求就会被应用程序认为是用户的合法请求。\nXSS利用站点内的信任用户，CSRF则通过伪装来自受信任用户的请求来利用受信任的网站。 反射型XSS的目的是在客户端执行脚本，而CSRF是在WEB应用中执行操作。 11.什么是命令执行漏洞？什么是文件包含漏洞？什么是文件上传漏洞？\n命令执行漏洞：PHP Web应用程序 PHP中包含很多可以执行命令的函数，如果过滤不严格就会产生此漏洞。攻击者可以将恶意代码提交给包含漏洞的Web应用服务器，从而执行一些系统命令。\n文件包含漏洞：注入一段用户能控制的脚本或代码，让服务器端执行。\n文件上传漏洞：用户上传一个可执行的脚本文件，通过此脚本文件获得执行服务器端命令的能力。\n第五章（7） # 1.什么是软件的生命周期？软件生命周期通常包括哪几个阶段？\n软件定义时期 软件定义时期的任务是， 确定软件开发工程必须完成的总目标； 确定工程的可行性； 导出实现工程目标应该采用的策略及系统必须完成的功能； 估计完成该项工程需要的资源和成本，并且制定工程进度表。 这个时期的工作通常又称为系统分析，由系统分析员负责完成。 软件定义时期通常进一步划分成3个阶段，即同题定义、可行性研究和需求分析。 软件开发时期 软件开发时期的任务是，设计和实现在前一个时期定义的软件，它通常由4个阶段组成：总体设计、详细设计、编码和单元测试，以及综合测试。其中前两个阶段又称为系统设计，后两个阶段又称为系统实现。 软件维护时期： 软件维护时期的任务是，使软件持久地满足用户的需要。具体地说， 当软件在使用过程中发现错误时应该加以改正；当环境改变时应该修改软件以适应新的环境； 当用户有新要求时应该及时改进软件以满足用户的新需要。 通常对维护时期不再进一步划分阶段，但是每一次维护活动本质上都是一次压缩和简化了的定义和开发过程。 2.什么是软件过程？什么是软件开发(过程)模型？为什么从20世纪90年代以后，人们更多使用\u0026quot;软件过程\u0026quot;来替代\u0026quot;传统的软件开发模型\u0026quot;？\n所谓软件过程，是指为了获得高质量软件所需要完成的一系列任务的框架，它规定了完成各项任务的工作步骤。\n软件开发模型是跨越整个软件生存周期的系统开发、运行和维护所实施的全部工作和任务的结构框架，它给出了软件开发活动各阶段之间的关系。\n进入20世纪90年代，软件工程领域提出了软件过程的概念。所谓软件过程，是指为了获得高质量软件所需要完成的一系列任务的框架，它规定了完成各项任务的工作步骤。1995年，国际标准化组织公布了软件开发的国际标准《ISO/IEC 12207 信息技术 软件生存期过程》，该标准将软件开发需要完成的活动概括为主要过程、支持过程和组织工程三大活动，每个大的活动又包括具体过程，共17个。我国也发布了相应的标准：《信息技术 软件生存周期过程》（CB/T 8566—2007）和《信息技术 软件生存周期过程 重用过程》（CB/T 26224—2010）。通常使用软件生命周期模型简洁地描述软件过程。软件生命周期模型规定了把生命周期划分为哪些阶段及各个阶段的执行顺序，因此，也称为软件过程模型。\n3.有哪些典型的软件开发模型？这些软件开发模型有什么区别与联系？\n（1） 瀑布模型（Waterfall Model）\n瀑布模型是在20世纪80年代之前唯一被广泛采用的生命周期模型，现在它仍然是软件工程中应用得最为广泛的开发模型。瀑布模型的本质是一次通过，即每个活动只执行一次，最后得到软件产品，因此也称为\u0026quot;线性顺序模型\u0026quot;或者\u0026quot;传统生命周期\u0026quot;。它的优势在于它是规范的、文档驱动的方法。它的主要缺陷如下：\n由于开发模型星线性，所以当开发成果尚未经过测试时，用户无法看到软件的效果。这样软件与用户见面的时间间隔较长，也增加了一定的风险。 在软件开发前期未发现的错误传到后面的开发活动中时，可能会扩散，进而可能会造成整个软件项目开发失败。 在软件需求分析阶段，完全确定用户的所有需求是比较困难的，也是不太可能的。 （2）快速原型模型（Rapid Prototype Model）\n快速原型模型是为了克服瀑布模型的缺点而提出来的。它通过快速构建一个可在计算机上运行的原型系统，让用户试用原型并收集用户反馈意见，获取用户的真实需求。这种模型的主要问题是，快速建立起来的原型系统结构加上连续的修改可能会导致产品质量低下。\n（3） 增量模型 （Incremental Model）\n增量模型是把待开发的软件系统模块化，将每个模块作为一个增量组件，从而分批次地分析、设计、编码和测试这些增量组件。运用增量模型的软件开发过程是递增式的过程。相对于瀑布模型而言，采用增量模型进行开发，开发人员不需要一次性地把整个软件产品提交给用户，而是可以分批次进行提交。该模型具有可在软件开发的早期阶段使投资获得明显回报和较易维护的优点，但是，要求软件具有开放的结构是使用这种模型时固有的困难。\n（4） 螺旋模型（Spiral Model）\n螺旋模型的基本做法是，在瀑布模型的每一个开发阶段前引人非常严格的风险识别、风险分析和风险控制。它把软件项目分解成一个个小项目，每个小项目都标识一个或多个主要风险，直到所有的主要风险因素都被确定。通过及时对风险进行识别及分析，决定采取何种对策，进而消除或减少风险的损害。螺旋模型强调风险分析，因此特别适用于庞大、复杂并具有高风险的系统。\n与瀑布模型相比，螺旋模型支持用户需求的动态变化，为用户参与软件开发的所有关键决策提供方便，有助于提高目标软件的适应能力，并且为项目管理人员及时调整管理决策提供便利，从而降低软件开发风险。\n螺旋模型的缺点如下：\n采用螺旋模型需要具有相当丰富的风险评估经验和专门知识，在风险较大的项目开发中，如果未能够及时识别风险，势必造成重大损失。 过多的选代次数会增加开发成本，延迟软件提交时间。 （5）喷泉模型（Fountain Model）\n喷泉模型是一种以用户需求为动力，以对象为驱动的模型，主要用于描述面向对象的软件开发过程。该模型较好地体现了面向对象软件开发过程无缝迭代的特性，是典型的面向对象的软件过程模型之一。\n（6） Rational 统一过程 （Ratiomal Unified Process, RUP）\nRUP 强调采用选代和检查的方式来开发软件，整个项目开发过程由多个迭代过程组成。在每次迭代中只考虑系统的一部分需求，针对这部分需求进行分析、设计、实现、测试和部署等工作，每次选代都是在系统已完成部分的基础上进行的，每次给系统增加一些新的功能，如此循环往复地进行下去，直至完成最终项目。\n（7） 极限编程和敏捷开发（eXtreme Programming \u0026amp; Agile Development）\n极限编程 XP是一种近螺旋式的开发方法，它将复杂的开发过程分解为一个个相对比较简单的小周期，通过积极的交流、反馈及其他一系列的方法，开发人员和客户可以非常清楚开发进度、变化、待解决的问题和潜在的困难等，并根据实际情况及时调整开发过程。极限编程方法强调开发者与用户的沟通，让客户全面参与软件的开发设计，保证变化的需求及时得到修正。\n以极限编程为代表的敏捷开发，具有对变化和不确定性的更快速、更敏捷的反应特性。敏提就是\u0026quot;快\u0026quot;，因此，敏捷开发过程能够较好地适应商业竞争环境下对小型项目提出的有限资源和有限开发时间的约束，可以作为对 RUP 的补充和完善。但是，要快就要多发挥个人的个性思维，虽然通过结队编程、代码共有或团队替补等方式可减少个人对软件的影响力，但个性思维的增多也会造成软件开发继承性的下降，因此敏捷开发是一个新的思路，但不是软件开发的终极选择。作为一种软件开发模式，敏捷开发远不如 RUP 全面和完整。\n（8） 微软过程（Microsoft Process）\n多年的实践经验证明，微软过程是非常成功和行之有效的。\n一方面，可以把微软过程看作 RUP 的一个精简配置版本，整个过程包含若干个生命周期的持续递进循环，每个生命周期由5个阶段组成：规划阶段、设计阶段、开发阶段、稳定阶段和发布阶段，每个阶段精简为由一次迭代完成； 另一方面，可以把微软过程看作敏捷过程的一个扩充版本，它扩充了每个生命周期内的各个阶段的具体工作流程。 4.SD3+C原则是SDL模型实施的基本原则，试简述其内容。\n安全设计（Secure by Design）。在架构设计和实现软件时，需要考虑保护其自身及其存储和处理的信息，并能抵御攻击。\n安全配置（Secure by Default）。在现实世界中，软件达不到绝对安全，所以设计者应假定其存在安全缺陷。为了使攻击者针对这些缺陷发起攻击时造成的损失最小，软件在默认状态下应具有较高的安全性。例如，软件应在最低的所需权限下运行，非广泛需要的服务和功能在默认情况下应被禁用或仅可由少数用户访问。\n安全部署（Security by Deployment）。软件需要提供相应的文档和工具，以帮助最终用户或管理员安全地使用。此外，更新应该易于部署。\n沟通（Communication）。软件开发人员应为产品漏洞的发现准备响应方案，并与系统应用的各类人员不断沟通，以帮助他们采取保护措施（如打补丁或部署变通办法）。\n5.微软的SDL模型与传统的瀑布模型的关系是怎样的？\nSDL模型是由软件工程的瀑布模型发展而来，是在瀑布模型的各个阶段添加了安全活动和业务活动目标。\n6.什么是敏捷SDL？敏捷SDL和经典SDL的主要区别是什么？\n敏捷SDL与典型SDL的差别主要有两点：\n敏捷SDL不采用传统的瀑布模型而是采用无阶段的迭代开发模型，以实现软件版本的快速更新和发布。\n在敏捷SDL中，并不是每个发布版本（或每次\u0026quot;突击发布\u0026quot;）都需要达到所有的要求。\n10.试从软件各个开发阶段所进行活动的角度，对几种软件安全开发模型进行对比分析。\n书中好几页太多了\n第六章（14） # 1.为什么要进行需求分析？通常对软件系统有哪些需求？\n为了开发出真正满足用户需求的软件产品，首先必须知道用户的需求。对软件需求的深入理解是软件开发工作获得成功的前提条件。\n通常对软件系统有下述几个方面的综合要求：\n功能需求：划分出系统必须完成的所有功能。 性能需求：指定系统必须满足的定时约束或容量约束，通常包括速度（响应时间）、信息量速率、主存容量、磁盘容量和安全性等方面的需求。 可靠性和可用性需求：可靠性需求定量地指定系统的可靠性。可用性与可靠性密切相关，它量化了用户可以使用系统的程度。 出错处理需求：说明系统对环境错误应该怎样响应。 接口需求：描述应用系统与它的环境通信的格式。 约束：描述在设计或实现应用系统时应遵守的限制条件。 逆向需求：说明软件系统不应该做什么。理论上有无限多个逆向需求，人们应该仅选取能澄清真实需求且可消除可能发生的误解的那些逆向需求。 将来可能提出的要求：明确地列出那些虽然不属于当前系统开发范畴，但是据分析将 来很可能会提出来的要求。这样做的目的是，在设计过程中对系统将来可能的扩充和修改预做准备，以便一旦确实需要时能比较容易地进行扩充和修改。 2.为什么要进行安全需求分析？通常对软件系统有哪些安全需求？138\n软件安全需求分析的目的是描述为了实现信息安全目标，软件系统应该做什么，才能有效地提高软件产品的安全质量，减少进而消减软件安全漏洞。\n软件运行的情境通常可以分为外部情境和内部情境，因此安全需求可以从外部需求和内部需求两个方面来分类。\n外部安全霈求：外部安全需求通常主要指法律、法规等遵从性需求，包括相应国家和地区关于安全技术与管理的法规、标准及要求等。这些安全技术和管理的合规性要求往往是已有安全威胁的经验性对策的总结，因而遵循这些要求不仅是法规制度上的要求，也是软件安全性保障的要求。 内部安全需求：内部安全需求通常包括两个部分，一是组织内部需要遵守的政策、标准、指南和实践模式，二是与软件业务功能相关的安全需求。 3.软件安全需求分析的主要工作是什么？它和软件需求分析有什么区别与联系？\n软件安全需求分析的主要工作：在软件开发生命周期的需求分析阶段，首先确定目标系统的业务运行环境、规则环境及技术环境，然后在了解各类软件安全需求内容的基础上，通过一定的安全需求获取过程，对软件应该包含的安全需求进行分析，而对于如何实现这些安全需求将在软件安全设计和开发部分进行讨论。\n软件安全需求分析与软件需求分析的联系：软件安全需求是软件需求的一个必要组成部分。安全需求应该与业务功能需求具有同样的需求水平，并对业务功能需求具有约束力。\n软件安全需求分析与软件需求分析的区别\n（1）软件安全需求的客观性\n软件安全需求由系统的客观属性决定。安全需求与一般需求的一个主要不同之处在于：安全需求并不是从使用者的要求和兴趣出发，而是由系统的客观展性所决定的。因此，需求分析员将承担更多软件需求的分析工作。\n（2）软件安全需求的系统性\n软件安全需求分析不能只从软件本身出发，必须从系统角度进行分析。这是因为，虽然软件（包括操作系统、数据库等）本身可能会由于逻辑、数据和时序等设计敏陷导致安全问题，但同时，由于软件属于逻辑产品，很多情况下并不是软件失效，而是在软件正常工作时，在某种特条件下软硬件相互作用，以及由于人的使用问题而导致不安全情况发生。因此，软件安全需求分析必须在系统安全件分析的基础上进行\n（3）软件安全需求的经济性和适用性\n软件安全的需求内容非常丰富，并不是所有的应用安全需求控制都要采纳和实施。组织应当根据具体业务的重要性，对安全措施进行成本控制。安全控制的成木应该与较件所有者或者管理部门要求的目标水平相当。\n4.为什么说软件安全需求更多地来源于遵从性需求？\n软件需求分析中，分析员和用户都起着至关重要的作用。然而，在软件安全性需求分析中，软件用户由于安全知识的缺乏，很难从专业角度提出安全需求。因此，软件安全需求更多地来自于对组织内部和外部的一些安全政策和标准的遵从。安全需求分析人员对这些政策需求和标准进行深入理解，并将它们转化为软件安全属性需求，是安全需求分析阶段要完成的艰巨任务。\n5.本章中介绍的安全需求遵从性标准有哪些类别，它们之间有何联系与区别？144\n信息系统安全测评国际标准 信息安全管理国际标准 信息系统安全工程国际标准 我国信息安全标准概述 信息安全标准从适用地域范围可以分为：国际标准、国家标准、地方标准、区域标准、行业标准和企业标准。 信息安全标准从涉及的内容可以分为：信息安全体系标准、信息安全机制标准、信息安全测评标准、信息安全管理标准、信息安全工程标准、信息系统等级保护标准和信息安全产品标准等类别。 6.我国为什么要实行网络安全等级保护制度？网络安全保护能力划分为哪些等级？具体每个等级有什么要求？145\n等级化保护是信息安全发展规律。按组织业务应用区域，分层、分类、分级进行保护和管理，分阶段推进等级保护制度建设，这是做好国家信息安全保护必须遵循的客观规律。\n等级保护是国家法律和政策要求。为了提高我国信息安全的保障能力和防护水平，维护国家安全、公共利益和社会稳定，保障和促进信息化建设的健康发展，1994年国务院颁布的《中华人民共和国计算机信息系统安全保护条例》规定：\u0026ldquo;计算机信息系统实行安全等级保护，安全等级的划分标准和安全等级保护的具体方法，由公安部会同有关部门制定\u0026rdquo;。\n网络安全等级保护制度将网络划分为如下五个安全保护等级，从第一级到第五级逐级增高。\n7.对网络安全等级划分通常有两种描述形式，即根据安全保护能力划分安全等级的描述，以及根据主体遭受破坏后对客体的破坏程度划分安全等级的描述这两种形式。试谈谈这两种等级划分的对应关系。\n题6为根据安全保护能力划分安全等级的描述，下面为根据主体遭受破坏后对客体的破坏程度划分安全等级，应该是一对一关系。\n第一级，属于一般网络，其一旦受到破坏，会对公民、法人和其他组织的合法权益造成损害，但不危害国家安全、社会秩序和社会公共利益。 第二级，属于一般网络，其一旦受到破坏，会对公民、法人和其他组织的合法权益造成严重损害，或者对社会秩序和社会公共利益造成危害，但不危害国家安全。 第三级，属于重要网络，其一旦受到破坏，会对公民、法人和其他组织的合法权益造成特别严重损害，或者会对社会秩序和社会公共利益造成严重危害，或者对国家安全造成危害。 第四级，属于特别重要网络，其一旦受到破坏，会对社会秩序和社会公共利益造成特别严重危害，或者对国家安全造成严重危害。 第五级，展于极其重要网络，其一旦受到破坏，会对国家安全造成特别严重危告。 网络安全等级保护是对网络进行分等级保护、分等级监管，是将信息网络、信息系统、网络上的数据和信息，按照重要性和遭受损坏后的危害性分成五个安全保护等级（从第一级到第五级，逐级增高）；等级确定后，第二级（含）以上网络到公安机关备案，公安机关对备案材料和定级准确性进行审核，审核合格后颁发备紫证明；备案单位根据网络的安金等级，按照国家标准开展安全建设整改，建设安全设施、落实安全措施、落实安全责任、建立和落实安全管理制度；选择符合国家要求的测评机构开展等级测评；公安机关对第二级网络进行指导，对第三、第四级网络定期开展监督，检查。\n8.简述网络等级保护与信息安全管理体系的联系和区别。\n9.软件安全需求获取过程中涉及哪些相关方人员？他们各自主要的职责是什么？\n10.软件安全需求的获取方法有哪些？\n11.软件安全需求的获取方法中的策略分解是指什么？\n策略分解是指将组织需要遵守的内部和外部政策，包括外部法律法规、隐私和遵从性命令分解成详细的安全需求。策略分解过程是一个连续的、结构化的过程。\n12.软件安全需求的获取方法中的数据分类是指什么？\n数据分类是指，根据数据生命周期管理（Data Lifecycle Management, DL.M）对数据的分阶段划分来决定相应的安全需求；也可以根据数据的重要性对保护级别的划分来决定相应的安全需求。\n13.针对信息系统中的数据生命周期，通常应当考虑的安全需求有哪些？\n谁可以创建数据？谁可以对数据进行访问？访问权限是什么（授权的级别）？ 当进行数据处理时，是否需要数据泄漏保护（Data Leakage Prevention, DLP）？ 当数据被传输时，是否需要采用传输层或网络层安全协议（如SSL/TLS 或 IPSec）来保护数据？ 数据是以结构化还是非结构化的方式进行存储，数据存储和使用的环境（私人、公共或混合）面临哪些安全威胁？ 从数据的可访问性和可用性角度来看，一些需要频繁访问的关键业务数据必须存储在更快的存储介质上，但是，如何应对这些存储介质的可靠性、易失性等物理安全问题？ 当数据归档时，需要遵循企业的数据保留政策，或是当地法律法规对数据存档的要求。当数据失去其效用（即不再被业务操作或连续性所需要），且没有监管或规范要求保留时，数据是否需要通过数据逻辑删除、物理媒体破坏或存储媒体消磁等被安全地处理？ 14.软件安全需求的获取方法中的主/客体关系矩阵是指什么？\n采用主/客体关系矩阵来刻画一个基于使用用例的主/客体之间的操作关系。主/客体关系矩阵是角色和组件的二维表示，主体（角色）作为列，客体（对象/组件）作为行。当主/客体关系矩阵产生后，与主/客体关系矩阵所允许的对应动作相违背的事件就可以判定为威胁，在此基础之上可以确定安全需求。\n第七章（14） # 1.软件设计阶段的主要工作是什么？\n从技术的角度看软件设计阶段的主要工作包括：软件架构设计、界面接口设计、模块/子系统等构件设计、数据模型设计、过程/算法设计及部署设计等。\n2.软件安全设计阶段的主要工作是什么？\n软件安全设计的主要工作包括软件架构安全性设计、软件架构安全性分析及软件安全功能设计。\n3.为什么要进行软件架构设计？软件架构设计的主要工作是什么？软件架构安全性设计的主要工作是什么？\n为了达到控制软件复杂性、提高软件系统质量、支持软件开发和复用的目的，开发人员提出了软件架构的概念。\n软件架构设计对于开发高质量软件具有较大作用。一般而言，软件架构的设计首先需要理清业务逻辑的功能要求，了解业务逻辑的变化性要求，包括可维护性和可扩展性，分离出概要业务逻辑层。接着，设计业务逻辑层和系统其他部分的接口与交互关系，按照职责分离原则设计包、类、方法和消息，设计业务逻辑算法。然后，使用自底向上和自顶向下相结合的方式，不断渐进地迭代架构设计。\n软件架构安全设计首先需要进行系统描述，包括系统功能、安全要求、系统部署和技术需求，确定软件系统的安全级别。接着，设计软件网络、数据库等应具备的安全功能，根据软件具体安全需求的不同，设计的安全功能包括加密、完整性验证、数字签名、访问控制及安全管理等。在架构安全设计过程中，还需要解决软件安全功能的易用性、可维护性和独立性问题。\n4.为什么要进行软件架构安全性分析？软件架构安全性分析的基本过程是什么？\n（1）软件架构安全性分析的重要性\n在软件架构安全性设计的过程中，尤其是对于大而复杂的系统，要将安全属性一次性设计到软件架构中成为架构的有机组成部分，这是一项非常具有挑战性的工作。\n为此，一旦软件架构设计或是软件架构安全性设计完成，在退出设计阶段进入开发阶段之前，需要对软件（安全）架构和设计方案进行检查，以确保设计能够满足软件的安全需求。这不仅包括功能方面的设计检查，也包括安全设计检查。检查可以帮助开发人员在编码之前对安全设计要素进行验证，提供一个识别和处理任何安全漏洞的机会，减少后续阶段重新设计软件的需要。\n设计检查需要考虑安全政策和软件部署的目标环境，同时也需要对应用系统进行全局检查。网络和主机水平的安全保护都需要到位，保护措施之间不会相互矛盾从而削弱保护强度。需要特别关注软件安全设计基本原则和软件核心安全属性需求的设计，以确保保密性、完整性和可用性。此外，还需要逐层对软件架构进行分析以保证纵深防御控制措施到位。攻击面评估、威胁建模和滥用案例建模、安全体系结构和设计检查等几个方面都是非常有用的，它们可以确保软件不仅能实现预期的功能，同时也不会违反任何安全策略。\n（2）软件架构安全性分析的基本过程\n软件架构安全性分析的基本过程如下图所示，首先进行架构建模，然后根据软件的安全需求描述或相关标准，对架构模型是否满足要求进行检查，如果不满足则需要修改设计架构，如此反复，直至满足所有安全需求和相关标准。\n目前，国内外关于软件架构安全性分析的理论和应用研究还处于探索阶段。软件架构安全性分析可以分为形式化和工程化两类分析方法。\n5.软件受攻击面是指什么？举例说明软件设计时可以采取哪些策略来降低受攻击面。\n软件受攻击面是指，用户或其他程序及潜在的攻击者都能够访问到的所有功能和代码的总和，它是一个混合体，不仅包括代码、接口和服务，也包括对所有用户提供服务的协议，尤其是那些未被验证的或远程用户都可以访问到的协议。一个软件的攻击面越大，安全风险就越大。减少软件受攻击面就是去除、禁止一切不需要使用的模块、协议和服务，其目的是减少攻击可以利用的漏洞。\n采取减少软件受攻击面原则的实例如下：\n重要性低的功能可取消；重要等级为中的功能可设置为非默认开启，需要用户配置后才予以开启；重要性高的功能则关闭或增加一些安全措施进行限制。 重用那些经过测试、已证明安全的现有库和通用组件，而不是用户自己开发的共享库。 6.什么是最小授权原则？试举例说明软件设计时哪些措施是采用了最小授权原则。\n最小授权原则是指，系统仅授予实体（用户、管理员、进程、应用和系统等）完成规定任务所必需的最小权限，并且该权限的持续时间也尽可能短。最小授权原则可使无意识的、不需要的、不正确的特权使用的可能性降到最低，从而确保系统安全。\n软件设计中采用最小授权原则的实例如下：\n将超级用户的权限划分为一组细粒度的权限，分别授予不同的系统操作员/管理员。对管理员账户分配安全资源的访问权限也要设置为受限访问，而不是超级用户权限。 采用高内聚、低耦合的模块化编程方法，也就是模块之间的依赖关系是弱链接（低耦合），每一个模块只负责执行一个独立的功能（高内聚）。 7.什么是权限分离原则？试举例说明软件设计时哪些措施是采用了权限分离原则。\n权限分离原则在教件设计中是指，将软件功能设计为需要在两个或更多条件下才能实现，以防止一旦出现问题，整个软件都可能面临风险。实际上这一原则也是最小权限原则的一种体现。\n软件设计中采用权限分离原则的实例如下：\n清晰的模块划分，将风险分散到各个模块中去。这样，如果出现问题就可以快速定位到模块，以便进行修复；其次，还可以对单个模块进行测试，保证各个模块的正确性；还可以重复使用已经开发的模块，并且可以在已有模块上增加和替换模块，同时不影响原有模块的功能。 不允许程序员检查自己编写的代码。 8.针对第6章介绍的核心安全需求，软件安全功能设计通常有哪些内容？\n设计模式是对软件设计中普遍存在、反复出现的各种问题，根据多次处理的经验，提出的一套能够快速、准确响应此类问题的解决方案。设计模式描述在各种不同情况下，应解决共性问题。\n应用安全设计\n应用安全功能设计：1）身份认证；2）授权；3）输入和输出验证；4）配置管理；5）会话管理；6）参数操作；7）异常管理；8）审核与日志 应用交互安全设计：1）明确交互系统；2）接口方式安全设计 数据安全设计\n机密性要求：1）数据传输应确保保密性；2）数据使用应确保保密性；3）数据删除应确保保密性 完整性要求：1）数据传输应确保完整性；2）数据使用应确保完整性；3）敏感数据的使用应在应用程序中进行检错和校验操作，保证原始数据的正确性和完整性 可用性要求：1）数据采集应确保可用性，验证的方式包括数据格式验证、数据长度验证和数据类型验证等；2）数据传输应确保可用性；3）数据处理应确保可用性；4）数据使用应确保可用性，验证的方式包括数据格式验证、数据长度验证和数据类型验证等 9.什么是软件设计模式？有哪些软件设计模式？\n设计模式是对软件设计中普遍存在、反复出现的各种问题，根据多次处理的经验，提出的一套能够快速、准确响应此类问题的解决方案。设计模式描述在各种不同情况下，应解决共性问题。\n常见的有23种模式，下面只列举6种\n抽象工厂横式（Abstract Factory）：提供一个创建一系列相关或相互依赖对象的接口，而无需指定它们具体的类。 适配器模式（Adaptor）：将一个类的授口转换成客户希望的另外一个接口。适配器模式使得原本由于接口不兼容而不能一起工作的类可以一起工作。 桥梁模式（Bridge）：将抽象部分与已的实现部分分离，使它们都可以独立地查化。 建造模式（Builder）：将一个复杂对象的构建与它的表示分离，使同样的构建过程可以创建不同的表示。 责任链模式（Chain of Responsibity）：为解除请求的发送者和接收者之间耦合，而使多个对象都有机会处理这个请求，将这些对象连成一条链，并沿着这条链传递该请求，直到有一个对象处理它。 命令横式（Command）：将一个请求封装为一个对象，从而可用不同的请求对客户进行参数化；对请求排队或记录请求日志，以及支持可取消的操作。 \u0026hellip; 10.什么是安全模式？为什么说能够利用安全模式来快速、准确地进行软件安全设计？\n安全模式是在给定的场景中，为控制、阻止或消减一组特定的威胁而采取的通用解决方案。该解决方案需要应对一系列的问题，并且可以使用UML类图、时序图、状态图和活动图等进行表述。\n安全模式封装了反复出现的系统问题的解决方案，同时精确地表述了系统要求和解决方案。采用模式的系统架构描述比较容易让人看懂，也为设计和分析提供了指南，还定义了使架构更安全的方法。安全模式使得不具备专业安全知识的应用开发人员也可以使用安全措施。还可以通过分析现有系统看它们是否包含特定的模式，进而评估它们的安全性。此外，可以在改造旧有系统时，利用模式来添加系统中缺失的安全特性。\n安全模式与威胁直接相关，特定的威胁可能是由一个或多个漏洞引起的。在软件设计阶段应用安全模式来控制、阻止或削弱威胁，可以从根本上消减软件安全漏洞。\n11.试给出一种利用安全模式进行软件安全设计的方法。\n一种基于安全模式的软件安全设计方法过程可以分为3个阶段：风险确定阶段、系统安全架构阶段和系统设计细化阶段。在每一个阶段，需要进行一系列相应的实践活动，通过实行这些实践活动，完成每一个阶段的任务，最终实现系统的安全架构。\n风险确定阶段：该阶段主要有两个工作：识别风险和评估风险。该阶段通过对业务需求、用户需求及安全需求等的分析，利用历史威胁记录及经验，识别并评估系统面临的风险。 系统安全架构阶段：该阶段对风险进行消解，并对解决方案进行评估，在此基础上构建系统的高层架构图。主要工作包括浏览模式库、选择安全模式、评估安全模式和建立系统高层架构。 系统设计细化阶段：该阶段的主要工作包括 构建业务类图、实例化安全模式，以及整合系统并适当重构。在该阶段，在前一阶段选定的安全模式集和系统高层架构图的基础上，细化业务分析和实例化安全模式，并将它们整合到一起形成系统的完整设计类图。 12.什么是威胁建模？试简述威胁建模的过程。\n虽然威胁建模还没有一个标准的定义，本书试图给出这样的一个解释，软件威胁建模是指，通过抽象的概念模型对影响软件系统的威胁进行系统的识别和评价。\n13.为什么说组织自身的威胁建模能力水平对提升组织的整体安全保障能力起到至关重要的作用？\n采用威胁建模方法，可以系统性地分析其架构、软件体系和程序部署，分析网络和信息系统可能面临的潜在威胁，确认有哪些攻击面，之后提出有针对性的安全防范措施，这才是有效解决网络安全对抗的良策。组织自身的威胁建模能力水平对提升组织的整体安全保障能力将起到至关重要的作用。\n威胁建模具有重要的存在价值，包括早期发现安全缺陷，理解安全需求，设计和交付更安全的产品，解决其他技术无法解决的问题等作用。\n事实上，尽管威胁模型在系统设计甚至于需求分析阶段就应该完成，但它的作用可以跨越整个软件生命周期。一个完整的威胁模型是设计、开发、测试、部署和运营团队的代表性输入项。在设计阶段，应该由软件架构团队来识别威胁进而建立威胁模型；开发团队可以使用威胁模型来实现安全控制和编写安全的代码；测试人员不仅可以使用威胁模型生成安全测试用例，还需要验证威胁模型中已识别威胁的控制措施的有效性；最后，操作人员可以使用威胁模型配置软件安全设置，保证所有入口点和出口点都有必要的保护控制措施。\n14.在威胁排序的几种计算方法中，为什么说相比Delphi法和平均排序法，P×I排序方法更科学？\n相比 Delphi 法和平均排序法，PxI排序方法更科学。PxI排序考虑业务影响（潜在损失和受影响的用户）和发生概率（可再现性、可利用性和可发现性）。PxI排序法对事件发生概率、业务影响及它们合并的影响进行深入分析，使得设计团队能够灵活地掌握如何降低事件发生的概率、减小业务影响或二者同时降低；此外，PxI排序方法还给出了更精确的风险图谱。\n第八章（8） # 软件安全编码阶段的主要工作有哪些？ 选择安全的编程语言，所谓安全的编程语言，是指那些具有对缓冲区、指针和内存进行管理能力而避免发生软件安全问题的语言。类型安全语言就属于安全的编程语言。 版本(配置)管理，软件版本管理或控制不仅能够保证开发团队正在使用的程序版本是正确的，同时在必要的情况下也能提供回退到上一个版本的功能；另外，软件版本管理还提供了跟踪所有权和程序代码变化的能力。 代码检测，这里的代码主要是指源代码。代码检测是指对代码质量进行检查，以发现是否存在可利用漏洞的过程。根据代码检测时代码所处的状态，可以将代码分析分为两种类型：代码静态检测和代码动态检测。 安全编译，编译是指将程序员编写的源代码转换为计算机可以理解的目标代码的过程。 2.什么是类型安全语言？哪些程序开发语言是类型安全的？\n类型安全简单来说就是访问可以被授权访问的内存位置，类型安全的代码不会试图访问自己未被授权的内存区域。提供类型安全的保障机制的编程语言是类型安全语言；\nJava和C#属于类型安全语言。\n3.安全编译是指在代码编译阶段采取的哪些安全措施？\n采用最新的集成编译环境，并选择使用这些编译环境提供的安全编译选项和安全编译机制来保护软件代码的安全性。 代码编译需要在一个安全的环境中进行。编译环境的完整性对于保证最终目标代码的正确性是很重要的。可以采用以下一些保证措施： 在物理环境上，对代码编译系统实施安全访问控制，防止人为地破坏和篡改。 在逻辑上，使用访问控制列表防止未授权用户的访问。 使用软件版本控制方法，保证代码编译版本的正确性。 尽量使用自动化编译工具和脚本，保证目标代码的安全性。 对应用环境的真实模拟也是软件编译需要考虑的问题。很多软件在开发和测试环境中运行得很好，而到了生产环境中就会出现很多问题，主要原因就是开发和测试环境与实际开发环境不匹配。由于应用环境比较复杂，因此要开发出能够适应所有环境的应用软件并不是一件简单的工作，对环境的适配也是反映软件应用弹性的一个重要指标。 多样化编译技术作为一种提高软件安全性的方法已经得到了应用。 4. 试列举几条安全编码原则，并举例说明这些原则的重要意义。\n验证输入。对于不可信任数据源的输入应当进行验证。正确的输入验证能减少大量软件漏洞。这些数据源包括命令行参数、网络接口、环境变量，以及用户文件。 留意编译器警告。应采用实现了安全特性的编译器，并启用编译器的警告和错误提示功能。不仅要处理和解决代码中的错误，而且也应处理和解决所有的警告，确保不将任何一个警告带入到程序的最终编译版本中。 默认拒绝。默认的访问权限是拒绝，除非明确是允许的。 坚持最小权限原则。每个进程拥有完成工作所需的最小权限，任何权限的拥有时间要尽可能短，以阻止攻击者利用权限提升执行任意代码的机会。 最少反馈。最少反馈是指在程序内部处理时，尽量将最少的信息反馈到运行界面，即避免给不可靠用户过多的可利用信息，防止其据此猜测软件程序的运行处理过程。最少反馈可以用在成功执行的流程中，也可以用在发生错误执行的流程中。典型的例子如用户名和口令认证程序，不管是用户名输入错误还是口令输入错误，认证端都只反馈统一的\u0026quot;用户名/口令错误\u0026quot;，而不是分别告知\u0026quot;用户名错误\u0026quot;或\u0026quot;口令错误\u0026quot;，这样可以避免攻击者根据输入正确的用户名或口令来猜测未知口令或用户名。 5.为什么要避免使用C语言中原有的字符串函数？所谓的安全字符串函数解决了原有C字符串函数的什么安全漏洞？\n因为c语言没用提供字符串类型，字符串以字符数组形式存储，是以\\0结尾的一段内存，并且c语言对缓冲区溢出不做检查，因此c语言原有的字符串函数在使用时有内存泄漏，缓冲区溢出的隐患。 安全字符串函数要求程序员在参数中给出目标缓冲区的大小，以此避免缓冲区溢出。 6.Java提供的沙箱安全机制的核心思想是什么？\n\u0026ldquo;沙箱\u0026quot;模型的核心思想是：本地环境中的代码能够访问系统中的关键资源(如文件系统等)，而从远程下载的程序则只能访问\u0026quot;沙箱\u0026quot;内的有限资源。该模型的目的是在可靠环境中运行可疑程序。\n7.试谈谈Java提供的安全机制。\n语言层安全，语言层安全是通过编译器的编译来实现，即编译成功则说明达到了语言层安全性。Java在语言层提供如下安全机制： 通过某些关键字（如private、protected）定义代码的可见性范围（即权限）。 通过类型规则确保程序运行时变量的值始终与声明的类型一致，在函数或方法调用时形参与实参的类型匹配。 Java还采用自动内存管理、垃圾收集站、字符串和数组的范围检查等方法，来确保Java语言的安全性。 字节码层安全，在字节码层次，Java提供两种保障安全的机制： 类加载器，类加载器主要分为四类：启动类加载器、标准扩展类加载器、路径类加载器和网络类加载器。 字节码验证器，验证分成静态和动态两个阶段。 应用层安全，一旦类加载器加载了一个类并由字节码验证器验证了它，Java平台的第3种安全机制，即安全管理器就开始运行： 安全管理器是一个由Java API提供的类，即：java.lang. SecurityManager类，它的作用是说明一个安全策略以及实施这个安全策略。 安全策略描述了哪些代码允许做哪些操作。由安全管理器对象定义的安全检查方法构成了当前系统的安全策略。当这些检查方法被调用时，安全策略就得以实施。 第十二章（7） # 1.试解释以下与恶意代码程序相关的计算机系统概念，以及各概念之间的联系与区别：\n进程、线程、动态链接库、服务、注册表。\n线程是执行任务、完成功能的基本单位，而进程则为线程提供了生存空间和线程所需要的其他资源；\n动态链接库（DLL）是共享函数库的可执行文件。动态链接库提供了一种方法，使进程可以调用不属于其可执行代码的函数。函数的可执行代码位于一个DLL中，该DLL包含一个或多个已被编译、链接并与使用它们的进程分开存储的函数。同一个动态链接库可以同时被多个进程加载到内存中，并且执行DLL中的功能。计算机病毒通常将病毒代码写到一个DLL文件中，然后想尽一切办法将此病毒代码加载到系统的某个进程中，如Explorer.exe桌面进程，这样Explorer.exe就会运行病毒代码了。这也是通常所说的病毒注入技术。\nwindows 系统的许多功能都是通过服务来实现的。简单来讲可以将服务理解为在后台完成系统任务的程序，比如自动获取更新或者管理打印服务等。\n注册表指在Windows中使用的中央分层数据库，用于存储一个或多个用户、应用程序和硬件设备配置系统所必须的信息。注册表包含Windows在运行期间不断引用的信息，例如，每个用户的配置文件、计算机上安装的应用程序可以创建的文档类型、正在使用哪些端口以及包含了有关计算机如何运行的信息。\n2.从危害、传播、激活和隐藏4个主要方面分析计算机病毒、蠕虫、木马、后门、Rootkit及勒索软件这几类恶意代码类型的工作原理。\n计算机病毒\n危害：病毒侵入系统的目的就是要破坏系统的机密性、完整性和可用性等。计算机病毒编制者的目的和所入侵系统的环境决定了破坏程度，较轻者可能只是显示一些无聊的画面文字、发出点声音，稍重一点的可能是消耗系统资源，严重者可以窃取或损坏用户数据，甚至是瘫痪系统、毁坏硬件。 传播：传染模块完成病毒的传播和感染。传染过程通常包括以下3个步骤。 寻找目标文件。病毒的传染有其针对性，或针对不同的系统，或针对同种系统的不同环境。 检测目标文件。感染模块检查寻找到的潜在目标文件是否带有感染标记或设定的感染条件是否满足。 实施感染。如果潜在目标文件没有感染标记或满足感染条件，感染模块就实施感染，将病毒代码和感染标记放入宿主程序。 激活：计算机病毒的触发条件多种多样，常见的有日期触发、时间触发、键盘触发、感染触发、启动触发、访问磁盘次数触发和调用中断功能触发等。多数病毒采用的是组合触发条件，而且通常先基于时间，再辅以访问磁盘、击键操作等其他条件实现触发。 0 隐藏：病毒程序为了隐藏自己，一般不独立存在，而是寄生在别的有用的程序或文档之上。同时，计算机病毒还采取隐藏窗口、隐藏进程、隐藏文件，以及远程DLL注入、远程代码注入和远程进程（线程）注入等方式来隐藏执行。 蠕虫\n危害：蠕虫的功能结构框架中包含宿主破坏模块，用于摧毁或破坏被感染主机，破坏网络正常运行，在被感染主机上留下后门等。蠕虫病毒入侵并完全控制一台计算机之后，就会把这台机器作为宿主，进而扫描并感染其他计算机。 传播：它会主动扫描和攻击网络上存在系统漏洞的结点主机，通过局域网或者因特网从一个结点传播到另外一个结点。 激活：网络蠕虫可以进行对主机的漏洞检测，从而决定采用何种攻击类型。 隐藏：蠕虫的功能结构框架中包含实体隐藏模块，包括对蠕虫各个实体组成部分的隐藏、变形、加密及进程的隐藏，主要用于提高蠕虫的生存能力。 木马\n危害：木马的主要攻击类型分为三种：控制类、信息窃取类和下载者类，分别进行控制目标机器、窃取信息和下载恶意软件的操作。 传播：木马一般需要诱骗用户上当后进行传播。 激活：当用户将木马的被控端安装在机器上后，木马会运行dropper组件，进行木马植入，成功植入后木马完成激活。 隐藏：在木马植入过程中dropper模块会根据系统环境、杀毒软件等影响因素判断是否进行植入，植入完成后使用rootkit技术隐藏木马的主要组件模块。 后门\n危害：后门能够绕过安全控制获取程序或系统的访问权。 隐藏：普通用户一般无从得知后门的存在。 rootkit\n危害：rootkit属于木马的范畴，通过修改现有的操作系统软件，使攻击者获得访问权限并隐藏在计算机中，它能够修改代码、数据和程序逻辑，破坏系统内核数据结构和更改指令执行流程，从而达到隐匿自身及相关行为痕迹的目的。 勒索软件\n危害：勒索软件可以用来劫持用户资产或资源，以此为条件向用户勒索钱财，一般会将用户的某些文件进行加密等操作使之不可用，然后发出勒索通知。 传播：垃圾邮件传播、漏洞传播、捆绑传播、可移动存储介质或驱动器传播和社交网络传播。 3.病毒程序与蠕虫程序的主要区别有哪些？\n病毒的传播需要人为干预，而蠕虫则无需用户干预而自动传播，传统病毒主要感染计算机文件系统，蠕虫主要影响计算机系统和网络性能。\n4.什么是Rootkit？它与木马和后门有什么区别与联系？\n最初，内核套件Rootkit是攻击者用来修改UNIX操作系统和保持根（Root）权限且不被发现的工具，正是由于它是用来获得root后门访问的 kit工具包，所以被命名为\u0026quot;root\u0026rdquo;+\u0026ldquo;kit\u0026rdquo;。目前通常所说的Rootkit是指:一类木马后门工具，通过修改现有的操作系统软件，使攻击者获得访问权限并隐藏在计算机中。\nRootkit与木马、后门等既有联系又有区别。首先，Rootkit属于木马的范畴，它用恶意的版本替换修改现有操作系统软件来伪装自己，从而掩盖其真实的恶意的目的，而这种伪装和隐藏机制正是木马的特性。此外，Rootkit还作为后门行使其职能，各种Rootkit通过后门口令、远程Shell或其他可能的后门途径，为攻击者提供绕过检查机制的后门访问通道，这是后门工具的又一特性。Rootkit 强调的是强大的隐藏功能、伪造和欺骗功能，而木马、后门强调的是窃取功能、远程侵入功能。两者的侧重点不一样，两者结合起来则可以使得攻击者的攻击手段更加隐蔽、强大。\n5.什么是勒索软件？为什么勒索软件成为近年来数量增长最快的恶意代码类型？\n勒索软件是黑客用来劫持用户资产或资源，并以此为条件向用户勒索钱财的一种恶意软件。勒索软件通常会将用户系统中的文档、邮件、数据库、源代码及图片等多种文件进行某种形式的加密操作，使之不可用，或者通过修改系统配置文件，干扰用户正常使用系统的方法，使系统的可用性降低，然后通过弹出窗口、对话框或生成文本文件等方式向用户发出勒索通知，要求用户向指定账户汇款来获得解密文件的密码或者获得恢复系统正常运行的方法。\n勒索软件是近年数量增长最快的恶意代码类型，主要原因有以下几个：\n加密手段有效，解密成本高。勒索软件都采用成熟的密码学算法，使用高强度的对称和非对称加密算法对文件进行加密。除非在实现上有漏洞或密钥泄密，否则在没有私钥的情况下几乎没有可能解密。当受害者数据非常重要又没有备份的情况下，除了支付赎金没有别的方法去恢复数据，正是因为这点勒索者能源源不断地获取高额收益，推动了勒索软件的快速增长。\n使用电子货币支付赎金，变现快，追踪困难。几乎所有勒索软件支付赎金的手段都是采用比特币来进行的。比特币因为匿名、变现快、追踪困难，再加上大众比较熟知比特币，支付起来困难不是很大而被攻击者大量使用。可以说比特币帮助勒索软件解决了支付赎金的问题，进一步推动了勒索软件的发展。\n勒索软件即服务(Ransomware-as-a-server)的出现，降低了攻击的技术门槛。勒索软件服务化，开发者提供整套勒索软件的解决方案，从勒索软件的开发、传播到赎金的收取都提供完整的服务。攻击者不需要任何知识，只要支付少量的租金即可租赁他们的服务，从而开展勒索软件的非法勾当。这大大降低了使用勒索软件的技术门槛，推动了勒索软件的大规模爆发。\n6.恶意代码防范的基本措施包括哪些？\n安装维护防病毒软件、谨慎使用链接和附件、阻止弹出式广告、使用权限有限的账户、禁用外部媒体自动运行和自动播放功能、更改密码、保持软件更新、资料备份、安装或启用防火墙、使用反间谍软件工具、监控账户、避免使用公共 Wi-Fi等。\n7.试为所在学院或单位拟定恶意代码防治管理制度。\n关于计算机防毒软件的安装、使用和维护管理 连网计算机、重要系统的关键计算机要安装防计算机病毒软件，使用计算机时必须启动防计算机病毒软件，并定期或及时（随时）更新（升级）计算机病毒防范产品的版本。要使用国家规定的、具有计算机使用系统安全专用产品销售许可证的计算机防病毒产品。·重要计算机要定期进行计算机病毒检查，系统中的程序要定期进行比较测试和检查。 能用硬盘启动的，尽量不要使用U盘或光盘启动计算机。 使用光盘、U盘等移动存储设备，运行外来的系统和软件，下载软件时，要先进行计算机病毒检查，确认无计算机病毒后才可以使用，严禁使用未经清查的、来历不明的优盘、U盘等。 对新购进的计算机及设备，为预防计算机病毒的侵害，要组织专业人员检查后方可投入运行。 严禁使用盗版软件，特别是盗版的杀毒软件，严禁在工作计算机上安装和运行各类游戏软件。 随时关注软件提供商的安全漏洞公示，及时为系统安装最新的漏洞补丁。 关于网络接口的管理 本单位的计算机信息系统，如内部网与局域网，要与因特网隔离，如要使用因特网时，应断开本系统的内网连接，同时必须启动计算机病毒防火墙。 对获准上因特网的计算机，要设卡建档，责任到人。 单位内部的业务网(如生产过程控制系统等)要与本单位的办公计算机网分开，内网的电子邮件系统要与因特网的电子邮件系统分开。 在接入因特网时，严格控制软件下载，谨慎接收电子邮件;在接收电子邮件时，严禁打开来历不明的邮件附件，对邮件附件要先查毒再打开。 对服务器，特别是邮件服务器，要采用可靠的网络防计算机病毒软件，并对经过服务器的信息进行监控，防止计算机病毒通过邮件服务器扩散和传播。 关键服务器要尽量做到专机专用，特别是具有读写权限、身份确认功能的认证服务器一定要专用。 对共享的网络文件服务器，应特别加以维护，控制读写权限，尽量不在服务器上运行软件程序。 关于数据备份 系统的重要数据资源要采取措施加以保护。 关键数据要经常备份，或自动异地备份。 备份介质要由专人保管，并有检索标记。 关于恶意代码防范预报预警机制的建立 跟踪恶意代码发展的最新动态，特别是有严重破坏力的恶意代码的爆发日期或爆发条件。 将恶意代码爆发的情况、查杀的措施及时通知单位的所有部门及相关用户，进行有效防范。 关于计算机病毒防范的日常管理 随时注意计算机的各种异常现象，一旦发现异常应该立即用查毒软件仔细检查。 经常更新与升级防杀计算机病毒软件的版本。 对重点岗位的计算机要定点、定时、定人做查毒、杀毒巡检。 经常关心防杀计算机病毒厂商公布的计算机病毒情报，及时了解新产生的、传播面广的计算机病毒，并知道它们的发作特征和存在形态，及时分析计算机系统出现的异常是否与新的计算机病毒有关。 制定关于计算机病毒防范工作人员的培训要求。·制定关于计算机病毒防范工作的激励与奖惩。·制定关于计算机信息安全保密方面的要求条款等。 发现或受到恶意代码攻击时的管理措施 当出现恶意代码传染迹象时，立即隔离被感染的系统和网络，并进行处理，不应带毒继续运行。 发现恶意代码后，一般应利用杀毒软件清除文件中的计算机病毒;杀毒完成后，重启计算机，再次用防杀计算机病毒软件检查系统中是否还存在计算机病毒，并确定被感染破坏的数据是否确实完全恢复。 如果破坏程度比较严重，或感染的是重要数据文件，则自己不要盲目修复，而要请计算机病毒防范的专业人员来处理。 对于杀毒软件无法杀除的计算机病毒，应将计算机病毒样本送交有关部门，以供详细分析。 一旦发生计算机病毒疫情，要启动应急计划，采取应急措施，将损失降到最小。 恶意代码防治管理制度的实施与检查 要明确本单位恶意代码防范的责任体系，明确主管部门与责任人，主管部门与其他部门之间的关系，以及主要责任人的职责与权利等。 要建立检查监管小组，以定期常规检查与特别日期专项检查、集中检查与分散检查相结合的方式，对本单位的恶意代码防治管理制度的实施情况进行检查。 要接受各级政府及公安机关有关部门的监督、检查和指导。 要建立重大事故报告制度。对因恶意代码引起的计算机信息系统瘫痪、程序和数据严重破坏等重大事故，以及发生的计算机犯罪案件，应保护现场并及时向上级主管部门，以及各级公安机关有关监察部门和管理计算机病毒防范工作的政府职能部门报告。 第十四章（4） # 我国对于软件的知识产权有哪些法律保护途径？ 《计算机软件保护条例》 《中华人民共和国专利法》 商业秘密所有权保护 《中华人民共和国商标法》 《互联网著作权行政保护办法》 《信息网络传播权保护条例》 《移动互联网应用程序信息服务管理规定》 2.根据我国法律，软件著作权人有哪些权利？在日常学习和生活中，有哪些违反软件著作权的行为？\n软件著作权人享有如下权利：\n发表权，即决定软件是否公之于众的权利; 署名权，即表明开发者身份，在软件上署名的权利; 修改权，即对软件进行增补、删节，或者改变指令、语句顺序的权利; 复制权，即将软件制作一份或者多份的权利; 发行权，即以出售或者赠与方式向公众提供软件的原件或者复制件的权利; 出租权，即有偿许可他人临时使用软件的权利，但是软件不是出租的主要标的的除外; 信息网络传播权，即以有线或者无线方式向公众提供软件，使公众可以在其个人选定的时间和地点获 得软件的权利; 翻译权，即将原软件从一种自然语言文字转换成另一种自然语言文字的权利; 应当由软件著作权人享有的其他权利。 软件著作权人可以许可他人行使其软件著作权，并有权获得报酬。\n软件著作权人可以全部或者部分转让其软件著作权，并有权获得报酬。\n以下行为是违反软件著作权的行为：\n未经软件著作权人许可，发表、登记、修改或翻译其软件。 将他人软件作为自己的软件发表或者登记，在他人软件上署名或者更改他人软件上的署名。 未经合作者许可，将与他人合作开发的软件作为自己单独完成的软件发表或者登记。 复制或者部分复制著作权人的软件。 向公众发行、出租或通过信息网络传播著作权人的软件。 故意避开或者破坏著作权人为保护其软件著作权而采取的技术措施。 故意删除或者改变软件权利管理电子信息。 转让或者许可他人行使著作权人的软件著作权。 3.试述软件版权的概念。针对软件的版权，有哪些侵权行为？有哪些保护措施？\n软件版权属于知识产权的著作权范畴，具有知识产权的特征，即时间性，专有性和地域性。软件版权在法律上称为\u0026quot;计算机软件著作权\u0026quot;。属于著作权（知识产权）的一种。\n有以下侵权行为：\n软件盗版，对软件进行非法复制和使用。 逆向工程，即软件被非法修改或剽窃软件设计思想等。 信息泄露，即对软件载体及涉及数据的泄露。 有以下保护措施：\n基于硬件的保护技术如：对发行介质的保护、软件狗、可信计算芯片\n基于软件的保护技术如：注册验证、软件水印、代码混淆、软件加壳、虚拟机保护\n云环境下的云授权保护模型\n软件版权保护的目标有哪些？它与软件保护的目标有什么联系与区别？\n软件版权保护的目标有如下几点：\n防软件盗版，即对软件进行防非法复制和使用的保护。 防逆向工程，即防止软件被非法修改或剽窃软件设计思想等。 防信息泄露，即对软件载体及涉及数据的保护，如加密硬件、加密算法的密钥等。 与软件保护的联系与区别如下：\n软件版权保护的目标是软件保护目标的一个子集。软件保护除了确保软件版权不受侵害以外，还要防范针对软件的恶意代码感染、渗透、篡改和执行等侵害。 软件版权保护的许多措施同样可以应用于软件保护。 ","date":"22 December 2023","permalink":"/posts/reviews/%E7%A0%94%E7%A9%B6%E7%94%9F%E5%A4%8D%E4%B9%A0/%E8%BD%AF%E4%BB%B6%E5%AE%89%E5%85%A8%E6%8A%80%E6%9C%AF/problems/","section":"博客","summary":"南京大学软件学院软件安全复习资料","title":"《软件安全技术-机械工业出版社》复习笔记"},{"content":"","date":"22 December 2023","permalink":"/posts/reviews/%E7%A0%94%E7%A9%B6%E7%94%9F%E5%A4%8D%E4%B9%A0/%E8%BD%AF%E4%BB%B6%E5%AE%89%E5%85%A8%E6%8A%80%E6%9C%AF/","section":"博客","summary":"软件安全技术是保护计算机系统、网络和数据免受未经授权的访问、破坏、窃取或损坏的措施和实践，旨在确保数据完整性、机密性和可用性。","title":"软件安全技术"},{"content":"名词解释 # 软件工程 # 软件工程是 应用系统的、规范的、可量化的方法来开发、运行和维护软件，即将工程应用到软件 对以上各种方法的研究 抽象与分解 # 分解是横向将系统分割为几个相对简单的子系统以及子系统之间的关系，分解之后每次只需关注经过抽象的相对简单的子系统以及相互间的关系，从而降低了复杂度 抽象是在纵向上聚焦各自系统的接口，抽象可以分离接口与实现过程，让人更好的关注系统本质，减低复杂度 分解和抽象是一起使用的，可以将系统分解为子系统，又通过抽象分离接口与实现 描述软件体系结构的分层风格 # 根据不同的抽象层次将系统组织为层次式结构，每个层次被建立为一个部件，不同的部件之间通常用程序调用的方式进行连接\n优点：设计机制清晰易于理解。支持并行开发。更好的可复用性和内部可修改性 缺点：交互协议难以修改。性能损失。难以确定层次数量和粒度。 描述软件体系结构的面向对象风格 # 该风格将系统组织为多个独立的对象，每个对象封装其内部的数据，并基于数据对外提供服务，不同的对象之间通过协作机制共同完成系统任务。\n优点：内部实现的可修改性。易开发、易理解、易服用的结构组织。 缺点：无法消除接口的耦合。印记耦合。面向对象编程中的副作用。 重构 # 修改软件系统的严禁方法，在不改变代码外部表现的情况下改变其内部结构。重构发生在新功能增加完成后，用于消除新功能带来的负面影响\n软件需求 # 需求就是用户的一种期望，用户为了解决问题或达到某些目标所需要具备的条件和能力。 系统或系统部件为了满足合同、标准、规范或其他正式文档所规定的要求所需要具备的条件或能力 对上述的一个条件或能力的一种文档化表达 功能性需求和非功能需求 # 功能性需求是软件系统中最常见、最主要和最重要的需求，同时它也是最为复杂的需求，是一个软件产品能够解决用户问题和产生价值的基础 非功能需求主要包括性能需求、质量需求、对外接口、约束、数据需求等。 其中质量需求是非功能需求中影响最大的需求。 可用性、可靠性、可维护性、可移植性、安全性、易用性 需求的三个层次分别描述了什么 # 业务需求 系统建立的战略出发点，表现为高层次的目标，它描述了组织为什么要开发系统； 为了满足用户的业务需求，需求工程师需要描述系统高层次的解决方案，定义系统应具备怎样的特性 高层次解决方案与系统特性说明了系统为用户提供的各项功能，限定了系统的范围，帮助用户和开发者确定系统的边界 用户需求 执行实际工作的用户对系统所能完成的具体任务的期望，描述了系统能够帮助用户做些什么 系统级需求 用户对系统行为的期望，每个系统级别需求反映了一次外界与系统的交互行为，或者系统的一个实现细节； 直接映射为系统行为，定义了系统中需要实现的功能，描述了开发人员需要实现什么 一系列系统级需求联系在一起满足一个用户需求，进而满足业务需求 软件配置管理活动 # 标识符配置：首先要确定有哪些配置项需要被保存和管理。其次要给配置项确定标识，设置唯一的id。最后要详细说明配置项的特征。 版本管理：为每一个刚纳入配置管理的配置项赋予一个初始的版本号，并在发生变更时更新版本号 变更管理：已经纳入配置管理中的配置项发生变化时，需要依据变更控制过程进行处理 配置审计：配置审计的目标是确定一个项目满足需求的功能和物理特征的程度，确保软件开发工作按照需求规格和设计特征进行，验证配置项的完整性、正确性、一致性和可跟踪性 状态报告：配置状态报告是要标识、收集和维持演化中的配置状态信息，也就是在动态演化者的配置项信息及其度量取快照 软件发布管理：软件发布管理是将软件配置项发布到开发活动之外，例如发布给客户 单元测试、集成测试、系统测试 # 单元测试：测试一个单元接口，是对程序单元进行正确性检验的测试工作 集成测试：测试多个单元接口，即对程序模块一次性或采用增量方式组装起来，对系统的接口进行正确性检验的测试工作 系统测试：测试全部单元接口，测试关注整个系统的行为 单元测试和集成测试更加关注技术上的正确性，重在发现设计缺陷和代码缺陷。系统测试更关注不符合需求的缺陷和需求自身的内在缺陷 持续集成 # 集成是将所有模块组合起来形成整个软件原型系统\n持续集成是一种增量性集成，但其提倡尽早集成（不需等到模块开发完才集成，开发之初就利用stub集成）和频繁集成（每次完成一些开发任务，就用完成的程序替换）。\n缺陷、错误、失效是什么？有什么关系 # 错误：产生不正确的认为行为。人为的原因导致一个不正确的结果。它可以是程序内部的错误，也可以是文档内的错误。甚至是环境方面的问题 缺陷：程序或者软件中不正确的步骤、过程或者数据定义等。比如错误的语句或者错误的标量定义。缺陷是错误的具体表现，可以是不正确的文档、程序段以及指令或者数据定义 失效：软件系统或单元无法实现需求文档中规定的功能特性或者非功能特性。或者说单元/系统产生的结果与期望交付的服务或者结果存在变差。外部的失效/失败是内部缺陷在执行测试软件的外部反映。它是规范说明的期望值与实际观察到的值、现象等存在偏差。比如不正确的系统反映、系统崩溃、系统死机等 关系：人为造成的错误引入到软件工作产品中就编程了缺陷，或者环境因素导致软件中存在瑕疵。加入存在缺陷的代码，进入了运行，这些缺陷就可能会导致系统的不正常，或者导致系统的失效和失败。 4个体系结构视角 # 组合视角：关注功能分解和运行时分解、子系统的构造，构建的复用 逻辑视角：关注静态结构、类型与实现的复用 依赖视角：关注互联、分享 信息视角：关注持久化信息 接口视角：关注服务的定义、服务的访问 质量模型的可用性 # 易学性：新手用户容易学习，能够很快使用系统 效率：熟练用户使用系统完成任务的速度 易记性：以前使用过软件系统的用户，能够有效记忆或者快速地重新学会使用该系统。 出错率：用户使用系统时，会犯多少错，错误有多严重，以及能够从错误中很容易地恢复 主观满意度：让用户有良好的体验 软件质量保障三种手段 # 评审：由作者之外的其他人来检查产品问题 软件测试：主要包括单元、集成、系统测试 质量度量：用数字量化的方式描述软件产品 科学与工程的区别 # 科学是关于事物的基本原理和事实的有组织、有系统的知识。科学的主要任务是研究世界万物变化的客观规律，他解决“为什么”的问题 工程是自然科学或各种专门技术应用到生产部门中而形成的各种学科的总称，其目的在于利用改造自然学科来为人类服务。通过工程可以生产和开发出对社会有用的产品。科学可以作为工程的指导知识，譬如软件工程的指导知识是计算机科学。 软件系统的生命周期 # 软件系统的生命周期模型描述软件开发过程中各种活动如何执行的模型。有瀑布模型、原型模型、增量模型、迭代模型、螺旋模型等。 以瀑布模型为例，它要求软件开发分为需求分析、软件设计、软件构造、软件测试、软件交付与维护阶段，每个阶段要编写相应的文档，且只有经过审核才能进入下一个阶段。 黑盒与白盒测试的差别 # 黑盒测试是基于规格的，将测试对象堪称一个黑盒子，完全基于输入和输出数据来判定测试对象的正确性。 白盒测试是基于代码的，将测试对象看作透明，按照测试对象内部的程序结构来设计测试用例进行测试。 黑盒测试的方法有：等价类划分、边界值分析、决策表 白盒测试的方法有：语句覆盖、条件覆盖、路径覆盖 白优：覆盖率高、发现的缺陷数量较多 白缺：测试开销大、不能检验需求规格 黑优：测试效率高、可以检验规格需求 黑缺：覆盖率低、发现缺陷数量较少 策略模式 # 策略模式就是用来封装算法的，但是在实践中，我们发现可以用它来封装几乎任何类型的规则，只要在分析过程中听到需要在不同时间应用不同业务规则，就可以考虑使用策略模式处理这种变化的可能性\n适用的场景：\n如果在一个系统里有很多类，他们之间的区别仅在于它们的行为，那么使用策略模式可以动态地让一个对象在许多行为中选择一种行为\n一个系统需要动态地在许多行为中选择一种行为\n如果一个对象有很多行为，如果不用恰当的模式，这种行为就只好使用多重的条件选择语句来实现\n优点：\n代码可扩展性，符合面向接口编程ISP\n提供了可以替换继承关系的方法\n避免多重条件转移语句\n缺点：\n客户端必须知道所有策略类，并自行决定使用哪一个策略类。这就意味着客户端必须理解这些算法的区别，以便适时选择恰当的算法类\n策略模式造成很多的策略类，每个具体策略类都会产生一个新类\n迪米特法则 # 一个软件实体应尽可能少地与其他实体发生相互作用\nATM机取款任务为主题，编写取款的用例 # 取款处理 ID:WithDrawProcess 参与人：用户（取款人），目标是能够尽可能快速地完成取款业务 触发条件：用户将银行卡插入卡槽 前置条件：所插入磁卡是本银行发行且有效状态的银行卡 后置条件：更新账户信息，准确读出取款陷阱至取款槽 正常流程：1.系统对插入卡槽的银行卡进行识别 2.系统显示用户操作界面 3.用户选择取款业务，系统上显示卡上余额 4.用户输入取款金额，系统更新账户信息，并读出相应现金 5.用户选择退卡，系统退回主界面，并动态更新银行基金宣传信息 扩展流程：1a.无效磁卡，吐出银行卡，并提示错误信息 3a.非法字符，系统提示错误并拒绝输入 3b.用户输入金额超过余额，系统提示并拒绝提款 4a.用户提款完成后，系统语音提示用户退卡，防止丢失 特殊需求：1.用户所有需求在3s内得到相应 2.初次使用ATM取款的用户也能快速完成取款业务 代码 # 判断思路 # 内聚和耦合。 面向对象设计原则。OCP、SRP 2013年-1 # public class RationalNumber{ private int dividend; private int divisor; public RationalNumber(int dividend,int divisor){ if(divisor==0)throw new illegalArgumentException(\u0026#34;分母不能为0\u0026#34;); this.dividend=dividend; this.divisor=divisor; } getter setter public RationalNumber SimpleFraction(RationNumber num){ int x=num.getDividend(); int y=num.getDivisor(); int largestCommonDivisor=getLargestCommonDivisor(x,y); return new x/largestCommonDivisor,y/largestCommonDivisor } public int getLargestCommonDivisor(int x,int y){ x=Math.abs(x); y=Math.abs(y); int z=y; whils(x%y!=0){ z=x%y; x=y; y=z; } return z; } public int getLeastCommonMultiple(int x,int y){ return (x*y)/getLargestCommonDivisor(x,y); } //加法 public RationalNumber add(RaionalNumber num){ int dividendOfNum1=this.getDividend(); int divisorOfNum1=this.getDivisor(); int dividendOfNum2=num.getDividend(); int divisorOfNum2=num.getDivisor(); int fenmu=getLeastCommonMultiple(divisorOfNum1,divisorOfNum2); dividendOfNum1*=(fenmu/divisorOfNum1); dividendOfNum2*=(fenmu/divisorOfNum2); int fenzi=dividendOfNum1+dividendOfNum2; RationalNUmber result = simpleFraction(new RationalNumber(fenzi,fenmu)); return result; } } 2013年-2 # public class Square{ Rectangle rect; double edge; public Squre (double edge){ this.rect.setWidth(edge); this.rect.setLength(edge); } public double getArea(){ return rect.getArea(); } } // 单一职责原则 public class Square{ public void draw(){ // 依赖GUI对象和GeometricSquare对象绘图 } } public GeometricSquare{ public double area(){ //求面积 } } 2014年-1 # public interface Factory{ public Employee createEmp(String name); } public class EmployeeFactory implements Factory{ public Employee createEmp(String name){ return new SalariedEmployee(name); } } ... public class Department{ private List\u0026lt;Employee\u0026gt; employeeList; private Factory factory; ... public Employee addEmployee(String name){ Employee emp=factory.createEmp(name); emp.setDepartment(this); employeeList.add(emp); update(); } } 2015年1 # 时间内聚考初始化 2017年 # private Double getTotalSum(List amounts){ Double totalToPay=0.00; Interator amountSIeterator = amount.iterator(); while(amountsIterator.hasNext){ Amount amount=(Amount) amountSIterator.next(); if(!cancelstatuses.contains(amount.getStatus())){ totalTopay+=amount.doubleValue(); } } return new Double(totalToPay); } private Double getTotalSumExcludeCancelAmount(List amout){ List newAmount=filter(amounts); return getTotalSum(newAmounts); } private List filter(List amounts){ List res=new ArrayList(); Interator amountSIeterator = amount.iterator(); while(amountsIterator.hasNext()){ Amount amount=(Amount)amountIterator.next(); if(!amount.getIsToCancel()){ res.add(amount); } } return res; } 2019年-1 # public class Sales{ SaleList salesList = new SaleList(); ... public double getCommdityPriceById(long CommidityId){ return saleList.getCommdityPrice(CommidityId); } } public class SaleList{ HashMap\u0026lt;Integer, SalesLineItem\u0026gt;salesItemMap = new HashMap\u0026lt;Integer, SalesLineItem\u0026gt;(); public double getCommdityPriceBy(long CommidityId){ SalesLineItem item = salesItemMap.get(id); return item.getCommodityPrice(); } } public class SaleLineList{ Commodity commodity; ... public double getCommdityPriceBy(long CommidityId){ return commodity.getPrice(); } } public class Commodity{ double price; ... public double getPrice(){ return price; } } 2019年-2 # interface FlyBehavior{ public void fly(); } class SubSonicFly implements FlyBehavior{ @Override public void fly(); } class SuperSonicFly implements FlyBehavior{ @Override public void fly(); } interface TakeOffBehavior{ public void takeOff(); } class VerticalTakeOff implements TakeOffBehavior{ @Override public void takeOff(); } class LongDistanceTakeOff implements TakeOffBehavior{ @Override public void takeOff(); } abstract class AirCraft{ protected FlyBehavior flyBehavior; protected TakeOffBehavior takeOffBehavior; public void fly(){ flyBehavior.fly(); } public void takeoff(){ flyBehavior.takeoff(); } } class AirPlane extends AirCraft{ public AirPlane(){ flyBehavior=new SubSonicFly(); takeOffBehavior=new VerticalTakeoff(); } } ... 2020年 # public interface ticket{ public int getPrice(); } public class StudentTicket implements Ticket{ @Override public int getPrice(){ return 30; } } ... ","date":"21 December 2023","permalink":"/posts/reviews/%E7%A0%94%E7%A9%B6%E7%94%9F%E5%A4%8D%E4%B9%A0/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E4%B8%8E%E8%AE%A1%E7%AE%97/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E4%B8%8E%E8%AE%A1%E7%AE%97%E8%80%83%E8%AF%95%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/","section":"博客","summary":"南京大学软件学院软件工程与计算复习资料","title":"《软件工程与计算》考试复习笔记"},{"content":"CUE 是一种开源数据验证语言和推理引擎，源于逻辑编程。虽然该语言不是通用编程语言，但它有许多应用，如数据验证、数据模板、配置、查询、代码生成甚至脚本编写。推理引擎可用于验证代码中的数据，或将其作为代码生成管道的一部分。\ncuelang with kubevela # 在 KubeVela 中，Component 最终被渲染为 Kubernetes 平台上的实际资源对象，通常是一组 Kubernetes 资源，例如 Deployment、Service 等，这取决于 Component 的定义中所包含的信息。\n具体而言，KubeVela 的核心引擎会根据 CueLang 文件中的定义，将 Component 转换为一个或多个 Kubernetes 资源的 YAML 表示。这些资源的种类和内容是根据 CueLang 文件中的输出部分定义的。\n以下是一个简单的示例，展示一个 CueLang 文件定义一个简单的 Component，并将其渲染为一个 Deployment 资源：\nparameter: { image: string port: int } output: { apiVersion: \u0026#34;apps/v1\u0026#34; kind: \u0026#34;Deployment\u0026#34; metadata: name: context.name spec: { selector: matchLabels: app: context.name template: { metadata: labels: app: context.name spec: { containers: [{ name: context.name image: parameter.image ports: [{ containerPort: parameter.port }] }] } } } } 上述 CueLang 文件定义了一个 Deployment，其中包含一个容器，该容器的镜像和端口由参数指定。在渲染过程中，KubeVela 将根据这个定义生成相应的 YAML 表示，例如：\napiVersion: apps/v1 kind: Deployment metadata: name: myapp spec: selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: - name: myapp image: nginx:latest ports: - containerPort: 80 webservice\n# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file. # Definition source cue file: vela-templates/definitions/registry/webserver.cue apiVersion: core.oam.dev/v1beta1 kind: ComponentDefinition metadata: annotations: definition.oam.dev/description: webserver was composed by deployment and service name: webserver namespace: vela-system spec: schematic: cue: template: | output: { apiVersion: \u0026#34;apps/v1\u0026#34; kind: \u0026#34;Deployment\u0026#34; spec: { selector: matchLabels: \u0026#34;app.oam.dev/component\u0026#34;: context.name template: { metadata: labels: \u0026#34;app.oam.dev/component\u0026#34;: context.name spec: containers: [{ name: context.name image: parameter.image if parameter[\u0026#34;cmd\u0026#34;] != _|_ { command: parameter.cmd } if parameter[\u0026#34;env\u0026#34;] != _|_ { env: parameter.env } if context[\u0026#34;config\u0026#34;] != _|_ { env: context.config } ports: [{ containerPort: parameter.port }] if parameter[\u0026#34;cpu\u0026#34;] != _|_ { resources: { limits: cpu: parameter.cpu requests: cpu: parameter.cpu } } }] } } } // workload can have extra object composition by using \u0026#39;outputs\u0026#39; keyword outputs: service: { apiVersion: \u0026#34;v1\u0026#34; kind: \u0026#34;Service\u0026#34; spec: { selector: \u0026#34;app.oam.dev/component\u0026#34;: context.name ports: [ { port: parameter.port targetPort: parameter.port }, ] } } parameter: { image: string cmd?: [...string] port: *80 | int env?: [...{ name: string value?: string valueFrom?: secretKeyRef: { name: string key: string } }] cpu?: string } workload: definition: apiVersion: apps/v1 kind: Deployment Deployment\napiVersion: apps/v1 kind: Deployment spec: selector: matchLabels: app.oam.dev/component: context.name template: metadata: labels: app.oam.dev/component: context.name spec: containers: - name: context.name image: parameter.image ports: - containerPort: parameter.port # 根据条件判断是否包含 command {{- if parameter[\u0026#34;cmd\u0026#34;] != _|_ }} command: parameter.cmd {{- end }} # 根据条件判断是否包含 env {{- if parameter[\u0026#34;env\u0026#34;] != _|_ }} env: parameter.env {{- end }} # 根据条件判断是否包含 config {{- if context[\u0026#34;config\u0026#34;] != _|_ }} env: context.config {{- end }} # 根据条件判断是否包含 resources {{- if parameter[\u0026#34;cpu\u0026#34;] != _|_ }} resources: limits: cpu: parameter.cpu requests: cpu: parameter.cpu {{- end }} apiVersion: v1 kind: Service spec: selector: app.oam.dev/component: context.name ports: - port: parameter.port targetPort: parameter.port Resources # 官网 ","date":"17 December 2023","permalink":"/posts/language/dsl/cuelang/","section":"博客","summary":"CUE 是一种开源数据验证语言和推理引擎，源于逻辑编程。虽然该语言不是通用编程语言，但它有许多应用，如数据验证、数据模板、配置、查询、代码生成甚至脚本编写。推理引擎可用于验证代码中的数据，或将其作为代码生成管道的一部分。","title":"cuelang 简介"},{"content":" ","date":"17 December 2023","permalink":"/posts/language/dsl/","section":"博客","summary":"领域特定语言（DSL）是一种用于特定领域或任务的编程语言，针对特定需求进行优化。DSL具有专门的语法和语义，使其更容易用于解决特定领域的问题，提高了代码的可读性和表达力。DSL广泛应用于领域如数据分析、硬件描述、自然语言处理等，有助于提高开发效率和降低错误风险。","title":"Domain Specific Language"},{"content":"","date":"17 December 2023","permalink":"/tags/dsl/","section":"Tags","summary":"","title":"Dsl"},{"content":"","date":"16 December 2023","permalink":"/tags/data/","section":"Tags","summary":"","title":"Data"},{"content":"Why data preprocessing? # 现实世界中的数据存在问题 不完整：缺少属性值，缺少感兴趣的某些属性，或仅包含聚合数据 有噪声：包含错误或异常值 不一致：包含代码或名称上的差异 没有高质量的数据，就没有高质量的挖掘结果！ 质量决策必须基于质量数据 数据仓库需要对质量数据进行一致的集成 Major Tasks in Data Preprocessing\nData cleaning 数据清理，填充缺失值，平滑嘈杂数据，识别或删除异常值，并解决不一致性 Data integration 数据集成，集成多个数据库、数据立方体或文件 Data transformation 数据转换，归一化和聚合 Data reduction 数据缩减，以体积较小的形式获得减少表示，但产生相同或相似的分析结果 Data discretization 数据离散化，针对数值数据尤其重要的数据缩减的一部分 Forms of data preprocessing\nData cleaning # Data cleaning tasks\n填充缺失值 识别异常值和平滑嘈杂数据 更正不一致的数据 Missing Data # 数据不总是可用 例如，许多元组对于几个属性没有记录的值，例如销售数据中的客户收入 缺失数据可能是由于 设备故障 与其他记录的数据不一致，因此被删除 由于误解而没有输入数据 在输入时可能不认为某些数据重要 不注册数据的历史或更改 可能需要推断缺失的数据。 How to Handle Missing Data?\n忽略元组：通常在缺少类标签时执行（假设分类中的任务）——当每个属性的缺失值的百分比变化很大时，效果不佳。 手动填写缺失值：繁琐 + 不可行？ 使用全局常数填充缺失值：例如，“unknown”，一个新的类别？ 使用属性均值填充缺失值 使用同一类别中所有样本的属性均值填充缺失值：更智能 使用最有可能的值填充缺失值：基于推理的方法，如贝叶斯公式或决策树 Noisy Data # 噪声：测量变量中的随机错误或方差 不正确的属性值可能是由于 错误的数据收集工具 数据输入问题 数据传输问题 技术限制（例如，输入缓存容量） 命名约定的不一致性 需要数据清理的其他数据问题 重复的记录 不完整的数据 不一致的数据 How to Handle Noisy Data?\n分箱法： 首先对数据进行排序并分为（等深度）箱 然后可以通过箱均值、箱中位数、箱边界等进行平滑。 聚类 检测并删除异常值 计算机和人工检查的组合 检测可疑值并由人类检查 回归 通过将数据拟合到回归函数中平滑数据 简单的离散化方法：分箱\n等宽（距离）分区： 将范围等分为 $N$ 个相等大小的间隔：均匀网格 最直接的方法：如果 $A$ 和 $B$ 是属性的最低和最高值，间隔的宽度将为：$W = (B-A)/N$。 但是异常值可能主导呈现，不处理偏斜的数据效果不佳。 等深（频率）分区： 将范围划分为 N 个间隔，每个间隔包含大致相同数量的样本 良好的数据缩放，管理分类属性可能会有些棘手。 Binning Methods for Data Smoothing\n按价格（以美元计）排序的数据: 4, 8, 9, 15, 21, 21, 24, 25, 26, 28, 29, 34 划分为（等深）箱： Bin 1: 4, 8, 9, 15 Bin 2: 21, 21, 24, 25 Bin 3: 26, 28, 29, 34 通过箱均值进行平滑： Bin 1: 9, 9, 9, 9 Bin 2: 23, 23, 23, 23 Bin 3: 29, 29, 29, 29 通过箱边界进行平滑： Bin 1: 4, 4, 4, 15 Bin 2: 21, 21, 25, 25 Bin 3: 26, 26, 26, 34 Cluster Analysis\nRegression\nData integration and transformation # Data Integration # 数据集成\n将来自多个来源的数据合并到一个一致的存储中\n模式集成 集成来自不同来源的元数据 实体识别问题：从多个数据源中识别真实世界的实体，例如，$A.cust-id \\eq B.cust-#$ 检测和解决数据值冲突 对于相同的真实世界实体，来自不同来源的属性值不同 可能的原因：不同的表示，不同的比例，例如，米制与英制 在数据集成中处理冗余数据\n在集成多个数据库时，经常会出现冗余数据 同一属性在不同数据库中可能有不同的名称 一个属性可能是另一个表中的“派生”属性，例如，年收入 通过相关性分析可能能够检测到冗余数据 仔细地集成来自多个来源的数据可能有助于减少/避免冗余和不一致，并提高挖掘速度和质量 Data Transformation # 数据转换\n平滑：从数据中去除噪音 聚合：总结、数据立方体构建 泛化：概念层次爬升 归一化：缩放到一个小范围内 min-max normalization z-score normalization normalization by decimal scaling 属性/特征构建 从给定属性构建新属性 Data Transformation: Normalization\nmin-max normalization\n一个属性：收入，其值从 12000 到 98000 如果我们想要将值 73000 映射到一个新范围 [0.0,1.0] 那么 (73000-12000)/(98000-12000)(1.0-0)=0.716 z-score normalization\n如果属性 \u0026ldquo;income\u0026rdquo; 的平均值为 54000，标准偏差为 16000 那么 (73000-54000)/16000=1.225 normalization by decimal scaling\n给定一个属性 A，其值从 -986 到 987，最大的绝对值是 987，所以我们得到 $j=3$ (即 1000) -986 将被转换为 -0.986 Data reduction # 数据减少策略\n仓库可能存储数千兆字节的数据：对完整数据集运行复杂的数据分析/挖掘可能需要很长时间 数据减少：获得数据集的减小表示，其体积要小得多，但产生相同（或几乎相同）的分析结果 数据减少策略 数据立方体聚合 降维 数值减少 离散化和概念层次生成 Data Cube Aggregation # 数据立方体聚合\n数据立方体的最低级别 与关注的个体的聚合数据 例如，电话呼叫数据仓库中的客户。 数据立方体中的多个级别 进一步减小要处理的数据大小 引用适当的级别 使用足以解决任务的最小表示 Dimensionality Reduction # 降维\n特征选择（即属性子集选择）： 选择一组最小的特征，使得给定这些特征的值的情况下不同类别的概率分布尽可能接近原始分布给定所有特征的值的情况 减少模式中的模式数量，更容易理解 有 $d$ 个特征，有 $2^d$ 个可能的子特征 如果特征数量太大，则测试所有这些子集可能是不可能的 因此，通常使用启发式方法来解决问题 Heuristic Feature Selection Methods # 几种启发式特征选择方法： 在特征独立性假设下选择最佳单个特征：通过显著性检验选择。 最佳逐步特征选择： 首先选择最佳的单个特征 然后下一个最佳特征条件于第一个，\u0026hellip; 逐步特征消除： 反复消除最差的特征 最佳组合特征选择和消除 决策树：ID3、C4.5 等。 Example of Decision Tree Induction\nData Compression # 数据压缩\n字符串压缩 有广泛的理论和调优的算法 通常是无损的 但只能在不扩展的情况下进行有限的操作 音频/视频压缩 通常是有损压缩，具有渐进改进 有时可以在不重建整个信号的情况下重建信号的小片段 Numerosity Reduction # 数值压减\n参数方法 假设数据适合某个模型，估计模型参数，仅存储参数，并丢弃数据（除了可能的异常值） 回归 对数线性模型：在适当的边缘子空间上获得 m-D 空间中的点的值的乘积 非参数方法 不假设模型 主要有：直方图、聚类、抽样 Regression and Log-Linear Models # 回归和对数线性模型\n线性回归：对数据建模以适应一条直线\n通常使用最小二乘法拟合直线 多元回归：允许将响应变量 Y 建模为多维特征向量的线性函数\n对数线性模型：逼近离散多维概率分布\n线性回归： 两个参数， $\\alpha$ 和 $\\beta$ 指定该线，并且通过使用现有数据的 $Y_1$, $Y_2$, …, $X_1$, $X_2$, … 的已知值对它们进行估计 多元回归： 许多非线性函数可以转换为上述形式 对数线性模型：\n联合概率的多维表通过较低阶表的乘积来逼近 概率： 直方图 # 一种流行的数据减少技术 将数据划分为桶，并存储每个桶的平均值（和） 在一维上使用动态规划可以最优化地构建 与量化问题相关 聚类 # 将数据集分为簇，可以仅存储簇表示 如果数据聚类，这可能非常有效，但如果数据“涂抹”则效果可能不好 可以具有层次聚类并存储在多维索引树结构中 有许多聚类定义和聚类算法的选择 抽样 # 允许挖掘算法在与数据规模的潜在子线性复杂性下运行 选择数据的代表子集 简单的随机抽样在存在偏斜时可能性能非常差 开发自适应抽样方法 分层抽样： 近似每个类别（或感兴趣的子群体）在整个数据库中的百分比 与偏斜的数据一起使用 Discretization and concept hierarchy generation # 离散化和概念层次生成\n离散化\n三种类型的属性：\nNominal — 来自无序集的值 Ordinal — 来自有序集的值 Continuous — 实数 离散化:\n将连续属性的范围划分为间隔 一些分类算法仅接受分类属性。 通过离散化减小数据大小 为进一步分析做准备 离散化\n通过将属性范围划分为间隔，减少给定连续属性的值的数量。然后可以使用间隔标签替换实际数据值。 概念层次\n通过收集并替换低层次概念（例如年龄属性的数值）为高层次概念（例如年轻、中年、老年），减少数据量。 Discretization and concept hierarchy generation for numeric data\n用于数值数据的离散化和概念层次生成\n分箱（参见前面的部分） 直方图分析（参见前面的部分） 聚类分析（参见前面的部分） 基于熵的离散化（稍后介绍） 自然分区的分割 Entropy-Based Discretization\n给定样本集 $S$，如果使用边界 $T$ 将 $S$ 划分为两个间隔 $S_1$ 和 $S_2$，分区后的熵为\n选择使熵函数在所有可能的边界上最小的边界作为二元离散化。\n该过程递归应用于获得的分区，直到满足某些停止准则，例如\n实验证明这可能减少数据量并提高分类准确性\nSegmentation by natural partitioning\n自然分区的分割\n3-4-5 规则可用于将数值数据分割为相对均匀的“自然”间隔。\n如果一个区间包含最显著数字上的 3、6、7 或 9 个不同的值，则将范围划分为 3 个等宽区间。 如果它包含最显著数字上的 2、4 或 8 个不同的值，则将范围划分为 4 个区间。 如果它包含最显著数字上的 1、5 或 10 个不同的值，则将范围划分为 5 个区间。 Concept hierarchy generation for categorical data\n用于分类数据的概念层次生成\n在模式级别或专家级别上由用户或专家显式指定属性的部分排序 通过显式的数据分组对层次的一部分进行指定 对属性集的一组属性进行指定，但不指定它们的部分排序 仅对一部分属性进行指定 Specification of a set of attributes\n指定一组属性\n可以基于给定属性集中每个属性的不同值的数量自动生成概念层次结构 具有最多不同值的属性放置在层次结构的最低级别。 Summary # Data preparation is a big issue for both warehousing and mining Data preparation includes Data cleaning and data integration Data reduction and feature selection Discretization A lot of methods have been developed but still an active area of research ","date":"16 December 2023","permalink":"/posts/reviews/%E7%A0%94%E7%A9%B6%E7%94%9F%E5%A4%8D%E4%B9%A0/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%8E%E7%9F%A5%E8%AF%86%E5%8F%91%E7%8E%B0/data-preprocessing/","section":"博客","summary":"现实世界中的数据存在不完整、有噪声、不一致的问题，没有高质量的数据，就没有高质量的挖掘结果！质量决策必须基于质量数据，数据仓库需要对质量数据进行一致的集成。","title":"Data Preprocessing"},{"content":"What is a data warehouse? # 数据仓库以许多不同的方式定义，但并不严格。\n一个与组织的运营数据库分开维护的决策支持数据库 通过提供一个坚实的平台，整合历史数据，支持信息处理以进行分析。 A data warehouse is a subject-oriented, integrated, time-variant, and nonvolatile collection of data in support of management\u0026rsquo;s decision-making process. —- W. H. Inmon\n数据仓库是一个面向主题的、集成的、时变的和非易失性的数据集合，支持管理决策过程。\n数据仓库：构建和使用数据仓库的过程\nSubject-Oriented # 围绕主要主题组织，如客户、产品、销售。 侧重于为决策者建模和分析数据，而不是日常运营或交易处理。 通过排除在决策支持过程中无用的数据，提供关于特定主题问题的简单而简明的视图。 Integrated # 通过整合多个异构数据源构建 关系数据库、平面文件、在线交易记录 应用数据清理和数据集成技术。 确保不同数据源之间的命名约定、编码结构、属性度量等的一致性 例如，酒店价格：货币、税收、包括早餐等。 将数据移动到仓库时进行转换。 Time Variant # 数据仓库的时间范围明显长于操作系统的时间范围。 操作数据库：当前值数据。 数据仓库数据：提供历史透视的信息（例如，过去5-10年）。 数据仓库中的每个关键结构 包含明确或隐含的时间元素 但操作数据的关键可能包含或不包含时间元素。 Non-Volatile # 从操作环境转换而来的数据的物理分离存储。 数据仓库环境中不发生数据的操作更新。 不需要事务处理、恢复和并发控制机制 仅需要两个数据访问操作：数据的初始加载和数据的访问。 Data Warehouse vs. Heterogeneous DBMS # Traditional heterogeneous DB integration（传统的异构数据库集成）:\n在异构数据库之上构建包装器/中介 查询驱动的方法 当向客户站点提出查询时，使用元字典将查询转换为适用于涉及的各个异构站点的查询，并将结果集成到全局答案集中 复杂的信息过滤，争夺资源 Data warehouse: 更新驱动，高性能\n信息来自异构来源，在预先集成并存储在仓库中，以供直接查询和分析。 OLTP vs OLAP # OLTP (On-Line Transaction Processing，联机事务处理)\n传统关系数据库管理系统的主要任务 日常运营：采购、库存、银行业务、制造、工资、注册、会计等。 OLAP (On-Line Analytical Processing，联机分析处理)\n数据仓库系统的主要任务 数据分析和决策制定 Distinct features (OLTP vs. OLAP):\nUser and system orientation: customer vs. market Data contents: current, detailed vs. historical, consolidated Database design: ER + application vs. star + subject View: current, local vs. evolutionary, integrated Access patterns: update vs. read-only but complex queries Why Separate Data Warehouse? # 两个系统都具有高性能 DBMS — 针对OLTP进行调整：访问方法、索引、并发控制、恢复 仓库 — 针对OLAP进行调整：复杂的OLAP查询、多维视图、汇总。 不同的功能和不同的数据： 缺失的数据：决策支持需要操作数据库通常不维护的历史数据 数据汇总：决策支持需要从异构来源聚合（汇总、摘要）的数据 数据质量：不同的来源通常使用不一致的数据表示、代码和格式，需要协调。 A multi-dimensional data model # 从表格和电子表格到数据立方体\n数据仓库基于多维数据模型，以数据立方体的形式查看数据。 数据立方体（如销售）允许数据在多个维度上建模和查看 维度表，如项目（项目名称、品牌、类型）或时间（日、周、月、季度、年） 事实表包含度量（如销售额）和与每个相关维度表的键 Multi-Dimensional Data Model（多维数据模型，立方体）\n立方体：立方体的晶格\n在数据仓库文献中，n-D基本立方体被称为基本小立方体。最顶层的0-D小立方体，它包含最高级别的摘要，被称为尖点小立方体。小立方体的晶格形成一个数据立方体。\n数据仓库的概念建模 # 建模数据仓库：维度和度量\nStar schema: 星型模式，中间有一个事实表与一组维度表相连 Example of Star Schema\nSnowflake schema: 雪花模式，星型模式的一种改进，其中一些维度层次被规范化为一组较小的维度表，形成类似雪花的形状 Example of Snowflake Schema\nFact constellations: 事实星座，多个事实表共享维度表，被视为一组星型，因此被称为星系模式或事实星座 Example of Fact Constellation\nA Data Mining Query Language, DMQL # Cube Definition (Fact Table)\ndefine cube \u0026lt;cube_name\u0026gt; [\u0026lt;dimension_list\u0026gt;]: \u0026lt;measure_list\u0026gt; Dimension Definition ( Dimension Table )\ndefine dimension \u0026lt;dimension_name\u0026gt; as (\u0026lt;attribute_or_subdimension_list\u0026gt;) Special Case (Shared Dimension Tables)\nFirst time as \u0026ldquo;cube definition\u0026rdquo; Other, define dimension \u0026lt;dimension_name\u0026gt; as \u0026lt;dimension_name_first_time\u0026gt; in cube \u0026lt;cube_name_first_time\u0026gt; Defining a Star Schema in DMQL # define cube sales_star [time, item, branch, location]: dollars_sold = sum(sales_in_dollars), avg_sales = avg(sales_in_dollars), units_sold = count(*) define dimension time as (time_key, day, day_of_week, month, quarter, year) define dimension item as (item_key, item_name, brand, type, supplier_type) define dimension branch as (branch_key, branch_name, branch_type) define dimension location as (location_key, street, city, province_or_state, country) Defining a Snowflake Schema in DMQL # define cube sales_snowflake [time, item, branch, location]: dollars_sold = sum(sales_in_dollars), avg_sales = avg(sales_in_dollars), units_sold = count(*) define dimension time as (time_key, day, day_of_week, month, quarter, year) define dimension item as (item_key, item_name, brand, type, supplier(supplier_key, supplier_type)) define dimension branch as (branch_key, branch_name, branch_type) define dimension location as (location_key, street, city(city_key, province_or_state, country)) Defining a Fact Constellation in DMQL # define cube sales [time, item, branch, location]: dollars_sold = sum(sales_in_dollars), avg_sales = avg(sales_in_dollars), units_sold = count(*) define dimension time as (time_key, day, day_of_week, month, quarter, year) define dimension item as (item_key, item_name, brand, type, supplier_type) define dimension branch as (branch_key, branch_name, branch_type) define dimension location as (location_key, street, city, province_or_state, country) define cube shipping [time, item, shipper, from_location, to_location]: dollar_cost = sum(cost_in_dollars), unit_shipped = count(*) define dimension time as time in cube sales define dimension item as item in cube sales define dimension shipper as (shipper_key, shipper_name, location as location in cube sales, shipper_type) define dimension from_location as location in cube sales define dimension to_location as location in cube sales Measures: Three Categories # distributive: 如果将函数应用于n个聚合值的结果与在不分区的所有数据上应用函数的结果相同。 E.g., count(), sum(), min(), max(). algebraic: 如果它可以通过具有M个参数的代数函数计算（其中M是有界整数），每个参数都是通过应用分布式聚合函数获得的。 E.g., avg(), standard_deviation(). holistic: 如果描述子聚合所需的存储大小没有常量界限。 E.g., median(), mode(), rank(). A Concept Hierarchy: Dimension (location)\nMultidimensional Data\nSales volume as a function of product, month, and region\nBrowsing a Data Cube\nVisualization OLAP capabilities Interactive manipulation Typical OLAP Operations # Roll up (drill-up): 汇总数据 通过沿着层次结构向上爬升或通过减少维度来实现 Drill down (roll down): reverse of roll-up 从较高级别的摘要到较低级别的摘要或详细数据，或引入新的维度 Slice and dice: 投影和选择 Pivot (rotate): 重新定位立方体，可视化，从3D到一系列2D平面。 Other operations drill across: 涉及（跨越）多个事实表 drill through: 通过底层的立方体穿透到其后端关系表（使用SQL） A Star-Net Query Model # Data Warehousing and OLAP Technology # 多层次体系结构\nThree Data Warehouse Models # Enterprise warehouse\n收集整个组织跨足的有关主题的所有信息 Data Mart\n是对公司范围内数据的子集，对特定用户组有价值。其范围限于特定的、选择的群体，如营销数据集市 独立 vs. 依赖（直接来自仓库）数据集市 Virtual warehouse\n对操作性数据库的一组视图 可能仅有一些可能的摘要视图会被实体化 Data Warehouse Development: A Recommended Approach # OLAP Server Architectures # Relational OLAP (ROLAP) 使用关系型或扩展关系型数据库管理系统存储和管理仓库数据，使用OLAP中间件支持缺失的部分 包括优化DBMS后端、实现聚合导航逻辑以及其他工具和服务 更大的可伸缩性 Multidimensional OLAP (MOLAP) 基于数组的多维存储引擎（稀疏矩阵技术） 针对预先计算的汇总数据进行快速索引 Hybrid OLAP (HOLAP) 用户灵活性，例如，低级别：关系型，高级别：数组 Specialized SQL servers 针对星型/雪花模式的SQL查询提供专门支持 Data warehouse implementation # Data Warehouse Design # 数据仓库设计和数据库设计的差异\n面向需求不同：数据库面向具体应用，需求一开始就很明确，而数据仓库是一个渐进的过程 设计目标不同：OLTP vs. OLAP 处理类型不同：面向操作型应用 vs. 面向分析型应用 数据来源不同：业务员输入 vs. 已存在的业务系统数据 系统设计的方法不同： 数据仓库可以采用数据驱动的设计方法 数据仓库设计可以分为数据仓库模型设计和数据装载接口设计两部分 数据仓库的设计步骤\n例如，将企业模型映射到数据仓库系统的过程\n分析建立企业模型并映射到数据仓库概念模型 逻辑模型设计 物理模型设计 OLAP Modeling Methods # 维表设计\n维的变化 维表的共享 层次信息和分类信息的位置 事实表设计\n事实表的特性 通用数据和专用数据事实表 维表的变化 # 维表通过记录因素的属性描述事件中包含的诸多因素 维表的本质是多维分析空间在某个角度上的投影 由于维表描述的是事物的属性，因此随着事物本身的变化，其属性也会产生改变 如果该属性与决策没有太大关系，例如电话号码属性对于分析顾客购买行为没有什么作用，则此属性的变化可以忽略不计 如果该属性与决策有关，例如某位顾客搬家后离超市更远了，我们试图分析其购买行为与家里距离变远有何关系，则不能将之忽略 对于需要记录其改变的维，有若干方法可以进行处理 当属性进行变化时，创建一个新记录 例如： 缺点：由于ID产生变化，被认为是两条记录 创建一个新的字段，将新地址填入 例如： 缺点：可扩展性不佳 增加一个修订号码字段和当前标记字段 例如： 缺点：维表和事实表连接时需要采用主关键字＋修订号码，增加了事实表的复杂性 最为理想的解决方案：新建一个关键字客户ID，通过主关键字与之相连，使用时间字段标志当前的值 例如： 缺点：相对较为复杂 维表的共享 # 多个维表中可能包含相同的属性： 供货商维中包含地址维，而销售商维中可能也包含地址维，因而可能共享维表 由于数据仓库中时间维的重要性，各个维中都有可能包含时间维，因而可能共享时间维表 可以采用扩展的星座结构来描述共享维表 具体内容请参见 上节 层次信息的位置 # 将维层次信息放入事实表\n优点：计算极为方便 缺点：事实表会因此变得极为庞大 将维层次放在各自的维表中，通过主关键字与事实表相连\n优点：减少了事实表的大小 缺点：OLAP性能比上一种方式差 分类信息的位置 # 在事实表中体现维分类\n优点： OLAP性能好 缺点： 事实表的字段数和记录数增加 在维表中体现维分类\n事实表的特征 # 与维表相比，事实表具有以下特征： 记录数量非常多 除了度量外，其他字段都是维表或者中间维表（雪花模型）的关键字 如果事实相关的维度很多，则事实表的字段数也会比较多 因此应当尽量减小一条记录的长度，才能避免事实表过大而难于管理 数据的粒度是影响事实表大小的关键因素，因而必须认真设计 通用数据和专用数据事实表 # 对应一个问题通常采用一个事实表，但在特殊情况下，也允许采用多个事实表 例如：超市里出售多种商品，由于商品本身分类不同，因此所采用的量度可能也不相同 如果将这些量度全部置于一个事实表中，由于某种类型的商品的量度其他商品可能不具备，因此则不可避免将在事实表中造成- 大量的数据空缺 解决办法：采用多个事实表，分为通用数据事实表与专用数据事实表加以管理 Optimization of Logical Model/Physical Model # 数据仓库的逻辑模型设计 # 系统数据量估算 数据粒度的选择 数据的分割 表的合理划分 去除纯操作数据 增加导出字段 定义关系模式 定义元数据存储 定义记录系统 系统数据量估算 # 设在概念模型中出现的表的个数为 $N$（这些表中应不包含不会放进数据仓库的表），对于每个表 $i$ ($0\\leq u \\leq N$), 计算表的大小$S_i$和表的主关键字大小$K_i$，然后估计每张表$i$在单位时间内最大记录数$L_{max}$和最少记录数$L_{min}$,则数据仓库的粗略数据量在以下范围： 其中，$T$是数据在数据仓库中存在的周期。$\\alpha$是考虑由于数据冗余和数据索引而使数据量增大的冗余因子,通常取$1.2~2$\n本公式的含义是：\n数据仓库数据量＝{累加[(每张表记录大小＋每张表主关键字大小)*每张表单位时间内记录的数量]*存储时间}*冗余因子\n公式估算出的结果仅能作为参考\n数据粒度的选择 # 数据量较小的情况下使用单一的数据粒度，即直接存储细节数据并定期在细节数据基础上进行数据综合\n对于大数据量需要采用双重粒度，对于细节数据只保存近期的数据在数据仓库之中，当保留周期到达时，将距离当前较远的数据导出到磁带或存储设备上\n数据粒度策略\n数据的分割 # 为何要进行数据分割？ 数据仓库中数据量过大时，检索速度很慢 数据分割是指将数据分散到各自的物理单元里以便能够独立处理，以提高数据处理的效率 数据分割没有固定的标准，分割的方法和粒度应当根据实际情况确定 通常选择时间、地点、业务等划分 一般按照时间分割数据分布比较均匀，因此按照时间分割最为常见 合理的表划分 # 直接存储字段数目很大的表，会造成以下问题：\n各个字段的更新频率不一，放在一张表里造成数据追加工作的浪费 各个字段的访问频率不一，放在一张表里影响访问效率 因此需要对表中的内容进行合理的划分\n按数据的稳定性划分 按业务规则进行表划分(略) 按数据稳定性进行表划分\n删除纯操作数据及增加导出字段 # 在将业务系统中的数据抽取到数据仓库系统中的过程里，如果发现某些数据对于决策没有作用，属于纯操作型数据,则可以将之删除 例：收款人字段 导出数据本身是冗余的，但是增加导出字段有利于数据以后的使用 例：在按月综合表中，可以加入平均价格，供货总价，供货总数量等导出字段 Meta Data # What is meta data?\n元数据是描述数据的数据。\n在数据仓库中，有几个数据层次：\nmeta data，元数据 current detailed data，当前详细数据 older detailed data，较旧的详细数据 lightly summarized data，轻度汇总数据 highly summarized data，高度汇总数据 元数据是定义数据仓库对象的数据。它具有以下类型：\n仓库结构的描述 模式、视图、维度、层次结构、派生数据定义、数据集市位置和内容 操作元数据 数据谱系（迁移数据和转换路径的历史）、数据的状态（活动、存档或清除）、监控信息（仓库使用统计、错误报告、审计追踪） 用于汇总的算法 从操作环境到数据仓库的映射 业务数据 业务术语和定义、数据所有权、收费政策 记录系统的定义 # 记录系统是操作型元数据的一部分 记录系统指明数据仓库中关系表的各个字段来源于业务数据库何处 例： 数据仓库物理模型设计 # 确定数据的存储结构 索引策略 数据存储策略与性能优化 多路聚集优化 表的归并 分割表的存放 按列存储 存储分配优化 数据装载接口设计 并行优化设计 Define the Storage Structure # 一般的数据库数据量相对较小，除非业务要求必须保证数据的安全性和可恢复性，否则可以不采用并行存储结构\n数据仓库由于数据的巨量存储，必须采用并行存储结构，例如RAID\n索引技术：为什么不使用B树？ # B树是数据库中广泛使用的技术 在数据库中查找记录时，B树索引具有很高的性能 虽然B树对于数据仓库来说并不是一个好的技术，为什么呢？ B-tree Indexing Technology\n原因是：\nB树要求属性必须有许多不同的值，例如itemID，customer ID等。 B树要求查询条件较简单且结果较少 创建B树的空间复杂度和时间复杂度都很大 Indexing OLAP Data: Bitmap Index\n特定列上的索引 列中的每个值都有一个位向量：位运算速度快 位向量的长度：基表中的记录数 如果基表的第i行具有索引列的值，则第i位设置为1 不适用于高基数域 Indexing OLAP Data: Join Index\n连接索引：JI(R-id, S-id)，其中 R（R-id，…）\u0026gt;\u0026lt; S（S-id，…） 传统索引将值映射到记录id列表 在数据仓库中，连接索引将星型模式的维度值与事实表中的行相关联。 例如，事实表：销售和两个维度城市和产品 对于每个不同的城市，连接索引维护一个包含记录城市销售的元组的R-ID列表 连接索引可以跨越多个维度 Data Storage Strategy # Table Mergence\nAdd Redundancy\nDividing Table\n在逻辑设计中，大表可以分成小表。当访问大表时，可以用小表替代。 在物理设计中，可以使用分布式存储方法。 表可以存储到磁盘阵列中。 Efficient Methods for Data Cube Computation # Data Cube: High Efficiency Computing\n数据立方体可以看作是一个立方体格点的网格 最底部的立方体是基本立方体 最顶部的立方体（顶点）仅包含一个单元格 在具有L级的n维立方体中有多少个立方体？ Materialization of data cube\n物化每个（立方体）（完全物化）、无（不物化）或一些（部分物化） 选择要物化的哪些立方体 基于大小、共享、访问频率等 Data Cube Operation # 在DMQL中定义和计算立方体 define cube sales[item, city, year]: sum(sales_in_dollars) compute cube sales 转换为类似SQL的语言 (with a new operator cube by, introduced by Gray et al.\u0026lsquo;96) SELECT item, city, year, SUM (amount) FROM SALES CUBE BY item, city, year 需要计算以下的Group-Bys (date, product, customer), (date,product),(date, customer), (product, customer), (date), (product), (customer) () Iceberg Cube\n仅计算计数或其他满足条件（如 HAVING COUNT(*) \u0026gt;= minsup）的立方体单元格！\nMotivation\n在稀疏立方体中，只有一小部分立方体单元格可能位于“水面”之上 仅计算“有趣”的单元格——超过一定阈值的数据 避免立方体的爆炸性增长 Multiway Array Aggregation # 将数组分成块（适应内存的小子立方体）。 压缩稀疏数组寻址：（chunk_id, offset） 通过按最小化每个单元格访问的次数的顺序访问立方体单元格，减少内存访问和存储成本。 方法：应按照其大小升序排序并计算平面。 思路：将最小的平面保留在主内存中，仅为最大平面获取和计算一个块 该方法的限制：仅对小型维数计算良好 如果维度较多，可以研究冰山立方体计算方法 有效处理OLAP查询 # 确定应在可用立方体上执行哪些操作： 将钻取、卷动等转换为相应的SQL和/或OLAP操作，例如，切片=选择+投影 确定相关操作应应用于哪些物化立方体 在MOLAP中探索索引结构和稠密数组结构与压缩数组结构的差异 Bottom-Up Computation (BUC) # 自底向上计算（BUC） BUC（Beyer＆Ramakrishnan，SIGMOD'99） 自底向上的立方体计算 将维度划分为分区，并促进冰山修剪 如果一个分区不满足min_sup，则可以修剪其后代 如果minsup = 1 =\u0026gt; 计算完整的CUBE！ BUC: Partitioning\n通常，整个数据集无法适应主内存 对不同的值进行排序，将其分为适合的块 Plan and Implementation of Data Warehouse # 数据仓库的应用目标 企业的核心业务 优化企业内部管理控制 为企业增加商业机会 建设数据仓库的必要性 可通过计算ROI（Return of Investment）来衡量投资回报的价值\n数据仓库主题的选择和阶段规划\n数据仓库的实施是一个极为复杂的长期过程，因此，应选择当前最急需、能在短期内产生效益、业务模型清晰的任务首期实现 选择首期实现主题的参考原则： 优先实现管理者目前需求最迫切和最关心的主题 优先选择能在短期内产生效益的主题 推后选择业务逻辑准备不充分的主题 推后实施技术难度较大、可实现性较低、投资风险大的主题 维护阶段 数据仓库的维护极为重要，一般数据仓库在建立完成之后，都需要一至两年的维护 数据仓库的维护过程就是DSS逐步产生效益的过程 数据仓库后端工具 # 数据抽取(Data extraction): get data from multiple, heterogeneous, and external sources 数据清洗(Data cleaning): detect errors in the data and rectify them when possible 数据转换(Data transformation):convert data from legacy or host format to warehouse format 数据装载(Load): sort, summarize, consolidate, compute views, check integrity, and build indicies and partitions 刷新(Refresh): propagate the updates from the data sources to the warehouse Further development of data cube technology # Discovery-Driven Exploration of Data Cubes # Hypothesis-driven: 由用户进行探索，搜索空间巨大 Discovery-driven (Sarawagi et al.\u0026lsquo;98) 预先计算指示异常的度量，引导用户在所有聚合级别进行数据分析 异常：基于统计模型，与预期值明显不同的值 使用背景颜色等视觉提示反映每个单元格的异常程度 异常指标的计算（建模拟合和计算SelfExp、InExp和PathExp值）可以与立方体构建重叠 Examples: Discovery-Driven Data Cubes\nFrom data warehousing to data mining # Data Warehouse Usage # 三种数据仓库应用 信息处理 支持查询、基本统计分析和使用十字表、表格、图表和图形的报告 分析处理 对数据仓库数据进行多维分析 支持基本的OLAP操作，如切片、切块、钻取、旋转 数据挖掘 从隐藏的模式中发现知识 支持关联、构建分析模型、执行分类和预测，并使用可视化工具呈现挖掘结果。 三个任务之间的差异 From On-Line Analytical Processing to On Line Analytical Mining (OLAM) # 为什么联机分析挖掘？ 数据仓库中数据的高质量 DW包含集成的、一致的、清理过的数据 围绕数据仓库的可用信息处理结构 ODBC、OLEDB、Web访问、服务设施、报告和OLAP工具 基于OLAP的探索性数据分析 带有钻取、切片、切块、旋转等的挖掘 在线选择数据挖掘功能 集成和交换多个挖掘功能、算法和任务 Summary # Data warehouse A subject-oriented, integrated, time-variant, and nonvolatile collection of data in support of management\u0026rsquo;s decision-making process A multi-dimensional model of a data warehouse Star schema, snowflake schema, fact constellations A data cube consists of dimensions \u0026amp; measures OLAP operations: drilling, rolling, slicing, dicing and pivoting OLAP servers: ROLAP, MOLAP, HOLAP Data warehouse implementation Efficient computation of data cubes Partial vs. full vs. no materialization Multiway array aggregation Bitmap index and join index implementations Further development of data cube technology Discovery-drive and multi-feature cubes From OLAP to OLAM (on-line analytical mining) ","date":"15 December 2023","permalink":"/posts/reviews/%E7%A0%94%E7%A9%B6%E7%94%9F%E5%A4%8D%E4%B9%A0/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%8E%E7%9F%A5%E8%AF%86%E5%8F%91%E7%8E%B0/data-warehousing/","section":"博客","summary":"数据仓库是一个面向主题的、集成的、时变的和非易失性的数据集合，支持管理决策过程。","title":"Data Warehousing"},{"content":"","date":"13 December 2023","permalink":"/posts/reviews/%E7%A0%94%E7%A9%B6%E7%94%9F%E5%A4%8D%E4%B9%A0/%E8%BD%AF%E4%BB%B6%E5%AE%89%E5%85%A8%E6%8A%80%E6%9C%AF/analysis/","section":"博客","summary":"静态程序分析（英语：Static program analysis）是指在不运行程序的条件下，进行程序分析的方法。","title":"Code Analysis"},{"content":"","date":"13 December 2023","permalink":"/tags/flawfinder/","section":"Tags","summary":"","title":"Flawfinder"},{"content":"Resources # https://dwheeler.com/flawfinder/ https://github.com/david-a-wheeler/flawfinder 下载 # 需要检测的c语言项目 # easy demo # 首先选取一个简单的demo\ndemo.cpp\n#include \u0026lt;iostream\u0026gt; using namespace std; int main() { int exponent; float base, result = 1; cout \u0026lt;\u0026lt; \u0026#34;Enter base and exponent respectively: \u0026#34;; cin \u0026gt;\u0026gt; base \u0026gt;\u0026gt; exponent; cout \u0026lt;\u0026lt; base \u0026lt;\u0026lt; \u0026#34;^\u0026#34; \u0026lt;\u0026lt; exponent \u0026lt;\u0026lt; \u0026#34; = \u0026#34;; while (exponent != 0) { result *= base; --exponent; } cout \u0026lt;\u0026lt; result; return 0; } grpc # https://github.com/grpc/grpc 使用分析 # 直接输入下面命令即可对代码进行静态分析。\nflawfinder [要测试的文件目录] demo.cpp # 结果显示没有问题。\n当然也可以将结果输出为 html 形式\nflawfinder --quiet --html ./ \u0026gt; results.html 也可以输出为sarif格式\nflawfinder --quiet --sarif ./ \u0026gt; results.sarif grpc # 搜集并了解其他的 c/c++代码分析工具,如 RATS、Splint 等，比较这些工具的功能。 # Flawfinder: # 功能： Flawfinder主要用于查找C/C++代码中的安全漏洞，特别是与安全性相关的缺陷。 使用场景： 适用于快速、简单的安全性检查，特别是对于较小的项目。 优点： 简单易用，不需要深厚的专业知识，能够快速找出常见的代码漏洞。 缺点： 不能进行深层次的静态分析，适用于快速的初步检查。 RATS # Rough Auditing Tool for Security\n功能： RATS用于查找C语言中的安全漏洞，特别关注与安全性相关的问题，如潜在的缓冲区溢出、格式字符串漏洞等。 使用场景： 适合需要更灵活配置和深度检查的项目，可以根据具体需求进行调整。 优点： 灵活，可以通过配置文件进行定制，能够检查代码中的多种潜在问题。 缺点： 有时可能会产生误报，需要仔细审查结果。 Splint # 功能： Splint是一个C语言的静态分析工具，旨在查找代码中的潜在错误、不规范的用法和潜在的安全问题。 使用场景： 适用于需要深入代码静态分析、关注规范性和可维护性的项目。 优点： 提供详细的警告和错误信息，能够进行较为深入的代码静态分析。 缺点： 需要一定的学习曲线，有时可能会产生大量的警告，需要谨慎使用。 Clang Static Analyzer # 功能： Clang Static Analyzer是基于Clang编译器的静态代码分析工具，用于查找C/C++代码中的潜在问题，如内存泄漏、空指针引用等。 使用场景： 适用于项目使用Clang编译器，希望充分利用编译器集成静态分析的优势。 优点： 集成于Clang编译器，能够提供高质量的静态分析结果，支持多种分析检查。 缺点： 部分项目可能需要调整编译过程以适配Clang。 Coverity # 功能： Coverity是一款商业静态分析工具，用于查找C/C++代码中的缺陷和安全漏洞。 使用场景： 适用于对代码质量有严格要求，愿意投资于高级静态分析工具的项目。 优点： 高度精准的静态分析，能够提供低误报率的结果，支持大型项目。 缺点： 是商业软件，可能需要付费许可。 ","date":"13 December 2023","permalink":"/posts/reviews/%E7%A0%94%E7%A9%B6%E7%94%9F%E5%A4%8D%E4%B9%A0/%E8%BD%AF%E4%BB%B6%E5%AE%89%E5%85%A8%E6%8A%80%E6%9C%AF/analysis/flawfinder/","section":"博客","summary":"Flawfinder主要用于查找C/C++代码中的安全漏洞，特别是与安全性相关的缺陷。","title":"Flawfinder"},{"content":"","date":"13 December 2023","permalink":"/tags/kythe/","section":"Tags","summary":"","title":"Kythe"},{"content":"Terminology Overview # 以下基本术语：\nsemantic node : 指代码中的语义单元，它代表了代码中的一个特定概念或实体。这些语义单元可以是函数、变量、类、接口等程序实体。Semantic Node 用于在 Kythe 中建立代码元数据的索引，使得代码的语义结构能够被有效地表示和查询。Semantic Node 通常包含以下信息： 名字（Name）： 表示语义单元的名称，例如函数名、变量名等。 种类（Kind）： 表示语义单元的类型，例如函数、变量、类等。 位置信息（Location）： 表示语义单元在源代码中的位置，包括文件路径、行号、列号等。 其他属性（Attributes）： 可能包含其他与语义单元相关的属性信息，例如访问修饰符、类型信息等。 Anchor : 指代码中的一个位置或节点，通常对应于源代码中的一个特定位置，例如函数调用、变量引用、类定义等。Anchor 主要用于标识源代码中的关键点，并作为索引的起点，连接到其他 Kythe 节点。Anchor 具有以下特性： 位置信息： Anchor 包含了源代码中的位置信息，通常包括文件路径、行号、列号等，以确定具体的代码位置。 关联边（Edges）： Anchor 通过关联边与其他 Kythe 节点建立关系。常见的关联边包括 ref、childof 等，用于表示不同节点之间的关系。 语义连接： Anchor 可能连接到与代码中的语义单元相关的 Semantic Node，从而帮助建立代码的语义结构。 Edge : 是连接两个节点的关系。边承载着关于代码的语义和结构信息，构建了 Kythe 索引中的有向图。边的存在意味着两个节点之间存在某种关联或关系，这有助于在代码库中跟踪、理解和查询不同代码元素之间的连接。 类型（Type）： 边有一个指定的类型，用于描述连接的两个节点之间的关系。例如，ref 表示引用关系，childof 表示父子关系等。这些类型的边有助于解释节点之间的语义关系。 起始节点和目标节点： 边连接两个节点，其中一个是起始节点，另一个是目标节点。起始节点是边的源头，目标节点是边的终点。 方向性： 边是有向的，即从起始节点指向目标节点。这意味着边有一个明确的方向，从一个节点到另一个节点。 语境信息： 边可能携带一些额外的信息，以提供关于关系的上下文。例如，一个 ref 边可能包含引用的具体位置信息，帮助定位源代码中的引用点。 Edge and Node Examples # Jump-to-Definition # 跳转到定义允许从某个实体（如变量或函数）的用法导航到其定义。为了支持这一点，索引器至少必须发出一个定义站点锚点、一个表示被定义实体的语义节点和一个用法站点锚点。在这个示例中，我们看到了一个名为 matchRevision 的变量的一个定义和两个引用（来自 kcd.go）：\nfunc (rf *RevisionsFilter) Compile() (func(Revision) bool, error) { if rf == nil || (rf.Revision == \u0026#34;\u0026#34; \u0026amp;\u0026amp; rf.Corpus == \u0026#34;\u0026#34; \u0026amp;\u0026amp; rf.Until.IsZero() \u0026amp;\u0026amp; rf.Since.IsZero()) { return func(Revision) bool { return true }, nil } var matchRevision, matchCorpus func(...string) bool // definition var err error if matchRevision, err = singleMatcher(rf.Revision); err != nil { // reference 1 return nil, err } if matchCorpus, err = singleMatcher(regexp.QuoteMeta(rf.Corpus)); err != nil { return nil, err } return func(rev Revision) bool { return matchRevision(rev.Revision) \u0026amp;\u0026amp; // reference 2 matchCorpus(rev.Corpus) \u0026amp;\u0026amp; (rf.Until.IsZero() || !rf.Until.Before(rev.Timestamp.In(time.UTC))) \u0026amp;\u0026amp; (rf.Since.IsZero() || !rev.Timestamp.In(time.UTC).Before(rf.Since)) }, nil } matchRevision 的第一次提及（注释为 \u0026ldquo;定义\u0026rdquo;）记录了一个带有 defines/binding 边的锚，该锚指向该变量的语义节点。引用（注释为 \u0026ldquo;reference 1\u0026rdquo; 和 \u0026ldquo;reference 2\u0026rdquo;）具有指向同一语义节点的 ref 边锚。\n一般来说，给定语义节点的定义站点可能不止一个，在某些情况下可能根本就没有定义。当确实存在一个唯一的定义时，我们称其为节点的 target definition。\nJava 中的隐式构造函数就是一个没有定义站点的例子。如果您定义了一个类，但没有编写一个构造函数，编译器会为您编写一个构造函数，并参与交叉引用，但它在程序的源文本中没有位置。\n许多 C++ 对象都有多个定义位置，包括类或函数原型的前向声明及其补全。类型（重新）声明的锚点也使用 defines/binding 边。\nCallgraph # 调用图由一组三元组组成，每个三元组关联一个 call site 、一个 caller 和一个 callee 。调用点是源代码中发生调用的位置（通常是某种调用表达式）；调用者是包含调用点的函数，被调用者是被调用的函数。\n在 Kythe 模式中，这种结构由三个节点和两条边表示：一个位于 call site 的 anchor，以及分别代表调用者和被调用者的 semantic nodes 。调用点锚有一条指向调用者的 childof 边和一条指向被调用者的 ref/call 边：\nkcd.go\nfunc (rf *RevisionsFilter) Compile() (func(Revision) bool, error) { memdb.go\ntype DB struct { ... } ... // Revisions implements a method of kcd.Reader. func (db DB) Revisions(_ context.Context, want *kcd.RevisionsFilter, f func(kcd.Revision) error) error { revisionMatches, err := want.Compile() 本图并未显示这些构造所产生的所有节点和边。特别是，完整的图将包括 memdb.go 中接口函数的覆盖（实现）的额外节点。为简洁起见，我们省略了该图的分支。\n这里我们还有 kcd.go 的两个部分，围绕着 Compile 函数的使用。该函数在 kcd.go 中定义，然后在 memdb.go 中通过函数调用引用。在 kcd.go 中，Compile 的代码锚有一个 childof 边指向 Revisions 的函数语义节点。\nClass/Interface Hierarchy \u0026amp; Overrides # 继承关系由 extends 和 satisfies 边沿捕捉。覆盖关系由 overrides 边表示。Go 的这个示例对这些关系进行了说明（该示例使用 satisfies 边来表达 Go 的隐式接口满足关系）。在这个示例中，childof 边捕捉了方法及其类型之间的包含关系：\nkcd.go\ntype Reader interface { Revisions(_ context.Context, filter *RevisionsFilter, f func(Revision) error) error } memdb.go\ntype DB struct { ... } ... // Revisions implements a method of kcd.Reader. func (db DB) Revisions(_ context.Context, want *kcd.RevisionsFilter, f func(kcd.Revision) error) error { 这里我们再次看到 kcd.go 中的顶级阅读器接口。同一文件中的接口函数 Revisions 有一条 childof 边指向该接口的节点。memdb.go中的Reader实现有一条satisfies边指向父接口，而它自己的Revisions函数有一条overrides边指向kcd.go中的顶层Revisions函数。对于 C++ 和 Java，接口、抽象类等在 Kythe 模式中也有类似的关系。\n对于明确声明继承和接口实现的语言（如 Java），Kythe 使用 extends 边标签而不是 satisfies 边标签。\n","date":"13 December 2023","permalink":"/posts/reviews/%E7%A0%94%E7%A9%B6%E7%94%9F%E5%A4%8D%E4%B9%A0/%E8%BD%AF%E4%BB%B6%E5%AE%89%E5%85%A8%E6%8A%80%E6%9C%AF/analysis/kythe/","section":"博客","summary":"Kythe是一个开源软件工程生态系统，用于建立和维护大型代码库的跨语言代码索引。它提供了强大的代码导航和分析工具，支持多语言和多仓库项目。Kythe的目标是提高软件工程师的开发效率，使他们能够更轻松地理解和管理复杂的代码结构。","title":"Kythe Schema Overview"},{"content":"Resources # How To Use Option As Meta Key In MacOS Terminal? ","date":"12 December 2023","permalink":"/posts/skills/meta/","section":"博客","summary":"在MacOS终端中，可以将“Option”键设置为“Meta”键。打开终端偏好设置，选择“Profiles” \u0026gt; “Keyboard” \u0026gt; “Use Option as Meta key”。","title":"Meta Key"},{"content":"","date":"10 December 2023","permalink":"/posts/language/lsp/","section":"博客","summary":"Language Server Protocol（LSP）是一种用于提供编程语言相关功能的开放式协议。由微软首创，旨在增强集成开发环境（IDE）的性能。LSP允许不同IDE和编辑器与语言服务器通信，实现代码补全、语法检查等功能，而无需了解特定语言的细节。这提高了跨平台、跨编辑器的互操作性，为开发人员提供了一致的编程体验。 LSP已被广泛采用，支持多种编程语言，为开发者提供更高效、统一的编码环境。","title":"Language Server Protocol"},{"content":"Language Server Protocol （语言服务器协议，简称 LSP）是微软于 2016 年提出的一套统一的通讯协议方案。该方案定义了一套编辑器或 IDE 与语言服务器之间使用的协议，该语言服务器提供自动完成、转到定义、查找所有引用等语言功能。\n同学们可能对语言服务器（Language Server）不是很了解。举个例子，我们在使用在线编程工具的时候，是不是也有代码提示、代码错误诊断等功能？其实背后是跑着一个对应这门语言的 language server 进程实例（也有开发者工具本身和 Language Server 耦合在一起的，比如 Eclipse），这个 Language Server 实例负责分析你当前打开的代码文件。\n市面上的 编辑器 / IDE，本质上提供给用户的代码编辑（如打开文件、编辑文集、查找引用、打开工作区等）以及编辑器的响应行为（如补全提示、代码诊断等）其实都大同小异，可能在个别功能上实现不一样，但是逃不开上述内容。或者说，上述这些功能都可以抽象为一系列的「行为事件」。\n微软提出 LSP 的目的是，之前各个编辑器（VSCode, Vim, Atom, Sublime\u0026hellip;）各自为战，编辑器内部实现的特性和协议都不同。每换一个编辑器，就有可能要给该编辑器中支持的每门语言写一个对应的 Language Server，也就是说假设有 $n$ 门语言，$m$ 个编辑器，那全部编辑器适配所有语言的开发成本和复杂度为 $n * m$。\n能不能在中间层做一个抽象，让语言的「静态分析服务」和「编辑器 / IDE」分离开来？这样上述情景下开发成本和复杂度就可以降低为线性的 $n + m$。\n例如，每个编辑器（客户端）都在用户产生某些通用的行为时（比如点击跳转到定义）负责生成标准中的行为事件，然后以 JSON-RPC 的形式去调用 Language Server 的接口方法。Language Server 相对应地，也必须实现全部 LSP 规范（或者至少实现其中关键部分）定义的接口。\n这么做的好处在于，对于某门编程语言，一个编辑器工具不需要再去关心怎么去做代码分析，而是只需要关注如何在界面上发起或响应 LSP 规定的 RPC 事件。而在语言服务器这边也是同理，只需要关注协议本身的事件并响应 \u0026amp; 发起事件即可。\n这种中间层分离的思想非常常见，例如编译器就分为前端和后端，前端生产中间语言 IR，后端负责把中间语言再翻译为 CPU 特定的指令集。典型的代表如 JVM 字节码、 LLVM IR 等\n另外，由于编辑器和 Language Server 是两个进程，所以如果 Language Server 挂了，编辑器进程本身也还会存在，用户不用担心还没修改好的代码因此丢失的问题。\n有没有缺点？肯定有，那就是市面上所有的 编辑器 和 Language Server 的 maintainer 都需要花时间和精力去兼容这个协议，并且这个协议本身也会随着自身版本更新而要求服务端 / 客户端响应新的协议行为。但是总体来说，利大于弊。\nLSP 的运作机制 # LSP 是一个「双工协议」。不只是开发者工具（客户端）会主动向 Language Server （服务端）通信，服务端也可能主动向开发者工具发起 RPC 请求（比如代码诊断事件 textDocument/Diagnostics ，只能从服务端向客户端主动发送）。在 LSP 规范定义文档 中，每个 RPC 事件会标注可能的发起方以及是否需要对方做出响应。\n例子\n例如一个客户端发起，且要求服务端返回的请求事件（小标题的括号中有一个从左至右然后转弯的箭头）： 例如一个服务端发起，且要求客户端返回的请求事件（小标题的括号中有一个从右至左然后转弯的箭头）： 也有单方面发送，不需要响应的（分别为工具向服务端单方面发送 / 服务端向工具单方面发送）： 我们以 Goto Type Definition Request 为例，具象化地理解一下整个流程。这个 RPC 请求的发起可能是来自 VSCode 中用户右键菜单中点击 跳转到类型定义 （Goto Type Definition） 这个事件：\nVSCode 会向 Language Server 进程以 IPC 形式发送如下信息（仅举例，实际参数结构比较复杂）：\n{ \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;id\u0026#34;: 24, \u0026#34;method\u0026#34;: \u0026#34;textDocument/typeDefinition\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;textDocument\u0026#34;: { \u0026#34;uri\u0026#34;: \u0026#34;file:///User/bytedance/java-hello/src/main/java/Main.java\u0026#34; }, \u0026#34;position\u0026#34;: { \u0026#34;line\u0026#34;: 3, \u0026#34;character\u0026#34;: 13 }, // ...其他参数 }, } 然后 Language Server 拿到这条指令，会执行如下动作：\n调用的方法是 textDocument/typeDefinition，也就是分析一个符号的类型定义信息。 根据参数，指令的来源文件是 Main.java 第 3 行第 13 个字符 —— 分析后可知是 foo 这个符号。 Server 寻找 foo 的符号对应的类型 Foo 所在位置。找到之后，同样通过 IPC 返回结果 JSON-RPC： { \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, // Request 中的 id 为 24，因此 Server 端对应的 Response id 也必须为 24 \u0026#34;id\u0026#34;: 24, \u0026#34;result\u0026#34;: { \u0026#34;uri\u0026#34;: \u0026#34;file:///User/bytedance/java-hello/src/main/java/Main.java\u0026#34;, \u0026#34;range\u0026#34;: { \u0026#34;start\u0026#34;: { \u0026#34;line\u0026#34;: 7, \u0026#34;character\u0026#34;: 25 }, \u0026#34;end\u0026#34;: { \u0026#34;line\u0026#34;: 7, \u0026#34;character\u0026#34;: 28 } } }, } 只有客户端根据返回值中的参数，让当前用户的编辑光标跳转到指定位置。\nLSP 的生命周期 # 上一节中的例子只是 Language Server 和开发者工具之间通信的其中一个特例场景。在编辑代码的整个过程中，Language Server / 开发者工具双方会持续不断地通过各式各样的请求体通信。\n为了规范，Language Server Protocol 中的交互一般需要遵循如下生命周期。\n用户在打开一个项目或者代码文件后，开发者工具就需要视情况启动一个 Language Server 子进程并建立通信。在 Language Server 开始接收消息后，一般从客户端发出初始化请求开始。\n1. 初始化 (Initialize) # 由于 Language Server 启动后，并不知道当前编辑器的状态。因此，所有符合 LSP 规范的开发者工具在和符合 LSP 规范的 Language Server 建立连接后，第一个 RPC 请求永远是 initialize 指令。initialize 指令的结构体比较复杂，主要是告知 Language Server 当前的工作区在哪里、客户端提供的能力（capacities）有哪些等等。\nServer 根据编辑器工具请求体内的配置信息初始化完成后，会响应 InitializeResult 结构体作为结果，同时告知客户端当前 Server 具有哪些能力。\n由于不同编辑器的功能实现不一，因此 LSP 中大部分的服务端/客户端能力都是可选的：比如有的客户端不提供 codeLens 功能，有的服务端不提供代码补全功能等。双方是否具备这些能力都会在初始化阶段互相告知，以避免后续产生某些无效的功能请求。\n按照 LSP 规范，客户端对 textDocument/didOpen、textDocument/didChange 和 textDocument/didClose 通知的支持是强制性的，客户端不能选择不支持它们。\n2. 打开文件 (textDocument/didOpen) # 然后，每当开发者工具侧的用户在打开（或者在 Language Server 初始化前已经打开）了某个文件，开发者工具会向 Language Server 发出 textDocument/didOpen 通知，告知 Language Server 某个文件被打开。\n「文档打开通知」从客户端发送到服务器，以表示新打开的文本文档。文档的内容现在由客户端管理，语言服务器不得尝试使用文档的 Uri 读取文档的内容。 从这个意义上说，「打开」意味着它由客户端「管理」。 这并不一定表示其内容会显示在编辑器中。在没有相应的「关闭通知」之前发送的情况下，客户端不能多次发送打开通知 —— 也就是说，打开和关闭通知必须一一匹配，并且特定 textDocument 的最大打开计数为 1。 请注意，服务器满足请求的能力，与文本文档是打开还是关闭无关。\n举个例子，我们通过 VSCode 打开 /workspace 下的 main.go 文件：\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { fmt.Println(\u0026#34;Hello World go!\u0026#34;) } 会发送的 textDocument/didOpen 通知结构体为：\n{ \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;textDocument/didOpen\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;textDocument\u0026#34;: { \u0026#34;uri\u0026#34;: \u0026#34;file:///workspace/main.go\u0026#34;, \u0026#34;languageId\u0026#34;: \u0026#34;go\u0026#34;, \u0026#34;version\u0026#34;: 2, // 这里的文件内容为 Language Server 中虚拟文件的内容初始状态 \u0026#34;text\u0026#34;: \u0026#34;package main\\n\\nimport (\\n\\t\u0026#34;fmt\u0026#34;\\n)\\n\\nfunc main() {\\n fmt.Println(\u0026#34;Hello World go!\u0026#34;)\\n}\u0026#34; } } } 整体流程图如下：\n我们注意到，Language Server 在得知文件被打开后，会试图维护一个\u0026quot;虚拟\u0026quot;的文件结构体，而不会去读取文件系统中对应文件的实际内容。后续的保存文件等操作是交由开发者工具直接写入文件系统完成的，Language Server 不负责同步文件内容。\n之后用户的编辑行为，都会通过事件通知的形式告知 Language Server。而 Language Server 则是根据编辑行为，维护和调整上述虚拟文件对象的数据结构，进而做出响应。\n当然，大家也不要产生误解，Language Server 仍有可能访问文件系统。\n3. 编辑文件 (textDocument/didChange) # 编辑文件总是发生在打开事件之后。\n根据 LSP 规范，Language Server 允许的编辑操作的更新方式有三种：不更新、全量更新、增量更新。但大部分 Language Server 一般采用增量更新模式，即发送编辑产生的 \u0026ldquo;diff\u0026rdquo; 而非更新后的整体内容。举例来说，我们在代码中新增一行 \u0026ldquo;a\u0026rdquo;：\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { fmt.Println(\u0026#34;Hello World go!\u0026#34;) + a } 客户端会产生如下的 JSON-RPC 请求：\n{ \u0026#34;jsonrpc\u0026#34;:\u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;:\u0026#34;textDocument/didChange\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;textDocument\u0026#34;: { \u0026#34;uri\u0026#34;: \u0026#34;file:///workspace/main.go\u0026#34;, \u0026#34;version\u0026#34;: 37 // 这个版本号用于确认 change 的先后顺序 }, \u0026#34;contentChanges\u0026#34;: [{ \u0026#34;range\u0026#34;: { \u0026#34;start\u0026#34;: { \u0026#34;line\u0026#34;:8, \u0026#34;character\u0026#34;:4 }, \u0026#34;end\u0026#34;: { \u0026#34;line\u0026#34;: 8, \u0026#34;character\u0026#34;: 4 } }, \u0026#34;rangeLength\u0026#34;: 0, \u0026#34;text\u0026#34;: \u0026#34;a\u0026#34; }] } } 然后，服务端根据当前 change 的内容，更新内部的数据结构，决定是否产生某些 \u0026ldquo;行为\u0026rdquo;（比如代码诊断等）。\n4. 关闭文件 (textDocument/didClose) # 按照规范内容，关闭的文件一般对应着一个已经由客户端打开的文件对象。这里不再赘述。\n当文档在客户端关闭时，文档关闭通知从客户端发送到服务器。 文档的主文件现在存在于文档的 URI 指向的位置（例如，如果文档的 URI 是文件 URI，则主文件现在存在于磁盘上）。 与打开通知一样，关闭通知是关于管理文档内容的。 收到关闭通知并不意味着该文档之前曾在编辑器中打开过。 关闭通知需要发送先前的打开通知。 请注意，服务器满足请求的能力与文本文档是打开还是关闭无关。\n关于 LSP 的常见问题 # 1. 语言服务器不会访问文件系统中的文件么？ # 不，Language Server 还是有可能读取文件系统中未被编辑器打开的文件。\n协议中仅仅规定，textDocument/didOpen 仅是不允许 Language Server 去打开 \u0026ldquo;客户端已经打开的\u0026rdquo; 对应 URI 文件的内容，但允许 Language Server 读取工作区和已打开文件上下文中其他「未打开的文件」。\n例如，import 其他库的时候的代码补全功能，Language Server 就需要访问文件系统以获取索引信息。\n2. 代码诊断是怎么实现的？ # 一般是通过建立抽象语法树，做语法分析检查语法错误。\n一些插件或者代码诊断工具，如 ESLint，可以在语法规范的 AST 中的节点中遍历访问，找出更多的 Lint 警告/错误。\n3. 代码补全是怎么实现的？ # 根据 LSP 中的规定，代码补全由客户端根据事件发起请求，遵循如下触发类型：\n用户输入某个标识符（大部分情况下编辑器会自动执行这个事件）或敲击 Ctrl/Cmd + Space 用户正在输入某个关键字符（比如 \u0026ldquo;.\u0026quot;） 补全列表不完整，需要重新触发一次 之后，服务端会根据当前 输入光标的所在位置 以及 文件的上下文信息 来判断如何做代码补全。这一块背后的原理相对比较复杂，后续可以单独列一篇文章讲述。\nResources # 官方： https://microsoft.github.io/language-server-protocol/ 非官方博客： https://juejin.cn/post/7051453384645148680 ","date":"10 December 2023","permalink":"/posts/language/lsp/lsp-principle/","section":"博客","summary":"Language Server Protocol （语言服务器协议，简称 LSP）是微软于 2016 年提出的一套统一的通讯协议方案。该方案定义了一套编辑器或 IDE 与语言服务器之间使用的协议，该语言服务器提供自动完成、转到定义、查找所有引用等语言功能。","title":"Language Server Protocol 的工作原理"},{"content":"","date":"10 December 2023","permalink":"/tags/lsp/","section":"Tags","summary":"","title":"Lsp"},{"content":"Resources # 官网： https://microsoft.github.io/monaco-editor/ https://github.com/microsoft/monaco-editor 非官方博客 Building a code editor with Monaco ","date":"10 December 2023","permalink":"/posts/language/lsp/monaco-editor/","section":"博客","summary":"Resources # 官网： https://microsoft.","title":"Monaco Editor"},{"content":"What are data? What is knowledge? # 我们可以轻松地获取大量数据，而这些数据对我们来说毫无意义。那么我们真正需要的是什么？knowledge 是从 data 中提取出的有意义的信息。knowledge 就是对你有用的东西。\n数据仓储和在线分析处理 从大型数据库中提取有趣的知识（规则、规律、模式、约束） What Is Data Mining? # 数据挖掘（ Knowledge discovery(mining) in databases ，KDD）：从大型数据库中提取有趣（非平凡、隐含、先前未知且潜在有用）的信息或模式。\nAlternative names and their \u0026ldquo;inside stories\u0026rdquo;: Knowledge discovery(mining) in databases (KDD), knowledge extraction, data/pattern analysis, data archeology, data dredging, information harvesting, business intelligence, etc.\nWhy Data Mining? # 潜在应用\n数据库分析和决策支持：\n市场分析与管理：目标营销，客户关系管理，市场篮子分析，交叉销售，市场细分 风险分析与管理：预测，客户保留，改进承保，质量控制，竞争分析 诈骗检测与管理 其他应用\n文本挖掘（新闻组，电子邮件，文档）和Web分析 智能查询回答 数据挖掘：知识发现过程的核心\nSteps of a KDD Process\n学习应用领域：相关的先前知识和应用的目标 创建目标数据集：数据选择 数据清理和预处理：（可能占60%的工作量！） 数据减少和转换：找到有用的特征，降低维度/变量，不变表示 选择数据挖掘功能：摘要，分类，回归，关联，聚类 选择挖掘算法 数据挖掘：寻找感兴趣的模式 模式评估和知识呈现：可视化，转换，去除冗余模式等 利用发现的知识 Data Mining: On What Kind of Data?\n关系数据库 数据仓库 事务性数据库 高级数据库和信息库 面向对象和面向对象关系数据库 空间数据库 时间序列数据和临时数据 文本数据库和多媒体数据库 异构和遗留数据库 万维网 Data Mining Functionalities # 概念描述：表征和区分 推广、总结和对比数据特征，例如，干燥区域与湿润区域 关联（相关性和因果关系） 多维与单维关联 age(X, “20..29”) ^ income(X, “20..29K”) à buys(X, “PC”) [support = 2%, confidence = 60%] contains(T, “computer”) à contains(x, “software”) [1%, 75%] 分类和预测 查找描述和区分未来预测的类别或概念的模型（函数）。例如，根据气候对国家进行分类，或根据油耗对汽车进行分类 展示：决策树、分类规则、神经网络 预测：预测一些未知或缺失的数值 聚类分析 类别标签未知：将数据分组形成新类别，例如，聚类房屋以找到分布模式 基于原则的聚类：最大化类内相似性，最小化类间相似性 离群值分析 离群值：不符合数据一般行为的数据对象 它可以被视为噪声或异常，但在欺诈检测、罕见事件分析中非常有用 趋势和演变分析 趋势和偏差：回归分析 时序模式挖掘，周期性分析 基于相似性的分析 其他面向模式或统计分析 Are All the \u0026ldquo;Discovered\u0026rdquo; Patterns Interesting?\n数据挖掘系统/查询可能生成数千个模式，其中并非所有都有趣。建议的方法：以人为中心，基于查询，重点挖掘 Interestingness measures: 模式易于被人类理解、在新数据或测试数据上具有一定程度的准确性、潜在有用、新颖或验证用户希望确认的某些假设时，模式是有趣的 客观与主观的有趣性度量: 客观：基于模式的统计和结构，例如，支持度、置信度等 主观：基于用户对数据的信仰，例如，出乎意料性、新颖性、可操作性等。 Can We Find All and Only Interesting Patterns?\n找到所有有趣的模式：完整性。数据挖掘系统是否能够找到所有有趣的模式？ 仅寻找有趣的模式：优化。数据挖掘系统是否能够找到仅有趣的模式？ Approaches 首先生成所有模式，然后过滤掉不有趣的模式 仅生成有趣的模式——挖掘查询优化 Data Mining: Classification Schemes\n一般功能 描述性数据挖掘 预测性数据挖掘 不同视图，不同分类 要挖掘的数据库种类 要发现的知识种类 使用的技术种类 数据挖掘分类的多维视图\n要挖掘的数据库 关系型、事务型、面向对象、面向对象关系、主动、空间、时序、文本、多媒体、异构、遗留、万维网等。 要挖掘的知识 表征、区分、关联、分类、聚类、趋势、偏差、离群值分析等。 多个/集成功能和多层次挖掘 使用的技术 面向数据库、数据仓库（OLAP）、机器学习、统计学、可视化、神经网络等。 适应的应用 零售、电信、银行、欺诈分析、DNA挖掘、股票市场分析、Web挖掘、Web日志分析等。 OLAP挖掘：数据挖掘和数据仓库的集成\n数据挖掘系统、DBMS、数据仓库系统的耦合 无耦合，松耦合，半紧耦合，紧耦合 在线分析挖掘数据 挖掘和OLAP技术的集成 交互挖掘多层次知识 通过钻取/滚动、切片/切块等在不同抽象层次上挖掘知识的必要性 多个挖掘功能的集成 表征分类、首先聚类，然后关联 OLAM架构\nMajor Issues in Data Mining # 挖掘方法和用户交互 挖掘数据库中不同类型的知识 在多个抽象层次上交互挖掘知识 合并背景知识 数据挖掘查询语言和临时数据挖掘 表达和可视化数据挖掘结果 处理噪声和不完整数据 模式评估：有趣性问题 性能和可伸缩性 数据挖掘算法的效率和可伸缩性 并行、分布和增量挖掘方法 与数据类型多样性相关的问题 处理关系和复杂类型的数据 从异构数据库和全球信息系统（WWW）挖掘信息 与应用和社会影响相关的问题 应用发现的知识 领域特定的数据挖掘工具 智能查询回答 过程控制和决策制定 将发现的知识与现有知识整合：知识融合问题 保护数据安全、完整性和隐私 ","date":"9 December 2023","permalink":"/posts/reviews/%E7%A0%94%E7%A9%B6%E7%94%9F%E5%A4%8D%E4%B9%A0/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%8E%E7%9F%A5%E8%AF%86%E5%8F%91%E7%8E%B0/overview/","section":"博客","summary":"数据挖掘（ Knowledge discovery(mining) in databases ，KDD）：从大型数据库中提取有趣（非平凡、隐含、先前未知且潜在有用）的信息或模式。","title":"数据仓库与知识发现概览"},{"content":"一、书名和作者 # 《Extreme Programming Explained》 作者：Kent Beck 二、书籍概览 # 主要论点和结构：该书主要介绍了极限编程（Extreme Programming，XP）的理念和实践方法。作者Kent Beck以问题为出发点，提出了解决软件开发中困扰人们的风险和成本问题的方法。书中分为两部分，第一部分探讨了问题，第二部分提出了解决方案。 目标读者和应用场景：适合软件开发领域的从业者，尤其是团队领导者和项目经理。适用于那些希望改善软件开发流程、提高质量和降低成本的团队。 三、核心观点与主题 # 1. 主题一: 风险解决之道 # 子观点1XP对软件开发中基本问题的视角\n在软件开发中，作者将风险视为基本问题，强调了它对项目成功的影响。通过极限编程（XP）的价值观和原则，作者提供了一种解决风险的方法。XP鼓励团队通过沟通、简单性、反馈和勇气来处理风险，这些价值观有助于构建强大的团队文化，提高项目的成功概率。\n子观点2四个变量的相互关系\n四个变量——范围、时间、资源和质量，构成了软件开发的基本元素。作者在书中强调这些变量之间的相互作用和关联，指出它们在项目中的调整可能影响其他变量。例如，更改项目的范围可能会影响时间和资源的分配。通过理解和平衡这些变量，团队可以更好地规划和执行项目，降低风险。\n实例或案例变化需求的应对策略\n举例来说，假设一个软件项目的客户提出了新的功能需求，这可能影响项目的范围。在传统的开发方法中，这可能导致重新规划整个项目，增加时间和资源的需求。然而，XP提倡的灵活性使团队能够迅速适应这种变化。通过及时的沟通和简单的设计，团队可以快速调整项目范围，最小化对时间和资源的影响，从而降低变更的成本。\n2. 主题二: XP的核心价值观和基本原则 # 子观点1XP的核心价值观\n极限编程（XP）的核心价值观包括沟通、简单性、反馈和勇气。这些价值观不仅仅是口号，而是在实践中体现出来的团队行为。沟通鼓励成员分享信息，简单性促使团队设计出最简洁的解决方案，反馈则确保持续改进，而勇气则使成员愿意面对困难。\n子观点2基本原则的重要性\n基本原则是XP的指导方针，强调了回归基础、编码、测试、倾听和设计的重要性。这些原则提供了团队在整个开发周期中的指导，确保项目的可维护性和高质量。通过回归基础，团队保持一致性；编码和测试确保可靠性；倾听和设计促进创新和可维护性。\n实例或案例实践中的XP价值观\n以沟通为例，团队成员可以使用日常站会来分享他们的进展和遇到的问题。这种开放的沟通方式有助于及时解决问题，保持团队的整体进度。在设计方面，持续的反馈循环确保了代码的不断改进。通过编写测试，团队能够更加自信地进行重构，改进系统设计而不担心引入错误。这些实践有助于构建高效、创新和可维护的软件。\n3. 主题三: XP的解决方案和管理策略 # 子观点1XP的解决方案实践\n极限编程（XP）的解决方案涵盖了规划游戏、小版本发布、简单设计、测试、重构等核心实践。这些实践构成了XP的开发周期，强调了迭代和持续改进的重要性。规划游戏和小版本发布使团队能够更好地应对变化，简单设计和测试确保了高质量的代码，而重构则保持了代码的可维护性。\n子观点2管理策略的重要性\n在管理策略方面，强调了分离业务和技术责任，同时提供了指标、辅导、跟踪和干预等工具。这种分离有助于团队专注于各自的专业领域，提高效率。同时，指标和辅导等工具提供了管理层更好地监控和指导团队的手段。\n实例或案例小版本发布的灵活性\n以小版本发布为例，团队在每次发布中只引入少量新功能，这降低了风险，并使问题的定位更加容易。在一个迭代中，团队可能会遇到一些未预料的问题。通过及时的干预和辅导，团队能够快速调整，确保项目的成功。这种灵活性和实时的管理支持使XP在实践中更具可行性。\n4. 主题四: XP的全生命周期和角色分工 # 子观点1XP在不同生命周期阶段的应用\n探讨了XP在不同阶段的生命周期中的应用，从探索、规划到维护和结束。这种全生命周期的考虑有助于团队更好地规划和管理项目，并确保项目的长期成功。\n子观点2XP中不同角色的协作\n引入了XP中不同角色的概念，如程序员、客户、测试人员等，以及他们在项目中的作用。这种分工和协作方式有助于充分利用每个团队成员的专业技能，提高整体效能。\n实例或案例团队协作与系统稳定性\n在探索阶段，团队可能会面临技术上的不确定性，通过迅速的原型和实验，可以更好地了解项目的可行性。在维护阶段，通过不断的重构和持续集成，团队能够确保系统的稳定性和可维护性。不同角色的协作，如程序员与测试人员的紧密配合，有助于在项目中实现高质量的代码。\n这些案例展示了XP的实际应用，如何解决具体问题，并提供了在不同阶段的生命周期中的灵活性和可行性。\n四、亮点与启发 # 最有影响的观点或实例:\n《Extreme Programming Explained》中的亮点主要集中在强调的价值观、原则以及核心实践，这些对提升软件开发团队效能具有深远的影响。\n首先，书中强调的四个核心价值观——沟通、简单性、反馈和勇气，构成了XP的基石。通过强调沟通，团队成员能够更好地分享信息、解决问题，促进协作。简单性原则鼓励团队设计最简单的解决方案，从而提高代码可读性和可维护性。反馈机制确保了持续的改进，而勇气则激发团队成员面对挑战。这些价值观的贯彻，使得XP团队更有活力、适应性更强。\n其次，核心实践如规划游戏、小版本发布等方法也是书中的亮点。规划游戏通过迭代式规划，使团队更灵活地应对变化，提高项目的适应性。小版本发布策略降低了项目的风险，使得团队能够更频繁地发布功能，及时获取用户反馈。这些实践不仅提高了开发的效率，也确保了项目的质量和用户满意度。\n对个人或专业发展的启示:\n通过采用XP方法，不仅可以优化软件开发过程，还能够对个人和团队的成长产生积极影响。\n首先，XP的灵活性使得团队更好地适应变化。在当今快速变化的技术和市场环境中，拥有适应性是团队的核心竞争力。XP通过强调灵活性和迭代式开发，帮助团队更快速地调整项目方向，迎接变化。\n其次，沟通效率的提升是XP方法给个人带来的重要收益。通过强调沟通价值观，团队成员更倾向于分享信息、解决问题，构建更融洽的工作氛围。这有助于个人在团队中更好地理解项目需求，提高工作效率。\n最后，对于个人而言，XP方法追求卓越的理念激励着个人不断学习和改进。在XP的框架下，每个团队成员都被鼓励为卓越而努力。这种追求卓越的精神激发了个人的专业成长动力，使其在软件开发领域更上一层楼。\n五、批评与局限性 # 任何有争议、模糊或过时的信息:\n尽管《Extreme Programming Explained》提出了许多有益的实践，但其中也存在一些有争议、模糊或过时的信息。首先，XP的一些实践可能在某些项目或组织中引起争议。例如，对于大型、复杂项目，XP的一些简化设计和快速迭代的方法可能显得过于激进，难以适应项目的特定需求。\n其次，书中对于沟通和团队合作的强调可能在某些文化和组织环境中存在模糊性。不同文化对于沟通方式的理解和接受程度各异，有时可能需要根据实际情况进行调整。此外，某些组织可能对于强调团队平等和开放式沟通的理念持保留态度，这可能导致XP的实践难以在这些环境中顺利推行。\n可能的不足或缺陷:\n尽管XP提供了许多有益的理念和方法，但书中的案例有时可能过于理想化，实际应用中可能会面临挑战。首先，XP的实践可能并非适用于所有项目。在某些行业或特殊领域，项目的特性可能使得XP的某些实践不够灵活或难以实施。例如，一些对安全性要求极高的项目可能需要更加严格的开发流程，XP的一些快速迭代可能无法满足这种需求。\n其次，XP并非适用于所有组织。一些传统、保守的组织可能对于XP的激进和开放式的管理风格产生抵触，导致XP的实施难以融入组织文化。此外，XP强调团队的自我管理和自组织，这对于一些组织而言可能难以接受，尤其是在需要更严格层级控制的环境中。\n在实践中，XP可能需要在团队和组织层面做出一些定制化的调整，以适应具体项目和环境的需求。因此，在采用XP之前，团队和组织需要仔细评估其适用性，并在实践中灵活调整，以确保最佳的实施效果。\n六、实际应用和拓展 # 在实际工作/学习中如何应用这些概念:\n将XP的核心概念融入实际工作中是一个逐步的过程。首先，团队可以逐渐采用XP的核心实践，如规划游戏、小版本发布等。通过规划游戏，团队能够更灵活地应对需求变化，而小版本发布则提供了频繁发布新功能的机制。这些实践的逐步采纳可以在不破坏原有流程的情况下改善软件开发过程，提高团队的效率和适应性。\n此外，团队还可以重视XP所强调的价值观和原则，如沟通、简单性、反馈和勇气。通过加强团队成员之间的沟通，建立更紧密的合作关系，可以提高项目的整体协作效果。简单设计原则则鼓励团队设计出更易维护的代码，反馈机制确保团队在不断学习中不断改进。\n对未来研究或实践的建议:\n未来的研究和实践可以关注XP方法的定制化和灵活性。鼓励团队根据自身情况和项目需求进行调整和改进，以确保XP的实践更贴近实际应用。此外，随着科技的不断发展，团队还应该关注新兴的软件开发方法和工具，保持学习和改进的动力。对于未来的研究而言，可以深入探讨XP方法在不同领域和行业的适用性，以及如何在不同文化背景下实现最佳效果。\n七、总结与评价 # 对书籍的整体评价:\n《Extreme Programming Explained》深入阐述了软件开发中的核心问题，并提供了实用的解决方案，对软件开发者和管理者都有价值。书籍通过对XP的详细介绍，使读者能够深刻理解XP的核心概念和实践方法，为团队提升软件开发效能提供了有力的指导。\n书籍的长处和短处:\n优点在于提供了具体实践方法，使读者能够从理论到实践的过程中逐步学习。书籍详细介绍了XP的核心概念，包括价值观、原则和实践，为读者提供了深入的理解。然而，书中可能过于理想化，实际应用中需根据具体情况调整。在实践中，可能会面临一些挑战，需要灵活应对，而书中的案例可能没有全面考虑这些实际问题。\n","date":"5 December 2023","permalink":"/read/%E6%9E%81%E9%99%90%E7%BC%96%E7%A8%8B/","section":"阅读","summary":"主要介绍了极限编程（Extreme Programming，XP）的理念和实践方法。作者Kent Beck以问题为出发点，提出了解决软件开发中困扰人们的风险和成本问题的方法。书中分为两部分，第一部分探讨了问题，第二部分提出了解决方案。适合软件开发领域的从业者，尤其是团队领导者和项目经理。适用于那些希望改善软件开发流程、提高质量和降低成本的团队。","title":"《极限编程》读书笔记"},{"content":"原项目链接：\nmall-swarm：https://github.com/macrozheng/mall-swarm mall-app-web：https://github.com/macrozheng/mall-app-web 目前此项目只配置了后端 springcloud 项目，没有配置 前端的 vue 项目\n组织架构 # 为了快速理解该项目，首先该了解该项目的组织架构：\nmall ├── mall-common -- 工具类及通用代码模块 ├── mall-mbg -- MyBatisGenerator生成的数据库操作代码模块 ├── mall-auth -- 基于Spring Security Oauth2的统一的认证中心 ├── mall-gateway -- 基于Spring Cloud Gateway的微服务API网关服务 ├── mall-monitor -- 基于Spring Boot Admin的微服务监控中心 ├── mall-admin -- 后台管理系统服务 ├── mall-search -- 基于Elasticsearch的商品搜索系统服务 ├── mall-portal -- 移动端商城系统服务 ├── mall-demo -- 微服务远程调用测试服务 └── config -- 配置中心存储的配置 基础设施清单：\nmysql:5.7 redis:7 nginx:1.22 rabbitmq:3.9-management elasticsearch:7.17.3 logstash:7.17.3 mongo:4 minio/minio 部署步骤 # 打 jar 包 # 在根目录下，打项目的jar包\nmvn install mvn package 打 docker 镜像 # 然后将jar包build成docker，以 mall-auth 为例， docker build -t xxx/mall-auth::1.0-SNAPSHOT .\n其中 dockerfile 如下：\nFROM java:8 ADD mall-auth-1.0-SNAPSHOT.jar /mall-tiny-docker-file.jar RUN bash -c \u0026#39;touch /mall-tiny-docker-file.jar\u0026#39; EXPOSE 8401 ENTRYPOINT [\u0026#34;java\u0026#34;, \u0026#34;-jar\u0026#34;,\u0026#34;/mall-tiny-docker-file.jar\u0026#34;] MAINTAINER wfuing 其中，端口的映射关系如下：\nmall-auth：8401 mall-admin：8080 mall-gateway：8201 mall-monitor：8101 mall-portal：8085 mall-search：8081 打完 docker 后，将镜像推送到 dockerhub 上，使用 docker push xxx:xxx\n安装 kubevela # 请查看官方文档：https://kubevela.io/docs/installation/kubernetes/\n书写 kubevela 配置文件 # 这边给出我的配置文件，具体如下：\napiVersion: core.oam.dev/v1beta1 kind: Application metadata: name: mall-swarm namespace: mall spec: components: - name: mysql type: webservice properties: image: mysql:5.7 ports: - port: 3306 expose: true cmd: [ \u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;mysqld --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci\u0026#34;, ] env: - name: MYSQL_ROOT_PASSWORD value: root volumeMounts: hostPath: - name: volume1 #数据文件挂载 mountPath: /var/lib/mysql path: /mydata/mysql/data/db - name: volume2 #配置文件挂载 mountPath: /etc/mysql/conf.d path: /mydata/mysql/data/conf - name: volume3 #日志文件挂载 mountPath: /var/log/mysql path: /mydata/mysql/log traits: - type: scaler properties: replicas: 1 - name: redis type: webservice properties: image: redis:7 ports: - port: 6379 expose: true volumeMounts: hostPath: - name: volume1 #数据文件挂载 mountPath: /data path: /mydata/redis/data traits: - type: scaler properties: replicas: 1 - name: rabbitmq type: webservice properties: image: rabbitmq:3.9-management ports: - port: 5672 expose: true - port: 15672 expose: true env: - name: RABBITMQ_DEFAULT_VHOST value: /mall volumeMounts: hostPath: - name: volume1 mountPath: /var/lib/rabbitmq path: /mydata/rabbitmq/data - name: volume2 mountPath: /var/log/rabbitmq path: /mydata/rabbitmq/log traits: - type: scaler properties: replicas: 1 - name: elasticsearch type: webservice properties: image: elasticsearch:7.17.3 ports: - port: 9200 expose: true env: - name: cluster.name value: elasticsearch - name: discovery.type value: single-node - name: ES_JAVA_OPTS value: -Xms512m -Xmx1024m volumeMounts: hostPath: - name: volume4 mountPath: /usr/share/elasticsearch/plugins path: /mydata/elasticsearch/plugins - name: volume5 mountPath: /var/lib/elasticsearch/data path: /mydata/elasticsearch/data traits: - type: scaler properties: replicas: 1 - name: logstash type: webservice dependsOn: - elasticsearch properties: image: logstash:7.17.3 ports: - port: 4560 expose: true - port: 4561 expose: true - port: 4562 expose: true - port: 4563 expose: true env: - name: TZ value: Asia/Shanghai volumeMounts: hostPath: - name: volume1 mountPath: /usr/share/logstash/pipeline/logstash.conf path: /mydata/logstash/logstash.conf traits: - type: scaler properties: replicas: 1 - name: mongo type: webservice properties: image: mongo:4 # imagePullPolicy: Always ports: - port: 27017 expose: true volumeMounts: hostPath: - name: volume1 mountPath: /data/db path: /mydata/mongo/db traits: - type: scaler properties: replicas: 1 - name: nacos-registry type: webservice properties: image: nacos/nacos-server:v2.1.0 ports: - port: 8848 expose: true env: - name: MODE value: standalone traits: - type: scaler properties: replicas: 1 - name: minio type: webservice properties: image: minio/minio ports: - port: 9000 expose: true - port: 9001 expose: true args: [\u0026#34;server\u0026#34;, \u0026#34;--console-address\u0026#34;, \u0026#34;:9001\u0026#34;, \u0026#34;/data\u0026#34;] volumeMounts: hostPath: - name: volume1 mountPath: /data path: /mydata/minio/data env: - name: MINIO_ROOT_USER value: minioadmin - name: MINIO_ROOT_PASSWORD value: minioadmin traits: - type: scaler properties: replicas: 1 - name: mall-auth type: webservice properties: image: september9/mall-auth:1.0-SNAPSHOT imagePullPolicy: Always ports: - port: 8401 expose: true env: - name: TZ value: Asia/Shanghai traits: - type: scaler properties: replicas: 1 - name: mall-portal type: webservice dependsOn: - mysql - nacos-registry - mongo - redis - rabbitmq properties: image: september9/mall-portal:1.0-SNAPSHOT imagePullPolicy: Always ports: - port: 8085 expose: true env: - name: TZ value: Asia/Shanghai traits: - type: scaler properties: replicas: 1 - name: mall-gateway type: webservice dependsOn: - nacos-registry - redis properties: image: september9/mall-gateway:1.0-SNAPSHOT imagePullPolicy: Always ports: - port: 8201 expose: true env: - name: TZ value: Asia/Shanghai traits: - type: scaler properties: replicas: 1 - name: mall-monitor dependsOn: - nacos-registry - mall-search - mall-admin type: webservice properties: image: september9/mall-monitor:1.0-SNAPSHOT imagePullPolicy: Always ports: - port: 8101 expose: true env: - name: TZ value: Asia/Shanghai traits: - type: scaler properties: replicas: 1 - name: mall-search type: webservice dependsOn: - mysql - nacos-registry properties: image: september9/mall-search:1.0-SNAPSHOT imagePullPolicy: Always ports: - port: 8081 expose: true env: - name: TZ value: Asia/Shanghai traits: - type: scaler properties: replicas: 1 - name: mall-admin type: webservice dependsOn: - mysql - nacos-registry - minio - redis properties: image: september9/mall-admin:1.0-SNAPSHOT imagePullPolicy: Always ports: - port: 8080 expose: true env: - name: TZ value: Asia/Shanghai traits: - type: scaler properties: replicas: 1 在这里要指出的是，如果你需要将 properties.image 改成你自己的docker镜像，当然用上面的也行。\n上面的配置都采用的 Kubevela 中的 webservice 组件进行定义，如果需要查看 webservice 的文档，可以在命令行输入 vela show webservice --web 之后就可以在浏览器中查看文档。\n有了配置文件以后，就可以在 kubevela 上进行部署，在上述的 mall-swarm.yaml 文件的目录下执行 vela up -f mall-swarm.yaml 即可实现部署。\n之后可以在 VelaUX 的可视化界面上查看相关的部署情况，VelaUX 的教程请见官网 https://kubevela.io/docs/reference/addons/velaux/ 。\n验证部署情况 # 通过 vela status -n mall mall-swarm --endpoint 即可知道所有项目的 DNS 地址，如下：\n~$ vela status -n mall mall-swarm --endpoint Please access mall-swarm from the following endpoints: +---------+----------------+-----------------------------+-----------------------------+-------+ | CLUSTER | COMPONENT | REF(KIND/NAMESPACE/NAME) | ENDPOINT | INNER | +---------+----------------+-----------------------------+-----------------------------+-------+ | local | mysql | Service/mall/mysql | mysql://mysql.mall:3306 | true | | local | redis | Service/mall/redis | redis://redis.mall:6379 | true | | local | rabbitmq | Service/mall/rabbitmq | rabbitmq.mall:5672 | true | | local | rabbitmq | Service/mall/rabbitmq | rabbitmq.mall:15672 | true | | local | elasticsearch | Service/mall/elasticsearch | elasticsearch.mall:9200 | true | | local | mongo | Service/mall/mongo | mongo.mall:27017 | true | | local | nacos-registry | Service/mall/nacos-registry | nacos-registry.mall:8848 | true | | local | minio | Service/mall/minio | minio.mall:9000 | true | | local | minio | Service/mall/minio | minio.mall:9001 | true | | local | mall-auth | Service/mall/mall-auth | mall-auth.mall:8401 | true | | local | logstash | Service/mall/logstash | logstash.mall:4560 | true | | local | logstash | Service/mall/logstash | logstash.mall:4561 | true | | local | logstash | Service/mall/logstash | logstash.mall:4562 | true | | local | logstash | Service/mall/logstash | logstash.mall:4563 | true | | local | mall-portal | Service/mall/mall-portal | mall-portal.mall:8085 | true | | local | mall-gateway | Service/mall/mall-gateway | mall-gateway.mall:8201 | true | | local | mall-search | Service/mall/mall-search | mall-search.mall:8081 | true | | local | mall-admin | Service/mall/mall-admin | http://mall-admin.mall:8080 | true | | local | mall-monitor | Service/mall/mall-monitor | mall-monitor.mall:8101 | true | +---------+----------------+-----------------------------+-----------------------------+-------+ 有了 DNS 地址就可以将集群中的内容 port-forward 到 localhost。\nmysql配置 # 在上面的部署中，已经将 mysql 的 data 挂载到了本地\n首先进入 mysql vela exec mysql -n mall -- mysql -h mysql.mall -P 3306 --user=root --password=root\n之后在 mysql 中执行一次 document/sql/mall.sql 即可实现数据挂载。\nRabbitMQ配置 # 在配置过程中会遇到一些问题，主要来说还是挂载路径的权限问题，在 RabbitMQ挂载文件权限问题-踩坑经历这篇博文中有详细记载。\n简单来说，就是修改log目录权限\nsudo chmod 777 mydata/rabbitmq/log 然后如果一开始初始化出错，一定要把挂载的路径下的文件全不删除后，再重新启动。\nport-forward mall-monitor 的地址 # 在这个项目中可以 port-forward mall-monitor 的地址，可以查看微服务部署的情况。\n~$ vela port-forward -n mall mall-swarm 8101:8101 ? There are 14 services match your filter conditions. Please choose one: Cluster | Component | Service local | mall-monitor | mall-monitor trying to connect the remote endpoint svc/mall-monitor 8101:8101 ..Forwarding from 127.0.0.1:8101 -\u0026gt; 8101 Forwarding from [::1]:8101 -\u0026gt; 8101 Forward successfully! Opening browser ... Failed to open browser: exec: \u0026#34;xdg-open\u0026#34;: executable file not found in $PATHHandling connection for 8101 Handling connection for 8101 Handling connection for 8101 Handling connection for 8101 在本地的浏览器打开\n用户名和密码如下：\nmacro 123456 进入 mall-monitor 界面，可以监控项目中的微服务。\nport-forward mall-gateway 的地址 # 也可以 port-forward mall-monitor 的地址，可以微服务部署的 api。\n~$ vela port-forward mall-swarm 8201:8201 ? There are 14 services match your filter conditions. Please choose one: Cluster | Component | Service local | mall-gateway | mall-gateway trying to connect the remote endpoint svc/mall-gateway 8201:8201 ..Forwarding from 127.0.0.1:8201 -\u0026gt; 8201 Forwarding from [::1]:8201 -\u0026gt; 8201 Forward successfully! Opening browser ... Failed to open browser: exec: \u0026#34;xdg-open\u0026#34;: executable file not found in $PATHHandling connection for 8201 Handling connection for 8201 Handling connection for 8201 Handling connection for 8201 在浏览器中输入http://localhost:8201/doc.html 即可进入。\n当然这个项目中使用了 token 验证，可以使用 postman 发送 post 请求\n之后可以将这个 token 放到下图的参数值中\n之后就可以访问所有微服务的 api，并进行访问，但是注意在访问的时候需要在 header 中加上 Authorization ，值为上图中请求返回的 token，即可访问数据库。\n","date":"5 December 2023","permalink":"/posts/architecture/virtualization/k8s/kubevela-mall-swarm/","section":"博客","summary":"本项目用kubevela部署mall-swarm，mall-swarm是一套微服务商城系统，采用了 Spring Cloud 2021 \u0026amp; Alibaba、Spring Boot 2.7、Oauth2、MyBatis、Elasticsearch、Docker、Kubernetes等核心技术。","title":"使用 Kubevela 部署 mall-swarm 项目"},{"content":"","date":"5 December 2023","permalink":"/tags/ceph/","section":"Tags","summary":"","title":"Ceph"},{"content":"研一云计算的课程作业之一是用ceph和flink实现一个实时数据分析工具，我们是四个人一个小组，我负责部署ceph。互联网时代的网络资源层次不齐，记录几个比较好的博客，信息熵比较大。\nResources # 官网： https://docs.ceph.com/en/latest/start/intro/ https://docs.ceph.com/en/mimic/start/quick-ceph-deploy/ 非官方资源： 手把手带你用docker部署Ceph集群 如何使用fdisk进行分区 使用ceph-deploy部署ceph集群 ceph部署常见错误 架构 # Ceph部分 # 1、关闭防火墙和selinux\nsed -i \u0026#34;s/SELINUX=enforcing/SELINUX=permissive/g\u0026#34; /etc/selinux/config setenforce 0 systemctl stop firewalld systemctl disable firewalld 2、配置hosts文件\n保证集群内主机名与ip解析正常（每个节点都需要配置）\n[root@ceph-node1 ~]# cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.56.125 ceph-node1 192.168.56.126 ceph-node2 192.168.56.127 ceph-node3 [root@ceph-node1 ~]# ping ceph-node2 PING ceph-node2 (192.168.56.126) 56(84) bytes of data. 64 bytes from ceph-node2 (192.168.56.126): icmp_seq=1 ttl=64 time=0.616 ms ………… 3、创建部署用户及配置sudo权限（所有节点都执行）\na.考虑到使用root用户的安全性问题，所以这里创建一个 ceph-admin 普通用户做为部署及运维使用 b.再加上ceph-deploy会在节点安装软件包，所以创建的用户需要无密码 sudo 权限\n[root@ceph-node1 ~]# useradd ceph-admin [root@ceph-node1 ~]# echo \u0026#34;123456\u0026#34; | passwd --stdin ceph-admin Changing password for user ceph-admin. passwd: all authentication tokens updated successfully. [root@ceph-node1 ~]# echo \u0026#34;ceph-admin ALL = NOPASSWD:ALL\u0026#34; | tee /etc/sudoers.d/ceph-admin ceph-admin ALL = NOPASSWD:ALL [root@ceph-node1 ~]# chmod 0440 /etc/sudoers.d/ceph-admin [root@ceph-node1 ~]# ll /etc/sudoers.d/ceph-admin -r--r-----. 1 root root 30 Oct 19 16:06 /etc/sudoers.d/ceph-admin 测试\n[root@ceph-node1 ~]# su - ceph-admin Last login: Mon Oct 19 16:11:51 CST 2020 on pts/0 [ceph-admin@ceph-node1 ~]$ sudo su - Last login: Mon Oct 19 16:12:04 CST 2020 on pts/0 [root@ceph-node1 ~]# exit logout [ceph-admin@ceph-node1 ~]$ exit logout 4、配置ssh无密码访问（在主节点node1上执行）\n[root@ceph-node1 ~]# su - ceph-admin [ceph-admin@ceph-node1 ~]$ ssh-keygen （每一步都按回车，口令密码留空） [ceph-admin@ceph-node1 ~]$ ssh-copy-id ceph-admin@ceph-node1 [ceph-admin@ceph-node1 ~]$ ssh-copy-id ceph-admin@ceph-node2 [ceph-admin@ceph-node1 ~]$ ssh-copy-id ceph-admin@ceph-node3 5、配置ntp时间同步\n配置时间同步目的：因在时间一致的情况下，才可保证集群正常运行 配置时间同步方式：node1连接网络上的ntp服务器同步时间，node2,3连接node1同步时间（即node1既为ntp服务端，也为客户端） 注：ntpd启动后需要等待几分钟去同步\nyum -y intall ntp（安装ntp，全部节点都需要执行） node1节点操作： vim /etc/ntp.conf 注释掉默认的配置项： #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst 添加配置项： server ntp1.aliyun.com #阿里云ntp服务器 server 127.127.1.0 #本地ntp服务器，配置此项是为了在外网ntp连接异常的情况下还能保证ntp正常，维护集群稳定 node2/node3节点操作： vim /etc/ntp.conf 同样注释掉默认的server配置项： 添加配置项： server 192.168.56.125 #node1-ntp服务器 全部节点都执行： systemctl restart ntpd systemctl enable ntpd 查看ntp连接情况和状态 [root@ceph-node1 ~]# ntpq -p remote refid st t when poll reach delay offset jitter ============================================================================== *120.25.115.20 10.137.53.7 2 u 41 128 377 30.382 -1.019 1.001 LOCAL(0) .LOCL. 5 l 806 64 0 0.000 0.000 0.000 [root@ceph-node2 ~]# ntpq -p remote refid st t when poll reach delay offset jitter ============================================================================== *ceph-node1 120.25.115.20 3 u 20 64 377 2.143 33.254 10.350 [root@ceph-node1 ~]# ntpstat synchronised to NTP server (120.25.115.20) at stratum 3 time correct to within 27 ms polling server every 128 s 二、开始部署Ceph集群\n1、添加阿里云的base源和epel源（所有节点都执行）\n备份系统原本的源 [root@ceph-node1 ~]# mkdir /mnt/repo_bak [root@ceph-node1 ~]# mv /etc/yum.repos.d/* /mnt/repo_bak 添加新源 [root@ceph-node1 ~]# wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo [root@ceph-node1 ~]# wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo 2、添加ceph的yum源（所有节点都执行）\n注意事项： 这里的yum源是确定了ceph的版本，在源中的baseurl项中rpm-nautilus即代表着是ceph的nautilus版本的rpm包（nautilus是ceph的14.x版本）如果需要安装其他版本，还需要替换为其他版本号，12.x版本是luminous，13.x版本是rpm-mimic。 详情可以去ceph官方源中查看：download.ceph.com/\nvim /etc/yum.repos.d/ceph.repo [Ceph] name=Ceph baseurl=http://download.ceph.com/rpm-nautilus/el7/x86_64 enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc priority=1 [Ceph-noarch] name=Ceph noarch packages baseurl=http://download.ceph.com/rpm-nautilus/el7/noarch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc priority=1 [ceph-source] name=Ceph source packages baseurl=http://download.ceph.com/rpm-nautilus/el7/SRPMS enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc priority=1 更新yum缓存及系统软件\nyum makecache yum -y update 可查看ceph版本，判断yum是否配置正确\n[root@ceph-node1 yum.repos.d]# yum list ceph --showduplicates |sort -r * updates: mirrors.cn99.com Loading mirror speeds from cached hostfile Loaded plugins: fastestmirror * extras: mirrors.163.com ceph.x86_64 2:14.2.9-0.el7 Ceph ceph.x86_64 2:14.2.8-0.el7 Ceph ceph.x86_64 2:14.2.7-0.el7 Ceph ceph.x86_64 2:14.2.6-0.el7 Ceph ceph.x86_64 2:14.2.5-0.el7 Ceph ceph.x86_64 2:14.2.4-0.el7 Ceph ceph.x86_64 2:14.2.3-0.el7 Ceph ceph.x86_64 2:14.2.2-0.el7 Ceph ceph.x86_64 2:14.2.11-0.el7 Ceph ceph.x86_64 2:14.2.1-0.el7 Ceph ceph.x86_64 2:14.2.10-0.el7 Ceph ceph.x86_64 2:14.2.0-0.el7 Ceph ceph.x86_64 2:14.1.1-0.el7 Ceph ceph.x86_64 2:14.1.0-0.el7 Ceph * base: mirrors.163.com Available Packages [root@ceph-node1 yum.repos.d]# yum list ceph-deploy --showduplicates |sort -r * updates: mirrors.cn99.com Loading mirror speeds from cached hostfile Loaded plugins: fastestmirror * extras: mirrors.163.com ceph-deploy.noarch 2.0.1-0 Ceph-noarch ceph-deploy.noarch 2.0.0-0 Ceph-noarch ceph-deploy.noarch 1.5.39-0 Ceph-noarch ceph-deploy.noarch 1.5.38-0 Ceph-noarch ceph-deploy.noarch 1.5.37-0 Ceph-noarch ceph-deploy.noarch 1.5.36-0 Ceph-noarch ceph-deploy.noarch 1.5.35-0 Ceph-noarch ceph-deploy.noarch 1.5.34-0 Ceph-noarch ceph-deploy.noarch 1.5.33-0 Ceph-noarch ceph-deploy.noarch 1.5.32-0 Ceph-noarch ceph-deploy.noarch 1.5.31-0 Ceph-noarch ceph-deploy.noarch 1.5.30-0 Ceph-noarch ceph-deploy.noarch 1.5.29-0 Ceph-noarch * base: mirrors.163.com Available Packages 3、安装ceph-deploy（在主节点node1上执行）\n[root@ceph-node1 ~]# su - ceph-admin [ceph-admin@ceph-node1 ~]$ sudo yum -y install python-setuptools #安装ceph依赖包 [ceph-admin@ceph-node1 ~]$ sudo yum install ceph-deploy （默认会选择安装2.0最新版本） 查看ceph-deploy安装版本 [root@ceph-node1 ~]# ceph-deploy --version 2.0.1 4、初始化集群（在主节点node1上执行） 创建集群安装目录（ceph-deploy部署程序会将文件输出到当前目录）\n[ceph-admin@ceph-node1 ~]$ mkdir cluster [ceph-admin@ceph-node1 ~]$ cd cluster/ 创建集群（后边是指定哪些节点做为mon监视器使用，所以选择规划中部署mon的节点-node1） [ceph-admin@ceph-node1 cluster]$ ceph-deploy new ceph-node1 [ceph_deploy.conf][DEBUG ] found configuration file at: /home/ceph-admin/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (2.0.1): /bin/ceph-deploy new ceph-node1 [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] func : \u0026lt;function new at 0x7f14c44c9de8\u0026gt; [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] cd_conf : \u0026lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f14c3c424d0\u0026gt; [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] ssh_copykey : True [ceph_deploy.cli][INFO ] mon : [\u0026#39;ceph-node1\u0026#39;] [ceph_deploy.cli][INFO ] public_network : None [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] cluster_network : None [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.cli][INFO ] fsid : None [ceph_deploy.new][DEBUG ] Creating new cluster named ceph [ceph_deploy.new][INFO ] making sure passwordless SSH succeeds [ceph-node1][DEBUG ] connection detected need for sudo [ceph-node1][DEBUG ] connected to host: ceph-node1 [ceph-node1][DEBUG ] detect platform information from remote host [ceph-node1][DEBUG ] detect machine type [ceph-node1][DEBUG ] find the location of an executable [ceph-node1][INFO ] Running command: sudo /usr/sbin/ip link show [ceph-node1][INFO ] Running command: sudo /usr/sbin/ip addr show [ceph-node1][DEBUG ] IP addresses found: [u\u0026#39;192.168.56.125\u0026#39;] [ceph_deploy.new][DEBUG ] Resolving host ceph-node1 [ceph_deploy.new][DEBUG ] Monitor ceph-node1 at 192.168.56.125 [ceph_deploy.new][DEBUG ] Monitor initial members are [\u0026#39;ceph-node1\u0026#39;] [ceph_deploy.new][DEBUG ] Monitor addrs are [\u0026#39;192.168.56.125\u0026#39;] [ceph_deploy.new][DEBUG ] Creating a random mon key... [ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring... [ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf... [ceph-admin@ceph-node1 cluster]$ ls ceph.conf ceph-deploy-ceph.log ceph.mon.keyring 在当前目录下的ceph.conf中添加以下两行内容 public_network = 192.168.56.0/24 cluster_network = 192.168.56.0/24 安装Ceph包至其他节点 （其中 --no-adjust-repos 参数含义：使用本地配置的源，不更改源。以防出现问题） [ceph-admin@ceph-node1 cluster]$ ceph-deploy install --no-adjust-repos ceph-node1 ceph-node2 ceph-node3 如果出现“RuntimeError: Failed to execute command: ceph \u0026ndash;version”报错，是因为服务器网络问题导致，下载ceph安装包速度太慢，达到5分钟导致超时，可以重复执行，或者单独在所有节点执行yum -y install ceph即可\n初始化mon节点\n在2.0.1版本的ceph-deploy中在该初始化的时候就会做收集密钥的动作，无需再执行 ceph-deploy gatherkeys {monitor-host} 这个命令\n[ceph-admin@ceph-node1 cluster]$ ceph-deploy mon create-initial\n5、添加OSD\n如果是里边有数据的磁盘，还需先清除数据：（详细可查看 ceph-depoy disk zap \u0026ndash;help）\n列出所有节点上所有可用的磁盘 [ceph-admin@ceph-node1 cluster]$ ceph-deploy disk list ceph-node1 ceph-node2 ceph-node3 清除数据 sudo ceph-deploy disk zap {osd-server-name} {disk-name} eg：sudo ceph-deploy disk zap ceph-node2 /dev/sdb 如果是干净的磁盘，可忽略上边清除数据的操作，直接添加OSD即可 （我这里是新添加的/dev/sdb磁盘） [ceph-admin@ceph-node1 cluster]$ ceph-deploy osd create --data /dev/sdb ceph-node1 [ceph-admin@ceph-node1 cluster]$ ceph-deploy osd create --data /dev/sdb ceph-node2 [ceph-admin@ceph-node1 cluster]$ ceph-deploy osd create --data /dev/sdb ceph-node3 可以看到cpeh将新增OSD创建为LVM格式加入ceph集群中 [ceph-admin@ceph-node1 cluster]$ sudo pvs PV VG Fmt Attr PSize PFree /dev/sdb ceph-ab1b8533-018e-4924-8520-fdbefbb7d184 lvm2 a-- \u0026lt;10.00g 0 6、允许主机以管理员权限执行 Ceph 命令 将ceph-deploy命令将配置文件和 admin key复制到各个ceph节点，其他节点主机也能管理ceph集群 [ceph-admin@ceph-node1 cluster]$ ceph-deploy admin ceph-node1 ceph-node2 ceph-node3\n7、部署MGR用于获取集群信息 [ceph-admin@ceph-node1 cluster]$ ceph-deploy mgr create ceph-node1\n查看集群状态\n[ceph-admin@ceph-node1 cluster]$ sudo ceph health detail HEALTH_OK [ceph-admin@ceph-node1 cluster]$ sudo ceph -s cluster: id: e9290965-40d4-4c65-93ed-e534ae389b9c health: HEALTH_OK services: mon: 1 daemons, quorum ceph-node1 (age 62m) mgr: ceph-node1(active, since 5m) osd: 3 osds: 3 up (since 12m), 3 in (since 12m) data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 3.0 GiB used, 27 GiB / 30 GiB avail pgs: 如果查看集群状态为“HEALTH_WARN mon is allowing insecure global_id reclaim”，是因为开启了不安全的模式，将之禁用掉即可： [ceph-admin@ceph-node1 cluster]$ sudo ceph config set mon auth_allow_insecure_global_id_reclaim false 因/etc/ceph/下key文件普通用户没有读权限，所以普通用户无权直接执行ceph命令 如果需要ceph-admin普通用户也可直接调用集群，增加对ceph配置文件的读权限即可 （想要每个节点普通用户都可以执行ceph相关命令，那就所有节点都修改权限） [ceph-admin@ceph-node1 ~]$ ll /etc/ceph/ total 12 -rw-------. 1 root root 151 Oct 21 17:33 ceph.client.admin.keyring -rw-r--r--. 1 root root 268 Oct 21 17:35 ceph.conf -rw-r--r--. 1 root root 92 Oct 20 04:48 rbdmap -rw-------. 1 root root 0 Oct 21 17:30 tmpcmU035 [ceph-admin@ceph-node1 ~]$ sudo chmod +r /etc/ceph/ceph.client.admin.keyring [ceph-admin@ceph-node1 ~]$ ll /etc/ceph/ total 12 -rw-r--r--. 1 root root 151 Oct 21 17:33 ceph.client.admin.keyring -rw-r--r--. 1 root root 268 Oct 21 17:35 ceph.conf -rw-r--r--. 1 root root 92 Oct 20 04:48 rbdmap -rw-------. 1 root root 0 Oct 21 17:30 tmpcmU035 [ceph-admin@ceph-node1 ~]$ ceph -s cluster: id: 130b5ac0-938a-4fd2-ba6f-3d37e1a4e908 health: HEALTH_OK services: mon: 1 daemons, quorum ceph-node1 (age 20h) mgr: ceph-node1(active, since 20h) osd: 3 osds: 3 up (since 20h), 3 in (since 20h) data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 3.0 GiB used, 27 GiB / 30 GiB avail pgs: 三、配置Mgr-Dashboard模块 开启dashboard模块\n[ceph-admin@ceph-node1 ~]$ sudo ceph mgr module enable dashboard 如果报错如下： Error ENOENT: all mgr daemons do not support module \u0026#39;dashboard\u0026#39;, pass --force to force enablement 那是因为没有安装ceph-mgr-dashboard，在mgr节点上安装即可 [ceph-admin@ceph-node1 ~]$ sudo yum -y install ceph-mgr-dashboard 默认情况下，仪表板的所有HTTP连接均使用SSL/TLS进行保护。 要快速启动并运行仪表板，可以使用以下命令生成并安装自签名证书 [ceph-admin@ceph-node1 ~]$ sudo ceph dashboard create-self-signed-cert Self-signed certificate created 创建具有管理员角色的用户: [ceph-admin@ceph-node1 ~]$ sudo ceph dashboard set-login-credentials admin admin ****************************************************************** *** WARNING: this command is deprecated. *** *** Please use the ac-user-* related commands to manage users. *** ****************************************************************** Username and password updated 之前用的“admin admin”，现在好像不能直接这样写了，需要将密码写在一个文件中读取，不然会报错 “dashboard set-login-credentials \u0026lt;username\u0026gt; : Set the login credentials. Password read from -i \u0026lt;file\u0026gt;” 那就加上-i参数来创建也是一样 [ceph-admin@ceph-node1 cluster]$ echo admin \u0026gt; userpass [ceph-admin@ceph-node1 cluster]$ sudo ceph dashboard set-login-credentials admin -i userpass ****************************************************************** *** WARNING: this command is deprecated. *** *** Please use the ac-user-* related commands to manage users. *** ****************************************************************** Username and password updated 查看ceph-mgr服务: [ceph-admin@ceph-node1 ~]$ sudo ceph mgr services { \u0026#34;dashboard\u0026#34;: \u0026#34;https://ceph-node1:8443/\u0026#34; } 浏览器访问测试:\nFlink部分 # 所有jar包可在对应module的target目录中获取。\nFilerWatcher # jar包，用于监听是否有新数据被爬取，将新数据输入Kafka的某个topic。 arg0:kafka topic名 arg1:监听路径\njava -jar FileWatcher-1.0-SNAPSHOT.jar arg0 arg1\nFilerWriter # jar包，用于监听Kafka的某个topic是否有新消息，并将新消息写入文件。 arg0:kafka topic名 arg1:写入文件全路径名\njava -jar FileWriter-1.0-SNAPSHOT.jar arg0 arg1\nFlink # 依赖：\nzookeeper3.4.13 配置在2181端口\nkafka2.1.1 配置在9092端口\nFlink集群配置了三个节点master，worker1，worker2，每个节点中有一个slot。在启动集群后，浏览器打开 master:8081 进入flink dashboard提交任务。\n任务jar包：\nciyun-1.0-SNAPSHOT.jar\n任务入口：\n# 计算北京地区Python岗位的描述关键词词频 com.zmy.CiyunJob flink_python-1.0-SNAPSHOT.jar\n任务入口：\n# 计算北京各地区Python岗位的数量 com.zmy.PythonAreaJob # 计算北京地区Python岗位的学历要求 com.zmy.PythonDegreeJob salary-1.0-SNAPSHOT.jar\n任务入口：\n# 计算北京地区Python岗位按地区分组计算平均工资 com.zmy.SalaryJob 结果 # ","date":"5 December 2023","permalink":"/posts/architecture/distributed/ceph/","section":"博客","summary":"Ceph是一种开源分布式存储系统，为大规模数据提供可扩展性和高性能。它使用分布式对象存储、块存储和文件系统，通过智能数据复制和动态数据分布，确保高可用性和容错性。Ceph的设计使其适用于云计算和大数据环境，提供灵活、可靠的存储解决方案，同时支持自动负载平衡和故障恢复。","title":"Ceph集群部署"},{"content":"","date":"5 December 2023","permalink":"/tags/distributed/","section":"Tags","summary":"","title":"Distributed"},{"content":"一、书名和作者 # 书名：《凤凰项目》 作者： Gene Kim、Kevin Behr、George Spafford 二、书籍概览 # 主要论点和结构：本书通过虚构的故事，介绍了IT运维和软件开发领域的最佳实践，强调快速交付、稳定运行、持续学习的三大支柱。以及业务、开发、IT运维、信息安全四个价值流的优化，倡导DevOps文化。 目标读者和应用场景：面向IT从业者、软件开发人员、运维工程师等，适用于希望优化IT流程、提高交付效率的组织和个人。 三、核心观点与主题 # 主题一、三大支柱 # 子观点1：快速交付是《凤凰项目》中的首要支柱，其核心理念是通过引入自动化工具，显著缩短软件开发周期，从而更迅速地满足业务需求。自动化不仅仅包括代码构建和部署，还包括测试、集成和交付过程的自动化。 实例或案例：在某软件公司，通过采用持续集成（CI）和持续交付（CD）工具，开发团队成功地将代码集成并自动部署到生产环境。这一改进使得他们能够更频繁地发布新功能和修复bug，从而提高了业务的灵活性和竞争力。通过自动化，开发人员能够专注于创造性的工作，减少了手动操作引起的错误，进而缩短了整个开发周期。 子观点2：稳定运行作为凤凰项目的第二支柱，旨在通过监控和自动化来减少系统故障，确保业务连续性和可靠性。通过不断改进系统架构和加强监控，可以更早地检测到潜在问题，并采取预防性措施，减少故障对业务的影响。 实例或案例：在某电商平台，引入了先进的监控系统，能够实时监测服务器性能、网络流量和数据库负载。当系统出现异常时，自动触发报警，通知运维团队及时介入。通过自动化的故障恢复机制，很多故障可以在用户察觉之前得到解决，有效降低了系统宕机的风险。 子观点3：持续学习是凤凰项目的第三支柱，着眼于通过实施反馈机制，促进团队的学习和不断改进。这一支柱的目标是建立一个文化，使得团队成员不仅在成功中学到东西，也能在失败和问题中获得宝贵的经验，并能够及时地应用这些经验。 实例或案例：在一家科技创新公司，团队引入了每日站会和定期回顾会议，以促使成员分享他们的经验、教训和发现。同时，建立了一个开放的知识分享平台，鼓励成员发布技术文章、分享解决方案。通过这些实践，团队建立了一种开放、透明的学习氛围，使得每个成员都能够不断改进自己的工作方法。 主题二、四个价值流 # 子观点1：业务价值流着眼于紧密对齐业务需求，确保软件开发和交付符合业务目标。在这一流程中，关键在于深刻理解业务需求，使开发出的软件能够真正为业务创造价值。 实例或案例：在一家金融机构，业务团队与开发团队建立了紧密的协作关系。通过定期的需求沟通会议和敏捷开发方法，开发团队深入了解业务需求，并及时调整开发计划。这种紧密对齐业务需求的实践，使得开发出的软件更符合业务目标，减少了后续的修改和调整，提高了整体的开发效率。 子观点2：开发价值流关注于优化开发流程，提高交付速度和质量。这一流程包括代码编写、测试、代码审查等环节，通过精益和敏捷方法，使开发过程更加高效、灵活。 实例或案例：在一家软件开发公司，团队采用了敏捷开发和持续集成的实践。通过自动化测试、代码审查和持续集成工具，他们实现了快速的迭代开发。开发人员能够及时地发现和修复bug，确保每个版本的软件都是可靠且高质量的。这种优化开发流程的实践，不仅提高了交付速度，也减少了开发过程中的错误。团队更加灵活地应对变化，客户能够更早地体验到新功能，从而提高了产品的市场竞争力。 子观点3：IT运维价值流关注于通过自动化减少运维工作量，提高系统的稳定性。这一流程包括监控、故障处理、性能优化等环节，通过精细的运维流程，确保系统持续稳定运行。 实例或案例：在一家电商平台，引入了自动化的运维工具，包括自动故障检测和自动恢复。通过这些工具，运维团队能够更快速地发现并解决潜在的系统问题，减少了系统宕机的风险。此外，他们还实施了容量规划和性能优化的自动化，确保系统在高峰时段依然能够高效运行。 子观点4：信息安全价值流强调安全性，旨在保护系统和数据，防范潜在的威胁和攻击。通过在整个开发和运维流程中加强安全性措施，确保系统对外界的威胁具有较高的抵御能力。 实例或案例：在一家医疗健康科技公司，信息安全团队通过制定严格的安全开发标准和进行定期的漏洞扫描，确保所有的软件开发过程都符合最高的安全标准。此外，他们还采用了网络入侵检测系统和行为分析工具，及时发现和应对潜在的安全威胁。 主题三、DevOps文化 # 子观点1：DevOps文化的第一支柱强调协作和沟通，旨在打破开发和运维之间的壁垒，促进更紧密的合作关系。通过加强开发与运维之间的合作，团队能够更好地理解彼此的需求，缩短反馈循环，从而更迅速地响应变化。 实例或案例：在一家电信公司，通过实施DevOps文化，开发和运维团队开始定期的联合工作坊。这些工作坊不仅包括技术方面的内容，还有沟通和协作的培训。开发人员学习了更多关于运维的知识，而运维人员也更深入地了解了开发过程。这种密切的合作关系使得问题能够更快地被解决，新功能更快地被推出，整个团队更具灵活性。 子观点2：DevOps文化的第二支柱是自动化，强调通过自动化工具提高效率，减少手动操作。自动化不仅包括软件开发和部署的自动化，还包括测试、监控、日志分析等各个环节的自动化。 实例或案例：在一家云服务提供商，团队引入了持续交付工具和自动化测试框架。通过这些工具，他们实现了从代码提交到部署的全自动化流程。测试环节也通过自动化脚本覆盖，大大减少了手动测试的工作量。同时，自动化监控系统能够及时发现和响应生产环境中的问题，降低了系统故障的风险。 主题四、金丝雀发布 # 子观点1：金丝雀发布是一种逐步引入新功能的策略，旨在降低发布新功能的风险。通过在一小部分用户中先进行测试，可以及时发现潜在问题，从而在全面发布之前进行修复。 实例或案例：在一家社交媒体公司，他们采用金丝雀发布的方式推出新的社交功能。首先，在小部分用户中启用这一功能，通过用户的反馈和监控数据进行实时的评估。如果发现有不良影响，团队可以迅速回退或修复问题。只有在新功能被充分验证后，才会在整个平台中启用。通过这种逐步引入新功能的方式，公司能够最大程度地减少对用户的干扰，确保新功能的质量，并避免可能的业务风险。这也使得团队更加敏捷，能够更快速地响应市场变化。 四、亮点与启发 # 最有影响的观点或实例\nDevOps文化的强调和实践。DevOps不仅仅是一种方法论，更是一种文化和价值观的体现。书中通过生动的故事情节展现了在实际工作中，通过协作、沟通和自动化的实践，团队如何打破传统的开发和运维之间的壁垒，实现更加敏捷、高效的工作流程。这种强调文化的观点对于组织在数字化时代的转型至关重要，为读者提供了深刻的启示。\n对个人或专业发展的启示\n通过自动化、协作和持续学习，提高工作效率和质量。书中的实践原则为个人和专业发展提供了深刻的启示。自动化工具的引入可以减少繁琐的手动操作，提高工作效率，使个人更加专注于创造性的工作。协作和沟通的重要性强调了团队合作对于项目成功的关键性作用。持续学习的理念则意味着个人需要保持对新知识和技能的敏感性，不断适应行业的变化。这些启示不仅对当前的职业生涯有指导作用，也对未来的发展提供了方向。\n五、批评与局限性 # 任何有争议、模糊或过时的信息\n书中可能过于理想化，实际应用中可能遇到一些挑战。尽管《凤凰项目》提供了丰富的实践经验，但有人指出书中的情节可能过于理想化，与实际工作环境存在差距。实际应用中，组织可能面临各种挑战，包括文化转变的阻力、技术实施的难度等。因此，读者在应用书中的原则时需要充分考虑组织的具体情况，量体裁衣地进行调整。\n可能的不足或缺陷\n某些实践可能不适用于所有组织，需要根据具体情况调整。书中介绍的一些实践可能并非适用于所有组织。每个组织都有独特的文化、业务需求和技术架构，因此在采纳某些实践时，需要根据具体情况进行调整。过于死板地套用模板化的解决方案可能带来适应性问题，读者需要理解实践的本质，并根据组织的具体情况进行灵活应用。\n六、实际应用和拓展 # 在实际工作/学习中如何应用这些概念\n引入自动化工具、加强团队协作，根据业务需求优化价值流。在实际应用中，读者可以首先考虑引入自动化工具，从而提高工作效率。通过自动化流程，可以减少手动操作引起的错误，提高整体质量。其次，加强团队协作是实现DevOps文化的关键，通过定期的协作会议和团队培训，可以促使团队更加紧密合作。最后，根据业务需求优化价值流，确保软件开发和交付符合业务目标。\n对未来研究或实践的建议\n深入研究新兴技术，持续关注行业最佳实践。随着技术的不断发展，读者被鼓励深入研究新兴技术，保持对行业最佳实践的关注。持续学习和不断更新技术知识是适应快速变化的行业环境的关键。同时，对于一些新兴的DevOps实践，可以通过参与社区活动、行业研讨会等方式，积极了解并与其他从业者交流经验。\n七、总结与评价 # 对书籍的整体评价\n《凤凰项目》是一本实用的IT运维和软件开发指南，通过故事情节生动地阐释了实践原则，为读者提供了一套实用的DevOps实践原则，通过生动的故事情节，使抽象的理论变得更加具体。书籍以小说的形式呈现，将复杂的技术概念融入故事情节中，使读者更容易理解和接受。同时，书中的实践原则和案例也为读者提供了在实际工作中应对挑战的指导方针。\n书籍的长处和短处\n强调了重要的IT实践，但可能过于理想化，需要结合实际情况灵活应用。《凤凰项目》的长处在于其强调了DevOps文化的重要性，提供了实践原则和案例供读者参考。然而，一些读者指出书中的情节可能过于理想化，与实际工作环境存在一定差距。因此，读者在应用书中的原则时需要谨慎，结合实际情况进行灵活调整。\n","date":"28 November 2023","permalink":"/read/%E5%87%A4%E5%87%B0%E9%A1%B9%E7%9B%AE/","section":"阅读","summary":"本书通过虚构的故事，介绍了IT运维和软件开发领域的最佳实践，强调快速交付、稳定运行、持续学习的三大支柱。以及业务、开发、IT运维、信息安全四个价值流的优化，倡导DevOps文化。面向IT从业者、软件开发人员、运维工程师等，适用于希望优化IT流程、提高交付效率的组织和个人。","title":"《凤凰项目》读书笔记"},{"content":"最近在用 kubevela 部署微服务，遇到了不少麻烦，主要还是 docker 和 K8s 的命令不熟悉，以后一定补上。在这里先记录下几个非常 nice 的博客，以免迷路。\n官网，不仅仅是文档，后面的博客写的也很好，几乎所有跟 Kubevela 相关的内容都在这里了。 交付完整模块，其中有如何使用 cuelang 定义 addon，并且部署到相关 vela component 仓库。 kubevela addon添加组件-微服务应用，不是官方的博客，但是很适合入门 kubevela addon，当时也是用 spring cloud 和 vela 的关键词找到的，缺点是没有整个项目的源代码链接，不能对照着学习。 Examples，官方的负载案例，很全面，值得学习。 vela show webservice --web ：查看文档。 k8s简易文档，这几天没时间全面地了解k8s，看的就是这个简易文档，快速了解概念。 Kubernetes 部署 Mysql 8.0 数据库(单节点)，使用 kubevela 部署的第一个 docker 镜像是 mysql:5.7，重点在于如何用 OAM 刻画 Application。 在Kubernetes上创建mysql容器时如何初始化 k8s官方文档，正式的官方文档，很详细。 将 Docker Compose 文件转换为 Kubernetes 资源 mall-swarm，最后需要实现的微服务项目部署，选择的原因也很简单，它的 文档比较全。 最后感慨一下，ChatGPT 真的是一项跨时代的技术，在毫无头绪的时候，它能提供思路，虽然正确率不是很高，但是至少有了思路，但是如果想要全面地了解一个技术，最直接有效的还是阅读官方的文档，无论你找了多久的资料，浏览了多久的网页，别人的博客都是在官网的基础上进行了扩展，甚至只是照搬了官网，所以在学习新技术的时候，先克服恐惧，这种恐惧是来自对未知的恐惧，不知道自己能不能处理这个问题，不知道能不能在截止日期前完成，所以会盲目地去寻找，试图找到一个与自己项目最接近的案例，但是很多情况是根本找不到这样的案例，最快的方法还是学习官网上的一则则文档，然后找一个案例学习。\n","date":"27 November 2023","permalink":"/posts/architecture/virtualization/k8s/kubevela/","section":"博客","summary":"KubeVela 是一个开箱即用的现代化应用交付与管理平台，它使得应用在面向混合云环境中的交付更简单、快捷。使用 KubeVela 的软件开发团队，可以按需使用云原生能力构建应用，随着团队规模的发展、业务场景的变化扩展其功能，一次构建应用，随处运行。","title":"Kubevela"},{"content":"","date":"17 November 2023","permalink":"/tags/oam/","section":"Tags","summary":"","title":"Oam"},{"content":"1. Purpose and Goals # Open Application Model 的目标是定义一种标准的、与基础设施无关的方法，用于描述跨混合环境、云甚至边缘设备的应用部署。\n该模型要解决的核心问题是如何组成分布式应用程序，然后成功地将其交给负责操作的人员。问题不在于如何编写程序，而在于如何采用面向服务（或面向微服务）架构的组件，并简化围绕此类应用的工作流程。\n例如，当代的云应用程序可能由几十个微服务组成，每个微服务负责广义上的 application 的一个独立部分。此类应用程序需要进行配置、部署、审核、更新和删除。有时必须将应用程序作为一个整体来处理，有时则需要更精细的粒度。最重要的是，此类应用程序通常不是由一个人或一个团队管理，而是由多个团队管理，他们必须通力合作，以实现可靠性、稳定性和及时性。\n该模型提供了对此类 workflow 的描述，描述本身具有声明性、可扩展性和最佳清晰度。此外，它还提出了操作此类应用程序的模式和流程。该模型将重点关注 cloud native（即 highly distributed）应用，涵盖公共云技术、内部部署解决方案以及物联网/边缘技术。这为现代应用交付系统奠定了坚实的基础，为云原生应用部署提供了标准但更高层次的描述。\nNon-goals 包括\n定义或规定特定的协调工具。 定义运行资源的模式，例如（但不限于）： Secrets (secure, encrypted values) Networks Volumes 描述或定义运行时基础设施本身。 2. Overview and Terminology # 本节概述了 Open Application Model (OAM) 及其术语。本节首先确定了运行云本地应用程序过程中涉及的组织角色。然后，介绍本文档中使用的特定术语。\n本文档提出的云本地应用程序定义模型如下：\n云本地应用程序是一系列相互关联但又互不关联的组件（服务、任务、工作者）的集合，当这些组件与配置耦合并在合适的运行时基础架构中实例化时，可共同完成统一的功能目的。\n在当前版本中，该应用模式定义了以下内容：\nComponents 代表一个可运行的单元。 Workload types 标识组件可执行的不同工作负载 Traits 是覆盖层，可通过附加的特定操作功能来增强组件。Traits 代表操作员关注的问题，而不是开发人员/软件所有者关注的问题。 Application scopes 通过对具有共同属性或依赖性的组件进行分组，代表了应用的边界。 Application configuration 集合了一组组件实例、它们的特性、它们所在的应用范围，并结合了配置参数和元数据。 因此，应用程序是具有一系列运行特征的组件的集合，并被集中到一个或多个应用程序边界中。\n3. Component Model # 本节定义组件模型。\n组件描述了可作为更大的分布式应用程序的一部分进行实例化的功能单元。应用程序部分将介绍如何将组件组合在一起以及如何配置这些组件的实例，而本节将重点介绍组件模型本身。\n组件定义（ComponentDefinition）实体的作用是允许组件提供者以基础设施中立格式声明此类执行单元的运行时特性。\n例如，应用程序中的每个微服务都被描述为一个组件。请注意，ComponentDefinition 本身并不是该微服务的实例，而是该微服务可配置属性的声明。这些可配置属性应作为参数列表公开，以便应用团队在部署时设置并实例化该组件。\n在实践中，一个简单的容器化工作负载、一个 Helm 图表或一个云数据库都可以建模为一个组件。\nTop-Level Attributes # 以下是提供组件定义顶层信息的属性。\nAttribute Type Required Default Value Description apiVersion string Y A string that identifies the version of the schema the object should have. The core types uses core.oam.dev/v1beta1 in this version of model kind string Y Must be ComponentDefinition metadata Metadata Y Entity metadata. spec Spec Y The specification for the component definition. Metadata # 该元数据部分由几个 top-level keys 组成。元数据提供有关对象内容的信息。\nAttribute Type Required Default Value Description name string Y A name for the schematic. name is subject to the restrictions listed beneath this table. labels map[string]string N A set of string key/value pairs used as arbitrary labels on this component. See the \u0026ldquo;Label format\u0026rdquo; section immediately below. annotations map[string]string N A set of string key/value pairs used as arbitrary descriptive text associated with this object. See the \u0026ldquo;Annotations format\u0026rdquo; section immediately below. name：group、kind、name 的组合必须是唯一的。两个不同的种类（例如组件和特质）可以使用相同的名称，而不会造成冲突。Version 不是区分因素。 Okay: 每种类型都允许使用名称 foo。\napiVersion: core.oam.dev/v1alpha2 kind: Component metadata: name: foo --- apiVersion: core.oam.dev/v1alpha2 kind: Trait metadata: name: foo Okay: 每个组（one.dev 和 other.dev）都允许使用名称为 foo 的组件。\napiVersion: one.dev/v1alpha2 kind: Component metadata: name: foo --- apiVersion: other.dev/v1alpha2 kind: Component metadata: name: foo NOT Okay: Version 不是命名空间限定符。\napiVersion: core.oam.dev/v1alpha2 kind: Component metadata: name: foo --- apiVersion: core.oam.dev/v1 kind: Component metadata: name: foo name 字段的格式必须如下：\nname 字段为必填字段，必须在 63 个字符以内，以字母数字字符（[a-z0-9A-Z]）开头和结尾，中间包含破折号 (-)、下划线 (_)、点 (.)和字母数字。\n除非另有说明，否则 name 在 Group/Version/Kind 中必须是唯一的。\nLabel：标签遵循 Kubernetes 规范 进行标记： 有效的标签密钥有两部分：可选的前缀和名称，以斜线 (/) 分隔。名称部分是必需的，长度必须在 63 个字符以内，以字母数字字符（[a-z0-9A-Z]）开头和结尾，中间包含破折号 (-)、下划线 (_)、点 (.)和字母数字。前缀为可选项。如果指定了前缀，则前缀必须是 DNS 子域：由点（.）分隔的一系列 DNS 标签，总长度不超过 253 个字符，后跟斜线 (/)。\nAnnotations：注解提供了一种在对象元数据中附加任意文本的机制。注释对象遵循 Kubernetes 规范： 注释是键/值对。有效的注解键有两个部分：可选的前缀和名称，以斜线（/）分隔。名称部分是必填项，必须少于或等于 63 个字符，以字母数字字符（[a-z0-9A-Z]）开头和结尾，中间包含破折号 (-)、下划线 (_)、点 (.)和字母数字。前缀为可选项。如果指定了前缀，则前缀必须是 DNS 子域：由点（.）分隔的一系列 DNS 标签，总长度不超过 253 个字符，后跟斜线 (/)。\n下列注释标签是 _predefined_ 和 RECOMMENDED\nAttribute Type Required Default Value Description description string N A short description of the component. version string N A user provided string defining the semantic version of the component, e.g. the release version of this software 如果未提供 version ，则根据 SemVer 规范，默认版本假定为 0.1.0。\n例子：\nmetadata: name: alpine-task labels: app: my-app-v1 # Non-normative example annotations: version: \u0026#34;1.0.1\u0026#34; description: A task that is backed by an Alpine Linux filesystem 元数据部分用于所有示意图。它也与 Kubernetes 元数据部分兼容。但请注意，Kubernetes 元数据是 OAM 元数据的超集，包含 OAM 无法识别的属性。\nGroup, Version, and Kind # 本文档中描述的许多 API 对象都使用了一种名为 \u0026ldquo;Group, Version, Kind\u0026rdquo; 的命名方案。该方案由 Kubernetes 推广，为 API 对象的命名间距和版本提供了一致的方式，在此用于 OAM API 对象的版本控制。本节将介绍该方案。\nGroup # 组是收集多个相关 kinds 的 namespace 。组使用 DNS 命名约定。组的示例有\ncomponents.oam.dev functions.azure.com my.dev oam.dev 域下的所有组都被视为 OAM 对象的保留组。此处指定的所有对象都属于该域中的组。\n组必须是全局唯一的。\nVersion # version 字符串是 API 的版本。按照通用范例，API 仅按主版本号进行版本控制。API 版本中省略了次版本号和补丁号。底层引擎的实际次要版本和补丁版本可能会重复，但它们必须重复此版本以处理任何破坏性更改。换句话说，主版本号是兼容性的保证，次版本号和补丁号不应改变这一保证。因此，API 的用户不能指定比主版本更细的粒度。\nAPI 版本号的前缀总是 v，后面跟一个或多个数字。\nv1 v2 v973 主要版本号中还有两个附加修饰符：\nalphaN（其中 N 为一位或多位数）表示该功能是试验性的，可能会被移除，但其当前的兼容性标记为 1。 betaN（其中 N 为一位或多位数）表示该特征尚未稳定。N 是兼容性标记。 标记为 alpha 或 beta 的 API 版本被认为是不稳定的，容易出现破坏性更改。\n一次只能使用两种修改器中的一种：\nv1alpha1 v973beta231 兼容性只能通过精确匹配来确定。v1 与 v2、v1alpha1 或 v1beta2 不兼容。\n版本没有唯一性要求。\nKind # 种类是类型的名称。例如，组件的种类是 \u0026ldquo;组件\u0026rdquo;（Component），而特质的种类是 \u0026ldquo;特质\u0026rdquo;（Trait）。种类总是由首字母大写的单词组成，每个单词的第一个字母都要大写。种类应大写首字母缩写的每个字母（例如，HTTP，而不是 Http）。\n在一个组中，种类必须是唯一的。\nGroup/Version/Kind 的表述 # Group/Version/Kind 的完全限定表示法是 GROUP/VERSION.KIND。下面是一些示例：\nlocal.dev/v7alpha2.Proxy cache.example.com/v1.Redis azure.com/v2.Functions 在示意图中，Group 和 Version 在一个字段中显示，Kind 在另一个字段中显示：\napiVersion: local.dev/v7alpha2 kind: Proxy 在极少数情况下，有必要链接组和种类，但不指定版本。例如，在声明默认工作负载时就需要这样做。一般情况下，不推荐这种行为，但在必要时，本规范会遵循 Kubernetes 的模式，用复数种类名称和组来构建 DNS 名称：\nProxies.local.dev # allowed but discouraged 这种形式不被接受为完全合格版本的替代形式。只有在模型明确规定接受这种形式的情况下才会被接受。\nSpec # Attribute Type Required Default Value Description workload WorkloadTypeDescriptor Y Identifier to workload type of this component. schematic Schematic Y Schematic information for this component. WorkloadTypeDescriptor # Attribute Type Required Default Value Description type string Y A reference to a WorkloadDefinition via name. definition WorkloadGVK Y Mutually exclusive to type, a reference to WorkloadDefinition via group, version, and kind. WorkloadGVK # Attribute Type Required Default Value Description apiVersion string Y The API version the workload type. kind string Y The API kind of the workload type. Schematic # 本节声明了一个组件的示意图，该组件可在以后的部署工作流程中作为应用程序的一部分实例化。请注意，OAM 本身并不强制要求如何实现示意图，只要它能做到以下几点即可：\n为可部署单元建模； 公开 JSON 模式或等效参数列表。 在 KubeVela 中，目前支持以下方案实现（\u0026ldquo;cue\u0026rdquo;、\u0026ldquo;helm \u0026ldquo;和 \u0026ldquo;kube\u0026rdquo;）。\nExample # 下面是一个完整的基于 CUE 的组件定义示例，名为 \u0026ldquo;webserver\u0026rdquo;：\napiVersion: core.oam.dev/v1beta1 kind: ComponentDefinition metadata: name: webserver annotations: definition.oam.dev/description: \u0026#34;webserver is a combo of Deployment + Service\u0026#34; spec: workload: definition: apiVersion: apps/v1 kind: Deployment schematic: cue: template: | output: { apiVersion: \u0026#34;apps/v1\u0026#34; kind: \u0026#34;Deployment\u0026#34; spec: { selector: matchLabels: { \u0026#34;app.oam.dev/component\u0026#34;: context.name } template: { metadata: labels: { \u0026#34;app.oam.dev/component\u0026#34;: context.name } spec: { containers: [{ name: context.name image: parameter.image if parameter[\u0026#34;cmd\u0026#34;] != _|_ { command: parameter.cmd } if parameter[\u0026#34;env\u0026#34;] != _|_ { env: parameter.env } if context[\u0026#34;config\u0026#34;] != _|_ { env: context.config } ports: [{ containerPort: parameter.port }] if parameter[\u0026#34;cpu\u0026#34;] != _|_ { resources: { limits: cpu: parameter.cpu requests: cpu: parameter.cpu } } }] } } } } // an extra template outputs: service: { apiVersion: \u0026#34;v1\u0026#34; kind: \u0026#34;Service\u0026#34; spec: { selector: { \u0026#34;app.oam.dev/component\u0026#34;: context.name } ports: [ { port: parameter.port targetPort: parameter.port }, ] } } parameter: { image: string cmd?: [...string] port: *80 | int env?: [...{ name: string value?: string valueFrom?: { secretKeyRef: { name: string key: string } } }] cpu?: string } 在平台中安装了上述 webserver 后，用户就可以在应用程序中部署该组件，如下所示：\napiVersion: core.oam.dev/v1beta1 kind: Application metadata: name: webserver-demo spec: components: - name: hello-world type: webserver # claim to deploy webserver component definition properties: # setting parameter values image: crccheck/hello-world port: 8000 # this port will be automatically exposed to public env: - name: \u0026#34;foo\u0026#34; value: \u0026#34;bar\u0026#34; cpu: \u0026#34;100m\u0026#34; 4. Workload Types # 工作负载类型（Workload Types）的使用将在组件模型部分介绍，但这里将对工作负载类型定义进行说明。\n工作负载类型是给定组件定义的关键特征。工作负载类型由平台提供，以便用户检查平台并了解有哪些工作负载类型可供使用。请注意，工作负载类型不可扩展给最终用户（只能扩展给平台操作员）。因此，终端用户不得创建新的工作负载类型。\nWorkload Definition # 工作负载类型以 \u0026ldquo;工作负载定义\u0026rdquo;（WorkloadDefinition）的形式呈现，此外，为了便于发现，工作负载定义还带有 特性信息，可提示平台如何将 特性 附加到引用该工作负载类型的给定组件上（即特性系统的适用于特性）。\nTop-Level Attributes # Attribute Type Required Default Value Description apiVersion string Y A string that identifies the version of the schema the object should have. The core types uses core.oam.dev/v1beta1 in this version of model kind string Y Must be WorkloadDefinition metadata Metadata Y Entity metadata. spec Spec Y The specification for the workload definition. Spec # Attribute Type Required Default Value Description definitionRef DefinitionRef Y Identifier to workload capability in the platform. DefinitionRef # Attribute Type Required Default Value Description name string N Name identifier of the workload capability. Mutually exclusive to apiVersion and kind. apiVersion string N API version of the workload capability. kind string N Kind of the workload capability. Characteristics of Workload Types # 下面工作负载定义中保留的 \u0026ldquo;metadata.labels\u0026rdquo; 用于表示工作负载类型的 _the distinguishing characteristics_。\n这些特征将通过 traits 系统的 _applies to_ 功能来实现。\nLabel Type Explain workload.oam.dev/replicable boolean Whether they are replicable. If not, no replication or scaling traits may be assigned. workload.oam.dev/daemonized boolean Whether they are daemonized. For daemon types, if the workload exits, this is considered a fault, and the system must fix it. For non-daemonized types, exit is considered a success if no error is reported. workload.oam.dev/exposed boolean Whether they are exposed, i.e. have a service endpoint with a stable name for network traffic. Workload types that have a service endpoint need a virtual IP address (VIP) with a DNS name to represent the component as a whole, addressable within their network scope and can be assigned traffic routing traits. workload.oam.dev/podspecable boolean Whether this workload can be addressed by Kubernetes PodSpec. If yes, the implementation could manipulate the workload by leveraging PodSpec structure, instead of being agnostic of the workload\u0026rsquo;s schematic. Categories of Workload Types # Workload Type 可分为几类。\nOfficially Maintained Workload Type # 正式维护的工作负载类型必须属于 \u0026ldquo;oam.dev \u0026ldquo;组。\n下面是一个官方维护工作负载类型的示例：\nkind: WorkloadDefinition metadata: name: Server spec: definitionRef: name: containerizedworkloads.core.oam.dev 该 Workload Type 的说明：\nName Category Schema Exposed Replicable Daemonized Server Core ContainerizedWorkload Yes Yes Yes Extended Workload Type # 每个 OAM 运行时都可以定义本组之外的工作负载类型，它们将被视为扩展工作负载类型。扩展工作负载类型的名称和模式完全由 OAM 实施自行决定。\n下面是一个示例：\nkind: WorkloadDefinition metadata: name: redis.cache.aliyun.com spec: definitionRef: name: redis.cache.aliyun.com # this is an extended workload type 对于扩展工作负载类型，建议使用以下约定：\n使用 Group/Version/Kind 唯一标识工作负载能力。 name 遵循 Group/Version/Kind 中描述的格式。 WorkloadDefinition 的 name 与它所指向的 name 相同。 kind: WorkloadDefinition metadata: name: schema.example.com spec: definitionRef: name: schema.example.com Server # 服务器是一种 OAM 核心工作负载类型，用于定义长期运行、可扩展的工作负载，这些负载有一个名称稳定的网络端点，用于接收整个组件的网络流量。常见用例包括暴露 API 的网络应用程序和服务。服务器工作负载类型具有以下特征：\n定义了一个容器运行时，同一容器的零个或多个副本可同时运行。 应用程序操作员可以通过应用和配置可用特性来增加或减少组件副本的数量。 服务器是守护进程化的。无论错误代码如何，运行时都必须尝试重新启动退出的副本。 服务器有一个网络端点，该端点具有自动分配的虚拟 IP 地址（VIP）和 DNS 名称，可在组件所属的网络范围内寻址。 如果服务器没有在至少一个容器上提供至少一个端口，实现就应该发出错误信息。\nHow to use? # 在基于 OAM 的平台中安装以下工作负载定义： apiVersion: core.oam.dev/v1beta1 kind: WorkloadDefinition metadata: name: Server spec: definitionRef: name: containerizedworkloads.core.oam.dev # the reference of schema for this workload type. In Kubernetes it should be a full name of API resource 将组件引用服务器定义为工作负载： apiVersion: core.oam.dev/v1beta1 kind: ComponentDefinition metadata: name: webserver annotations: definition.oam.dev/description: \u0026#34;webserver is a combo of Deployment + Service\u0026#34; spec: workload: type: Server # reference above workload definition by name. schematic: ... # CUE code to define user schematic. Containerized Workload # ContainerizedWorkload 是无服务器容器风格的工作负载定义，可作为 Azure ACI、AWS Fargate 或 Kubernetes 的简单无状态工作负载等运行时平台的长期运行容器化工作负载类型的模式。\n注意：根据设计，ContainerizedWorkload 模式并不等同于 Kubernetes Pod 规范。作为无服务器风格工作负载的模式，ContainerizedWorkload 计划只关注面向开发人员的基元，并且是自包含的，因此开发人员无需定义 ConfigMap 或 Secret 等对象。此外，它默认公开容器端口。因此，如果您正在为基于 Kubernetes 的 PaaS 寻找通用工作负载模式，我们建议您查看 PodSpecWorkload 规范。\n以下是 ContainerizedWorkload 的规范示意图。\nTop-Level Attributes of a containerized workload # 这些属性提供了有关容器化工作负载的顶级信息。\nAttribute Type Required Default Value Description apiVersion string Y core.oam.dev/v1alpha2 kind string Y ContainerizedWorkload metadata Metadata Y containerized workload metadata. spec Spec Y A container for the containerized workload spec. Spec # Attribute Type Required Default Value Description osType string N The OS required to host (all of) the component\u0026rsquo;s containers (since containers share a kernel with the underlying host). Possible values include:linuxwindows For extended runtimes, this is passed in unaltered. Default can be none and let the runtime decide where to place the component. arch string N The CPU architecture required to host (all of) the component\u0026rsquo;s containers (since containers share physical hardware with the underlying host). Possible values include:i386amd64armarm64 Default can be none and let the runtime chose architecture. containers []Container Y The OCI container(s) that implement the component. Container # 本节介绍为该组件运行容器化工作负载所需的运行时配置。\nAttribute Type Required Default Value Description name string Y The container\u0026rsquo;s name. Must be unique per component. image string Y A path-like or URI-like representation of the location of an OCI image. Where applicable, this MAY be prefixed with a registry address, SHOULD be suffixed with a tag. resources Resources Y Resources required by the container. cmd []string N Entrypoint array. args []string N Arguments to the entrypoint. The container image\u0026rsquo;s CMD is used if this is not provided. env []Env N Environment variables for the container. config []ConfigFile N Locations to write configuration as files accessible within the container ports []Port N Ports exposed by the container. livenessProbe HealthProbe N Instructions for assessing whether the container is alive. readinessProbe HealthProbe N Instructions for assessing whether the container is in a suitable state to serve traffic. imagePullSecret string N Key that can be used to retrieve the credentials for pulling this secret. Resources # 资源描述附加到运行时的计算资源。\nAttribute Type Required Default Value Description cpu CPU Y Specifies the attributes of the cpu resource required for the container. memory Memory Y Specifies the attributes of the memory resource required for the container. gpu GPU N Specifies the attributes of the gpu resources required for the container. volumes []Volume N Specifies the attributes of the volumes that the container uses. extended []ExtendedResource N Implementation-specific extended resource requirements 对于底层平台无法满足的任何资源，平台必须返回错误并停止部署。资源被视为一种需求，无法满足该需求意味着运行时不得部署应用程序。例如，如果应用程序请求 1P 内存，而内存量不可用，则应用程序部署必须失败。同样，如果应用程序需要 1 个 GPU，而运行时没有提供 GPU，则应用程序部署必须失败。\nCPU # Attribute Type Required Default Value Description required double Y The minimum number of logical cpus required for running this container. Memory # Attribute Type Required Default Value Description required string Y The minimum amount of memory required for running this container. The value should be a positive integer with/without unit suffix: P, T, G, M, K. If no unit is given, it defaults to \u0026lsquo;bytes\u0026rsquo;. GPU # Attribute Type Required Default Value Description required double Y The minimum number of gpus required for running this container. Volume # 卷描述了名称、挂载卷的位置、访问模式（如读/写或只读）以及挂载的共享策略。它还描述了卷所需的底层磁盘属性。\n路径格式取决于消费组件的操作系统，但实现应支持类似 UNIX 的路径表示法。\nAttribute Type Required Default Value Description name string Y Specifies the name used to reference the path. mountPath string Y Specifies the actual mount path in the filesystem. accessMode string N RW Specifies the access mode. Allowed values are RW (read/write) and RO (read-only). sharingPolicy string N Exclusive The sharing policy for the mount, indicating if it is expected to be shared or not. Allowed values are Exclusive and Shared. disk Disk N Specifies the attributes of the underneath disk resources required by the volume. Example:\nname: \u0026#34;configuration\u0026#34; mountPath: /etc/config accessMode: RO sharingPolicy: Shared disk: required: \u0026#34;2G\u0026#34; ephemeral: n 上述操作要求在路径 /etc/config 下挂载一个只读卷，该卷由一个至少提供 2G 非短暂存储空间的卷支持。\nDisk # 磁盘指定卷所使用磁盘的属性。它描述了最小磁盘大小和磁盘是否短暂等信息。短暂磁盘表示组件需要节点上最小的磁盘大小才能运行。例如，图像处理组件可能需要节点上更大的缓存才能运行，这时可以使用短暂磁盘。当短暂磁盘设置为假时，表示将使用外部磁盘。\nAttribute Type Required Default Value Description required string Y The minimum disk size required for running this container. The value should be a positive value, greater than zero. ephemeral boolean N Specifies whether external disk needs to be mounted or not. ExtendedResource # 扩展资源是针对特定实施资源的资源需求声明。例如，兼容 OAM 的平台可能会暴露出特殊的硬件。通过该字段，容器可以指出，为了让容器运行，需要提供此类特殊硬件。\nAttribute Type Required Default Value Description name string Y The name of the resource, as a Group/Version/Kind required string Y The required condition. name 字段必须是标识特定资源的 group/version/kind。\nExample:\nextended: - name: ext.example.com/v1.MotionSensor required: \u0026#34;1\u0026#34; - name: ext.example.com/v2beta4.ServoModel required: z141155-t100 如果已命名的扩展资源因故不可用，则在创建组件实例时，实现必须返回错误信息。\nEnv # Env 将环境变量描述为 name/value 的字符串。\nAttribute Type Required Default Value Description name string Y The environment variable name. Must be unique per container. value string N The environment variable value. name 字段必须由有效的 Unicode 字母和数字字符以及 _ 和 - 组成。\nExample:\nenv: - name: \u0026#34;ADMIN_USER\u0026#34; value: \u0026#34;admin\u0026#34; # This is a literal value ConfigFile # ConfigFile 描述了容器内可用文件的路径，以及将写入该文件的数据。这提供了一种将配置文件注入容器的方法。\nAttribute Type Required Default Value Description path string Y An absolute path within the container. value string N The data to be written into the file at the specified path. If this is not supplied, fromParam must be supplied fromParam string N The parameter whose value should be written into this file as a value path 字段必须包含一个遵守底层操作系统路径规则的路径。如果给出的是相对路径，实现必须假定该路径是相对于容器根目录的。如果使用这样的路径会违反安全措施或路径布局要求，实现可能会产生错误\nExample:\nconfig: - path: \u0026#34;/etc/access/default_user.txt\u0026#34; value: \u0026#34;admin\u0026#34; # This is a literal value - path: \u0026#34;/var/run/db-data\u0026#34; fromParam: \u0026#34;sourceData\u0026#34; # This will cause the value to be read from the parameter whose name is `sourceData` 如果同时指定了 fromParam 和 value，则必须优先使用 fromParam，即使参数值是空值。如果两者都未指定，运行时必须产生错误。\nPort # Attribute Type Required Default Value Description name string Y A descriptive name for the port. Must be unique per container. containerPort integer Y The port number. Must be unique per container. protocol string N TCP Indicates the transport layer protocol used by the server listening on the port. Valid values are TCP and UDP. name 字段必须是 ASCII 字符集（0061-007A）中的小写字母字符。\nHealthProbe # HealthProbe 描述了如何执行探测操作以确定组件的健康状况。\nAttribute Type Required Default Value Description exec Exec N Instructions for assessing container health by executing a command. Either this attribute or the httpGet attribute or the tcpSocket attribute MUST be specified. This attribute is mutually exclusive with both the httpGet attribute and the tcpSocket attribute. httpGet HTTPGet N Instructions for assessing container health by executing an HTTP GET request. Either this attribute or the exec attribute or the tcpSocket attribute MUST be specified. This attribute is mutually exclusive with both the exec attribute and the tcpSocket attribute. tcpSocket TCPSocket N Instructions for assessing container health by probing a TCP socket. Either this attribute or the exec attribute or the httpGet attribute MUST be specified. This attribute is mutually exclusive with both the exec attribute and the httpGet attribute. initialDelaySeconds integer N 0 Number of seconds after the container is started before the first probe is initiated. periodSeconds integer N 10 How often, in seconds, to execute the probe. timeoutSeconds integer N 1 Number of seconds after which the probe times out. successThreshold integer N 1 Minimum consecutive successes for the probe to be considered successful after having failed. failureThreshold integer N 3 Number of consecutive failures required to determine the container is not alive (liveness probe) or not ready (readiness probe). Exec # Attribute Type Required Default Value Description command []string Y A command to be executed inside the container to assess its health. Each space delimited token of the command is a separate array element. Commands exiting 0 are considered to be successful probes, whilst all other exit codes are considered failures. HTTPGet # Attribute Type Required Default Value Description path string Y The endpoint, relative to the port, to which the HTTP GET request should be directed. port integer Y The TCP socket within the container to which the HTTP GET request should be directed. httpHeaders []HTTPHeader N Optional HTTP headers. HTTPHeader # Attribute Type Required Default Value Description name string Y An HTTP header name. This must be unique per HTTP GET-based probe. value string Y An HTTP header value. name 和 value 都必须遵守 HTTP/1.1 关于有效[标头值]的规范( https://www.w3.org/Protocols/rfc2616/rfc2616-sec4.html#sec4.2)\nTCPSocket # Attribute Type Required Default Value Description port integer Y The TCP socket within the container that should be probed to assess container health. 端口必须是大于 0 的整数。\nUnits for Time, CPU, Memory, and Disk # 本规范的某些地方使用了某些度量单位。本节将介绍这些计量单位。\nTiming (Intervals) # 对于计时，默认的时间单位是秒，用整数表示。\nCPU count # CPU 数量用浮点数表示，其中 \u0026ldquo;1 \u0026ldquo;表示一个 CPU，\u0026ldquo;2 \u0026ldquo;表示两个 CPU，\u0026ldquo;0.5 \u0026ldquo;表示半个 CPU。\n该单位的确切含义因平台而异。 实施者应考虑逻辑 cpu 相当于一个 AWS vCPU 或一个 Azure vCore 或一个 GCP Core 或一个 IBM vCPU。允许使用分数值。如果运行时不支持小数单位，则必须向上舍入（上限函数）到下一个整数值。\nMemory and Disk # 内存和磁盘空间使用 bytes/kilo/mega/giga/tera/peta 的符号，只使用主要单位：\n1024 is 1024 bytes 88K is 88 kilobytes 5M is 5 megabytes 7G is 7 gigabytes 100T is 100 terabytes 9999P is 9999 petabytes 如果在单位字母后附加 B，则必须忽略。因此，5M 和 5MB 被视为相同。大小写并不重要。15k 和 15K 必须视为相同的值。\nContainerized Workload JSON schema # 对于非 Kubernetes 运行时实现，可以使用 JSON schema 作为有效的容器化工作负载模式。\n{ \u0026#34;$schema\u0026#34;: \u0026#34;http://json-schema.org/draft-07/schema#\u0026#34;, \u0026#34;$id\u0026#34;: \u0026#34;http://oam.dev/v1/oam.workload.schema.json\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Open Application Model Containerized Workload JSON Schema\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;apiVersion\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The specific version of the Open Application Model specification in use. This version of the specification covers apiVersions in core.oam.dev/v1alpha2.\u0026#34; }, \u0026#34;kind\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;must be ContainerizedWorkload.\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;ContainerizedWorkload\u0026#34;] }, \u0026#34;metadata\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;ContainerizedWorkload metadata.\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34; }, \u0026#34;labels\u0026#34;: { \u0026#34;type\u0026#34;:\u0026#34;object\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A set of string key/value pairs used as arbitrary labels on this workload.\u0026#34;, \u0026#34;additionalProperties\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;annotations\u0026#34;: { \u0026#34;type\u0026#34;:\u0026#34;object\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A set of string key/value pairs used as arbitrary annotations on this workload.\u0026#34;, \u0026#34;additionalProperties\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } } } }, \u0026#34;spec\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A container for all remaining attributes.\u0026#34;, \u0026#34;additionalProperties\u0026#34;: { \u0026#34;$ref\u0026#34;: \u0026#34;#/definitions/containerSpec\u0026#34; } } }, \u0026#34;required\u0026#34;: [\u0026#34;apiVersion\u0026#34;, \u0026#34;kind\u0026#34;, \u0026#34;metadata\u0026#34;, \u0026#34;spec\u0026#34;], \u0026#34;additionalProperties\u0026#34;: false, \u0026#34;definitions\u0026#34;: { \u0026#34;containerSpec\u0026#34;: { \u0026#34;osType\u0026#34;:{ \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;default\u0026#34;: \u0026#34;linux\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The OS required to host (all of) the component\u0026#39;s containers (since containers share a kernel with the underlying host).\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;linux\u0026#34;, \u0026#34;windows\u0026#34;] }, \u0026#34;arch\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;default\u0026#34;: \u0026#34;amd64\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The CPU architecture required to host (all of) the component\u0026#39;s containers (since containers share physical hardware with the underlying host).\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;i386\u0026#34;, \u0026#34;amd64\u0026#34;, \u0026#34;arm\u0026#34;, \u0026#34;arm64\u0026#34;] }, \u0026#34;containers\u0026#34;:{ \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The OCI container(s) that implement the component.\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;$ref\u0026#34;: \u0026#34;#/definitions/container\u0026#34; } }, \u0026#34;required\u0026#34;: [\u0026#34;containers\u0026#34;], \u0026#34;additionalProperties\u0026#34;: false }, \u0026#34;container\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The runtime configuration necessary to run a containerized workload for this component.\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The container\u0026#39;s name. Must be unique per component.\u0026#34; }, \u0026#34;image\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A path-like or URI-like representation of the location of an OCI image. Where applicable, this MAY be prefixed with a registry address, SHOULD be suffixed with a tag, and MUST be suffixed with a digest in OCI format. The digest may be used to compute the integrity of the image.\u0026#34; }, \u0026#34;resources\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Resources required by the container.\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;cpu\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the attributes of the cpu resource required for the container.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;required\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;number\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The minimum number of logical cpus required for running this container.\u0026#34; } } }, \u0026#34;memory\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the attributes of the memory resource required for the container.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;required\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The minimum amount of memory in MB required for running this container. The value should be a positive integer, greater than zero.\u0026#34; } } }, \u0026#34;gpu\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the attributes of the gpu resources required for the container.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;required\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;number\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The minimum number of gpus required for running this container.\u0026#34; } } }, \u0026#34;volumes\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the attributes of the volumes that the container uses.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the name used to reference the path.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;mountPath\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the actual mount path in the filesystem.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;accessMode\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the access mode.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;default\u0026#34;: \u0026#34;RW\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;RW\u0026#34;, \u0026#34;RO\u0026#34;] }, \u0026#34;sharingPolicy\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The sharing policy for the mount, indicating if it is expected to be shared or not.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;default\u0026#34;: \u0026#34;Exclusive\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;Exclusive\u0026#34;, \u0026#34;Shared\u0026#34;] }, \u0026#34;disk\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the actual mount path in the filesystem.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;required\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The minimum disk size required for running this container. The value should be a positive integer, greater than zero.\u0026#34; }, \u0026#34;ephemeral\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Specifies whether external disk needs to be mounted or not.\u0026#34; } } } }, \u0026#34;required\u0026#34;: [\u0026#34;name\u0026#34;, \u0026#34;mountPath\u0026#34;] } } } }, \u0026#34;cmd\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;args\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;env\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Environment variables\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;a name/value pair\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The environment variable name. Must be unique per container.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;maxLength\u0026#34;: 256 }, \u0026#34;value\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The environment variable value. If this is not supplied, fromParam must be supplied\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;$comment\u0026#34;: \u0026#34;POSIX may dictate a max length on this\u0026#34;, \u0026#34;maxLength\u0026#34;: 2048 }, \u0026#34;fromParam\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The parameter whose value should be substituted into this variable as a value\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;required\u0026#34;: [\u0026#34;name\u0026#34;], \u0026#34;additionalProperties\u0026#34;: false } }, \u0026#34;config\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Configuration files\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;a path and the value to be written into a file at that location\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The file name as an absolute path. Must be unique per container.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;$comment\u0026#34;: \u0026#34;format of this value is operating system dependent.\u0026#34;, \u0026#34;maxLength\u0026#34;: 256 }, \u0026#34;value\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The value to be written into the file. If this is not supplied, fromParam must be supplied\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;fromParam\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The parameter whose value should be written into this file as a value\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;required\u0026#34;: [ \u0026#34;name\u0026#34; ], \u0026#34;additionalProperties\u0026#34;: false } }, \u0026#34;ports\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Ports exposed by the container.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;A descriptive name for the port. Must be unique per container.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;containerPort\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The port number. Must be unique per container.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34; }, \u0026#34;protocol\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Indicates the transport layer protocol used by the server listening on the port.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;default\u0026#34;: \u0026#34;TCP\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;TCP\u0026#34;, \u0026#34;UDP\u0026#34;] } }, \u0026#34;required\u0026#34;: [\u0026#34;name\u0026#34;, \u0026#34;containerPort\u0026#34;], \u0026#34;additionalProperties\u0026#34;: false } }, \u0026#34;livenessProbe\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Instructions for assessing whether the container is alive.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;additionalProperties\u0026#34;: { \u0026#34;$ref\u0026#34;: \u0026#34;#/definitions/healthProbe\u0026#34; } }, \u0026#34;readinessProbe\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Instructions for assessing whether the container is in a suitable state to serve traffic.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;additionalProperties\u0026#34;: { \u0026#34;$ref\u0026#34;: \u0026#34;#/definitions/healthProbe\u0026#34; } } }, \u0026#34;required\u0026#34;: [\u0026#34;name\u0026#34;, \u0026#34;image\u0026#34;, \u0026#34;resources\u0026#34;], \u0026#34;additionalProperties\u0026#34;: false }, \u0026#34;healthProbe\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Health Probe describes how a probing operation is to be executed as a way of determining the health of a component.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;exec\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Instructions for assessing container health by executing a command. Either this attribute or the httpGet attribute or the tcpSocket attribute MUST be specified. This attribute is mutually exclusive with both the httpGet attribute and the tcpSocket attribute.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;command\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;A command to be executed inside the container to assess its health. Each space delimited token of the command is a separate array element. Commands exiting 0 are considered to be successful probes, whilst all other exit codes are considered failures.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } } } }, \u0026#34;httpGet\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Instructions for assessing container health by executing an HTTP GET request. Either this attribute or the exec attribute or the tcpSocket attribute MUST be specified. This attribute is mutually exclusive with both the exec attribute and the tcpSocket attribute.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;path\u0026#34;:{ \u0026#34;description\u0026#34;: \u0026#34;The endpoint, relative to the port, to which the HTTP GET request should be directed.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;port\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The TCP socket within the container to which the HTTP GET request should be directed.\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;integer\u0026#34; }, \u0026#34;httpHeaders\u0026#34;:{ \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Optional HTTP headers.\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;An HTTP header name. This must be unique per HTTP GET-based probe.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;value\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;An HTTP header value.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;required\u0026#34;: [\u0026#34;value\u0026#34;, \u0026#34;name\u0026#34;], \u0026#34;additionalProperties\u0026#34;: false } }, \u0026#34;required\u0026#34;: [\u0026#34;path\u0026#34;, \u0026#34;port\u0026#34;], \u0026#34;additionalProperties\u0026#34;: false }, \u0026#34;tcpSocket\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Instructions for assessing container health by probing a TCP socket. Either this attribute or the exec attribute or the httpGet attribute MUST be specified. This attribute is mutually exclusive with both the exec attribute and the httpGet attribute.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;port\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The TCP socket within the container that should be probed to assess container health.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34; } }, \u0026#34;required\u0026#34;: [\u0026#34;port\u0026#34;], \u0026#34;additionalProperties\u0026#34;: false }, \u0026#34;initialDelaySeconds\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Number of seconds after the container is started before the first probe is initiated.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;default\u0026#34;: 0 }, \u0026#34;periodSeconds\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;How often, in seconds, to execute the probe.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;default\u0026#34;: 10 }, \u0026#34;timeoutSeconds\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Number of seconds after which the probe times out.\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;integer\u0026#34;, \u0026#34;default\u0026#34;: 1 }, \u0026#34;successThreshold\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Minimum consecutive successes for the probe to be considered successful after having failed.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;default\u0026#34;: 1 }, \u0026#34;failureThreshold\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Number of consecutive failures required to determine the container is not alive (liveness probe) or not ready (readiness probe).\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;default\u0026#34;: 3 } } } } } The Relationship of Component and Workload Type # 简而言之，组件是工作负载类型的封装，预先定义了面向用户的示意图。组件需要明确声明其工作负载类型，以便 OAM 系统正常工作，即 OAM 特性系统将检查该工作负载类型信息，以验证特定特性是否可附加到该组件上。\n下面的示例可能有助于更好地解释它们之间的关系：\nWeb 服务组件封装了 Kubernetes 部署和服务。 在这种情况下，apps/v1.Deployment 就是该组件的工作负载类型。 封装 Kubernetes 部署的 后端 Worker 组件。 在这种情况下，apps/v1.Deployment 仍是该组件的工作负载类型。 将 Kubernetes 部署 + Ingress 作为模板的 Helm 图表也是一个组件。 该组件的工作负载类型是 apps/v1.Deployment。 从另一个角度看，组件定义 总是由某些组件提供商或软件构建商定义，而 工作负载定义 作为一种平台能力，总是由基础设施运营商维护。因此，OAM 平台的工作负载定义数量通常非常有限，而且变化不大，但总是有无数的组件定义由不同的提供商和用户维护。\n5. Application Scopes # Application scopes 通过提供具有共同 group 行为的不同形式的应用边界，将组件组合成逻辑应用。\nApplication scopes 具有以下一般特征：\n在定义一组组件实例共有的行为或元数据时，应使用 Application scopes。 一个组件可以同时部署到多个不同类型的应用范围中。 Application scopes 类型可决定组件是否可同时部署到同一应用范围类型的多个实例中。 Application scopes 可用作组件组与基础设施（如网络）或外部功能（如身份提供商）所提供功能之间的连接机制。 下图说明了如何将组件分组到重叠的应用范围中以创建不同的应用边界：\n此示例显示了两个作用域类型，其中分布着四个组件。\n组件 A、B 和 C 部署在同一个健康作用域中。健康作用域将收集其组成组件的总体健康信息，并在组件升级操作期间对这些信息进行评估。需要根据一组组件的总体健康状况进行评估和/或执行操作的特性或组件可进一步使用健康范围提供的查询信息。这是应用程序的一个基本分组结构，它为组件之间的依赖关系提供了一个宽松的定义。\n组件 A 在自己的网络范围内与组件 B、C 和 D 隔离。这样，基础设施运营商就可以为不同的组件组提供不同的 SDN 设置，例如对后端组件的入站/出站规则进行更严格的限制。\nDefining an application scope # 应用范围由 ScopeDefinition 实体表示。\nTop-Level Attributes # 这些属性提供了有关应用范围的顶级信息。\nAttribute Type Required Default Value Description apiVersion string Y A string that identifies the version of the schema the object should have. The core types uses core.oam.dev/v1beta1 in this version of model. kind string Y Must end with ScopeDefinition. metadata Metadata Y Entity metadata. spec Spec Y A specification for scope attributes. Spec # 该规范定义了作用域的构成部分。\nAttribute Type Required Default Value Description allowComponentOverlap bool N true Determines whether a component is allowed to be in multiple instances of this scope type simultaneously. When false, the runtime implementation MUST produce an error and stop deployment if an attempt is made to place a component into more than one instance of this scope type simultaneously. definitionRef DefinitionRef Y Identifier to scope capability in the platform. DefinitionRef # Attribute Type Required Default Value Description name string N Name identifier of the scope capability. Mutually exclusive to apiVersion and kind. apiVersion string N API version of the scope capability. kind string N Kind of the scope capability. Examples # apiVersion: core.oam.dev/v1beta1 kind: ScopeDefinition metadata: name: healthscopes.core.oam.dev spec: allowComponentOverlap: true definitionRef: name: healthscopes.core.oam.dev Health Scope # 健康范围将组件分组为一个总体健康组。组内各组件的总体健康状况为升级和回滚机制提供信息。\n健康范围汇总了组件的健康状态。可以设置健康范围的参数，以确定必须有多少百分比的组件处于不健康状态，才能认为整个范围处于不健康状态。\n健康作用域本身不会根据健康状态采取任何行动。它只是一个群组健康状况聚合器，可被应用程序的其他进程和部分查询和使用，例如：应用程序升级特质可监控健康范围的总体健康状况，并决定何时启动自动回滚。监控应用程序可以监控健康状况范围的总体健康状况以发出警报。\nProperties # Attribute Type Required Default Value Description probe-timeout int32 Y ProbeTimeout is the amount of time in seconds to wait when receiving a response before marked failure. probe-interval int32 Y ProbeInterval is the amount of time in seconds between probing tries. Instance Example # 例如，健康范围实例可以如下所示：\napiVersion: core.oam.dev/v1alpha2 kind: HealthScope metadata: name: example-health-scope spec: probe-timeout: 5 probe-interval: 5 Usage Example # 应用程序可以如下引用上述作用域实例：\nkind: Application metadata: name: example-appconfig spec: components: - componentName: example-component traits: ... scopes: healthscopes.core.oam.dev: example-health-scope Network Scope # 网络范围将组件归入网络子网边界，并定义一般运行时网络模型。网络定义、规则和策略由基础设施的网络或 SDN 描述。\n网络范围将组件组合在一起，并将它们连接到网络或 SDN。网络本身必须由应用运营商定义和运营。\n流量管理功能也可查询网络范围，以确定服务网格的可发现性边界或 API 网关的 API 边界。\n如果未指定网络范围，平台必须将应用程序加入默认网络。在默认网络中，应用配置中的所有组件必须能够相互通信，健康探测器必须能够与定义了健康检查规则的任何组件联系。不过，加入的网络与平台有关。例如，基于集群的环境（如 Kubernetes）会声明集群范围内的网络。相反，真正的无服务器实现可能会将组件加入到仅包括组件和健康检查探针的网络中。\nProperties # Attribute Type Required Default Value Description networkId string Y The id of the network, e.g. vpc-id, VNet name. subnetIds array Y A comma separated list of IDs of the subnets within the network. For example, \u0026lsquo;vsw-123\u0026rsquo; or \u0026lsquo;vsw-123\u0026rsquo;,\u0026lsquo;vsw-456\u0026rsquo;. There could be more than one subnet because there is a limit in the number of IPs in a subnet. If IPs are taken up, operators need to add another subnet into this network. internetGatewayType string Y The type of the gateway, options are \u0026lsquo;public\u0026rsquo;, \u0026rsquo;nat\u0026rsquo;. Empty string means no gateway. Instance Example # 例如，网络作用域实例可以如下所示：\napiVersion: standard.oam.dev/v1alpha2 kind: NetworkScope metadata: name: example-vpc-network spec: allowComponentOverlap: true # can be skipped networkId: cool-vpc-network subnetIds: - cool-subnetwork - cooler-subnetwork - coolest-subnetwork internetGatewayType: nat Usage Example # 应用程序可以如下引用上述作用域实例：\nkind: Application metadata: name: example-appconfig spec: components: - componentName: example-component traits: ... scopes: networkscopes.standard.oam.dev: example-vpc-network 6. Traits # trait 是一种可自行决定的运行时叠加，可通过操作功能增强组件工作负载实例。它为应用程序操作员提供了一个机会，让他们可以对组件的配置做出特定的决定，而无需让组件提供者参与其中或破坏组件封装。\n例如，在 WordPress Helm 图表中注入一个 sidecar container 的 sidecar 特性。\n特性可以是适用于单个组件的分布式应用程序的任何配置，如流量路由规则（如负载平衡策略、网络入口路由、断路、速率限制）、自动扩展策略、升级策略等。\nTrait Definition # 本节具有规范性，因为性状是系统中可检查（也可能是可共享）的部分。所有特征都必须可以用以下格式表示。\n特质的定义与组件一样使用示意图。与工作负载定义一样，每个特质定义都有一个 definitionRef，它是对定义特质配置选项的模式的引用。\nTop-Level Attributes # Attribute Type Required Default Value Description apiVersion string Y A string that identifies the version of the schema the object should have. The core types uses core.oam.dev/v1beta1 in this version of documentation. kind string Y Must be TraitDefinition metadata Metadata Y Information about the trait. spec Spec Y A specification for trait attributes. Spec # Attribute Type Required Default Value Description appliesToWorkloads []string * The collection of workload type characteristics to which this trait applies. If this field is empty or unspecified or specified as \u0026quot;*\u0026quot;, it means applies any workload type. A trait must apply to at least one workload type. This attribute must contain at least one value. conflictsWith []string N A list of traits that would be conflict with this trait when applied to same workload type. For example, autoscaling may conflict with cron-autoscaling. definitionRef DefinitionRef Y Identifier of the trait capability. DefinitionRef # Attribute Type Required Default Value Description name string N Name identifier of the trait. Mutually exclusive to apiVersion and kind. apiVersion string N API version of the trait. kind string N Kind of trait. 建议使用 Group/Version/Kind 来唯一标识特征能力。如果使用 definitionRef.name，则必须包含可用于唯一标识的信息。\nApplication 部分将详细介绍用户如何将特质附加到组件。\nImplementing a Trait System # 警告：本节仅面向 OAM 平台实施者（即 KubVela），如果您只对 OAM 概念感兴趣，请跳过本节。\n在本文档中，_traits system_ 被定义为运行时通过 traits 对组件实例应用和管理操作行为的方式。\n模型的运行时实现必须提供将操作行为附加到组件实例的特征系统。\n当一个组件上有一个以上的特征时，它必须是：\n按照规定的顺序应用特征 确定兼容性，如果特性组合无法满足要求则失败 特征按顺序应用，如果某些特征集表示依赖关系（例如，必须在 SSL 之前设置入口），则可通过设置明确的顺序来解决。\n一个组件实例只能有一个给定特质类型的配置。\n在完成所有特性配置之前，部署不应被标记为已完成。例如，如果网络服务器组件是与自动分级器特质一起部署的，那么在 (a) 网络服务器本身运行（由健康检查确定）和 (b) 自动分级器特质运行（由底层平台确定）之前，网络服务器不应被视为 \u0026ldquo;正在运行\u0026rdquo;。\n组件可指定多个特性；因此，运行时必须支持对组件应用零个或多个特性。如果底层运行时无法满足所请求的特性，运行时必须产生错误并停止部署。\n没有明确要求组合特质的机制。例如，一个特质（ssl）不能指定它要求另一个特质（ingress）应用于一个对象。但是，如果一个特质在没有另一个特质的情况下无法履行合约，那么特质的_实现_就应该失败。系统可以支持一种机制，在这种机制中，一个特质（sslIngress）由两个不同的特质实现（ssl和ingress）不透明地支持。\n在运行时可用的任何特征都必须在该运行时实现。例如，运行时中的 autoscaling 特性不可能没有实现。\nCategories of Traits # 特质系统旨在作为运行时操作能力的扩展点，同时也为组件提供通用的、在某些情况下是必要的操作功能。\n特质的实施细节超出了本文档的范围。不过，为了允许运行时实施任意特质，同时保持一定程度的通用性，以实现不同运行时的应用程序可移植性，特质按以下方式进行分类：\nCore traits：这一类定义了一组类型名称为 core.oam.dev 组的特质定义。这些特性提供了某些工作负载类型和组件功能运行所必需的操作功能。运行时必须实现本模型中定义的这些特征。 Standard traits：该类别定义了一组类型名称为 standard.oam.dev 组的特征定义。这些特征提供了应用程序操作员常用的操作功能，并由大多数运行时实现。建议运行时使用本模型中定义的标准特性定义，以提供与标准特性定义中所列功能相当的操作功能。换句话说，如果运行时正在实现具有行为 foo 的特质，而行为 foo 的标准特质定义已经存在，那么运行时就应该使用标准 foo 特质定义。应用程序操作员如果希望最大限度地提高应用程序的可移植性，就应该使用这些 trait 定义。虽然这并不能保证应用程序的可移植性，但其目的是提高运行时之间的可移植性。 Extension traits：除核心特质和标准特质定义的特质外，该类别还允许运行时定义自己独有的特质定义集。运行时可选择以任意定义实现该类别中的任意一组特质。扩展特质类型名称不得位于 core.oam.dev 或 standard.oam.dev 组中。 Trait Characteristics # 单个特征可与特定工作负载绑定（也可适用于所有工作负载）。特性可以声明适用于哪种工作负载。\n本模型并不对特质的简单或复杂程度设定要求。有些特性可能会提供大量可配置选项，而有些特性则可能不提供任何可配置选项。也不要求特质与其底层技术一样详尽。例如，自动缩放技术的实现可能会提供多种方法来确定组件何时应该缩放。但该特性可能只涉及其中的几种。同样，也可以定义多个自动缩放特性（每个特性都有唯一的名称），每个特性显示不同的可配置选项子集。或者，一个大的特质可以显示所有可能的自动缩放配置。\nOAM 平台的实现应能以下面解释的格式发现所有支持的特征。\n7. Application # 应用程序实体定义了应用程序部署后将被实例化的组件列表。\n用户将指定每个组件的最终参数以及用于增强其功能或改变其行为的特性。此外，还可以指定一组范围，将不同的组件子集分组。\nTop-Level Attributes # Attribute Type Required Default Value Description apiVersion string Y A string that identifies the version of the schema the object should have. The core types uses core.oam.dev/v1beta1 in this version of documentation. kind string Y Must be Application metadata Metadata Y Information about the application. spec Spec Y A specification for application attributes. Spec # The specification of applications defines components to create, traits attached to each components, and a set of scopes the components drop in.\nAttribute Type Required Default Value Description components []Component Y Component specification. Component # Attribute Type Required Default Value Description name string Y The name of the component instance. type string Y A reference to the component definition that will be instantiated by the application. Optionally a namespaced component may be referenced as \u0026lt;namespace\u0026gt;/\u0026lt;component_name\u0026gt;. The meaning of namespace is implementation specific depending on the runtime being used and the target platform where entities are created. In the case of the Kubernetes, the concept of namespace matches the existing one in Kubernetes. properties Properties Y A set of values assigned to the parameters exposed from the component schematic. traits []Trait N The traits to attach to this component instance. scopes map[string]string N A map with the scopes the component belongs to. The map uses the qualified scope definition name as key (e.g., \u0026ldquo;scope.company.com\u0026rdquo;), and the name of the scope as value (e.g., \u0026ldquo;myscope\u0026rdquo;). Notice that this reference implies that an entity of the target scope with the specific name exists. name 必须遵循这些命名规则：\nname 字段为必填字段，必须为 63 个字符或更少，以字母数字字符（[a-z0-9A-Z]）开头和结尾，中间包含破折号 (-)、下划线 (_)、点 (.)和字母数字。\nTrait # Attribute Type Required Default Value Description type string N A reference to the name of trait definition. For one type of trait, there could be only one configuration in one component. properties Properties Y The properties values to use this trait. 注意 _Traits_ 不需要名称，因为 OAM 运行时负责实例化 traits。组件的名称预计将用于相关特质。\nProperties # 属性指定了与实体属性相关联的值。\n当与 _Components_ 关联时，所设置的值将覆盖组件的参数，这些参数遵循 component schematic 中定义的示意图。\n当属性用于 _Traits_ 或 _Scopes_ 时，它们会设置这些实体实例化所需的值。其结构由 definition reference 决定。它可能是一个简单的值，也可能是一个复杂的对象。属性根据适合于特质或范围的模式进行验证。\nExample # 下面是一个完整的 YAML 文件示例，其中表达了组件、特性和作用域。该示例展示了组件的四个可配置元素：type, properties, traits, and scopes。\napiVersion: core.oam.dev/v1beta1 kind: Application metadata: name: my-example-app annotations: version: v1.0.0 description: \u0026#34;Brief description of the application\u0026#34; spec: components: - name: publicweb type: web-ui properties: # properties targeting component parameters. image: example/web-ui:v1.0.2@sha256:verytrustworthyhash param_1: \u0026#34;enabled\u0026#34; # param_1 is defined on the web-ui component traits: - type: ingress # ingress trait providing a public endpoint for the publicweb component of the application. properties: # properties are defined by the trait CRD spec. This example assumes path and port. path: / port: 8080 scopes: \u0026#34;healthscopes.core.oam.dev\u0026#34;: \u0026#34;app-health\u0026#34; # An application level health scope including both components. - name: backend type: company/test-backend # test-backend is referenced from other namespace properties: debug: \u0026#34;true\u0026#34; # debug is a parameter defined in the test-backend component. traits: - type: scaler # scaler trait to specify the number of replicas for the backend component properties: replicas: 4 scopes: \u0026#34;healthscopes.core.oam.dev\u0026#34;: \u0026#34;app-health\u0026#34; # An application level health scope including both components. 上例展示了一个完整的应用程序，包括其作用域、组件及其特性。该应用程序假定存在两个 component definitions，分别代表网络用户界面和假想应用程序的后台。两个 traits 用于增强组件的功能。首先是后台的scaler，用于设置复制因子；其次是ingress，用于将应用程序对外开放。此外，还使用 HealthScope 来检查网络和后端组件的状态。这是本例中任意设置的一组组件，并不一定需要所有组件。\n在实例化应用程序时，运行时将根据目标系统（如 Kubernetes）生成所需的实体，并执行与每个实体（如 traits）相关的模式。由于 _Traits_ 可以作为扩展添加到现有环境中，因此运行时必须能够创建任何通过 TraitDefinition 在系统中注册的 Trait 实体。\nComponent Instances # _component instance_ 是在应用程序部署过程中创建的。它是在组件与配置一起部署时创建的。\n每次部署组件时，它都必须与应用程序一起部署。本规范的这一部分描述的是配置。 组件的每次后续 _upgrade_ 都会修改给定的实例，如果组件附加了任何修订感知特征，则会生成新的实例修订版。 当实例首次创建时，它处于 _initial revision state_。每次升级操作发生时，我们都会说该实例发生了新的_修订_。但是，我们并不假定在某个地方一定存储有相应的 revision 对象。 Releases # 在 Twelve-Factor Applications 中，一个版本被定义为一个 build plus a set of configs。也就是说，对构建或配置的任何更改都会产生一个新版本。在开放式应用程序模型中，类似的情况是 component、trait和scope objects与应用程序结合在一起共同构成一个版本。\n对于 OAM 应用程序，一个版本是这样定义的：\n版本是一个已命名的应用程序及其相关组件、作用域和特性的描述。\n此外，随着应用程序的发布，其组件实例也会被发布。\n为适应发布的这一定义，OAM 平台应做出以下假设：\n应用程序是可变的。 对 application 的任何更改（概念上）都会导致新版本的发布，并取代旧版本。 如果更新了应用程序，而新版本包含了原应用程序中没有的组件，则必须创建组件实例。 特质同样应该根据相同的准则进行附加和分离 组件与应用范围的关系应根据相同的准则应用或删除 8. Practical Considerations # Proposal Stages and the Maturity of the Specification # 该模型目前处于 \u0026ldquo;草稿 \u0026ldquo;阶段，正在向 \u0026ldquo;工作稿 \u0026ldquo;迈进。工作草案之后，该规范将成为最终规范（如 1.0）。\n在草稿阶段，模型的任何部分都不被认为是稳定的 在工作草案期间，可能会添加功能和修复问题。可能会出现破坏性修改，但这是不正常的情况 在最终规范期间，模型只会更新勘误、语法修正和明确标注的 \u0026ldquo;说明性文字\u0026rdquo;。 一旦发布了最终规范（如 1.0），新版本的规范（如 1.1）可能会在粗稿阶段开始。\nMedia Types # 示意图的媒体类型将在模型进入工作草案状态时确定。媒体类型初步将采用以下形式：\napplication/oam.TYPE.v1+json，其中 TYPE 替换为类型名称（如组件）。 Security # 该模式目前的形式并没有规定一套具体的安全政策。不过，它确实为安全的某些方面提供了指导：\nOCI/Docker 图像必须尽可能用 SHA 引用 文件格式必须转换为规范格式，以便进行哈希运算 满足这两个条件后，就可以构建系统，其中原理图的摘要验证将保持系统所有组件的不变性。也就是说，如果示意图用哈希值引用了图像，那么验证示意图摘要的过程也能确保所提取的图像与生成示意图时使用的引用相同。 其他安全细节，如网络传输安全或静态数据的安全，超出了本模型的考虑范围。\n9. Design Principles # Open Application Model 遵循一系列设计原则，以确保模型的清晰度、丰富性和可扩展性。\nSeparation of Concerns # Separation of Concerns 是一种设计理念，在这种理念下，架构选择是根据所要解决的离散问题来进行的。像 component 和 application，或 schematics 和 configuration 这样的工件划分，都是按照功能或行为划分的。通过确定不同用户群体的角色和责任，将规格划分为与问题空间相匹配的概念。\nRuntime Neutrality # Separation of Concerns 与运行时间无关。它不假定任何运行时特定的功能。相反，它旨在为应用程序所有者和操作员提供一个通用词汇表，以描述所需的拓扑结构和行为，而不受任何特定平台的影响。\nBalance (Elegance) # 在确保关注点分离的同时，OAM 力求避免对在较小团队中担任多种角色的用户造成不必要的复杂性。简单的方案应只需投入最少的时间和精力即可实现，但复杂的方案应在不需要重新平台化的情况下加以解决。\nOAM 提供了多个抽象层，这样就可以独立于开发人员的关注点来捕捉操作关注点。\nReusability # OAM 原理图中的组件可重复使用和共享。此外，这些组件独立于其所描述的代码，因此可以重复使用代码（容器），并防止出现 \u0026ldquo;锁定 \u0026ldquo;情况。\n该模型作为一个整体，旨在提供应用程序的 \u0026ldquo;分布式\u0026rdquo;，使同一个应用程序可以在不同的平台上执行而无需改动。应用程序的这种可移植性旨在使以下情况不仅成为可能，而且变得容易：\n将应用程序从开发人员工作站转移到生产集群或服务中 从一个实施方案迁移到另一个实施方案，无需更改代码 创建类似市场的环境，将应用程序部署到客户平台上 Application Models Are Not Programming Models # 应用程序模型和编程模型之间有明显的区别。应用程序模型描述应用程序的组成及其组件的拓扑结构。它不涉及如何实现每个组件（语言、设计模式等）。\n另一方面，编程模型描述的是单个软件是如何组成的。开发人员使用它来实现应用程序组件。开放式应用程序模型提供了一种不需要编程模型的应用程序模型。\n","date":"17 November 2023","permalink":"/posts/architecture/iac/oam/","section":"博客","summary":"Open Application Model 的目标是定义一种标准的、与基础设施无关的方法，用于描述跨混合环境、云甚至边缘设备的应用部署。该模型要解决的核心问题是如何组成分布式应用程序，然后成功地将其交给负责操作的人员。问题不在于如何编写程序，而在于如何采用面向服务（或面向微服务）架构的组件，并简化围绕此类应用的工作流程。","title":"Open Application Model"},{"content":"","date":"16 November 2023","permalink":"/tags/iot/","section":"Tags","summary":"","title":"Iot"},{"content":"vorto # InformationModel: \u0026#39;vortolang\u0026#39; 1.0 \u0026#39;namespace\u0026#39; qualifiedName \u0026#39;version\u0026#39; version (\u0026#39;displayname\u0026#39; string)? (\u0026#39;description\u0026#39; string)? (\u0026#39;category\u0026#39; ID(\u0026#39;/\u0026#39; ID)*)? (modelReference)* \u0026#39;infomodel\u0026#39; ID \u0026#39;{\u0026#39; \u0026#39;functionblocks\u0026#39; \u0026#39;{\u0026#39; (functionblockProperty)* \u0026#39;}\u0026#39; ; functionblockProperty: (\u0026#39;mandatory\u0026#39; | \u0026#39;optional\u0026#39;)? (\u0026#39;multiple\u0026#39;)? ID \u0026#39;as\u0026#39; [FunctionBlock::ID | qualifiedName] (description)? qualifiedName: ID (\u0026#39;.\u0026#39; ID)*; version : int(\u0026#39;.\u0026#39; int)*(\u0026#39;-\u0026#39;ID)?; ID: \u0026#39;^\u0026#39;?(\u0026#39;a\u0026#39;..\u0026#39;z\u0026#39;|\u0026#39;A\u0026#39;..\u0026#39;Z\u0026#39;|\u0026#39;_\u0026#39;) (\u0026#39;a\u0026#39;..\u0026#39;z\u0026#39;|\u0026#39;A\u0026#39;..\u0026#39;Z\u0026#39;|\u0026#39;_\u0026#39;|\u0026#39;0\u0026#39;..\u0026#39;9\u0026#39;)* ; description: STRING modelReference: \u0026#39;using\u0026#39; qualifiedName \u0026#39;;\u0026#39; version; vortolang 1.0 namespace com.mycompany version 1.0.0 description \u0026#34;Information model for FabLab.eu\u0026#39;s IoT Octopus\u0026#34; category multisensor using com.ipso.smartobjects.Accelerometer ; 1.1.0 using com.ipso.smartobjects.Barometer ; 1.1.0 infomodel IoTOctopus { functionblocks { mandatory accelerometer as Accelerometer \u0026#34;some description\u0026#34; optional barometer as Barometer } } FunctionBlock: \u0026#39;vortolang\u0026#39; 1.0 \u0026#39;namespace\u0026#39; qualifiedName \u0026#39;version\u0026#39; version (\u0026#39;displayname\u0026#39; string)? (\u0026#39;description\u0026#39; string)? (\u0026#39;category\u0026#39; ID(\u0026#39;/\u0026#39; ID)*)? (modelReference)* \u0026#39;functionblock\u0026#39; ID (\u0026#39;extends\u0026#39; [Functionblock::ID | qualifiedName])? \u0026#39;{\u0026#39; (\u0026#39;configuration\u0026#39; \u0026#39;{\u0026#39; (Property)* \u0026#39;}\u0026#39;)? (\u0026#39;status\u0026#39; \u0026#39;{\u0026#39; (Property)* \u0026#39;}\u0026#39;)? (\u0026#39;events\u0026#39; \u0026#39;{\u0026#39; (ID \u0026#39;{\u0026#39; (Property)* \u0026#39;}\u0026#39;)* \u0026#39;}\u0026#39;)? (\u0026#39;operations\u0026#39; \u0026#39;{\u0026#39; (Operation)* \u0026#39;}\u0026#39;)?\t\u0026#39;}\u0026#39;\t; qualifiedName: ID (\u0026#39;.\u0026#39; ID)*; version : int(\u0026#39;.\u0026#39; int)*(\u0026#39;-\u0026#39;ID)?; ID: \u0026#39;^\u0026#39;?(\u0026#39;a\u0026#39;..\u0026#39;z\u0026#39;|\u0026#39;A\u0026#39;..\u0026#39;Z\u0026#39;|\u0026#39;_\u0026#39;) (\u0026#39;a\u0026#39;..\u0026#39;z\u0026#39;|\u0026#39;A\u0026#39;..\u0026#39;Z\u0026#39;|\u0026#39;_\u0026#39;|\u0026#39;0\u0026#39;..\u0026#39;9\u0026#39;)*; modelReference: \u0026#39;using\u0026#39; qualifiedName \u0026#39;;\u0026#39; version; description: STRING Property: Please refer to chapter Model Property for details Operation: (\u0026#39;extension\u0026#39;)? (\u0026#39;mandatory\u0026#39; | \u0026#39;optional\u0026#39;)? (\u0026#39;breakable\u0026#39;)? ID \u0026#39;(\u0026#39; (OperationType (\u0026#39;,\u0026#39; OperationType)*)?\u0026#39;)\u0026#39; (\u0026#39;returns\u0026#39; OperationType)? (description)?; OperationType: DictonaryOperationType | OperationPrimitiveType | OperationObjectType; DictonaryOperationType: (\u0026#39;multiple\u0026#39;)? ID \u0026#39;as\u0026#39; DictionaryType (\u0026#39;\u0026lt;\u0026#39; ConstraintRule \u0026#39;\u0026gt;\u0026#39;)? (description)?; OperationPrimitiveType: (\u0026#39;multiple\u0026#39;)? ID \u0026#39;as\u0026#39; PrimitiveType (\u0026#39;\u0026lt;\u0026#39; ConstraintRule \u0026#39;\u0026gt;\u0026#39;)? (description)?; OperationObjectType: (\u0026#39;multiple\u0026#39;)? ID \u0026#39;as\u0026#39; ObjectType (description)?; vortolang 1.0 namespace com.mycompany version 1.0.0 functionblock Accelerometer { configuration { mandatory enable as boolean } status { mandatory xValue as float \u0026#34;The measured value along the X axis.\u0026#34; optional yValue as float \u0026#34;The measured value along the Y axis.\u0026#34; optional zValue as float \u0026#34;The measured value along the Z axis.\u0026#34; } operations { reset() returns boolean \u0026#34;resets the accelerometer\u0026#34; } } https://github.com/eclipse/vorto/blob/master/docs/vortolang-1.0.md Flask # Flask: Staged Functional Programming for Sensor Networks\nMainland G, Morrisett G, Welsh M. Flask: Staged Functional Programming for Sensor Networks[J]. 2008.\nIOTCollab # A data sharing strategy and a DSL for service discovery Adda M, Saad R. A data sharing strategy and a DSL for service discovery, selection and consumption for the IoT[J]. Procedia Computer Science, 2014, 37: 92-100.\nComPOS # Rules # ComPOS: A DSL for Composing IoT Systems with Weak Connectivity Alfred Åkesson, Görel Hedin, and Niklas Fors. 2023. ComPOS: A DSL for Composing IoT Systems with Weak Connectivity. In Proceedings of the 10th ACM SIGPLAN International Workshop on Reactive and Event-Based Languages and Systems (REBLS 2023). Association for Computing Machinery, New York, NY, USA, 31–42. https://doi.org/10.1145/3623506.3623577\n","date":"16 November 2023","permalink":"/posts/language/dsl/iot-dsl/","section":"博客","summary":"vorto # InformationModel: \u0026#39;vortolang\u0026#39; 1.","title":"iot 相关 dsl"},{"content":"\u0026ldquo;Aas\u0026rdquo; 是一种缩写，代表了一系列与计算机和信息技术相关的服务模型。以下是一些常见的 \u0026ldquo;as a Service\u0026rdquo;（Aas）模型：\nIaaS（Infrastructure as a Service）基础设施即服务：提供计算基础设施，如虚拟机、存储和网络。用户可以通过云平台按需使用这些资源，而不必购买和维护物理硬件。\nPaaS（Platform as a Service）平台即服务：提供了一个开发和运行应用程序的平台，包括开发工具、运行时环境和相关服务。用户无需担心底层的基础设施，只需关注应用程序的开发和部署。\nSaaS（Software as a Service）软件即服务：提供基于云的软件应用，用户可以通过互联网访问。这种模型下，软件通常由第三方提供和维护，用户只需使用而无需担心软件的安装和更新。\nDaaS（Desktop as a Service）桌面即服务：提供了虚拟桌面环境，允许用户通过云访问其个人桌面。这对于远程办公和移动设备的用户尤其有用。\nCaaS（Communication as a Service）通信即服务：提供通信服务，如语音通话、视频会议和实时消息。这样的服务可以帮助企业构建和扩展其通信基础设施。\nFaaS（Function as a Service）函数即服务：以事件驱动的方式执行单一功能的小块代码（函数）。这种模型对于处理特定任务或事件响应非常有效，用户只需支付其代码运行的实际时间。\nMaaS（Monitoring as a Service）监控即服务：提供基于云的监控和分析服务，帮助用户监视其应用程序和基础设施的性能、可用性和安全性。\nBaaS（Backend as a Service）后端即服务：提供应用程序开发所需的后端功能，如数据库、用户管理和文件存储。这简化了应用程序开发过程，使开发者可以专注于前端和业务逻辑。\n","date":"16 November 2023","permalink":"/posts/architecture/aas/","section":"博客","summary":"\u0026ldquo;Aas\u0026rdquo; 是一种缩写，代表了一系列与计算机和信息技术相关的服务模型。以下是一些常见的 \u0026ldquo;as a Service\u0026rdquo;（Aas）模型：","title":"as a Service 模型"},{"content":"","date":"16 November 2023","permalink":"/tags/baas/","section":"Tags","summary":"","title":"BaaS"},{"content":"","date":"16 November 2023","permalink":"/tags/caas/","section":"Tags","summary":"","title":"CaaS"},{"content":"","date":"16 November 2023","permalink":"/tags/daas/","section":"Tags","summary":"","title":"DaaS"},{"content":"","date":"16 November 2023","permalink":"/tags/faas/","section":"Tags","summary":"","title":"FaaS"},{"content":"","date":"16 November 2023","permalink":"/tags/iaas/","section":"Tags","summary":"","title":"IaaS"},{"content":"","date":"16 November 2023","permalink":"/tags/maas/","section":"Tags","summary":"","title":"MaaS"},{"content":"","date":"16 November 2023","permalink":"/tags/paas/","section":"Tags","summary":"","title":"PaaS"},{"content":"","date":"16 November 2023","permalink":"/tags/saas/","section":"Tags","summary":"","title":"SaaS"},{"content":"","date":"16 November 2023","permalink":"/tags/iac/","section":"Tags","summary":"","title":"Iac"},{"content":"基础架构即代码（IaC）是通过机器可读的定义文件而不是物理硬件配置或交互式配置工具来管理和配置计算机数据中心资源的过程。定义可以在版本控制系统中。定义文件中的代码可以使用脚本或声明式定义，而不是通过人工流程来维护代码，但 IaC 更经常使用声明式方法。\n起源 # IaC 是为了应对公用事业计算和第二代网络框架带来的困难而发展起来的。2006 年，亚马逊网络服务公司（Amazon Web Services）推出了弹性计算云（Elastic Compute Cloud），而就在几个月前，Ruby on Rails 推出了 1.0 版本，这在企业中造成了广泛的扩展问题，而以前只有大型跨国公司才会遇到这种问题。用代码对基础架构进行建模，然后利用已知的软件最佳实践来设计、实施和部署应用基础架构的想法吸引了软件开发人员和 IT 基础架构管理员。将基础架构视为代码，并使用与其他软件项目相同的工具，可以让开发人员快速部署应用程序。\n优势 # IaC 的价值可细分为三个可衡量的类别：成本、速度和风险。\n降低成本的目的不仅是在财务上帮助企业，而且在人力和精力方面也是如此，这意味着通过消除人工部分，人们能够将精力重新集中到其他企业任务上。 基础设施自动化通过在配置基础设施时更快地执行来提高速度，并旨在提供可视性，以帮助整个企业的其他团队更快、更高效地工作。 自动化消除了与人为错误相关的风险，如手动错误配置；消除这种风险可以减少停机时间并提高可靠性。这些成果和属性有助于企业实施 DevOps 文化，即开发和运营的结合工作。 方法类型 # IaC 通常有两种方法：声明式（功能性）与命令式（程序性）。声明式方法与命令式方法的区别主要在于 \u0026ldquo;是什么\u0026rdquo; 与 \u0026ldquo;如何做\u0026rdquo;。\n声明式方法关注的是最终目标配置应该是什么； 命令式方法关注的是如何改变基础架构来实现这一目标。命令式方法定义了需要按适当顺序执行的特定命令，以实现预期的结论。 Methods # IaC 有两种方法：push 和 pull。主要区别在于告诉服务器如何配置的方式。\n在 pull 中，待配置的服务器将从控制服务器拉动其配置。 在 push 中，控制服务器将配置推送到目标系统。 持续配置自动化 # 所有持续配置自动化（CCA）工具都可视为传统 IaC 框架的延伸。它们利用 IaC 来更改、配置和自动化基础架构，还能提供基础架构管理方式的可视性、效率和灵活性。\n社区内容是决定开源 CCA 工具质量的关键因素。正如 Gartner 所说，CCA 工具的价值 \u0026ldquo;既取决于用户社区贡献的内容和支持，也取决于自动化工具的商业成熟度和性能\u0026rdquo;。Chef 有 Chef Community Repository，Puppet 有 PuppetForge。其他供应商依靠邻近的社区，并利用其他 IaC 框架，如 PowerShell DSC。新的供应商正在出现，它们不是内容驱动型，而是模型驱动型，通过产品中的智能来提供内容。这些可视化、面向对象的系统非常适合开发人员使用，但对面向生产的 DevOps 和运营人员尤其有用，因为他们重视模型而不是脚本内容。随着该领域的不断发展和变化，基于社区的内容对于如何使用 IaC 工具将变得越来越重要，除非这些工具是模型驱动和面向对象的。\nTool Released by Method Approach Written in Comments Chef Chef (2009) Pull Declarative and imperative Ruby - Otter Inedo (2015) Push Declarative and imperative - Windows-oriented Puppet Puppet (2005) Push and Pull Declarative and imperative C++ \u0026amp; Clojure since 4.0, Ruby - SaltStack SaltStack (2011) Push and Pull Declarative and imperative Python - CFEngine Northern.tech Pull Declarative C - Terraform HashiCorp (2014) Push Declarative and imperative Go - Ansible / Ansible Tower Red Hat (2012) Push Declarative and imperative Python - 其他工具包括 AWS CloudFormation、cdist、StackStorm、Juju、Pulumi 和 Step CI。\n关系 # 与 DevOps 的关系 # IaC 是实现 DevOps 最佳实践的关键属性。开发人员会更多地参与到配置定义中，而运营团队则会更早地参与到开发过程中。利用 IaC 的工具可实现服务器状态和配置的可视性，并最终为企业内的用户提供可视性，目的是将团队团结起来，最大限度地发挥他们的作用。自动化的总体目标是消除人工流程中的混乱和易错之处，使其更加高效、富有成效。这样就可以灵活地创建更好的软件和应用程序，减少停机时间，并为公司带来整体成本效益。IaC 的目的是降低复杂性，因为复杂性会降低人工配置的效率。自动化和协作被认为是 DevOps 的核心要点；基础设施自动化工具通常是 DevOps 工具链的组成部分。\n与安全的关系 # Unit 42（网络安全提供商 Palo Alto Networks 的威胁情报部门）发布的《2020 年云威胁报告》指出，基础设施作为代码模板存在约 20 万个潜在漏洞。\n","date":"16 November 2023","permalink":"/posts/architecture/iac/concept/","section":"博客","summary":"基础架构即代码（IaC）是通过机器可读的定义文件而不是物理硬件配置或交互式配置工具来管理和配置计算机数据中心资源的过程。定义可以在版本控制系统中。定义文件中的代码可以使用脚本或声明式定义，而不是通过人工流程来维护代码，但 IaC 更经常使用声明式方法。","title":"IaC基本概念"},{"content":"一、书名和作者 # 书名：《黑客与画家》 作者： 保罗·格雷厄姆 二、书籍概览 # 主要论点和结构：《黑客与画家》聚焦于计算机科学、创新和创业，探讨了黑客文化、编程艺术、创意思维以及成功企业的共同特质。书中通过一系列独立的随笔和观点，呈现了作者对技术、艺术和创业的独到见解。 目标读者和应用场景：适合对计算机科学、创新和创业感兴趣的读者。尤其对那些想要了解黑客文化、程序员思维方式，以及创业者在技术领域取得成功的人群。 三、核心观点与主题 # 主题一、黑客精神的力量 # 子观点1：作者阐述了黑客文化的重要性，强调了创新和突破传统的思维方式。黑客精神包括对问题的独立思考和不断追求卓越的态度。 子观点2：书中通过介绍一些著名的黑客和创业者，展示了这种黑客文化是如何推动科技和社会变革的。 实例或案例\n以比尔·盖茨、马克·安德森等成功人士为例，深入阐释了《黑客与画家》中提到的黑客精神在实践中的深远影响。比尔·盖茨，作为微软创始人之一，体现了黑客文化的核心力量，他在早期的个人计算机时代，通过对计算机系统的钻研和创新，为个人计算机的普及和发展做出了巨大贡献。\n盖茨的黑客精神体现在对技术的热情和不断追求创新。他早年对基于Altair 8800微型计算机的BASIC编程语言的开发，展现了黑客对探索未知、挑战技术极限的勇气。此外，他对操作系统的独到见解和创新，使得微软成为全球最大的个人计算机软件公司。\n马克·安德森是互联网时代的代表性人物之一，作为Netscape公司的创始人之一，他在推动Web浏览器的发展方面发挥了关键作用。安德森的黑客精神体现在对互联网技术的前瞻性洞察和对创新思维的坚持。他不仅推动了互联网技术的发展，还促成了商业与技术的深度融合，对当时尚处于摸索阶段的互联网产业产生了深远的影响。\n这两位成功人士的经历生动展现了黑客精神在实践中的影响力，即对技术的激情、对创新的追求、对未知的勇气。这些特质不仅推动了个人的成功，更影响了整个科技产业的发展方向，为社会带来了深远的变革。他们的故事在《黑客与画家》中得到生动阐释，为读者呈现了黑客文化如何在实践中引领技术和社会的变革。\n主题二、编程艺术与创意思维 # 子观点1：探讨了编程艺术，强调编程是一门创意的艺术。程序员不仅仅是码农，更是创作者，通过编程表达自己的思想和创意。 子观点2：论述了编程语言的选择对于表达思想的重要性，以及如何通过编程语言来实现清晰、优雅的代码。 实例或案例\n通过Unix操作系统和Lisp语言的设计，《黑客与画家》生动展示了编程艺术对技术进步的深刻推动。Unix的设计理念体现了编程艺术的核心原则，为操作系统的模块化和可扩展性奠定了基础。\nUnix的创始人肯·汤普逊和丹尼斯·里奇在设计Unix时追求的是简洁、灵活和可组合的原则。他们将系统划分为小而独立的工具，每个工具都专注于一个特定的功能，通过简单的接口相互协作。这种模块化的设计使得Unix更易于维护和扩展，也为后来的操作系统提供了设计的范本。编程艺术在Unix的设计中得以体现，强调简单而优雅的解决方案。\nLisp语言的设计则展现了编程艺术对语言创新的巨大影响。Lisp以其强大的元编程能力和函数式编程范式而著称，这源于其灵活的语法和对代码即数据的看法。Lisp的设计者约翰·麦卡锡致力于创建一种能够自我改进的语言，这种追求源于对编程艺术的独特理解。\nUnix和Lisp的设计思想都强调了对技术的创新和对编程艺术的追求。它们的成功不仅在于技术上的突破，更在于对简洁、灵活和优雅的编程原则的坚持。这些设计不仅影响了操作系统和编程语言的发展，也深刻地影响了整个计算机科学领域，成为编程艺术对技术进步推动的典范。\n主题三、技术与商业的交汇 # 子观点1：强调创业的本质是创造价值，而不仅仅是追求金钱。成功的创业者是那些能够解决实际问题、满足人们需求的人。 子观点2：论述了创业过程中的坚持和创新的重要性，以及如何在竞争激烈的市场中脱颖而出。 实例或案例\n谷歌作为搜索引擎巨头，其创始人拉里·佩奇和谢尔盖·布林在技术创新上投入了大量资源。他们的搜索算法和广告模型的创新为公司带来了技术领域的领先地位。然而，谷歌不仅仅停留在技术创新，还成功将其技术实力与商业模式相结合，通过广告营销等商业手段实现盈利，最终成为全球最有价值的科技公司之一。\n苹果公司则以整合硬件和软件为核心的创新模式而著称。乔布斯作为创始人之一，注重产品的用户体验和设计，致力于将技术融入生活中。苹果在创新产品上的成功，如iPhone和iPad等，既源于技术上的巧妙设计，也得益于商业上的营销和销售策略。公司通过创造独特的用户体验，成功吸引了全球消费者，实现了技术和商业的良性互动。\n这些公司的成功经验表明，技术创新与商业策略的紧密结合是推动产业变革的重要动力。在竞争激烈的科技行业，单一追求技术创新或仅注重商业模式都难以取得长久的成功。相反，通过技术与商业的协同创新，公司能够更好地满足市场需求，实现可持续发展。这一观点深刻影响着科技产业的发展方向，体现了技术与商业相互促进的关系。\n主题四、创新的思维模式 # 子观点1：探讨了自由软件和开源文化，阐释了分享、合作和开放源代码对于技术发展的推动作用。 子观点2：论述了互联网时代下，开放式的合作模式如何改变了传统产业和社会结构。 实例或案例:\n互联网时代强调开放、共享、平台化的理念，这对传统的封闭式思维模式构成了挑战。开放源代码、开放式创新等概念成为互联网时代的代表，鼓励人们通过共同的努力推动技术的发展。Linux操作系统就是一个典型的例子，通过开源模式汇聚了全球程序员的智慧，取得了巨大成功。这种开放合作的思维模式有效促进了技术的迅猛发展。\n另一方面，互联网时代强调用户体验和个性化需求，这对传统的生产模式提出了新的挑战。以谷歌为代表的搜索引擎技术正是通过深度学习等创新手段，实现了更智能、个性化的搜索服务。这种以用户需求为中心的思维方式推动了技术的不断创新，加速了科技应用在日常生活中的普及。\n四、亮点与启发 # 最有影响的观点或实例\n本书最有影响的观点之一是强调黑客文化的重要性，它突显了独立思考、挑战传统的态度如何推动科技创新。此外，作者对编程艺术的阐述也是亮点之一，让人重新审视编程作为一门创意的表达方式。书中的案例和观点启发人们要追求卓越，不断追求创新，形成独特的思维方式。\n对个人或专业发展的启示 本书通过黑客文化、编程艺术、创业和开源文化等方面的讨论，告诉我要激发了追求独立思考、创造性表达的动力，同时保持对技术和创新的敏感性，培养自己的编程艺术，在创业过程中注重创造实际价值。\n五、批评与局限性 # 任何有争议、模糊或过时的信息\n书中对某些技术和创业观点的强调可能存在一定争议。随着科技的发展，科技的发展，一些观点可能已经过时，而书中并未对此进行充分更新。此外，某些案例和观点可能受到时间和技术发展的影响，需要在实际应用时进行审慎考虑。\n可能的不足或缺陷\n书中的一些观点可能相对主观，读者在接受时需保持批判性思维，结合实际情况进行理解。\n六、实际应用和拓展 # 在实际工作/学习中如何应用这些概念\n本书提供的关于黑客文化、编程艺术和创业的观点，可以在实际工作和学习中得到应用。读者可以通过培养独立思考的能力，追求创新，将编程视为一门艺术来提升个人技能。在创业方面，书中的创业观点可以指导创业者更好地理解市场、解决问题，注重产品的实际价值。\n对未来研究或实践的建议\n为了更深入地理解技术和创业领域的发展，建议读者在阅读本书的同时，继续关注行业的最新动态，参与相关的社区和活动。此外，可以拓展阅读其他与计算机科学、创新和创业相关的著作，以获取更广泛的知识，并在实践中不断积累经验。\n七、总结与评价 # 对书籍的整体评价\n《黑客与画家》是一本具有启发性和思考深度的书籍，通过多个主题的探讨，为读者呈现了作者对技术、艺术和创业的独到见解。书中的案例和观点引人深思，激发了读者对于计算机科学和创新领域的兴趣。尽管存在一些争议和可能过时的信息，但整体上对于推动读者思考和探索这一领域具有积极作用。\n书籍的长处和短处\n该书的长处在于其广泛的主题涵盖，深刻的观点和独特的思考方式。黑客文化、编程艺术、创业等方面的讨论都为读者提供了独特的视角。然而，书中的一些观点可能受到时间和技术发展的限制，需要读者在实际应用时保持警觉。此外，对于某些主题的论述可能显得过于简略，需要读者进一步深入研究。\n","date":"12 November 2023","permalink":"/read/%E9%BB%91%E5%AE%A2%E4%B8%8E%E7%94%BB%E5%AE%B6/","section":"阅读","summary":"聚焦于计算机科学、创新和创业，探讨了黑客文化、编程艺术、创意思维以及成功企业的共同特质。书中通过一系列独立的随笔和观点，呈现了作者对技术、艺术和创业的独到见解。适合对计算机科学、创新和创业感兴趣的读者。尤其对那些想要了解黑客文化、程序员思维方式，以及创业者在技术领域取得成功的人群。","title":"《黑客与画家》读书笔记"},{"content":" https://cloud.tencent.com/developer/article/1948324 类加载机制简介 # 类加载整体流程如下图所示，这也是类的生命周期：\n字节码文件需要经过加载，链接（包括验证、准备、解析），初始化才能转为类，然后才能根据类来创建对象\n需要注意的是，图中红框所代表的加载，验证，准备，初始化，卸载这五个阶段的顺序是确定的，类加载必须严格按照这五个阶段的顺序来开始，但解析阶段则未必，有可能在初始化之后才开始，主要是为了支持 Java 的动态绑定特性，那么各个阶段主要做了哪些事呢？\n加载 # 在加载阶段，虚拟机需要完成以下三件事\n通过一个类的全限定名来获取此类的二进制字节流 将这个字节流所代表的静态存储结构转化为方法区的运行时结构 在内存中生成一个代表这个类的 java.lang.Class 对象，作为方法区这个类的各种数据的访问入口 如上图所示，加载后生成的类对象与对象间的关系如上图所示，什么是类对象呢，比如实例的 getClass() 或 Foo.Class 即为类对象\n每个类只有一个对象实例（类对象），多个对象共享类对象，这里有个需要注意的点是类对象是在堆中而不是在方法区（这里针对的是 Java 7 及以后的版本），所有的对象都是分配在堆中的，类对象也是对象，所以也是分配在堆中，这点网上挺多人混淆了，需要注意一下\n有人可能会奇怪，只看上面这张图，对象和类对象貌似联系不起来，实际上在虚拟机底层，比如 Java Hotspot 虚拟机，对象和类是以一种被称为 oop-klass 的模型来表示的，每个对象或类都有对应的 C++ 类表示方式，它的底层其实是如下这样来表示的，通过下图可以看到，通过这种方式实例对象和 Class 对象就能联系起来了。\n有人可能会困惑，为啥需要做这些校验工作呢，字节码文件难道不安全？字节码文件一般来说是通过正常的 Java 编译器编译而成的，但字节码文件也是可以编辑修改的，也是有可能被篡改注入恶意的字节码的，就会对程序造成不可预知的风险，所以加载阶段的验证是非常有必要的。\n我们可以在执行 java 程序的时候加上 -verbose:class 或 -XX:+TraceClassLoading 这两个 JVM 参数来观察一下类的加载情况，比如我们写了如下测试类：\npublic class Test { public static void main(String[] args) { } } 编译后执行 java -XX:+TraceClassLoading Test\n可以看到如下加载过程\n[Opened /Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home/lib/rt.jar] [Loaded java.lang.Object from /Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home/lib/rt.jar] [Loaded java.lang.CharSequence from /Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home/lib/rt.jar] [Loaded java.lang.String from /Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home/lib/rt.jar] ... // 省略号表示加载了很多 lib/rt.jar 下的类 [Loaded Test from file:/Users/ronaldo/practice/] ... 注意看倒数第二行，可以看到 Test 类被加载了，这可以理解，因为执行了 Test 的 main 方法，Test 会被初始化，也就会被加载（之后会讲述初始化条件）， 但上述有挺多加载 lib/rt.jar 下的类又是怎么回事呢？\n要回答这个问题，我们必须得先搞清楚一个问题：我们说的类加载到底是由谁执行的？\n双亲委派模式 # 类加载必须由类加载器（classloader）来完成，类加载器+类的全限定名（包名+类名）唯一确定一个类，看到这有人可能会问了，类加载器难道会有多个？\n猜得没错！类加载器的确会有多个，为啥会有多个呢，主要有两个目的：安全性和责任分离\n首先说安全性，试想如果只有一个类加载器会出现什么情况，我们可能会定义一个 java.lang.virus 的类，这样的话由于此类与 Java.lang.String 等核心类处于同一个包名下，那么此类就具有访问这些核心类 package 方法的权限，此外如果用户自定义一个 java.lang.String 类，如果类加载器加载了这个类，有可能把原本的 String 类给替换掉，这显然会造成极大的安全隐患\n再来说责任分离，像 rt.jar 包下的核心类等没有什么特殊的要求显然可以直接加载，而且由于是核心类，程序一启动就会被加载，也可以进一步优化来提升加载速度，而有些字节码文件由于反编译等原因可能需要加密，此时类加载器就需要在加载字节码文件时对其进行解密，再比如实现热部署也需要类加载器从指定的目录中加载文件，这些功能如果都在一个类加载器里实现，会导致类加载器的功能很重，所以解决办法就是定义多个类加载器，各自负责加载指定路径下的字节码文件，从而针对指定路径下的类文件加载做相关的操作，达到责任分离的目的\n在 JVM 中有哪些类加载器呢 # 主要有以下三类加载器\n启动类加载器（BootstrapClassLoader）：，负责加载\\lib 下的 rt.jar，resources.jar 等核心类库或者 -Xbootclasspath 指定的文件 扩展类加载器（Extension ClassLoader）：负责加载\\lib\\ext目录或java.ext.dirs系统变量指定的路径中的所有类库。 应用程序类加载器（Application ClassLoader）。负责加载用户类路径（classpath）上的指定类库，我们可以直接使用这个类加载器。一般情况，如果我们没有自定义类加载器默认就是用这个加载器。 类加载器的主要作用就是负责加载字节码二进制流，将其最终转成方法区中的类对象\n现在我们知道了有以上几个种类的类加载器，那么这里有三个问题需要回答：\n怎么指定类由指定的类加载器加载的呢？ 类加载器是如何保证类的一致性的，由以上可知类加载器+类的全限定名唯一确定一个类，那怎么避免一个类被多个类加载器加载呢，毕竟你无法想象工程中有两个 Object 类，那岂不乱套了 类加载器（java.lang.ClassLoader）是用来加载类的，但其本身也是类，那么类加载器又是被谁加载的呢 为了解决上述问题，类加载器采用了双亲委派模型模式来设计类加载器的层次结构\n什么是双亲委派模式 # 可以看到，程序默认是由 AppClassLoader 加载的，每个类被相应的加载器加载后都会被缓存起来，这样下次再碰到相关的类直接从缓存里获取即可，避免了重复的加载，同时每个类由于只会被相应的类加载器加载，确保了类的唯一性，比如 java.lang.Object 只会被 BootstrapClassLoader 加载，保证了 Object 的唯一性\n类加载器是如何加载类的呢？ # 当类首次被加载时（假设此类为 ArrayList），AppClassLoader 并不会马上就加载它，而是会向上委托给它的 parent，即 ExtClassLoader，查看是否已加载了这个类，如果没有则继续向上委托给 BootsrapClassLoader 让其加载，此时 BootsrapClassLoader 就会从 lib/rt.jar 加载此类生成类对象并缓存起来，然后 BootsrapClassLoader 会把此类对象返回给 ExtClassLoader，ExtClassLoader 再把此类对象返回给 AppClassLoader，然后就可以基于此类对象来创建类的实例对象了 当再次调用 new ArrayList() 时，也会触发 ArrayList 的加载，此时 AppClassLoader 也会首先往上层层委托给 BootsrapClassLoader 给加载，由于其缓存里已经有此类对象了，所以直接在缓存里查找后递归返回给 AppClassLoader 即可。 实际上 AppClassLoader 和 ExClassLoader 都是 java.lang.ClassLoader 的子类，它们都是在应用启动时是由 BootstrapClassLoader 加载的，毕竟其它类要由这三个类加载器加载，所以这三个类加载器必须先存在，那么谁来加载 BootstrapClassLoader 呢，如果还是由另一个类加载器加载，那么还要设计一个类加载器来加载它，。。。，就陷入了无限循环之中，所以 BootstrapClassLoader 在 JVM 中本身是以 C++ 形式实现的，是 JVM 的一部分，在应用启动时就存在了，所以它本身可以说是 JVM 自身创建的，不需要由另外的加载器加载，所以它也被称为根加载器\njava.lang 下的一些核心类如 Object,String，Class 等核心类本身非常重要也很常用，所以在应用启动时 BootstrapClassLoader 也会提前把它们加载好，另外在加载 AppClassLoader 和 ExClassLoader 时在这两个类中也会遇到使用 List 等核心类的情况，所以也会把 rt.jar 中的这些核心类也一起加载了，这就是为什么我们在上文看到 Test 类被加载前也看到了这些核心类被加载的原因\n不是的，一个典型的应用场景就是 Tomcat 的类加载，由于 Tomcat 可能会加载多个 web 应用，而多个应用很有可能出现 包名+类名 都一样的类，最典型的比如两个应用采用了同样的第三方类库，但是它们的版本不同，这种情况下如果按双亲委派来加载，只会有一个类对象，显然有问题，这种情况要能区分各个应用的类，就得破坏双亲委派机制，如下：\n绿色部分是 java 项目在打 war 包的时候, tomcat 自动生成的类加载器, 也就是说 , 每一个项目打成一个war包, tomcat都会自动生成一个类加载器, 专门用来加载这个 war 包，当加载 war 包中的类时，首先由 webappClassLoader 加载，而不是首先委托给上一级类加载器加载，这样的话由于加载每一个 war 包的 webappClassLoader 都不一样，每个 war 包被加载的类最终生成的类对象也必然不一样！就达到了应用程序间类的隔离\n最后有一个需要注意的点是并不是所有的类都需要通过类加载器来加载创建，比如数组类就比较特殊，它是由 Java 虚拟机直接在内存中动态构造出来的，但由于类的特性（类加载器+类的全限定名惟一确定一个类），数组类依然最终会会被标识在某个加载器的命名空间下，到底标识在哪个类加载器的命名空间下，取决于数组的组件类型（比如 int[] 数组组件类型为 int，String[] 数组组件类型为 String），如果组件类型为 int 等基本类型，会标识在启动类加载器 bootstrapclassloader 下，如果为其它的引用类型（比如自定义的类 Test，数组为 Test）则标识为最终加载此类的类加载器下\n花了这么大的笔墨终于把加载阶段讲完了，这个阶段真的很重要，不仅是因为它是类加载的第一个阶段，还因为其中涉及到双亲委派等原理，如果没有搞明白的，建议多看几遍，应该都讲得比较清楚了。\n接下来我们再来看另外两个阶段：链接和初始化，首先需要明白的是，加载阶段完成后并不会马上就做之后的链接，初始化的操作，比如如果我有一个类 Test，在方法中定义了一个 Test[] list = new Test[10]; 这样的数组变量，此时会触发 Test 类的加载，但并不会触发 Test 类的链接，初始化。\n链接 # 验证，准备和解析，其中验证又包括字节码验证和符号引用验证\n这里的验证主要有两种字节码验证，符号引用验证\n字节码验证 # 这个阶段主要是对类的方法体（Class 文件中的 Code 属性）进行校验分析，保证被校验类的方法不会在运行时做出危害虚拟机安全的行为，比如：\n保证任何跳转指令不会跳转到方法体以外的字节码指令上 保证类的转换是有效的，比如可以把子类对象赋值给父类变量，反之则不行 … 符号引用验证 # 这个验证其实是在解析阶段发生，符号引用可以看作是对类自身以外（常用池引用中的各种符合引用）的各类信息进行匹配性的验证，我们知道在字节码方法中如果调用了或者说引用了某个类，那么这个类是在字节码中是以符号引用的形式存在的，所以就要确保真正用到此类的时候能找到此类，如果找不到就会报错，举个简单的例子，假设有以下两个类，显然编译时都能通过，但在编译后如果我把 B.class 删掉，A.class 保留着 B 类的符号引用，如果执行 A 的 main 方法需要加载 B 类，由于 B.class 文件缺失导致无法加载 B 类，就会报错\n// B.java public class B { } // A.java public class A { public static void main(String[] args) { B b = new B(); } } 符号引用验证不光验证类，还会验证方法，字段等\n注意，类的验证并不是必须的，如果你能确保你的 class 文件是绝对安全的，那么可以开启 -Xverify:none 来关闭类的验证，这样可以缩短类的加载时间以达到加快类加载的目的。\n准备 # 准备阶段的主要目的有两个\n为了给被加载类的静态字段分配内存，并为其赋默认初始值，如 int 类型的静态变量默认赋值为 0 部分 Java 虚拟机还会在此阶段构造其他跟类层次相关的数据结构，比如说用来实现虚方法的动态绑定的方法表。 解析 # 如前所述，这一阶段会进行符号引用验证，主要作用是在运行时把字节码类中的常量池符号引用解析成为能定位到内存方法区中对应类信息的直接引用（内存中的具体地址），以上述的代码为例\n// B.java public class B { } // A.java public class A { public static void main(String[] args) { B b = new B(); } } 在编译后，A 类的字节码文件 A.class 包括 B 的符号引用，那么在执行 main 方法后，由于碰到了 new B()，此时就会将 B 的符号引用转为指向 B 的类对象的直接引用，由于 B 未加载，所以，所以此时也会触发 B 的加载生成 B 的类对象，这样符号引用就可以转成直接引用了，这里是以类的解析举例，但实际上，常量，方法，字段等符号引用也都会被解析\n但需要注意的是这一阶段有可能发生在初始化之后，因为只有真正用到了比如需要调用某个类的方法时才需要去解析，如果在初始化时此方法还没有被用到，那解析自然也完全没有必要了\n初始化 # 这一阶段主要做两件事\n初始化静态变量，为其赋值 执行静态代码块内容 无论是初始化静态变量还是执行静态代码块，java 编译器编译后， 它们都会被一起置于一个被称为 clinit 的方法中，并且 JVM 会对其加锁以保证此方法只会被执行一次，只有在初始化完成之后，类才真正成为可执行状态，另外需要注意的，在子类的 clinit 完成之前，JVM 会确保父类的 clinit 也已经完成了，这从继承的角度也容易理解，子类毕竟继承着父类，只有父类初始化可用了，子类才能放心继承或者说使用父类的方法等。\n这里有一个需要注意的点是如果是 final 的静态变量，且其类型是基本类型或字符串时，该字段会被标记为常量值，其初始化由 JVM 完成，而不会被放入 clinit，比如如下类静态变量\npublic class Test { private static final int field = 1; } 这个 field 由于是常量值，所以并不会放入 clinit，而是由 JVM 来完成初始化\n那么什么时候会执行初始化呢，《Java 虚拟机规范》规定了六种情况必须立即对类进行初始化\n遇到 new、getstatic、putstatic 或 invokestatic 这四条字节码指令的时候,如果类没有进行初始化,则需要先触发其初始化. 生成这四条指令的最常见的java代码场景是：\n使用 new 关键字实例化对象的时候\n读取或设置一个类的静态字段(被final修饰、已在编译期把结果放入常量池的静态字段除外)的时候\npublic class SuperClass{ static { System.out.println(\u0026#34;SuperClass init\u0026#34;); } public static int value = 10; } public class SubClass extends SuperClass { static { System.out.println(\u0026#34;SubClass init\u0026#34;); } } public class NotInitialization { public static void main(String[] args) { System.out.println(SubClass.value); } } // output SuperClass init 10 调用一个类的静态方法的时候\n使用 java.lang.reflect 包的方法对类进行反射调用的时候,如果类没有进行初始化,则需要先触发其初始化\n当初始化一个类的时候,如果发现其父类还没有进行过初始化,则需要先触发其父类的初始化\n当虚拟机启动的时候,用户需要指定一个要执行的主类(包含 main 方法的那个类),虚拟机会先初始化这个主类\n当使用 jdk7 新加入的动态语言支持的时候,如果一个 java,lang.invoke.MethodHandler 实例的最后解析结果是REF_getStatic,REF_putStatic,REF_invokeStatic,REF_newInvokeSpecial 四种类的方法句柄，并且这个方法句柄对应的类没有进行过初始化,那么需要先触发其初始化.\n(新)当一个接口中定义了 JDK8 新加入的默认方法(被default关键字修饰的接口方法)时，如果这个接口的实现类发生了初始化,那么该接口要在其之前初始化\n当初次调用 MethodHandle 实例时，初始化该 MethodHandle 指向的方法所在的类。\n这六种场景的行为称为对一个类型的主动引用。除此之外，所有的引用类型的方式都不会触发其初始化，称为被动引用。\n","date":"7 November 2023","permalink":"/posts/language/java/class-loader/","section":"博客","summary":"字节码文件需要经过加载，链接（包括验证、准备、解析），初始化才能转为类，然后才能根据类来创建对象","title":"Java类加载机制"},{"content":"","date":"5 November 2023","permalink":"/tags/junit/","section":"Tags","summary":"","title":"Junit"},{"content":"Junit 是由 Kent Beck 和 Erich Gamma 于 1995 年底着手编写的框架，自此以后，Junit 框架日益普及，现在已经成为单元测试 Java 应用程序的事实上的标准。\n在软件开发领域中，从来没有这样的事情：少数几行代码对大量代码起着如此重要的作用 \u0026mdash; Martin Fowler\n从一个简单的例子开始认识 Junit # 本文注重点在于研究 Junit 运行的基本原理和执行单元测试的流程，所以对于一些额外的信息和数据不单独准备，本文所使用的测试 case 如下：\njunit4\nimport org.junit.*; public class JunitSamplesTest { @Before public void before(){ System.out.println(\u0026#34;.....this is before test......\u0026#34;); } @After public void after(){ System.out.println(\u0026#34;.....this is after test......\u0026#34;); } @BeforeClass public static void beforeClass(){ System.out.println(\u0026#34;.....this is before class test......\u0026#34;); } @AfterClass public static void afterClass(){ System.out.println(\u0026#34;.....this is after class test......\u0026#34;); } @Test public void testOne(){ System.out.println(\u0026#34;this is test one\u0026#34;); } @Test public void testTwo(){ System.out.println(\u0026#34;this is test two\u0026#34;); } } 执行结果如下：\n.....this is before class test...... Disconnected from the target VM, address: \u0026#39;127.0.0.1:65400\u0026#39;, transport: \u0026#39;socket\u0026#39; .....this is before test...... this is test one .....this is after test...... .....this is before test...... this is test two .....this is after test...... .....this is after class test...... 从代码和执行结果来看，BeforeClass 和 AfterClass 注解分别在测试类开始之前和之后执行，Before 和 After 注解在测试类中每个测试方法的前后执行。\n问题 # 从开发者的角度来看，对于任何一个技术产品组件，如果想要更好的使用它，就意味着必须了解它。通过上面提供的 case 可以看到，Junit 使用非常简单，基本 0 门槛上手，通过给测试的方法加一个 @Test 注解，然后将待测试逻辑放在 被 @Test 标注的方法内，然后 run 就好了。简单源于组件开发者的顶层抽象和封装，将技术细节屏蔽，然后以最简洁的 API 或者注解面向用户，这也是 Junit 能够让广大开发者容易接受的根本原因，值得我们借鉴学习。\n回归正题，基于上面分析，Junit 使用简单在于其提供了非常简洁的 API 和注解，那对于我们来说，这些就是作为分析 Junit 的基本着手点；通过这些，来拨开 Junit 的基本原理。基于第一节的小案例，这里抛出这样几个问题：\nJunit 是怎么触发执行的 为什么被标注 @Test 注解的方法会被执行，而没有标注的不会 Before 和 After 执行时机 BeforeClass 和 AfterClass 执行时机 Junit 是怎么将执行结果收集并返回的（这里不关注 IDE 提供的渲染） Junit 是如何执行的？ # 这里把断点直接打在目标测试方法位置，然后 debug 执行\n通过堆栈来找到用例执行的整个路径。因为本 case 是通过 vscode 启动执行，所以可以看到的入口实际是被 vscode 包装过的。但是这里也抓到了 JUnitCore 这样的一个入口。\nJUnitCore 是运行测试用例的门面入口，通过源码注释可以看到，JUnitCore 从 junit 4 才有，但是其向下兼容了 3.8.x 版本系列。我们在跑测试用例时，其实大多数情况下在本地都是通过 IDE 来触发用例运行，或者通过 mvn test 来运行用例，实际上，不管是 IDE 还是 mvn 都是对 JUnitCore 的封装。我们完全可以通过 main 方法的方式来运行，比如运行下面代码的 main 方法来通过一个 JUnitCore 实例，然后指定被测试类来触发用例执行，为了尽量使得堆栈更贴近 Junit 自己的代码，我们通过这种方式启动来减少堆栈对于代码执行路径的干扰。\n这里得到了最简化的测试执行入口：\n如果使用 java 命令来引导启动，其实就是从 JunitCore 内部自己的 main 方法开始执行的\n/** * Run the tests contained in the classes named in the \u0026lt;code\u0026gt;args\u0026lt;/code\u0026gt;. * If all tests run successfully, exit with a status of 0. Otherwise exit with a status of 1. * Write feedback while tests are running and write * stack traces for all failed tests after the tests all complete. * * @param args names of classes in which to find tests to run */ public static void main(String... args) { Result result = new JUnitCore().runMain(new RealSystem(), args); System.exit(result.wasSuccessful() ? 0 : 1); } 为什么被标注 @Test 注解的方法会被执行，而没有标注的不会 # 这里比较好理解，被打了 @Test 注解的方法，一定是 Junit 通过某种方式将其扫描到了，然后作为待执行的一个集合或者队列中。下面通过分析代码来论证下。\norg.junit.runners.BlockJUnit4ClassRunner#getChildren\n@Override protected List\u0026lt;FrameworkMethod\u0026gt; getChildren() { return computeTestMethods(); } 通过方法 computeTestMethods 方法名其实就可以看出其目的，就是计算出所有的测试方法。\ngetAnnotatedMethods 通过指定的 annotationClass 类型，将当前 TestClass 中类型为 annotationClass 类型注解标注的方法过滤出来，\ngetFilteredChildren 中最后将获取得到的测试方法放在 filteredChildren 中缓存起来。这里简单汇总下 @Test 注解被识别的整个过程（其他注解如 @Before 都是一样的）\nJunit 在初始化构建 Runner 的过程，内部会基于给定的 测试类创建一个 TestClass 对象模型，用于描述当前测试类在 Junit 中的表示。 // clazz 是待测试类 public TestClass(Class\u0026lt;?\u0026gt; clazz) { this.clazz = clazz; if (clazz != null \u0026amp;\u0026amp; clazz.getConstructors().length \u0026gt; 1) { // 测试类不能有有参构造函数 throw new IllegalArgumentException( \u0026#34;Test class can only have one constructor\u0026#34;); } Map\u0026lt;Class\u0026lt;? extends Annotation\u0026gt;, List\u0026lt;FrameworkMethod\u0026gt;\u0026gt; methodsForAnnotations = new LinkedHashMap\u0026lt;Class\u0026lt;? extends Annotation\u0026gt;, List\u0026lt;FrameworkMethod\u0026gt;\u0026gt;(); Map\u0026lt;Class\u0026lt;? extends Annotation\u0026gt;, List\u0026lt;FrameworkField\u0026gt;\u0026gt; fieldsForAnnotations = new LinkedHashMap\u0026lt;Class\u0026lt;? extends Annotation\u0026gt;, List\u0026lt;FrameworkField\u0026gt;\u0026gt;(); // 扫描待测试类中所有的 Junit 注解，包括 @Test @Before @After 等等 scanAnnotatedMembers(methodsForAnnotations, fieldsForAnnotations); // 过滤出打在方法上的注解， this.methodsForAnnotations = makeDeeplyUnmodifiable(methodsForAnnotations); // 过滤出打在变量上的注解 this.fieldsForAnnotations = makeDeeplyUnmodifiable(fieldsForAnnotations); } methodsForAnnotations 和 fieldsForAnnotations 缓存了当前待测试类所有被 junit 注解标注过的方法和变量\ngetFilteredChildren 中，从 methodsForAnnotations 中筛选出所有 @Test 注解标注的方法。（getDescription()-\u0026gt; getFilteredChildren -\u0026gt; computeTestMethods -\u0026gt; 从 methodsForAnnotations 按类型过滤） 返回所有 @Test 注解标注的方法 Before 和 After 执行时机 # 要搞定这个问题，其实有必要了解下 Junit 中一个比较重要的概念 Statement。\npublic abstract class Statement { /** * Run the action, throwing a {@code Throwable} if anything goes wrong. */ public abstract void evaluate() throws Throwable; } Statement 从 junit 4.5 版本被提出，Statement 表示在运行 JUnit 测试组件的过程中要在运行时执行的一个或多个操作，简单说就是，对于被 @Before @After 注解标注的方法，在 JUnit 会被作为一种 Statement 存在，分别对应于 RunBefores 和 RunnerAfter，这些 statement 中持有了当前运行所有的 FrameworkMethod。\nFrameworkMethod 是 JUnit 中所有被 junit 注解标注方式的内部描述，@Test, @Before, @After, @BeforeClass, @AfterClass 标注的方法最终都作为 FrameworkMethod 实例存在。\nStatement 的创建有两种方式，基于 FrameworkMethod 的 methodBlock 和基于 RunNotifier 的 classBlock，这里介绍 methodBlock ，classBlock 下节讨论。\nprotected Statement methodBlock(final FrameworkMethod method) { Object test; try { test = new ReflectiveCallable() { @Override protected Object runReflectiveCall() throws Throwable { return createTest(method); } }.run(); } catch (Throwable e) { return new Fail(e); } Statement statement = methodInvoker(method, test); statement = possiblyExpectingExceptions(method, test, statement); statement = withPotentialTimeout(method, test, statement); statement = withBefores(method, test, statement); statement = withAfters(method, test, statement); statement = withRules(method, test, statement); statement = withInterruptIsolation(statement); return statement; } withAfters、withBefores 会将 RunAfters 和 RunBefore 绑定到 statement，最后 形成一个 statement 链，这个链的执行入口时 RunAfters#evaluate。\n@Override public void evaluate() throws Throwable { List\u0026lt;Throwable\u0026gt; errors = new ArrayList\u0026lt;Throwable\u0026gt;(); try { next.evaluate(); } catch (Throwable e) { errors.add(e); } finally { // 在 finally 中执行 after 方法 for (FrameworkMethod each : afters) { try { invokeMethod(each); } catch (Throwable e) { errors.add(e); } } } MultipleFailureException.assertEmpty(errors); } next 链中包括 before 和待执行的测试方法\n所以我们看到的就是 before -\u0026gt; testMethod -\u0026gt; after。\n这里其实和预想的不太一样，关于 before 和 after 这种逻辑，第一想法是通过代理的方式，对测试方法进行代理拦截，类似 Spring AOP 中的 Before 和 After，其实不然。\nBeforeClass 和 AfterClass 执行时机 # 前面分析了 methodBlock，了解到 junit 中通过这个方法创建 statement 并且将 before 和 after 的方法绑定给 statement，以此推断，classBlock 的作用就是将 BeforeClass 和 AfterClass 绑定给statement 。\nprotected Statement classBlock(final RunNotifier notifier) { // childrenInvoker 这里会调用到 methodBlock Statement statement = childrenInvoker(notifier); if (!areAllChildrenIgnored()) { statement = withBeforeClasses(statement); statement = withAfterClasses(statement); statement = withClassRules(statement); statement = withInterruptIsolation(statement); } return statement; } BeforeClass 和 before 都会对应创建一个 RunnerBefores，区别在于 BeforeClass 在创建 RunnerBefores 时，不会指定目标测试方法。\nBeforeClass 在执行 statement 之前，运行该类和超类上所有非覆盖的@BeforeClass方法;如果有抛出异常，停止执行并传递异常。 AfterClass 在执行 statement链最后，在该类和超类上运行所有未覆盖的 @AfterClass 方法；始终执行所有 AfterClass 方法：如有必要，将前面步骤抛出的异常与来自 AfterClass 方法的异常合并到 org.junit.runners.model.MultipleFailureException 中。 Junit 是怎么将执行结果收集并返回的 # junit 所有执行的结果都存放在 Result 中\n// 所有 case 数 private final AtomicInteger count; // 忽略执行的 case 数（被打了 ignore） private final AtomicInteger ignoreCount; // 失败 case 数 private final AtomicInteger assumptionFailureCount; // 所有失败 case 的结果 private final CopyOnWriteArrayList\u0026lt;Failure\u0026gt; failures; // 执行时间 private final AtomicLong runTime; // 开始时间 private final AtomicLong startTime; Result 中内置了一个默认的来监听器，这个监听器会在每个 case 执行完成之后进行相应的回调，Listener 如下：\n@RunListener.ThreadSafe private class Listener extends RunListener { // 设置开始时间 @Override public void testRunStarted(Description description) throws Exception { startTime.set(System.currentTimeMillis()); } // 执行完所有 case @Override public void testRunFinished(Result result) throws Exception { long endTime = System.currentTimeMillis(); runTime.addAndGet(endTime - startTime.get()); } // 执行完某个 case @Override public void testFinished(Description description) throws Exception { count.getAndIncrement(); } // 执行完某个 case 失败 @Override public void testFailure(Failure failure) throws Exception { failures.add(failure); } // 执行完某个ignore case @Override public void testIgnored(Description description) throws Exception { ignoreCount.getAndIncrement(); } @Override public void testAssumptionFailure(Failure failure) { // Assumption 产生的失败 assumptionFailureCount.getAndIncrement(); } } JUnit 4 开始在测试中支持假设 Assumptions，在 Assumptions 中，封装了一组使用的方法，以支持基于假设的条件测试执行。假设实际就是指定某个特定条件，假如不能满足假设条件，假设不会导致测试失败，只是终止当前测试。这也是假设与断言的最大区别，因为对于断言而言，会导致测试失败。\n所以 JUnit 通过监听器机制收集所有的测试信息，最终封装到 Result 中返回。\n总结 # Junit 中有一些比较基本的概念，比如 Runner，statement 等；在初始化时，默认情况下 junit 会构建出 BlockJUnit4ClassRunner 这样的一个 Runner，并且在这个 Runner 中会持有被测试类的所有信息。Runner 运行测试并在执行此操作时将重要事件通知 RunNotifier。\n","date":"5 November 2023","permalink":"/posts/language/java/junit/","section":"博客","summary":"Junit 是由 Kent Beck 和 Erich Gamma 于 1995 年底着手编写的框架，自此以后，Junit 框架日益普及，现在已经成为单元测试 Java 应用程序的事实上的标准。","title":"Junit 运行流程"},{"content":"","date":"1 November 2023","permalink":"/tags/netty/","section":"Tags","summary":"","title":"Netty"},{"content":"Netty 基础 # Netty 是什么 # Netty 是 JBoss 开源项目，是异步的、基于事件驱动的网络应用框架，它以高性能、高并发著称。所谓基于事件驱动，说得简单点就是 Netty 会根据客户端事件（连接、读、写等）做出响应，关于这点，随着文章的论述的展开，读者自然会明白。 Netty 主要用于开发基于 TCP 协议的网络 IO 程序（TCP/IP 是网络通信的基石，当然也是 Netty 的基石，Netty 并没有去改变这些底层的网络基础设施，而是在这之上提供更高层的网络基础设施），例如高性能服务器段/客户端、P2P 程序等。 Netty 是基于 Java NIO 构建出来的，Java NIO 又是基于 Linux 提供的高性能 IO 接口/系统调用构建出来的。关于 Netty 在网络中的地位，下图可以很好地表达出来： Netty 的应用场景 # 在互联网领域，Netty 作为异步高并发的网络组件，常常用于构建高性能 RPC 框架，以提升分布式服务群之间调用或者数据传输的并发度和速度。例如 Dubbo 的网络层就可以（但并非一定）使用 Netty。 一些大数据基础设施，比如 Hadoop，在处理海量数据的时候，数据在多个计算节点之中传输，为了提高传输性能，也采用 Netty 构建性能更高的网络 IO 层。 在游戏行业，Netty 被用于构建高性能的游戏交互服务器，Netty 提供了 TCP/UDP、HTTP 协议栈，方便开发者基于 Netty 进行私有协议的开发。 …… Netty 作为成熟的高性能异步通信框架，无论是应用在互联网分布式应用开发中，还是在大数据基础设施构建中，亦或是用于实现应用层基于公私协议的服务器等等，都有出色的表现，是一个极好的轮子。\nJava 中的网络 IO 模型 # Java 中的网络 IO 模型有三种：BIO、NIO、AIO。\nBIO：同步的、阻塞式 IO。在这种模型中，服务器上一个线程处理一次连接，即客户端每发起一个请求，服务端都要开启一个线程专门处理该请求。这种模型对线程量的耗费极大，且线程利用率低，难以承受请求的高并发。BIO 虽然可以使用线程池+等待队列进行优化，避免使用过多的线程，但是依然无法解决线程利用率低的问题。 使用 BIO 构建 C/S 系统的 Java 编程组件是 ServerSocket 和 Socket。服务端示例代码为：\npublic static void main(String[] args) throws IOException { ExecutorService threadPool = Executors.newCachedThreadPool(); ServerSocket serverSocket = new ServerSocket(8080); while (true) { Socket socket = serverSocket.accept(); threadPool.execute(() -\u0026gt; { handler(socket); }); } } /** * 处理客户端请求 */ private static void handler(Socket socket) throws IOException { byte[] bytes = new byte[1024]; InputStream inputStream = socket.getInputStream(); socket.close(); while (true) { int read = inputStream.read(bytes); if (read != -1) { System.out.println(\u0026#34;msg from client: \u0026#34; + new String(bytes, 0, read)); } else { break; } } } NIO：同步的、非阻塞式 IO。在这种模型中，服务器上一个线程处理多个连接，即多个客户端请求都会被注册到多路复用器（后文要讲的 Selector）上，多路复用器会轮训这些连接，轮训到连接上有 IO 活动就进行处理。NIO 降低了线程的需求量，提高了线程的利用率。Netty 就是基于 NIO 的（这里有一个问题：前文大力宣扬 Netty 是一个异步高性能网络应用框架，为何这里又说 Netty 是基于同步的 NIO 的？请读者跟着文章的描述找寻答案）。 NIO 是面向缓冲区编程的，从缓冲区读取数据的时候游标在缓冲区中是可以前后移动的，这就增加了数据处理的灵活性。这和面向流的 BIO 只能顺序读取流中数据有很大的不同。\nJava NIO 的非阻塞模式，使得一个线程从某个通道读取数据的时候，若当前有可用数据，则该线程进行处理，若当前无可用数据，则该线程不会保持阻塞等待状态，而是可以去处理其他工作（比如处理其他通道的读写）；同样，一个线程向某个通道写入数据的时候，一旦开始写入，该线程无需等待写完即可去处理其他工作（比如处理其他通道的读写）。这种特性使得一个线程能够处理多个客户端请求，而不是像 BIO 那样，一个线程只能处理一个请求。\n使用 NIO 构建 C/S 系统的 Java 编程组件是 Channel、Buffer、Selector。服务端示例代码为：\npublic static void main(String[] args) throws IOException { ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); Selector selector = Selector.open(); // 绑定端口 serverSocketChannel.socket().bind(new InetSocketAddress(8080)); // 设置 serverSocketChannel 为非阻塞模式 serverSocketChannel.configureBlocking(false); // 注册 serverSocketChannel 到 selector，关注 OP_ACCEPT 事件 serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); while (true) { // 没有事件发生 if (selector.select(1000) == 0) { continue; } // 有事件发生，找到发生事件的 Channel 对应的 SelectionKey 的集合 Set\u0026lt;SelectionKey\u0026gt; selectionKeys = selector.selectedKeys(); Iterator\u0026lt;SelectionKey\u0026gt; iterator = selectionKeys.iterator(); while (iterator.hasNext()) { SelectionKey selectionKey = iterator.next(); // 发生 OP_ACCEPT 事件，处理连接请求 if (selectionKey.isAcceptable()) { SocketChannel socketChannel = serverSocketChannel.accept(); // 将 socketChannel 也注册到 selector，关注 OP_READ // 事件，并给 socketChannel 关联 Buffer socketChannel.register(selector, SelectionKey.OP_READ, ByteBuffer.allocate(1024)); } // 发生 OP_READ 事件，读客户端数据 if (selectionKey.isReadable()) { SocketChannel channel = (SocketChannel) selectionKey.channel(); ByteBuffer buffer = (ByteBuffer) selectionKey.attachment(); channel.read(buffer); System.out.println(\u0026#34;msg form client: \u0026#34; + new String(buffer.array())); } // 手动从集合中移除当前的 selectionKey，防止重复处理事件 iterator.remove(); } } } AIO：异步非阻塞式 IO。在这种模型中，由操作系统完成与客户端之间的 read/write，之后再由操作系统主动通知服务器线程去处理后面的工作，在这个过程中服务器线程不必同步等待 read/write 完成。由于不同的操作系统对 AIO 的支持程度不同，AIO 目前未得到广泛应用。因此本文对 AIO 不做过多描述。 使用 Java NIO 构建的 IO 程序，它的工作模式是：主动轮训 IO 事件，IO 事件发生后程序的线程主动处理 IO 工作，这种模式也叫做 Reactor 模式。使用 Java AIO 构建的 IO 程序，它的工作模式是：将 IO 事件的处理托管给操作系统，操作系统完成 IO 工作之后会通知程序的线程去处理后面的工作，这种模式也叫做 Proactor 模式。\n网路 IO 中阻塞、非阻塞、异步、同步这几个术语的含义和关系：\n阻塞：如果线程调用 read/write 过程，但 read/write 过程没有就绪或没有完成，则调用 read/write 过程的线程会一直等待，这个过程叫做阻塞式读写。 非阻塞：如果线程调用 read/write 过程，但 read/write 过程没有就绪或没有完成，调用 read/write 过程的线程并不会一直等待，而是去处理其他工作，等到 read/write 过程就绪或完成后再回来处理，这个过程叫做阻塞式读写。 异步：read/write 过程托管给操作系统来完成，完成后操作系统会通知（通过回调或者事件）应用网络 IO 程序（其中的线程）来进行后续的处理。 同步：read/write 过程由网络 IO 程序（其中的线程）来完成。 基于以上含义，可以看出：异步 IO 一定是非阻塞 IO；同步 IO 既可以是阻塞 IO、也可以是非阻塞 IO。\nJava NIO API 简单回顾 # BIO 以流的方式处理数据，而 NIO 以缓冲区（也被叫做块）的方式处理数据，块 IO 效率比流 IO 效率高很多。BIO 基于字符流或者字节流进行操作，而 NIO 基于 Channel 和 Buffer 进行操作，数据总是从通道读取到缓冲区或者从缓冲区写入到通道。Selector 用于监听多个通道上的事件（比如收到连接请求、数据达到等等），因此使用单个线程就可以监听多个客户端通道。如下图所示：\n关于上图，再进行几点说明：\n一个 Selector 对应一个处理线程 一个 Selector 上可以注册多个 Channel 每个 Channel 都会对应一个 Buffer（有时候一个 Channel 可以使用多个 Buffer，这时候程序要进行多个 Buffer 的分散和聚集操作），Buffer 的本质是一个内存块，底层是一个数组 Selector 会根据不同的事件在各个 Channel 上切换 Buffer 是双向的，既可以读也可以写，切换读写方向要调用 Buffer 的 flip()方法 同样，Channel 也是双向的，数据既可以流入也可以流出 缓冲区（Buffer） # 缓冲区（Buffer）本质上是一个可读可写的内存块，可以理解成一个容器对象，Channel 读写文件或者网络都要经由 Buffer。在 Java NIO 中，Buffer 是一个顶层抽象类，它的常用子类有（前缀表示该 Buffer 可以存储哪种类型的数据）：\nByteBuffer CharBuffer ShortBuffer IntBuffer LongBuffer DoubleBuffer FloatBuffer 涵盖了 Java 中除 boolean 之外的所有的基本数据类型。其中 ByteBuffer 支持类型化的数据存取，即可以往 ByteBuffer 中放 byte 类型数据、也可以放 char、int、long、double 等类型的数据，但读取的时候要做好类型匹配处理，否则会抛出 BufferUnderflowException。\n另外，Buffer 体系中还有一个重要的 MappedByteBuffer（ByteBuffer 的子类），可以让文件内容直接在堆外内存中被修改，而如何同步到文件由 NIO 来完成。本文重点不在于此，有兴趣的可以去探究一下 MappedByteBuffer 的底层原理。\n通道（Channel） # 通道（Channel）是双向的，可读可写。在 Java NIO 中，Buffer 是一个顶层接口，它的常用子类有：\nFileChannel：用于文件读写 DatagramChannel：用于 UDP 数据包收发 ServerSocketChannel：用于服务端 TCP 数据包收发 SocketChannel：用于客户端 TCP 数据包收发 选择器（Selector） # 选择器（Selector）是实现 IO 多路复用的关键，多个 Channel 注册到某个 Selector 上，当 Channel 上有事件发生时，Selector 就会取得事件然后调用线程去处理事件。也就是说只有当连接上真正有读写等事件发生时，线程才会去进行读写等操作，这就不必为每个连接都创建一个线程，一个线程可以应对多个连接。这就是 IO 多路复用的要义。\nNetty 的 IO 线程 NioEventLoop 聚合了 Selector，可以同时并发处理成百上千的客户端连接，后文会展开描述。\n在 Java NIO 中，Selector 是一个抽象类，它的常用方法有：\npublic abstract class Selector implements Closeable { ...... /** * 得到一个选择器对象 */ public static Selector open() throws IOException { return SelectorProvider.provider().openSelector(); } ...... /** * 返回所有发生事件的 Channel 对应的 SelectionKey 的集合，通过 * SelectionKey 可以找到对应的 Channel */ public abstract Set\u0026lt;SelectionKey\u0026gt; selectedKeys(); ...... /** * 返回所有 Channel 对应的 SelectionKey 的集合，通过 SelectionKey * 可以找到对应的 Channel */ public abstract Set\u0026lt;SelectionKey\u0026gt; keys(); ...... /** * 监控所有注册的 Channel，当其中的 Channel 有 IO 操作可以进行时， * 将这些 Channel 对应的 SelectionKey 找到。参数用于设置超时时间 */ public abstract int select(long timeout) throws IOException; /** * 无超时时间的 select 过程，一直等待，直到发现有 Channel 可以进行 * IO 操作 */ public abstract int select() throws IOException; /** * 立即返回的 select 过程 */ public abstract int selectNow() throws IOException; ...... /** * 唤醒 Selector，对无超时时间的 select 过程起作用，终止其等待 */ public abstract Selector wakeup(); ...... } 在上文的使用 Java NIO 编写的服务端示例代码中，服务端的工作流程为：\n当客户端发起连接时，会通过 ServerSocketChannel 创建对应的 SocketChannel。 调用 SocketChannel 的注册方法将 SocketChannel 注册到 Selector 上，注册方法返回一个 SelectionKey，该 SelectionKey 会被放入 Selector 内部的 SelectionKey 集合中。该 SelectionKey 和 Selector 关联（即通过 SelectionKey 可以找到对应的 Selector），也和 SocketChannel 关联（即通过 SelectionKey 可以找到对应的 SocketChannel）。 Selector 会调用 select()/select(timeout)/selectNow()方法对内部的 SelectionKey 集合关联的 SocketChannel 集合进行监听，找到有事件发生的 SocketChannel 对应的 SelectionKey。 通过 SelectionKey 找到有事件发生的 SocketChannel，完成数据处理。 /** * SocketChannel 继承 AbstractSelectableChannel */ public abstract class SocketChannel extends AbstractSelectableChannel implements ByteChannel, ScatteringByteChannel, GatheringByteChannel, NetworkChannel { ...... } public abstract class AbstractSelectableChannel extends SelectableChannel { ...... /** * AbstractSelectableChannel 中包含注册方法，SocketChannel 实例 * 借助该注册方法注册到 Selector 实例上去，该方法返回 SelectionKey */ public final SelectionKey register( // 指明注册到哪个 Selector 实例 Selector sel, // ops 是事件代码，告诉 Selector 应该关注该通道的什么事件 int ops, // 附加信息 attachment Object att) throws ClosedChannelException { ...... } ...... } public abstract class SelectionKey { ...... /** * 获取该 SelectionKey 对应的 Channel */ public abstract SelectableChannel channel(); /** * 获取该 SelectionKey 对应的 Selector */ public abstract Selector selector(); ...... /** * 事件代码，上面的 ops 参数取这里的值 */ public static final int OP_READ = 1 \u0026lt;\u0026lt; 0; public static final int OP_WRITE = 1 \u0026lt;\u0026lt; 2; public static final int OP_CONNECT = 1 \u0026lt;\u0026lt; 3; public static final int OP_ACCEPT = 1 \u0026lt;\u0026lt; 4; ...... /** * 检查该 SelectionKey 对应的 Channel 是否可读 */ public final boolean isReadable() { return (readyOps() \u0026amp; OP_READ) != 0; } /** * 检查该 SelectionKey 对应的 Channel 是否可写 */ public final boolean isWritable() { return (readyOps() \u0026amp; OP_WRITE) != 0; } /** * 检查该 SelectionKey 对应的 Channel 是否已经建立起 socket 连接 */ public final boolean isConnectable() { return (readyOps() \u0026amp; OP_CONNECT) != 0; } /** * 检查该 SelectionKey 对应的 Channel 是否准备好接受一个新的 socket 连接 */ public final boolean isAcceptable() { return (readyOps() \u0026amp; OP_ACCEPT) != 0; } /** * 添加附件（例如 Buffer） */ public final Object attach(Object ob) { return attachmentUpdater.getAndSet(this, ob); } /** * 获取附件 */ public final Object attachment() { return attachment; } ...... } 下图用于辅助读者理解上面的过程和源码：\n首先说明，本文以 Linux 系统为对象来研究文件 IO 模型和网络 IO 模型。\n零拷贝技术 # 注：本节讨论的是 Linux 系统下的 IO 过程。并且对于零拷贝技术的讲解采用了一种浅显易懂但能触及其本质的方式，因为这个话题，展开来讲实在是有太多的细节要关注。\n在\u0026quot;将本地磁盘中文件发送到网络中\u0026quot;这一场景中，零拷贝技术是提升 IO 效率的一个利器，为了对比出零拷贝技术的优越性，下面依次给出使用直接 IO 技术、内存映射文件技术、零拷贝技术实现将本地磁盘文件发送到网络中的过程。\n直接 IO 技术 使用直接 IO 技术实现文件传输的过程如下图所示。\n上图中，内核缓冲区是 Linux 系统的 Page Cahe。为了加快磁盘的 IO，Linux 系统会把磁盘上的数据以 Page 为单位缓存在操作系统的内存里，这里的 Page 是 Linux 系统定义的一个逻辑概念，一个 Page 一般为 4K。\n可以看出，整个过程有四次数据拷贝，读进来两次，写回去又两次：磁盘\u0026ndash;\u0026gt;内核缓冲区\u0026ndash;\u0026gt;Socket 缓冲区\u0026ndash;\u0026gt;网络。\n直接 IO 过程使用的 Linux 系统 API 为：\nssize_t read(int filedes, void *buf, size_t nbytes); ssize_t write(int filedes, void *buf, size_t nbytes); 等函数。\n内存映射文件技术 使用内存映射文件技术实现文件传输的过程如下图所示。\n可以看出，整个过程有三次数据拷贝，不再经过应用程序内存，直接在内核空间中从内核缓冲区拷贝到 Socket 缓冲区。\n内存映射文件过程使用的 Linux 系统 API 为：\nvoid *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset); 零拷贝技术 使用零拷贝技术，连内核缓冲区到 Socket 缓冲区的拷贝也省略了，如下图所示：\n内核缓冲区到 Socket 缓冲区之间并没有做数据的拷贝，只是一个地址的映射。底层的网卡驱动程序要读取数据并发送到网络上的时候，看似读取的是 Socket 的缓冲区中的数据，其实直接读的是内核缓冲区中的数据。\n零拷贝中所谓的零指的是内存中数据拷贝的次数为 0。\n零拷贝过程使用的 Linux 系统 API 为：\nssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count); 在 JDK 中，提供的：\nFileChannel.transderTo(long position, long count, WritableByteChannel target); 方法实现了零拷贝过程，其中的第三个参数可以传入 SocketChannel 实例。例如客户端使用以上的零拷贝接口向服务器传输文件的代码为：\npublic static void main(String[] args) throws IOException { SocketChannel socketChannel = SocketChannel.open(); socketChannel.connect(new InetSocketAddress(\u0026#34;127.0.0.1\u0026#34;, 8080)); String fileName = \u0026#34;test.zip\u0026#34;; // 得到一个文件 channel FileChannel fileChannel = new FileInputStream(fileName).getChannel(); // 使用零拷贝 IO 技术发送 long transferSize = fileChannel.transferTo(0, fileChannel.size(), socketChannel); System.out.println(\u0026#34;file transfer done, size: \u0026#34; + transferSize); fileChannel.close(); } Netty 的架构与原理 # 为什么要制造 Netty # 既然 Java 提供了 NIO，为什么还要制造一个 Netty，主要原因是 Java NIO 有以下几个缺点：\nJava NIO 的类库和 API 庞大繁杂，使用起来很麻烦，开发工作量大。 使用 Java NIO，程序员需要具备高超的 Java 多线程编码技能，以及非常熟悉网络编程，比如要处理断连重连、网络闪断、半包读写、失败缓存、网络拥塞和异常流处理等一系列棘手的工作。 Java NIO 存在 Bug，例如 Epoll Bug 会导致 Selector 空轮训，极大耗费 CPU 资源。 Netty 对于 JDK 自带的 NIO 的 API 进行了封装，解决了上述问题，提高了 IO 程序的开发效率和可靠性，同时 Netty：\n设计优雅，提供阻塞和非阻塞的 Socket；提供灵活可拓展的事件模型；提供高度可定制的线程模型。 具备更高的性能和更大的吞吐量，使用零拷贝技术最小化不必要的内存复制，减少资源的消耗。 提供安全传输特性。 支持多种主流协议。预置多种编解码功能，支持用户开发私有协议。 所谓支持 TCP、UDP、HTTP、WebSocket 等协议，就是说 Netty 提供了相关的编程类和接口，因此本文后面主要对基于 Netty 的 TCP Server/Client 开发案例进行讲解，以展示 Netty 的核心原理。\n我们从其中的几个关键词就能看出 Netty 的强大之处：\n零拷贝、可拓展事件模型； 支持 TCP、UDP、HTTP、WebSocket 等协议； 提供安全传输、压缩、大文件传输、编解码支持等等。 几种 Reactor 线程模式 # 传统的 BIO 服务端编程采用\u0026quot;每线程每连接\u0026quot;的处理模型，弊端很明显，就是面对大量的客户端并发连接时，服务端的资源压力很大；并且线程的利用率很低，如果当前线程没有数据可读，它会阻塞在 read 操作上。这个模型的基本形态如下图所示（图片来源于网络）。\nBIO 服务端编程采用的是 Reactor 模式（也叫做 Dispatcher 模式，分派模式），Reactor 模式有两个要义：\n基于 IO 多路复用技术，多个连接共用一个多路复用器，应用程序的线程无需阻塞等待所有连接，只需阻塞等待多路复用器即可。当某个连接上有新数据可以处理时，应用程序的线程从阻塞状态返回，开始处理这个连接上的业务。 基于线程池技术复用线程资源，不必为每个连接创建专用的线程，应用程序将连接上的业务处理任务分配给线程池中的线程进行处理，一个线程可以处理多个连接的业务。 下图反应了 Reactor 模式的基本形态（图片来源于网络）：\nReactor 模式有两个核心组成部分：\nReactor（图中的 ServiceHandler）：Reactor 在一个单独的线程中运行，负责监听和分发事件，分发给适当的处理线程来对 IO 事件做出反应。 Handlers（图中的 EventHandler）：处理线程执行处理方法来响应 I/O 事件，处理线程执行的是非阻塞操作。 Reactor 模式就是实现网络 IO 程序高并发特性的关键。它又可以分为单 Reactor 单线程模式、单 Reactor 多线程模式、主从 Reactor 多线程模式。\n单 Reactor 单线程模式 # 单 Reactor 单线程模式的基本形态如下（图片来源于网络）：\n这种模式的基本工作流程为：\nReactor 通过 select 监听客户端请求事件，收到事件之后通过 dispatch 进行分发 如果事件是建立连接的请求事件，则由 Acceptor 通过 accept 处理连接请求，然后创建一个 Handler 对象处理连接建立后的后续业务处理。 如果事件不是建立连接的请求事件，则由 Reactor 对象分发给连接对应的 Handler 处理。 Handler 会完成 read\u0026ndash;\u0026gt;业务处理\u0026ndash;\u0026gt;send 的完整处理流程。 这种模式的优点是：模型简单，没有多线程、进程通信、竞争的问题，一个线程完成所有的事件响应和业务处理。当然缺点也很明显：\n存在性能问题，只有一个线程，无法完全发挥多核 CPU 的性能。Handler 在处理某个连接上的业务时，整个进程无法处理其他连接事件，很容易导致性能瓶颈。 存在可靠性问题，若线程意外终止，或者进入死循环，会导致整个系统通信模块不可用，不能接收和处理外部消息，造成节点故障。 单 Reactor 单线程模式使用场景为：客户端的数量有限，业务处理非常快速，比如 Redis 在业务处理的时间复杂度为 O(1)的情况。\n单 Reactor 多线程模式 # 单 Reactor 单线程模式的基本形态如下（图片来源于网络）：\n这种模式的基本工作流程为：\nReactor 对象通过 select 监听客户端请求事件，收到事件后通过 dispatch 进行分发。 如果事件是建立连接的请求事件，则由 Acceptor 通过 accept 处理连接请求，然后创建一个 Handler 对象处理连接建立后的后续业务处理。 如果事件不是建立连接的请求事件，则由 Reactor 对象分发给连接对应的 Handler 处理。Handler 只负责响应事件，不做具体的业务处理，Handler 通过 read 读取到请求数据后，会分发给后面的 Worker 线程池来处理业务请求。 Worker 线程池会分配独立线程来完成真正的业务处理，并将处理结果返回给 Handler。Handler 通过 send 向客户端发送响应数据。 这种模式的优点是可以充分的利用多核 cpu 的处理能力，缺点是多线程数据共享和控制比较复杂，Reactor 处理所有的事件的监听和响应，在单线程中运行，面对高并发场景还是容易出现性能瓶颈。\n主从 Reactor 多线程模式 # 单 Reactor 单线程模式的基本形态如下（图片来源于网络）：\n主从 Reactor 多线程模式的基本形态如下（第一章图片来源于网络，第二章图片是 JUC 作者 Doug Lea 老师在《Scalable IO in Java》中给出的示意图，两张图表达的含义一样）：\n针对单 Reactor 多线程模型中，Reactor 在单个线程中运行，面对高并发的场景易成为性能瓶颈的缺陷，主从 Reactor 多线程模式让 Reactor 在多个线程中运行（分成 MainReactor 线程与 SubReactor 线程）。这种模式的基本工作流程为：\nReactor 主线程 MainReactor 对象通过 select 监听客户端连接事件，收到事件后，通过 Acceptor 处理客户端连接事件。 当 Acceptor 处理完客户端连接事件之后（与客户端建立好 Socket 连接），MainReactor 将连接分配给 SubReactor。（即：MainReactor 只负责监听客户端连接请求，和客户端建立连接之后将连接交由 SubReactor 监听后面的 IO 事件。） SubReactor 将连接加入到自己的连接队列进行监听，并创建 Handler 对各种事件进行处理。 当连接上有新事件发生的时候，SubReactor 就会调用对应的 Handler 处理。 Handler 通过 read 从连接上读取请求数据，将请求数据分发给 Worker 线程池进行业务处理。 Worker 线程池会分配独立线程来完成真正的业务处理，并将处理结果返回给 Handler。Handler 通过 send 向客户端发送响应数据。 一个 MainReactor 可以对应多个 SubReactor，即一个 MainReactor 线程可以对应多个 SubReactor 线程。 这种模式的优点是：\nMainReactor 线程与 SubReactor 线程的数据交互简单职责明确，MainReactor 线程只需要接收新连接，SubReactor 线程完成后续的业务处理。 MainReactor 线程与 SubReactor 线程的数据交互简单， MainReactor 线程只需要把新连接传给 SubReactor 线程，SubReactor 线程无需返回数据。 多个 SubReactor 线程能够应对更高的并发请求。 这种模式的缺点是编程复杂度较高。但是由于其优点明显，在许多项目中被广泛使用，包括 Nginx、Memcached、Netty 等。 这种模式也被叫做服务器的 1+M+N 线程模式，即使用该模式开发的服务器包含一个（或多个，1 只是表示相对较少）连接建立线程+M 个 IO 线程+N 个业务处理线程。这是业界成熟的服务器程序设计模式。\nNetty 的模样 # Netty 的设计主要基于主从 Reactor 多线程模式，并做了一定的改进。本节将使用一种渐进式的描述方式展示 Netty 的模样，即先给出 Netty 的简单版本，然后逐渐丰富其细节，直至展示出 Netty 的全貌。\n简单版本的 Netty 的模样如下：\n关于这张图，作以下几点说明：\nBossGroup 线程维护 Selector，ServerSocketChannel 注册到这个 Selector 上，只关注连接建立请求事件（相当于主 Reactor）。 当接收到来自客户端的连接建立请求事件的时候，通过 ServerSocketChannel.accept 方法获得对应的 SocketChannel，并封装成 NioSocketChannel 注册到 WorkerGroup 线程中的 Selector，每个 Selector 运行在一个线程中（相当于从 Reactor）。 当 WorkerGroup 线程中的 Selector 监听到自己感兴趣的 IO 事件后，就调用 Handler 进行处理。 我们给这简单版的 Netty 添加一些细节：\n关于这张图，作以下几点说明：\n有两组线程池：BossGroup 和 WorkerGroup，BossGroup 中的线程（可以有多个，图中只画了一个）专门负责和客户端建立连接，WorkerGroup 中的线程专门负责处理连接上的读写。 BossGroup 和 WorkerGroup 含有多个不断循环的执行事件处理的线程，每个线程都包含一个 Selector，用于监听注册在其上的 Channel。 每个 BossGroup 中的线程循环执行以下三个步骤： 轮训注册在其上的 ServerSocketChannel 的 accept 事件（OP_ACCEPT 事件） 处理 accept 事件，与客户端建立连接，生成一个 NioSocketChannel，并将其注册到 WorkerGroup 中某个线程上的 Selector 上 再去以此循环处理任务队列中的下一个事件 每个 WorkerGroup 中的线程循环执行以下三个步骤： 轮训注册在其上的 NioSocketChannel 的 read/write 事件（OP_READ/OP_WRITE 事件） 在对应的 NioSocketChannel 上处理 read/write 事件 再去以此循环处理任务队列中的下一个事件 我们再来看下终极版的 Netty 的模样，如下图所示（图片来源于网络）：\n关于这张图，作以下几点说明：\nNetty 抽象出两组线程池：BossGroup 和 WorkerGroup，也可以叫做 BossNioEventLoopGroup 和 WorkerNioEventLoopGroup。每个线程池中都有 NioEventLoop 线程。BossGroup 中的线程专门负责和客户端建立连接，WorkerGroup 中的线程专门负责处理连接上的读写。BossGroup 和 WorkerGroup 的类型都是 NioEventLoopGroup。 NioEventLoopGroup 相当于一个事件循环组，这个组中含有多个事件循环，每个事件循环就是一个 NioEventLoop。 NioEventLoop 表示一个不断循环的执行事件处理的线程，每个 NioEventLoop 都包含一个 Selector，用于监听注册在其上的 Socket 网络连接（Channel）。 NioEventLoopGroup 可以含有多个线程，即可以含有多个 NioEventLoop。 每个 BossNioEventLoop 中循环执行以下三个步骤： select：轮训注册在其上的 ServerSocketChannel 的 accept 事件（OP_ACCEPT 事件） processSelectedKeys：处理 accept 事件，与客户端建立连接，生成一个 NioSocketChannel，并将其注册到某个 WorkerNioEventLoop 上的 Selector 上 runAllTasks：再去以此循环处理任务队列中的其他任务 每个 WorkerNioEventLoop 中循环执行以下三个步骤： select：轮训注册在其上的 NioSocketChannel 的 read/write 事件（OP_READ/OP_WRITE 事件） processSelectedKeys：在对应的 NioSocketChannel 上处理 read/write 事件 runAllTasks：再去以此循环处理任务队列中的其他任务 在以上两个processSelectedKeys步骤中，会使用 Pipeline（管道），Pipeline 中引用了 Channel，即通过 Pipeline 可以获取到对应的 Channel，Pipeline 中维护了很多的处理器（拦截处理器、过滤处理器、自定义处理器等）。这里暂时不详细展开讲解 Pipeline。 基于 Netty 的 TCP Server/Client 案例 # 下面我们写点代码来加深理解 Netty 的模样。下面两段代码分别是基于 Netty 的 TCP Server 和 TCP Client。\n服务端代码为：\n/** * 需要的依赖： * \u0026lt;dependency\u0026gt; * \u0026lt;groupId\u0026gt;io.netty\u0026lt;/groupId\u0026gt; * \u0026lt;artifactId\u0026gt;netty-all\u0026lt;/artifactId\u0026gt; * \u0026lt;version\u0026gt;4.1.52.Final\u0026lt;/version\u0026gt; * \u0026lt;/dependency\u0026gt; */ public static void main(String[] args) throws InterruptedException { // 创建 BossGroup 和 WorkerGroup // 1. bossGroup 只处理连接请求 // 2. 业务处理由 workerGroup 来完成 EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try { // 创建服务器端的启动对象 ServerBootstrap bootstrap = new ServerBootstrap(); // 配置参数 bootstrap // 设置线程组 .group(bossGroup, workerGroup) // 说明服务器端通道的实现类（便于 Netty 做反射处理） .channel(NioServerSocketChannel.class) // 设置等待连接的队列的容量（当客户端连接请求速率大于 // NioServerSocketChannel 接收速率的时候，会使用该队列做缓冲） // option()方法用于给服务端的 ServerSocketChannel添加配置 .option(ChannelOption.SO_BACKLOG, 128) // 设置连接保活 // childOption()方法用于给服务端 ServerSocketChannel // 接收到的 SocketChannel 添加配置 .childOption(ChannelOption.SO_KEEPALIVE, true) // handler()方法用于给 BossGroup 设置业务处理器 // childHandler()方法用于给 WorkerGroup 设置业务处理器 .childHandler( // 创建一个通道初始化对象 new ChannelInitializer\u0026lt;SocketChannel\u0026gt;() { // 向 Pipeline 添加业务处理器 @Override protected void initChannel( SocketChannel socketChannel ) throws Exception { socketChannel.pipeline().addLast( new NettyServerHandler() ); // 可以继续调用 socketChannel.pipeline().addLast() // 添加更多 Handler } } ); System.out.println(\u0026#34;server is ready...\u0026#34;); // 绑定端口，启动服务器，生成一个 channelFuture 对象， // ChannelFuture 涉及到 Netty 的异步模型，后面展开讲 ChannelFuture channelFuture = bootstrap.bind(8080).sync(); // 对通道关闭进行监听 channelFuture.channel().closeFuture().sync(); } finally { bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); } } /** * 自定义一个 Handler，需要继承 Netty 规定好的某个 HandlerAdapter（规范） * InboundHandler 用于处理数据流入本端（服务端）的 IO 事件 * InboundHandler 用于处理数据流出本端（服务端）的 IO 事件 */ static class NettyServerHandler extends ChannelInboundHandlerAdapter { /** * 当通道有数据可读时执行 * * @param ctx 上下文对象，可以从中取得相关联的 Pipeline、Channel、客户端地址等 * @param msg 客户端发送的数据 * @throws Exception */ @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { // 接收客户端发来的数据 System.out.println(\u0026#34;client address: \u0026#34; + ctx.channel().remoteAddress()); // ByteBuf 是 Netty 提供的类，比 NIO 的 ByteBuffer 性能更高 ByteBuf byteBuf = (ByteBuf) msg; System.out.println(\u0026#34;data from client: \u0026#34; + byteBuf.toString(CharsetUtil.UTF_8)); } /** * 数据读取完毕后执行 * * @param ctx 上下文对象 * @throws Exception */ @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception { // 发送响应给客户端 ctx.writeAndFlush( // Unpooled 类是 Netty 提供的专门操作缓冲区的工具 // 类，copiedBuffer 方法返回的 ByteBuf 对象类似于 // NIO 中的 ByteBuffer，但性能更高 Unpooled.copiedBuffer( \u0026#34;hello client! i have got your data.\u0026#34;, CharsetUtil.UTF_8 ) ); } /** * 发生异常时执行 * * @param ctx 上下文对象 * @param cause 异常对象 * @throws Exception */ @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { // 关闭与客户端的 Socket 连接 ctx.channel().close(); } } 客户端端代码为：\n/** * 需要的依赖： * \u0026lt;dependency\u0026gt; * \u0026lt;groupId\u0026gt;io.netty\u0026lt;/groupId\u0026gt; * \u0026lt;artifactId\u0026gt;netty-all\u0026lt;/artifactId\u0026gt; * \u0026lt;version\u0026gt;4.1.52.Final\u0026lt;/version\u0026gt; * \u0026lt;/dependency\u0026gt; */ public static void main(String[] args) throws InterruptedException { // 客户端只需要一个事件循环组，可以看做 BossGroup EventLoopGroup eventLoopGroup = new NioEventLoopGroup(); try { // 创建客户端的启动对象 Bootstrap bootstrap = new Bootstrap(); // 配置参数 bootstrap // 设置线程组 .group(eventLoopGroup) // 说明客户端通道的实现类（便于 Netty 做反射处理） .channel(NioSocketChannel.class) // handler()方法用于给 BossGroup 设置业务处理器 .handler( // 创建一个通道初始化对象 new ChannelInitializer\u0026lt;SocketChannel\u0026gt;() { // 向 Pipeline 添加业务处理器 @Override protected void initChannel( SocketChannel socketChannel ) throws Exception { socketChannel.pipeline().addLast( new NettyClientHandler() ); // 可以继续调用 socketChannel.pipeline().addLast() // 添加更多 Handler } } ); System.out.println(\u0026#34;client is ready...\u0026#34;); // 启动客户端去连接服务器端，ChannelFuture 涉及到 Netty 的异步模型，后面展开讲 ChannelFuture channelFuture = bootstrap.connect(\u0026#34;127.0.0.1\u0026#34;, 8080).sync(); // 对通道关闭进行监听 channelFuture.channel().closeFuture().sync(); } finally { eventLoopGroup.shutdownGracefully(); } } /** * 自定义一个 Handler，需要继承 Netty 规定好的某个 HandlerAdapter（规范） * InboundHandler 用于处理数据流入本端（客户端）的 IO 事件 * InboundHandler 用于处理数据流出本端（客户端）的 IO 事件 */ static class NettyClientHandler extends ChannelInboundHandlerAdapter { /** * 通道就绪时执行 * * @param ctx 上下文对象 * @throws Exception */ @Override public void channelActive(ChannelHandlerContext ctx) throws Exception { // 向服务器发送数据 ctx.writeAndFlush( // Unpooled 类是 Netty 提供的专门操作缓冲区的工具 // 类，copiedBuffer 方法返回的 ByteBuf 对象类似于 // NIO 中的 ByteBuffer，但性能更高 Unpooled.copiedBuffer( \u0026#34;hello server!\u0026#34;, CharsetUtil.UTF_8 ) ); } /** * 当通道有数据可读时执行 * * @param ctx 上下文对象 * @param msg 服务器端发送的数据 * @throws Exception */ @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { // 接收服务器端发来的数据 System.out.println(\u0026#34;server address: \u0026#34; + ctx.channel().remoteAddress()); // ByteBuf 是 Netty 提供的类，比 NIO 的 ByteBuffer 性能更高 ByteBuf byteBuf = (ByteBuf) msg; System.out.println(\u0026#34;data from server: \u0026#34; + byteBuf.toString(CharsetUtil.UTF_8)); } /** * 发生异常时执行 * * @param ctx 上下文对象 * @param cause 异常对象 * @throws Exception */ @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { // 关闭与服务器端的 Socket 连接 ctx.channel().close(); } } 什么？你觉得使用 Netty 编程难度和工作量更大了？不会吧不会吧，你要知道，你通过这么两段简短的代码得到了一个基于主从 Reactor 多线程模式的服务器，一个高吞吐量和并发量的服务器，一个异步处理服务器……你还要怎样？\n对上面的两段代码，作以下简单说明：\nBootstrap 和 ServerBootstrap 分别是客户端和服务器端的引导类，一个 Netty 应用程序通常由一个引导类开始，主要是用来配置整个 Netty 程序、设置业务处理类（Handler）、绑定端口、发起连接等。 客户端创建一个 NioSocketChannel 作为客户端通道，去连接服务器。 服务端首先创建一个 NioServerSocketChannel 作为服务器端通道，每当接收一个客户端连接就产生一个 NioSocketChannel 应对该客户端。 使用 Channel 构建网络 IO 程序的时候，不同的协议、不同的阻塞类型和 Netty 中不同的 Channel 对应，常用的 Channel 有： NioSocketChannel：非阻塞的 TCP 客户端 Channel（本案例的客户端使用的 Channel） NioServerSocketChannel：非阻塞的 TCP 服务器端 Channel（本案例的服务器端使用的 Channel） NioDatagramChannel：非阻塞的 UDP Channel NioSctpChannel：非阻塞的 SCTP 客户端 Channel NioSctpServerChannel：非阻塞的 SCTP 服务器端 Channel \u0026hellip;\u0026hellip; 启动服务端和客户端代码，调试以上的服务端代码，发现：\n默认情况下 BossGroup 和 WorkerGroup 都包含 16 个线程（NioEventLoop），这是因为我的 PC 是 8 核的 NioEventLoop 的数量=coreNum*2。这 16 个线程相当于主 Reactor。 其实创建 BossGroup 和 WorkerGroup 的时候可以指定 NioEventLoop 数量，如下：\nEventLoopGroup bossGroup = new NioEventLoopGroup(1); EventLoopGroup workerGroup = new NioEventLoopGroup(16); 这样就能更好地分配线程资源。\n每一个 NioEventLoop 包含如下的属性（比如自己的 Selector、任务队列、执行器等）： 将代码断在服务端的 NettyServerHandler.channelRead 上： 可以看到 ctx 中包含的属性如下：\n可以看到：\n当前 ChannelHandlerContext ctx 是位于 ChannelHandlerContext 责任链中的一环，可以看到其 next、prev 属性 当前 ChannelHandlerContext ctx 包含一个 Handler 当前 ChannelHandlerContext ctx 包含一个 Pipeline Pipeline 本质上是一个双向循环列表，可以看到其 tail、head 属性 Pipeline 中包含一个 Channel，Channel 中又包含了该 Pipeline，两者互相引用 …… 从下一节开始，我将深入剖析以上两段代码，向读者展示 Netty 的更多细节。\nNetty 的 Handler 组件 # 无论是服务端代码中自定义的 NettyServerHandler 还是客户端代码中自定义的 NettyClientHandler，都继承于 ChannelInboundHandlerAdapter，ChannelInboundHandlerAdapter 又继承于 ChannelHandlerAdapter，ChannelHandlerAdapter 又实现了 ChannelHandler：\npublic class ChannelInboundHandlerAdapter extends ChannelHandlerAdapter implements ChannelInboundHandler { ...... public abstract class ChannelHandlerAdapter implements ChannelHandler { ...... 因此无论是服务端代码中自定义的 NettyServerHandler 还是客户端代码中自定义的 NettyClientHandler，都可以统称为 ChannelHandler。\nNetty 中的 ChannelHandler 的作用是，在当前 ChannelHandler 中处理 IO 事件，并将其传递给 ChannelPipeline 中下一个 ChannelHandler 处理，因此多个 ChannelHandler 形成一个责任链，责任链位于 ChannelPipeline 中。\n数据在基于 Netty 的服务器或客户端中的处理流程是：读取数据\u0026ndash;\u0026gt;解码数据\u0026ndash;\u0026gt;处理数据\u0026ndash;\u0026gt;编码数据\u0026ndash;\u0026gt;发送数据。其中的每个过程都用得到 ChannelHandler 责任链。\nNetty 中的 ChannelHandler 体系如下（第一张图来源于网络）：\n其中：\nChannelInboundHandler 用于处理入站 IO 事件 ChannelOutboundHandler 用于处理出站 IO 事件 ChannelInboundHandlerAdapter 用于处理入站 IO 事件 ChannelOutboundHandlerAdapter 用于处理出站 IO 事件 ChannelPipeline 提供了 ChannelHandler 链的容器。以客户端应用程序为例，如果事件的方向是从客户端到服务器的，我们称事件是出站的，那么客户端发送给服务器的数据会通过 Pipeline 中的一系列 ChannelOutboundHandler 进行处理；如果事件的方向是从服务器到客户端的，我们称事件是入站的，那么服务器发送给客户端的数据会通过 Pipeline 中的一系列 ChannelInboundHandler 进行处理。\n无论是服务端代码中自定义的 NettyServerHandler 还是客户端代码中自定义的 NettyClientHandler，都继承于 ChannelInboundHandlerAdapter，ChannelInboundHandlerAdapter 提供的方法如下：\n从方法名字可以看出，它们在不同的事件发生后被触发，例如注册 Channel 时执行 channelRegistred()、添加 ChannelHandler 时执行 handlerAdded()、收到入站数据时执行 channelRead()、入站数据读取完毕后执行 channelReadComplete()等等。\nNetty 的 Pipeline 组件 # 上一节说到，Netty 的 ChannelPipeline，它维护了一个 ChannelHandler 责任链，负责拦截或者处理 inbound（入站）和 outbound（出站）的事件和操作。这一节给出更深层次的描述。\nChannelPipeline 实现了一种高级形式的拦截过滤器模式，使用户可以完全控制事件的处理方式，以及 Channel 中各个 ChannelHandler 如何相互交互。\n每个 Netty Channel 包含了一个 ChannelPipeline（其实 Channel 和 ChannelPipeline 互相引用），而 ChannelPipeline 又维护了一个由 ChannelHandlerContext 构成的双向循环列表，其中的每一个 ChannelHandlerContext 都包含一个 ChannelHandler。（前文描述的时候为了简便，直接说 ChannelPipeline 包含了一个 ChannelHandler 责任链，这里给出完整的细节。）\n如下图所示（图片来源于网络）：\n还记得下面这张图吗？这是上文中基于 Netty 的 Server 程序的调试截图，可以从中看到 ChannelHandlerContext 中包含了哪些成分：\nChannelHandlerContext 除了包含 ChannelHandler 之外，还关联了对应的 Channel 和 Pipeline。可以这么来讲：ChannelHandlerContext、ChannelHandler、Channel、ChannelPipeline 这几个组件之间互相引用，互为各自的属性，你中有我、我中有你。\n在处理入站事件的时候，入站事件及数据会从 Pipeline 中的双向链表的头 ChannelHandlerContext 流向尾 ChannelHandlerContext，并依次在其中每个 ChannelInboundHandler（例如解码 Handler）中得到处理；出站事件及数据会从 Pipeline 中的双向链表的尾 ChannelHandlerContext 流向头 ChannelHandlerContext，并依次在其中每个 ChannelOutboundHandler（例如编码 Handler）中得到处理。\nNetty 的 EventLoopGroup 组件 # 在基于 Netty 的 TCP Server 代码中，包含了两个 EventLoopGroup——bossGroup 和 workerGroup，EventLoopGroup 是一组 EventLoop 的抽象。\n追踪 Netty 的 EventLoop 的继承链，可以发现 EventLoop 最终继承于 JUC Executor，因此 EventLoop 本质就是一个 JUC Executor，即线程，JUC Executor 的源码为：\npublic interface Executor { /** * Executes the given command at some time in the future. */ void execute(Runnable command); } Netty 为了更好地利用多核 CPU 的性能，一般会有多个 EventLoop 同时工作，每个 EventLoop 维护着一个 Selector 实例，Selector 实例监听注册其上的 Channel 的 IO 事件。\nEventLoopGroup 含有一个 next 方法，它的作用是按照一定规则从 Group 中选取一个 EventLoop 处理 IO 事件。\n在服务端，通常 Boss EventLoopGroup 只包含一个 Boss EventLoop（单线程），该 EventLoop 维护者一个注册了 ServerSocketChannel 的 Selector 实例。该 EventLoop 不断轮询 Selector 得到 OP_ACCEPT 事件（客户端连接事件），然后将接收到的 SocketChannel 交给 Worker EventLoopGroup，Worker EventLoopGroup 会通过 next()方法选取一个 Worker EventLoop 并将这个 SocketChannel 注册到其中的 Selector 上，由这个 Worker EventLoop 负责该 SocketChannel 上后续的 IO 事件处理。整个过程如下图所示：\nNetty 的 TaskQueue # 在 Netty 的每一个 NioEventLoop 中都有一个 TaskQueue，设计它的目的是在任务提交的速度大于线程的处理速度的时候起到缓冲作用。或者用于异步地处理 Selector 监听到的 IO 事件。\nNetty 中的任务队列有三种使用场景：\n处理用户程序的自定义普通任务的时候 处理用户程序的自定义定时任务的时候 非当前 Reactor 线程调用当前 Channel 的各种方法的时候。 对于第一种场景，举个例子，2.4 节的基于 Netty 编写的服务端的 Handler 中，假如 channelRead 方法中执行的过程很耗时，那么以下的阻塞式处理方式无疑会降低当前 NioEventLoop 的并发度：\n/** * 当通道有数据可读时执行 * * @param ctx 上下文对象 * @param msg 客户端发送的数据 * @throws Exception */ @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { // 借助休眠模拟耗时操作 Thread.sleep(LONG_TIME); ByteBuf byteBuf = (ByteBuf) msg; System.out.println(\u0026#34;data from client: \u0026#34; + byteBuf.toString(CharsetUtil.UTF_8)); } 改进方法就是借助任务队列，代码如下：\n/** * 当通道有数据可读时执行 * * @param ctx 上下文对象 * @param msg 客户端发送的数据 * @throws Exception */ @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { // 假如这里的处理非常耗时，那么就需要借助任务队列异步执行 final Object finalMsg = msg; // 通过 ctx.channel().eventLoop().execute()将耗时 // 操作放入任务队列异步执行 ctx.channel().eventLoop().execute(new Runnable() { public void run() { // 借助休眠模拟耗时操作 try { Thread.sleep(LONG_TIME); } catch (InterruptedException e) { e.printStackTrace(); } ByteBuf byteBuf = (ByteBuf) finalMsg; System.out.println(\u0026#34;data from client: \u0026#34; + byteBuf.toString(CharsetUtil.UTF_8)); } }); // 可以继续调用 ctx.channel().eventLoop().execute() // 将更多操作放入队列 System.out.println(\u0026#34;return right now.\u0026#34;); } 断点跟踪这个函数的执行，可以发现该耗时任务确实被放入的当前 NioEventLoop 的 taskQueue 中了。\n对于第二种场景，举个例子，2.4 节的基于 Netty 编写的服务端的 Handler 中，假如 channelRead 方法中执行的过程并不需要立即执行，而是要定时执行，那么代码可以这样写：\n/** * 当通道有数据可读时执行 * * @param ctx 上下文对象 * @param msg 客户端发送的数据 * @throws Exception */ @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { final Object finalMsg = msg; // 通过 ctx.channel().eventLoop().schedule()将操作 // 放入任务队列定时执行（5min 之后才进行处理） ctx.channel().eventLoop().schedule(new Runnable() { public void run() { ByteBuf byteBuf = (ByteBuf) finalMsg; System.out.println(\u0026#34;data from client: \u0026#34; + byteBuf.toString(CharsetUtil.UTF_8)); } }, 5, TimeUnit.MINUTES); // 可以继续调用 ctx.channel().eventLoop().schedule() // 将更多操作放入队列 System.out.println(\u0026#34;return right now.\u0026#34;); } 断点跟踪这个函数的执行，可以发现该定时任务确实被放入的当前 NioEventLoop 的 scheduleTasjQueue 中了。\n对于第三种场景，举个例子，比如在基于 Netty 构建的推送系统的业务线程中，要根据用户标识，找到对应的 SocketChannel 引用，然后调用 write 方法向该用户推送消息，这时候就会将这一 write 任务放在任务队列中，write 任务最终被异步消费。这种情形是对前两种情形的应用，且涉及的业务内容太多，不再给出示例代码，读者有兴趣可以自行完成，这里给出以下提示：\nNetty 的 Future 和 Promise # Netty**对使用者提供的多数 IO 接口（即 Netty Channel 中的 IO 方法）**是异步的（即都立即返回一个 Netty Future，而 IO 过程异步进行），因此，调用者调用 IO 操作后是不能直接拿到调用结果的。要想得到 IO 操作结果，可以借助 Netty 的 Future（上面代码中的 ChannelFuture 就继承了 Netty Future，Netty Future 又继承了 JUC Future）查询执行状态、等待执行结果、获取执行结果等，使用过 JUC Future 接口的同学会非常熟悉这个机制，这里不再展开描述了。也可以通过 Netty Future 的 addListener()添加一个回调方法来异步处理 IO 结果，如下：\n// 启动客户端去连接服务器端 // 由于 bootstrap.connect()是一个异步操作，因此用.sync()等待 // 这个异步操作完成 final ChannelFuture channelFuture = bootstrap.connect( \u0026#34;127.0.0.1\u0026#34;, 8080).sync(); channelFuture.addListener(new ChannelFutureListener() { /** * 回调方法，上面的 bootstrap.connect()操作执行完之后触发 */ public void operationComplete(ChannelFuture future) throws Exception { if (channelFuture.isSuccess()) { System.out.println(\u0026#34;client has connected to server!\u0026#34;); // TODO 其他处理 } else { System.out.println(\u0026#34;connect to serverfail!\u0026#34;); // TODO 其他处理 } } }); Netty Future 提供的接口有：\n注：会有一些资料给出这样的描述：\u0026ldquo;Netty 中所有的 IO 操作都是异步的\u0026rdquo;，这显然是错误的。Netty 基于 Java NIO，Java NIO 是同步非阻塞 IO。Netty 基于 Java NIO 做了封装，向使用者提供了异步特性的接口，因此本文说 Netty**对使用者提供的多数 IO 接口（即 Netty Channel 中的 IO 方法）**是异步的。例如在 io.netty.channel.ChannelOutboundInvoker（Netty Channel 的 IO 方法多继承于此）提供的多数 IO 接口都返回 Netty Future：\nPromise 是可写的 Future，Future 自身并没有写操作相关的接口，Netty 通过 Promise 对 Future 进行扩展，用于设置 IO 操作的结果。Future 继承了 Future，相关的接口定义如下图所示，相比于上图 Future 的接口，它多出了一些 setXXX 方法：\nNetty 发起 IO 写操作的时候，会创建一个新的 Promise 对象，例如调用 ChannelHandlerContext 的 write(Object object)方法时，会创建一个新的 ChannelPromise，相关代码如下：\n@Override public ChannelFuture write(Object msg) { return write(msg, newPromise()); } ...... @Override public ChannelPromise newPromise() { return new DefaultChannelPromise(channel(), executor()); } ...... 当 IO 操作发生异常或者完成时，通过 Promise.setSuccess()或者 Promise.setFailure()设置结果，并通知所有 Listener。\nNetty 源码分析 # NioEventLoopGroup源码分析 # 启动NettyServer的模版代码\nprivate void bing(int port) { EventLoopGroup parentGroup = new NioEventLoopGroup(); EventLoopGroup childGroup = new NioEventLoopGroup(); try { ServerBootstrap b = new ServerBootstrap(); b.group(parentGroup, childGroup) .channel(NioServerSocketChannel.class) //非阻塞模式 .option(ChannelOption.SO_BACKLOG, 128) .childHandler(new MyChannelInitializer()); ChannelFuture f = b.bind(port).sync(); System.out.println(\u0026#34;itstack-demo-netty server start done. {关注公众号：bugstack虫洞栈，获取源码}\u0026#34;); f.channel().closeFuture().sync(); } catch (InterruptedException e) { e.printStackTrace(); } finally { childGroup.shutdownGracefully(); parentGroup.shutdownGracefully(); } } 类结构树 # NioEventLoopGroup 通过实现Java的并发编程包的方法，来实现自己的相关功能。\nEventExecutorGroup # EventExecutorGroup 使用next()方法负责提供EventExecutor。除此之外，它还负责处理生命周期，并且可以以一种全局的方式进行关闭。\n/** * The {@link EventExecutorGroup} is responsible for providing the {@link EventExecutor}\u0026#39;s to use * via its {@link #next()} method. Besides this, it is also responsible for handling their * life-cycle and allows shutting them down in a global fashion. */ public interface EventExecutorGroup extends ScheduledExecutorService, Iterable\u0026lt;EventExecutor\u0026gt; { ... /** * Returns one of the {@link EventExecutor}s managed by this {@link EventExecutorGroup}. */ EventExecutor next(); ... } EventExecutorGroup.next() : 返回一个由EventExecutorGroup管理的事件执行器。组里包含了若干个EventExecutor。 EventLoopGroup # EventLoopGroup继承EventExecutorGroup的接口\nEventLoopGroup 本身是特殊的EventExecutorGroup，它的作用是会在事件循环（处理链接、输入输出消息等）的过程当中，进行selection操作当中允许注册一个一个的channel链接。\n/** * Special {@link EventExecutorGroup} which allows registering {@link Channel}s that get * processed for later selection during the event loop. * */ public interface EventLoopGroup extends EventExecutorGroup { /** * Return the next {@link EventLoop} to use */ @Override EventLoop next(); /** * Register a {@link Channel} with this {@link EventLoop}. The returned {@link ChannelFuture} * will get notified once the registration was complete. */ ChannelFuture register(Channel channel); /** * Register a {@link Channel} with this {@link EventLoop} using a {@link ChannelFuture}. The passed * {@link ChannelFuture} will get notified once the registration was complete and also will get returned. */ ChannelFuture register(ChannelPromise promise); /** * Register a {@link Channel} with this {@link EventLoop}. The passed {@link ChannelFuture} * will get notified once the registration was complete and also will get returned. * * @deprecated Use {@link #register(ChannelPromise)} instead. */ @Deprecated ChannelFuture register(Channel channel, ChannelPromise promise); } EventLoopGroup.next() 返回下一个事件循环 EventLoopGroup.register(Channel channel) 将一个通道注册到事件循环当中，所返回的ChannelFuture在注册完成之后就会收到一个通知。（ChannelFuture是一个异步方法，ChannelFuture是继承自jdk1.5里面的Future方法。 EventLoopGroup.register(ChannelPromise promise) 与上面的方法构成一个重载，ChannelPromise里面继承了ChannelFuture，里面包含了channel。在注册完成之后ChannelFuture会收到一个通知并且也会返回。 EventLoopGroup.register(Channel channel, ChannelPromise promise) 因为ChannelPromise已经包含了Channel，方法重复了所以被注释掉了。 NioEventLoopGroup # MultithreadEventLoopGroup是NioEventLoopGroup的一个父类，NioEventLoopGroup基于NIO选择器的Selector的一个实现。并提供多种不同入参的构造方法，在不同的构造方法内提供一些默认的初始化方法，以便于创建Netty服务配置信息。\n/** * {@link MultithreadEventLoopGroup} implementations which is used for NIO {@link Selector} based {@link Channel}s. */ public class NioEventLoopGroup extends MultithreadEventLoopGroup { /** * Create a new instance using the default number of threads, the default {@link ThreadFactory} and * the {@link SelectorProvider} which is returned by {@link SelectorProvider#provider()}. */ public NioEventLoopGroup() { this(0); } /** * Create a new instance using the specified number of threads, {@link ThreadFactory} and the * {@link SelectorProvider} which is returned by {@link SelectorProvider#provider()}. */ public NioEventLoopGroup(int nThreads) { this(nThreads, (Executor) null); } /** * Create a new instance using the specified number of threads, the given {@link ThreadFactory} and the * {@link SelectorProvider} which is returned by {@link SelectorProvider#provider()}. */ public NioEventLoopGroup(int nThreads, ThreadFactory threadFactory) { this(nThreads, threadFactory, SelectorProvider.provider()); } public NioEventLoopGroup(int nThreads, Executor executor) { this(nThreads, executor, SelectorProvider.provider()); } /** * Create a new instance using the specified number of threads, the given {@link ThreadFactory} and the given * {@link SelectorProvider}. */ public NioEventLoopGroup( int nThreads, ThreadFactory threadFactory, final SelectorProvider selectorProvider) { this(nThreads, threadFactory, selectorProvider, DefaultSelectStrategyFactory.INSTANCE); } public NioEventLoopGroup(int nThreads, ThreadFactory threadFactory, final SelectorProvider selectorProvider, final SelectStrategyFactory selectStrategyFactory) { super(nThreads, threadFactory, selectorProvider, selectStrategyFactory, RejectedExecutionHandlers.reject()); } public NioEventLoopGroup( int nThreads, Executor executor, final SelectorProvider selectorProvider) { this(nThreads, executor, selectorProvider, DefaultSelectStrategyFactory.INSTANCE); } public NioEventLoopGroup(int nThreads, Executor executor, final SelectorProvider selectorProvider, final SelectStrategyFactory selectStrategyFactory) { super(nThreads, executor, selectorProvider, selectStrategyFactory, RejectedExecutionHandlers.reject()); } public NioEventLoopGroup(int nThreads, Executor executor, EventExecutorChooserFactory chooserFactory, final SelectorProvider selectorProvider, final SelectStrategyFactory selectStrategyFactory) { super(nThreads, executor, chooserFactory, selectorProvider, selectStrategyFactory, RejectedExecutionHandlers.reject()); } public NioEventLoopGroup(int nThreads, Executor executor, EventExecutorChooserFactory chooserFactory, final SelectorProvider selectorProvider, final SelectStrategyFactory selectStrategyFactory, final RejectedExecutionHandler rejectedExecutionHandler) { super(nThreads, executor, chooserFactory, selectorProvider, selectStrategyFactory, rejectedExecutionHandler); } ... } NioEventLoopGroup() 在创建Netty服务端的时候，代码中实例化了两个EventLoopGroup分别是parentGroup、childGroup，parentGroup 主要用于接收请求链接，链接成功后交给childGroup处理收发数据等事件。 NioEventLoopGroup可以在构造方法中传入需要启动的线程数，默认的情况下他会在采用计算机核心数2的方式去启动线程数量。另外目前很多计算机采用了超线程技术，那么4核心的机器，超线程后就是8核心，Netty在启动的时候随时会启动82=16个线程。 MultithreadEventLoopGroup.java | 源码中NettyRuntime.availableProcessors() * 2 超线程（HT, Hyper-Threading）是英特尔研发的一种技术，于2002年发布。超线程技术原先只应用于Xeon 处理器中，当时称为\u0026quot;Super-Threading\u0026quot;。之后陆续应用在Pentium 4 HT中。早期代号为Jackson。 [1] 通过此技术，英特尔实现在一个实体CPU中，提供两个逻辑线程。之后的Pentium D纵使不支持超线程技术，但就集成了两个实体核心，所以仍会见到两个线程。超线程的未来发展，是提升处理器的逻辑线程。英特尔于2016年发布的Core i7-6950X便是将10核心的处理器，加上超线程技术，使之成为20个逻辑线程的产品。\npublic abstract class MultithreadEventLoopGroup extends MultithreadEventExecutorGroup implements EventLoopGroup { private static final InternalLogger logger = InternalLoggerFactory.getInstance(MultithreadEventLoopGroup.class); private static final int DEFAULT_EVENT_LOOP_THREADS; static { DEFAULT_EVENT_LOOP_THREADS = Math.max(1, SystemPropertyUtil.getInt( \u0026#34;io.netty.eventLoopThreads\u0026#34;, NettyRuntime.availableProcessors() * 2)); if (logger.isDebugEnabled()) { logger.debug(\u0026#34;-Dio.netty.eventLoopThreads: {}\u0026#34;, DEFAULT_EVENT_LOOP_THREADS); } } ... } 可以按照实际需要调整线程数 EventLoopGroup parentGroup = new NioEventLoopGroup(1); //单线程 EventLoopGroup parentGroup = new NioEventLoopGroup(4); //多线程 NioEventLoopGroup(int nThreads); 在此构造函数Executor的参数为NULL，最终在MultithreadEventExecutorGroup.MultithreadEventExecutorGroup中会进行创建线程任务执行器 if (executor == null) { executor = new ThreadPerTaskExecutor(newDefaultThreadFactory()); } NioEventLoopGroup(int nThreads, ThreadFactory threadFactory) 在此构造函数中提供了SelectorProvider.provider()用于通过静态方法来获取NIO实例 /** * Create a new instance using the specified number of threads, the given {@link ThreadFactory} and the * {@link SelectorProvider} which is returned by {@link SelectorProvider#provider()}. */ public NioEventLoopGroup(int nThreads, ThreadFactory threadFactory) { this(nThreads, threadFactory, SelectorProvider.provider()); } public static SelectorProvider provider() { synchronized (lock) { if (provider != null) return provider; return AccessController.doPrivileged( new PrivilegedAction\u0026lt;SelectorProvider\u0026gt;() { public SelectorProvider run() { if (loadProviderFromProperty()) return provider; if (loadProviderAsService()) return provider; provider = sun.nio.ch.DefaultSelectorProvider.create(); return provider; } }); } } NioEventLoopGroup(int nThreads, ThreadFactory threadFactory, final SelectorProvider selectorProvider) 在此构造函数中提供了DefaultSelectStrategyFactory.INSTANCE来创建默认选择策略工厂。 final class DefaultSelectStrategy implements SelectStrategy { static final SelectStrategy INSTANCE = new DefaultSelectStrategy(); private DefaultSelectStrategy() { } @Override public int calculateStrategy(IntSupplier selectSupplier, boolean hasTasks) throws Exception { return hasTasks ? selectSupplier.get() : SelectStrategy.SELECT; } } NioEventLoopGroup(int nThreads, Executor executor, final SelectorProvider selectorProvider,final SelectStrategyFactory selectStrategyFactory) 公开辅助方法，用于创建不同的拒绝执行处理器。 // RejectedExecutionHandlers.java private static final RejectedExecutionHandler REJECT = new RejectedExecutionHandler() { @Override public void rejected(Runnable task, SingleThreadEventExecutor executor) { throw new RejectedExecutionException(); } }; MultithreadEventExecutorGroup # 使用多个线程同时处理其任务的实现的抽象基类，其中的MultithreadEventExecutorGroup方法最终创建执行线程\n// MultithreadEventExecutorGroup.java /** * Abstract base class for {@link EventExecutorGroup} implementations that handles their tasks with multiple threads at * the same time. */ public abstract class MultithreadEventExecutorGroup extends AbstractEventExecutorGroup { ... /** * Create a new instance. * * @param nThreads the number of threads that will be used by this instance. * @param executor the Executor to use, or {@code null} if the default should be used. * @param chooserFactory the {@link EventExecutorChooserFactory} to use. * @param args arguments which will passed to each {@link #newChild(Executor, Object...)} call */ protected MultithreadEventExecutorGroup(int nThreads, Executor executor, EventExecutorChooserFactory chooserFactory, Object... args) { if (nThreads \u0026lt;= 0) { throw new IllegalArgumentException(String.format(\u0026#34;nThreads: %d (expected: \u0026gt; 0)\u0026#34;, nThreads)); } if (executor == null) { executor = new ThreadPerTaskExecutor(newDefaultThreadFactory()); } children = new EventExecutor[nThreads]; for (int i = 0; i \u0026lt; nThreads; i ++) { boolean success = false; try { children[i] = newChild(executor, args); success = true; } catch (Exception e) { // TODO: Think about if this is a good exception type throw new IllegalStateException(\u0026#34;failed to create a child event loop\u0026#34;, e); } finally { if (!success) { for (int j = 0; j \u0026lt; i; j ++) { children[j].shutdownGracefully(); } for (int j = 0; j \u0026lt; i; j ++) { EventExecutor e = children[j]; try { while (!e.isTerminated()) { e.awaitTermination(Integer.MAX_VALUE, TimeUnit.SECONDS); } } catch (InterruptedException interrupted) { // Let the caller handle the interruption. Thread.currentThread().interrupt(); break; } } } } } chooser = chooserFactory.newChooser(children); final FutureListener\u0026lt;Object\u0026gt; terminationListener = new FutureListener\u0026lt;Object\u0026gt;() { @Override public void operationComplete(Future\u0026lt;Object\u0026gt; future) throws Exception { if (terminatedChildren.incrementAndGet() == children.length) { terminationFuture.setSuccess(null); } } }; for (EventExecutor e: children) { e.terminationFuture().addListener(terminationListener); } Set\u0026lt;EventExecutor\u0026gt; childrenSet = new LinkedHashSet\u0026lt;EventExecutor\u0026gt;(children.length); Collections.addAll(childrenSet, children); readonlyChildren = Collections.unmodifiableSet(childrenSet); } ... } ","date":"1 November 2023","permalink":"/posts/reviews/network/netty/","section":"博客","summary":"Netty 是 JBoss 开源项目，是异步的、基于事件驱动的网络应用框架，以高性能、高并发著称。Netty 是基于 Java NIO 构建出来的，主要用于开发基于 TCP 协议的网络 IO 程序。","title":"Netty"},{"content":"","date":"1 November 2023","permalink":"/tags/https/","section":"Tags","summary":"","title":"Https"},{"content":"","date":"1 November 2023","permalink":"/posts/reviews/network/https-rsa/","section":"博客","summary":"","title":"HTTPS RSA 握手解析"},{"content":"HTTP # HTTP 基本概念 # HTTP 是什么？ # HTTP 是超文本传输协议，也就是HyperText Transfer Protocol。\n能否详细解释「超文本传输协议」？\nHTTP 的名字「超文本协议传输」，它可以拆成三个部分：\n超文本 传输 协议 1. 「协议」\n在生活中，我们也能随处可见「协议」，例如：\n刚毕业时会签一个「三方协议」； 找房子时会签一个「租房协议」； 生活中的协议，本质上与计算机中的协议是相同的，协议的特点：\n「协」字，代表的意思是必须有两个以上的参与者。例如三方协议里的参与者有三个：你、公司、学校三个；租房协议里的参与者有两个：你和房东。 「议」字，代表的意思是对参与者的一种行为约定和规范。例如三方协议里规定试用期期限、毁约金等；租房协议里规定租期期限、每月租金金额、违约如何处理等。 针对 HTTP 协议，我们可以这么理解。\nHTTP 是一个用在计算机世界里的协议。它使用计算机能够理解的语言确立了一种计算机之间交流通信的规范（两个以上的参与者），以及相关的各种控制和错误处理方式（行为约定和规范）。\n2. 「传输」\n所谓的「传输」，很好理解，就是把一堆东西从 A 点搬到 B 点，或者从 B 点 搬到 A 点。\n别轻视了这个简单的动作，它至少包含两项重要的信息。\nHTTP 协议是一个双向协议。\n我们在上网冲浪时，浏览器是请求方 A，百度网站就是应答方 B。双方约定用 HTTP 协议来通信，于是浏览器把请求数据发送给网站，网站再把一些数据返回给浏览器，最后由浏览器渲染在屏幕，就可以看到图片、视频了。\n数据虽然是在 A 和 B 之间传输，但允许中间有中转或接力。\n就好像第一排的同学想传递纸条给最后一排的同学，那么传递的过程中就需要经过好多个同学（中间人），这样的传输方式就从「A \u0026lt; \u0026mdash; \u0026gt; B」，变成了「A \u0026lt;-\u0026gt; N \u0026lt;-\u0026gt; M \u0026lt;-\u0026gt; B」。\n而在 HTTP 里，需要中间人遵从 HTTP 协议，只要不打扰基本的数据传输，就可以添加任意额外的东西。\n针对传输，我们可以进一步理解了 HTTP。\nHTTP 是一个在计算机世界里专门用来在两点之间传输数据的约定和规范。\n3. 「超文本」\nHTTP 传输的内容是「超文本」。\n我们先来理解「文本」，在互联网早期的时候只是简单的字符文字，但现在「文本」的涵义已经可以扩展为图片、视频、压缩包等，在 HTTP 眼里这些都算作「文本」。\n再来理解「超文本」，它就是超越了普通文本的文本，它是文字、图片、视频等的混合体，最关键有超链接，能从一个超文本跳转到另外一个超文本。\nHTML 就是最常见的超文本了，它本身只是纯文字文件，但内部用很多标签定义了图片、视频等的链接，再经过浏览器的解释，呈现给我们的就是一个文字、有画面的网页了。\nOK，经过了对 HTTP 里这三个名词的详细解释，就可以给出比「超文本传输协议」这七个字更准确更有技术含量的答案：\nHTTP 是一个在计算机世界里专门在「两点」之间「传输」文字、图片、音频、视频等「超文本」数据的「约定和规范」。\n那「HTTP 是用于从互联网服务器传输超文本到本地浏览器的协议」，这种说法正确吗？\n这种说法是不正确的。因为也可以是「服务器\u0026lt; \u0026ndash; \u0026gt;服务器」，所以采用两点之间的描述会更准确。\nHTTP 常见的状态码有哪些？ # 1xx 类状态码属于提示信息，是协议处理中的一种中间状态，实际用到的比较少。\n「101 Switching Protocols」协议切换，服务器已经理解了客户端的请求，并将通过 Upgrade 消息头通知客户端采用不同的协议来完成这个请求。比如切换到一个实时且同步的协议（如 WebSocket）以传送利用此类特性的资源。 2xx 类状态码表示服务器成功处理了客户端的请求，也是我们最愿意看到的状态。\n「200 OK」是最常见的成功状态码，表示一切正常。如果是非 HEAD 请求，服务器返回的响应头都会有 body 数据。\n「204 No Content」也是常见的成功状态码，与 200 OK 基本相同，但响应头没有 body 数据。\n「206 Partial Content」是应用于 HTTP 分块下载或断点续传，表示响应返回的 body 数据并不是资源的全部，而是其中的一部分，也是服务器处理成功的状态。\n3xx 类状态码表示客户端请求的资源发生了变动，需要客户端用新的 URL 重新发送请求获取资源，也就是重定向。\n「301 Moved Permanently」表示永久重定向，说明请求的资源已经不存在了，需改用新的 URL 再次访问。\n「302 Found」表示临时重定向，说明请求的资源还在，但暂时需要用另一个 URL 来访问。\n301 和 302 都会在响应头里使用字段 Location，指明后续要跳转的 URL，浏览器会自动重定向新的 URL。\n「304 Not Modified」不具有跳转的含义，表示资源未修改，重定向已存在的缓冲文件，也称缓存重定向，也就是告诉客户端可以继续使用缓存资源，用于缓存控制。 4xx 类状态码表示客户端发送的报文有误，服务器无法处理，也就是错误码的含义。\n「400 Bad Request」表示客户端请求的报文有错误，但只是个笼统的错误。\n「403 Forbidden」表示服务器禁止访问资源，并不是客户端的请求出错。\n「404 Not Found」表示请求的资源在服务器上不存在或未找到，所以无法提供给客户端。\n5xx 类状态码表示客户端请求报文正确，但是服务器处理时内部发生了错误，属于服务器端的错误码。\n「500 Internal Server Error」与 400 类似，是个笼统通用的错误码，服务器发生了什么错误，我们并不知道。\n「501 Not Implemented」表示客户端请求的功能还不支持，类似\u0026quot;即将开业，敬请期待\u0026quot;的意思。\n「502 Bad Gateway」通常是服务器作为网关或代理时返回的错误码，表示服务器自身工作正常，访问后端服务器发生了错误。\n「503 Service Unavailable」表示服务器当前很忙，暂时无法响应客户端，类似\u0026quot;网络服务正忙，请稍后重试\u0026quot;的意思。\nHTTP 常见字段有哪些？ # Host 字段\n客户端发送请求时，用来指定服务器的域名。\nHost: www.A.com 有了 Host 字段，就可以将请求发往「同一台」服务器上的不同网站。\nContent-Length 字段\n服务器在返回数据时，会有 Content-Length 字段，表明本次回应的数据长度。\nContent-Length: 1000 如上面则是告诉浏览器，本次服务器回应的数据长度是 1000 个字节，后面的字节就属于下一个回应了。\n大家应该都知道 HTTP 是基于 TCP 传输协议进行通信的，而使用了 TCP 传输协议，就会存在一个\u0026quot;粘包\u0026quot;的问题，HTTP 协议通过设置回车符、换行符作为 HTTP header 的边界，通过 Content-Length 字段作为 HTTP body 的边界，这两个方式都是为了解决\u0026quot;粘包\u0026quot;的问题。\nConnection 字段\nConnection 字段最常用于客户端要求服务器使用「HTTP 长连接」机制，以便其他请求复用。\nHTTP 长连接的特点是，只要任意一端没有明确提出断开连接，则保持 TCP 连接状态。\nHTTP/1.1 版本的默认连接都是长连接，但为了兼容老版本的 HTTP，需要指定 Connection 首部字段的值为 Keep-Alive。\nConnection: Keep-Alive 开启了 HTTP Keep-Alive 机制后，连接就不会中断，而是保持连接。当客户端发送另一个请求时，它会使用同一个连接，一直持续到客户端或服务器端提出断开连接。\nPS：大家不要把 HTTP Keep-Alive 和 TCP Keepalive 搞混了，这两个虽然长的像，但是不是一个东西。\nContent-Type 字段\nContent-Type 字段用于服务器回应时，告诉客户端，本次数据是什么格式。\nContent-Type: text/html; Charset=utf-8 上面的类型表明，发送的是网页，而且编码是 UTF-8。\n客户端请求的时候，可以使用 Accept 字段声明自己可以接受哪些数据格式。\nAccept: */* 上面代码中，客户端声明自己可以接受任何格式的数据。\nContent-Encoding 字段\nContent-Encoding 字段说明数据的压缩方法。表示服务器返回的数据使用了什么压缩格式\nContent-Encoding: gzip 上面表示服务器返回的数据采用了 gzip 方式压缩，告知客户端需要用此方式解压。\n客户端在请求时，用 Accept-Encoding 字段说明自己可以接受哪些压缩方法。\nAccept-Encoding: gzip, deflate GET 与 POST # GET 和 POST 有什么区别？ # 可见 一次完整的HTTP请求过程\nGET 和 POST 方法都是安全和幂等的吗？ # 先说明下安全和幂等的概念：\n在 HTTP 协议里，所谓的「安全」是指请求方法不会「破坏」服务器上的资源。 所谓的「幂等」，意思是多次执行相同的操作，结果都是「相同」的。 如果从 RFC 规范定义的语义来看：\nGET 方法就是安全且幂等的，因为它是「只读」操作，无论操作多少次，服务器上的数据都是安全的，且每次的结果都是相同的。所以，可以对 GET 请求的数据做缓存，这个缓存可以做到浏览器本身上（彻底避免浏览器发请求），也可以做到代理上（如 nginx），而且在浏览器中 GET 请求可以保存为书签。 POST 因为是「新增或提交数据」的操作，会修改服务器上的资源，所以是不安全的，且多次提交数据就会创建多个资源，所以不是幂等的。所以，浏览器一般不会缓存 POST 请求，也不能把 POST 请求保存为书签。 做个简要的小结。\nGET 的语义是请求获取指定的资源。GET 方法是安全、幂等、可被缓存的。\nPOST 的语义是根据请求负荷（报文主体）对指定的资源做出处理，具体的处理方式视资源类型而不同。POST 不安全，不幂等，（大部分实现）不可缓存。\n注意，上面是从 RFC 规范定义的语义来分析的。\n但是实际过程中，开发者不一定会按照 RFC 规范定义的语义来实现 GET 和 POST 方法。比如：\n可以用 GET 方法实现新增或删除数据的请求，这样实现的 GET 方法自然就不是安全和幂等。 可以用 POST 方法实现查询数据的请求，这样实现的 POST 方法自然就是安全和幂等。 曾经有个笑话，有人写了个博客，删除博客用的是 GET 请求，他觉得没人访问就连鉴权都没做。然后 Google 服务器爬虫爬了一遍，他所有博文就没了。。。\n如果「安全」放入概念是指信息是否会被泄漏的话，虽然 POST 用 body 传输数据，而 GET 用 URL 传输，这样数据会在浏览器地址拦容易看到，但是并不能说 GET 不如 POST 安全的。\n因为 HTTP 传输的内容都是明文的，虽然在浏览器地址拦看不到 POST 提交的 body 数据，但是只要抓个包就都能看到了。\n所以，要避免传输过程中数据被窃取，就要使用 HTTPS 协议，这样所有 HTTP 的数据都会被加密传输。\nGET 请求可以带 body 吗？\nRFC 规范并没有规定 GET 请求不能带 body 的。理论上，任何请求都可以带 body 的。只是因为 RFC 规范定义的 GET 请求是获取资源，所以根据这个语义不需要用到 body。\n另外，URL 中的查询参数也不是 GET 所独有的，POST 请求的 URL 中也可以有参数的。\nHTTP 缓存技术 # HTTP 缓存有哪些实现方式？ # 对于一些具有重复性的 HTTP 请求，比如每次请求得到的数据都一样的，我们可以把这对「请求 - 响应」的数据都缓存在本地，那么下次就直接读取本地的数据，不必在通过网络获取服务器的响应了，这样的话 HTTP/1.1 的性能肯定肉眼可见的提升。\n所以，避免发送 HTTP 请求的方法就是通过缓存技术，HTTP 设计者早在之前就考虑到了这点，因此 HTTP 协议的头部有不少是针对缓存的字段。\nHTTP 缓存有两种实现方式，分别是强制缓存和协商缓存。\n什么是强制缓存？ # 强缓存指的是只要浏览器判断缓存没有过期，则直接使用浏览器的本地缓存，决定是否使用缓存的主动性在于浏览器这边。\n如下图中，返回的是 200 状态码，但在 size 项中标识的是 from disk cache，就是使用了强制缓存。\n强缓存是利用下面这两个 HTTP 响应头部（Response Header）字段实现的，它们都用来表示资源在客户端缓存的有效期：\nCache-Control，是一个相对时间； Expires，是一个绝对时间； 如果 HTTP 响应头部同时有 Cache-Control 和 Expires 字段的话，Cache-Control 的优先级高于 Expires 。\nCache-control 选项更多一些，设置更加精细，所以建议使用 Cache-Control 来实现强缓存。具体的实现流程如下：\n当浏览器第一次请求访问服务器资源时，服务器会在返回这个资源的同时，在 Response 头部加上 Cache-Control，Cache-Control 中设置了过期时间大小； 浏览器再次请求访问服务器中的该资源时，会先通过请求资源的时间与 Cache-Control 中设置的过期时间大小，来计算出该资源是否过期，如果没有，则使用该缓存，否则重新请求服务器； 服务器再次收到请求后，会再次更新 Response 头部的 Cache-Control。 什么是协商缓存？ # 当我们在浏览器使用开发者工具的时候，你可能会看到过某些请求的响应码是 304，这个是告诉浏览器可以使用本地缓存的资源，通常这种通过服务端告知客户端是否可以使用缓存的方式被称为协商缓存。\n上图就是一个协商缓存的过程，所以协商缓存就是与服务端协商之后，通过协商结果来判断是否使用本地缓存。\n协商缓存可以基于两种头部来实现。\n第一种：请求头部中的 If-Modified-Since 字段与响应头部中的 Last-Modified 字段实现，这两个字段的意思是：\n响应头部中的 Last-Modified：标示这个响应资源的最后修改时间； 请求头部中的 If-Modified-Since：当资源过期了，发现响应头中具有 Last-Modified 声明，则再次发起请求的时候带上 Last-Modified 的时间，服务器收到请求后发现有 If-Modified-Since 则与被请求资源的最后修改时间进行对比（Last-Modified），如果最后修改时间较新（大），说明资源又被改过，则返回最新资源，HTTP 200 OK；如果最后修改时间较旧（小），说明资源无新修改，响应 HTTP 304 走缓存。 第二种：请求头部中的 If-None-Match 字段与响应头部中的 ETag 字段，这两个字段的意思是：\n响应头部中 Etag：唯一标识响应资源； 请求头部中的 If-None-Match：当资源过期时，浏览器发现响应头里有 Etag，则再次向服务器发起请求时，会将请求头 If-None-Match 值设置为 Etag 的值。服务器收到请求后进行比对，如果资源没有变化返回 304，如果资源变化了返回 200。 第一种实现方式是基于时间实现的，第二种实现方式是基于一个唯一标识实现的，相对来说后者可以更加准确地判断文件内容是否被修改，避免由于时间篡改导致的不可靠问题。\n如果在第一次请求资源的时候，服务端返回的 HTTP 响应头部同时有 Etag 和 Last-Modified 字段，那么客户端再下一次请求的时候，如果带上了 ETag 和 Last-Modified 字段信息给服务端，这时 Etag 的优先级更高，也就是服务端先会判断 Etag 是否变化了，如果 Etag 有变化就不用在判断 Last-Modified 了，如果 Etag 没有变化，然后再看 Last-Modified。\n为什么 ETag 的优先级更高？ 这是因为 ETag 主要能解决 Last-Modified 几个比较难以解决的问题：\n在没有修改文件内容情况下文件的最后修改时间可能也会改变，这会导致客户端认为这文件被改动了，从而重新请求； 可能有些文件是在秒级以内修改的，If-Modified-Since 能检查到的粒度是秒级的，使用 Etag 就能够保证这种需求下客户端在 1 秒内能刷新多次； 有些服务器不能精确获取文件的最后修改时间。 注意，协商缓存这两个字段都需要配合强制缓存中 Cache-Control 字段来使用，只有在未能命中强制缓存的时候，才能发起带有协商缓存字段的请求。\n下图是强制缓存和协商缓存的工作流程：\n当使用 ETag 字段实现的协商缓存的过程：\n当浏览器第一次请求访问服务器资源时，服务器会在返回这个资源的同时，在 Response 头部加上 ETag 唯一标识，这个唯一标识的值是根据当前请求的资源生成的； 当浏览器再次请求访问服务器中的该资源时，首先会先检查强制缓存是否过期： 如果没有过期，则直接使用本地缓存； 如果缓存过期了，会在 Request 头部加上 If-None-Match 字段，该字段的值就是 ETag 唯一标识； 服务器再次收到请求后，会根据请求中的 If-None-Match 值与当前请求的资源生成的唯一标识进行比较： 如果值相等，则返回 304 Not Modified，不会返回资源； 如果不相等，则返回 200 状态码和返回资源，并在 Response 头部加上新的 ETag 唯一标识； 如果浏览器收到 304 的请求响应状态码，则会从本地缓存中加载资源，否则更新资源。 HTTP 特性 # 到目前为止，HTTP 常见到版本有 HTTP/1.1，HTTP/2.0，HTTP/3.0，不同版本的 HTTP 特性是不一样的。\n这里先用 HTTP/1.1 版本给大家介绍，其他版本的后续也会介绍。\nHTTP/1.1 的优点有哪些？ # HTTP 最突出的优点是「简单、灵活和易于扩展、应用广泛和跨平台」。\n1. 简单\nHTTP 基本的报文格式就是 header + body，头部信息也是 key-value 简单文本的形式，易于理解，降低了学习和使用的门槛。\n2. 灵活和易于扩展\nHTTP 协议里的各类请求方法、URI/URL、状态码、头字段等每个组成要求都没有被固定死，都允许开发人员自定义和扩充。\n同时 HTTP 由于是工作在应用层（ OSI 第七层），则它下层可以随意变化，比如：\nHTTPS 就是在 HTTP 与 TCP 层之间增加了 SSL/TLS 安全传输层； HTTP/1.1 和 HTTP/2.0 传输协议使用的是 TCP 协议，而到了 HTTP/3.0 传输协议改用了 UDP 协议。 3. 应用广泛和跨平台\n互联网发展至今，HTTP 的应用范围非常的广泛，从台式机的浏览器到手机上的各种 APP，从看新闻、刷贴吧到购物、理财、吃鸡，HTTP 的应用遍地开花，同时天然具有跨平台的优越性。\nHTTP/1.1 的缺点有哪些？ # HTTP 协议里有优缺点一体的双刃剑，分别是「无状态、明文传输」，同时还有一大缺点「不安全」。\n1. 无状态双刃剑\n无状态的好处，因为服务器不会去记忆 HTTP 的状态，所以不需要额外的资源来记录状态信息，这能减轻服务器的负担，能够把更多的 CPU 和内存用来对外提供服务。\n无状态的坏处，既然服务器没有记忆能力，它在完成有关联性的操作时会非常麻烦。\n例如登录-\u0026gt;添加购物车-\u0026gt;下单-\u0026gt;结算-\u0026gt;支付，这系列操作都要知道用户的身份才行。但服务器不知道这些请求是有关联的，每次都要问一遍身份信息。\n这样每操作一次，都要验证信息，这样的购物体验还能愉快吗？别问，问就是酸爽！\n对于无状态的问题，解法方案有很多种，其中比较简单的方式用 Cookie 技术。\nCookie 通过在请求和响应报文中写入 Cookie 信息来控制客户端的状态。\n相当于，在客户端第一次请求后，服务器会下发一个装有客户信息的「小贴纸」，后续客户端请求服务器的时候，带上「小贴纸」，服务器就能认得了了，\n2. 明文传输双刃剑\n明文意味着在传输过程中的信息，是可方便阅读的，比如 Wireshark 抓包都可以直接肉眼查看，为我们调试工作带了极大的便利性。\n但是这正是这样，HTTP 的所有信息都暴露在了光天化日下，相当于信息裸奔。在传输的漫长的过程中，信息的内容都毫无隐私可言，很容易就能被窃取，如果里面有你的账号密码信息，那你号没了。\n3. 不安全\nHTTP 比较严重的缺点就是不安全：\n通信使用明文（不加密），内容可能会被窃听。比如，账号信息容易泄漏，那你号没了。 不验证通信方的身份，因此有可能遭遇伪装。比如，访问假的淘宝、拼多多，那你钱没了。 无法证明报文的完整性，所以有可能已遭篡改。比如，网页上植入垃圾广告，视觉污染，眼没了。 HTTP 的安全问题，可以用 HTTPS 的方式解决，也就是通过引入 SSL/TLS 层，使得在安全上达到了极致。\nHTTP/1.1 的性能如何？ # HTTP 协议是基于 TCP/IP，并且使用了「请求 - 应答」的通信模式，所以性能的关键就在这两点里。\n1. 长连接\n早期 HTTP/1.0 性能上的一个很大的问题，那就是每发起一个请求，都要新建一次 TCP 连接（三次握手），而且是串行请求，做了无谓的 TCP 连接建立和断开，增加了通信开销。\n为了解决上述 TCP 连接问题，HTTP/1.1 提出了长连接的通信方式，也叫持久连接。这种方式的好处在于减少了 TCP 连接的重复建立和断开所造成的额外开销，减轻了服务器端的负载。\n持久连接的特点是，只要任意一端没有明确提出断开连接，则保持 TCP 连接状态。\n当然，如果某个 HTTP 长连接超过一定时间没有任何数据交互，服务端就会主动断开这个连接。\n2. 管道网络传输\nHTTP/1.1 采用了长连接的方式，这使得管道（pipeline）网络传输成为了可能。\n即可在同一个 TCP 连接里面，客户端可以发起多个请求，只要第一个请求发出去了，不必等其回来，就可以发第二个请求出去，可以减少整体的响应时间。\n举例来说，客户端需要请求两个资源。以前的做法是，在同一个 TCP 连接里面，先发送 A 请求，然后等待服务器做出回应，收到后再发出 B 请求。那么，管道机制则是允许浏览器同时发出 A 请求和 B 请求，如下图：\n但是服务器必须按照接收请求的顺序发送对这些管道化请求的响应。\n如果服务端在处理 A 请求时耗时比较长，那么后续的请求的处理都会被阻塞住，这称为「队头堵塞」。\n所以，HTTP/1.1 管道解决了请求的队头阻塞，但是没有解决响应的队头阻塞。\n注意!!!\n实际上 HTTP/1.1 管道化技术不是默认开启，而且浏览器基本都没有支持，所以后面所有文章讨论 HTTP/1.1 都是建立在没有使用管道化的前提。大家知道有这个功能，但是没有被使用就行了。\n3. 队头阻塞\n「请求 - 应答」的模式加剧了 HTTP 的性能问题。\n因为当顺序发送的请求序列中的一个请求因为某种原因被阻塞时，在后面排队的所有请求也一同被阻塞了，会招致客户端一直请求不到数据，这也就是「队头阻塞」，好比上班的路上塞车。\n总之 HTTP/1.1 的性能一般般，后续的 HTTP/2 和 HTTP/3 就是在优化 HTTP 的性能。\nHTTP 与 HTTPS # HTTP 与 HTTPS 有哪些区别？ # HTTP 是超文本传输协议，信息是明文传输，存在安全风险的问题。HTTPS 则解决 HTTP 不安全的缺陷，在 TCP 和 HTTP 网络层之间加入了 SSL/TLS 安全协议，使得报文能够加密传输。\nHTTP 连接建立相对简单，TCP 三次握手之后便可进行 HTTP 的报文传输。而 HTTPS 在 TCP 三次握手之后，还需进行 SSL/TLS 的握手过程，才可进入加密报文传输。\n两者的默认端口不一样，HTTP 默认端口号是 80，HTTPS 默认端口号是 443。\nHTTPS 协议需要向 CA（证书权威机构）申请数字证书，来保证服务器的身份是可信的。\nHTTPS 解决了 HTTP 的哪些问题？ # HTTP 由于是明文传输，所以安全上存在以下三个风险：\n窃听风险，比如通信链路上可以获取通信内容，用户号容易没。 篡改风险，比如强制植入垃圾广告，视觉污染，用户眼容易瞎。 冒充风险，比如冒充淘宝网站，用户钱容易没。 HTTPS 在 HTTP 与 TCP 层之间加入了 SSL/TLS 协议，可以很好的解决了上述的风险：\n信息加密：交互信息无法被窃取，但你的号会因为「自身忘记」账号而没。 校验机制：无法篡改通信内容，篡改了就不能正常显示，但百度「竞价排名」依然可以搜索垃圾广告。 身份证书：证明淘宝是真的淘宝网，但你的钱还是会因为「剁手」而没。 可见，只要自身不做「恶」，SSL/TLS 协议是能保证通信是安全的。\nHTTPS 是如何解决上面的三个风险的？\n混合加密的方式实现信息的机密性，解决了窃听的风险。 摘要算法的方式来实现完整性，它能够为数据生成独一无二的「指纹」，指纹用于校验数据的完整性，解决了篡改的风险。 将服务器公钥放入到数字证书中，解决了冒充的风险。 1. 混合加密\n通过混合加密的方式可以保证信息的机密性，解决了窃听的风险。\nHTTPS 采用的是对称加密和非对称加密结合的「混合加密」方式：\n在通信建立前采用非对称加密的方式交换「会话秘钥」，后续就不再使用非对称加密。 在通信过程中全部使用对称加密的「会话秘钥」的方式加密明文数据。 采用「混合加密」的方式的原因：\n对称加密只使用一个密钥，运算速度快，密钥必须保密，无法做到安全的密钥交换。 非对称加密使用两个密钥：公钥和私钥，公钥可以任意分发而私钥保密，解决了密钥交换问题但速度慢。 2. 摘要算法 + 数字签名\n为了保证传输的内容不被篡改，我们需要对内容计算出一个「指纹」，然后同内容一起传输给对方。\n对方收到后，先是对内容也计算出一个「指纹」，然后跟发送方发送的「指纹」做一个比较，如果「指纹」相同，说明内容没有被篡改，否则就可以判断出内容被篡改了。\n那么，在计算机里会用摘要算法（哈希函数）来计算出内容的哈希值，也就是内容的「指纹」，这个哈希值是唯一的，且无法通过哈希值推导出内容。\n通过哈希算法可以确保内容不会被篡改，但是并不能保证「内容 + 哈希值」不会被中间人替换，因为这里缺少对客户端收到的消息是否来源于服务端的证明。\n举个例子，你想向老师请假，一般来说是要求由家长写一份请假理由并签名，老师才能允许你请假。\n但是你有模仿你爸爸字迹的能力，你用你爸爸的字迹写了一份请假理由然后签上你爸爸的名字，老师一看到这个请假条，查看字迹和签名，就误以为是你爸爸写的，就会允许你请假。\n那作为老师，要如何避免这种情况发生呢？现实生活中的，可以通过电话或视频来确认是否是由父母发出的请假，但是计算机里可没有这种操作。\n那为了避免这种情况，计算机里会用非对称加密算法来解决，共有两个密钥：\n一个是公钥，这个是可以公开给所有人的； 一个是私钥，这个必须由本人管理，不可泄露。 这两个密钥可以双向加解密的，比如可以用公钥加密内容，然后用私钥解密，也可以用私钥加密内容，公钥解密内容。\n流程的不同，意味着目的也不相同：\n公钥加密，私钥解密。这个目的是为了保证内容传输的安全，因为被公钥加密的内容，其他人是无法解密的，只有持有私钥的人，才能解密出实际的内容； 私钥加密，公钥解密。这个目的是为了保证消息不会被冒充，因为私钥是不可泄露的，如果公钥能正常解密出私钥加密的内容，就能证明这个消息是来源于持有私钥身份的人发送的。 一般我们不会用非对称加密来加密实际的传输内容，因为非对称加密的计算比较耗费性能的。\n所以非对称加密的用途主要在于通过「私钥加密，公钥解密」的方式，来确认消息的身份，我们常说的数字签名算法，就是用的是这种方式，不过私钥加密内容不是内容本身，而是对内容的哈希值加密。\n私钥是由服务端保管，然后服务端会向客户端颁发对应的公钥。如果客户端收到的信息，能被公钥解密，就说明该消息是由服务器发送的。\n引入了数字签名算法后，你就无法模仿你爸爸的字迹来请假了，你爸爸手上持有着私钥，你老师持有着公钥。\n这样只有用你爸爸手上的私钥才对请假条进行「签名」，老师通过公钥看能不能解出这个「签名」，如果能解出并且确认内容的完整性，就能证明是由你爸爸发起的请假条，这样老师才允许你请假，否则老师就不认。\n3. 数字证书\n前面我们知道：\n可以通过哈希算法来保证消息的完整性； 可以通过数字签名来保证消息的来源可靠性（能确认消息是由持有私钥的一方发送的）； 但是这还远远不够，还缺少身份验证的环节，万一公钥是被伪造的呢？\n还是拿请假的例子，虽然你爸爸持有私钥，老师通过是否能用公钥解密来确认这个请假条是不是来源你父亲的。\n但是我们还可以自己伪造出一对公私钥啊！\n你找了个夜晚，偷偷把老师桌面上和你爸爸配对的公钥，换成了你的公钥，那么下次你在请假的时候，你继续模仿你爸爸的字迹写了个请假条，然后用你的私钥做个了「数字签名」。\n但是老师并不知道自己的公钥被你替换过了，所以他还是按照往常一样用公钥解密，由于这个公钥和你的私钥是配对的，老师当然能用这个被替换的公钥解密出来，并且确认了内容的完整性，于是老师就会以为是你父亲写的请假条，又允许你请假了。\n好家伙，为了一个请假，真的是斗智斗勇。\n后面你的老师和父亲发现了你伪造公私钥的事情后，决定重新商量一个对策来应对你这个臭家伙。\n正所谓魔高一丈，道高一尺。\n既然伪造公私钥那么随意，所以你爸把他的公钥注册到警察局，警察局用他们自己的私钥对你父亲的公钥做了个数字签名，然后把你爸爸的「个人信息 + 公钥 + 数字签名」打包成一个数字证书，也就是说这个数字证书包含你爸爸的公钥。\n这样，你爸爸如果因为家里确实有事要向老师帮你请假的时候，不仅会用自己的私钥对内容进行签名，还会把数字证书给到老师。\n老师拿到了数字证书后，首先会去警察局验证这个数字证书是否合法，因为数字证书里有警察局的数字签名，警察局要验证证书合法性的时候，用自己的公钥解密，如果能解密成功，就说明这个数字证书是在警察局注册过的，就认为该数字证书是合法的，然后就会把数字证书里头的公钥（你爸爸的）给到老师。\n由于通过警察局验证了数字证书是合法的，那么就能证明这个公钥就是你父亲的，于是老师就可以安心的用这个公钥解密出清教条，如果能解密出，就证明是你爸爸写的请假条。\n正是通过了一个权威的机构来证明你爸爸的身份，所以你的伪造公私钥这个小伎俩就没用了。\n在计算机里，这个权威的机构就是 CA（数字证书认证机构），将服务器公钥放在数字证书（由数字证书认证机构颁发）中，只要证书是可信的，公钥就是可信的。\n数字证书的工作流程，我也画了一张图，方便大家理解：\n通过数字证书的方式保证服务器公钥的身份，解决冒充的风险。\nHTTPS 是如何建立连接的？其间交互了什么？ # SSL/TLS 协议基本流程：\n客户端向服务器索要并验证服务器的公钥。 双方协商生产「会话秘钥」。 双方采用「会话秘钥」进行加密通信。 前两步也就是 SSL/TLS 的建立过程，也就是 TLS 握手阶段。\nTLS 的「握手阶段」涉及四次通信，使用不同的密钥交换算法，TLS 握手流程也会不一样的，现在常用的密钥交换算法有两种：RSA 算法 和 ECDHE 算法。\n基于 RSA 算法的 TLS 握手过程比较容易理解，所以这里先用这个给大家展示 TLS 握手过程，如下图：\nTLS 协议建立的详细流程：\n1. ClientHello\n首先，由客户端向服务器发起加密通信请求，也就是 ClientHello 请求。\n在这一步，客户端主要向服务器发送以下信息：\n（1）客户端支持的 TLS 协议版本，如 TLS 1.2 版本。\n（2）客户端生产的随机数（Client Random），后面用于生成「会话秘钥」条件之一。\n（3）客户端支持的密码套件列表，如 RSA 加密算法。\n2. SeverHello\n服务器收到客户端请求后，向客户端发出响应，也就是 ServerHello。服务器回应的内容有如下内容：\n（1）确认 TLS 协议版本，如果浏览器不支持，则关闭加密通信。\n（2）服务器生产的随机数（Server Random），也是后面用于生产「会话秘钥」条件之一。\n（3）确认的密码套件列表，如 RSA 加密算法。\n（4）服务器的数字证书。\n3.客户端回应\n客户端收到服务器的回应之后，首先通过浏览器或者操作系统中的 CA 公钥，确认服务器的数字证书的真实性。\n如果证书没有问题，客户端会从数字证书中取出服务器的公钥，然后使用它加密报文，向服务器发送如下信息：\n（1）一个随机数（pre-master key）。该随机数会被服务器公钥加密。\n（2）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。\n（3）客户端握手结束通知，表示客户端的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供服务端校验。\n上面第一项的随机数是整个握手阶段的第三个随机数，会发给服务端，所以这个随机数客户端和服务端都是一样的。\n服务器和客户端有了这三个随机数（Client Random、Server Random、pre-master key），接着就用双方协商的加密算法，各自生成本次通信的「会话秘钥」。\n4. 服务器的最后回应\n服务器收到客户端的第三个随机数（pre-master key）之后，通过协商的加密算法，计算出本次通信的「会话秘钥」。\n然后，向客户端发送最后的信息：\n（1）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。\n（2）服务器握手结束通知，表示服务器的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供客户端校验。\n至此，整个 TLS 的握手阶段全部结束。接下来，客户端与服务器进入加密通信，就完全是使用普通的 HTTP 协议，只不过用「会话秘钥」加密内容。\n客户端校验数字证书的流程是怎样的？\n接下来，详细说一下实际中数字证书签发和验证流程。\n如下图图所示，为数字证书签发和验证流程：\nCA 签发证书的过程，如上图左边部分：\n首先 CA 会把持有者的公钥、用途、颁发者、有效时间等信息打成一个包，然后对这些信息进行 Hash 计算，得到一个 Hash 值； 然后 CA 会使用自己的私钥将该 Hash 值加密，生成 Certificate Signature，也就是 CA 对证书做了签名； 最后将 Certificate Signature 添加在文件证书上，形成数字证书； 客户端校验服务端的数字证书的过程，如上图右边部分：\n首先客户端会使用同样的 Hash 算法获取该证书的 Hash 值 H1； 通常浏览器和操作系统中集成了 CA 的公钥信息，浏览器收到证书后可以使用 CA 的公钥解密 Certificate Signature 内容，得到一个 Hash 值 H2； 最后比较 H1 和 H2，如果值相同，则为可信赖的证书，否则则认为证书不可信。 但事实上，证书的验证过程中还存在一个证书信任链的问题，因为我们向 CA 申请的证书一般不是根证书签发的，而是由中间证书签发的，比如百度的证书，从下图你可以看到，证书的层级有三级：\n对于这种三级层级关系的证书的验证过程如下：\n客户端收到 baidu.com 的证书后，发现这个证书的签发者不是根证书，就无法根据本地已有的根证书中的公钥去验证 baidu.com 证书是否可信。于是，客户端根据 baidu.com 证书中的签发者，找到该证书的颁发机构是\u0026quot;GlobalSign Organization Validation CA - SHA256 - G2\u0026quot;，然后向 CA 请求该中间证书。 请求到证书后发现\u0026quot;GlobalSign Organization Validation CA - SHA256 - G2\u0026quot;证书是由\u0026quot;GlobalSign Root CA\u0026quot;签发的，由于\u0026quot;GlobalSign Root CA\u0026quot;没有再上级签发机构，说明它是根证书，也就是自签证书。应用软件会检查此证书有否已预载于根证书清单上，如果有，则可以利用根证书中的公钥去验证\u0026quot;GlobalSign Organization Validation CA - SHA256 - G2\u0026quot;证书，如果发现验证通过，就认为该中间证书是可信的。 \u0026ldquo;GlobalSign Organization Validation CA - SHA256 - G2\u0026quot;证书被信任后，可以使用\u0026quot;GlobalSign Organization Validation CA - SHA256 - G2\u0026quot;证书中的公钥去验证 baidu.com 证书的可信性，如果验证通过，就可以信任 baidu.com 证书。 在这四个步骤中，最开始客户端只信任根证书 GlobalSign Root CA 证书的，然后\u0026quot;GlobalSign Root CA\u0026quot;证书信任\u0026quot;GlobalSign Organization Validation CA - SHA256 - G2\u0026quot;证书，而\u0026quot;GlobalSign Organization Validation CA - SHA256 - G2\u0026quot;证书又信任 baidu.com 证书，于是客户端也信任 baidu.com 证书。\n总括来说，由于用户信任 GlobalSign，所以由 GlobalSign 所担保的 baidu.com 可以被信任，另外由于用户信任操作系统或浏览器的软件商，所以由软件商预载了根证书的 GlobalSign 都可被信任。\n操作系统里一般都会内置一些根证书，比如我的 MAC 电脑里内置的根证书有这么多：\n这样的一层层地验证就构成了一条信任链路，整个证书信任链验证流程如下图所示：\n最后一个问题，为什么需要证书链这么麻烦的流程？Root CA 为什么不直接颁发证书，而是要搞那么多中间层级呢？\n这是为了确保根证书的绝对安全性，将根证书隔离地越严格越好，不然根证书如果失守了，那么整个信任链都会有问题。\nHTTPS 的应用数据是如何保证完整性的？ # TLS 在实现上分为握手协议和记录协议两层：\nTLS 握手协议就是我们前面说的 TLS 四次握手的过程，负责协商加密算法和生成对称密钥，后续用此密钥来保护应用程序数据（即 HTTP 数据）； TLS 记录协议负责保护应用程序数据并验证其完整性和来源，所以对 HTTP 数据加密是使用记录协议； TLS 记录协议主要负责消息（HTTP 数据）的压缩，加密及数据的认证，过程如下图：\n具体过程如下：\n首先，消息被分割成多个较短的片段，然后分别对每个片段进行压缩。\n接下来，经过压缩的片段会被加上消息认证码（MAC 值，这个是通过哈希算法生成的），这是为了保证完整性，并进行数据的认证。通过附加消息认证码的 MAC 值，可以识别出篡改。与此同时，为了防止重放攻击，在计算消息认证码时，还加上了片段的编码。\n再接下来，经过压缩的片段再加上消息认证码会一起通过对称密码进行加密。\n最后，上述经过加密的数据再加上由数据类型、版本号、压缩后的长度组成的报头就是最终的报文数据。\n记录协议完成后，最终的报文数据将传递到传输控制协议 (TCP) 层进行传输。\n如果你想详细了解记录协议是如何分片、压缩、计算 MAC 值、分组加密，可以看这篇： 理解 SSL/TLS 系列 (四) 记录协议\nHTTPS 一定安全可靠吗？ # 之前有读者在字节面试的时候，被问到：HTTPS 一定安全可靠吗？\n前两天字节一面时，面试官问我https一定安全可靠吗，如果有假基站起了转发全部信息的作用，这样是不是假基站就获取到全部信息了，从而造成信息泄露。这个该怎么回答呀？\n这个问题的场景是这样的：客户端通过浏览器向服务端发起 HTTPS 请求时，被「假基站」转发到了一个「中间人服务器」，于是客户端是和「中间人服务器」完成了 TLS 握手，然后这个「中间人服务器」再与真正的服务端完成 TLS 握手。\n具体过程如下：\n客户端向服务端发起 HTTPS 建立连接请求时，然后被「假基站」转发到了一个「中间人服务器」，接着中间人向服务端发起 HTTPS 建立连接请求，此时客户端与中间人进行 TLS 握手，中间人与服务端进行 TLS 握手； 在客户端与中间人进行 TLS 握手过程中，中间人会发送自己的公钥证书给客户端，客户端验证证书的真伪，然后从证书拿到公钥，并生成一个随机数，用公钥加密随机数发送给中间人，中间人使用私钥解密，得到随机数，此时双方都有随机数，然后通过算法生成对称加密密钥（A），后续客户端与中间人通信就用这个对称加密密钥来加密数据了。 在中间人与服务端进行 TLS 握手过程中，服务端会发送从 CA 机构签发的公钥证书给中间人，从证书拿到公钥，并生成一个随机数，用公钥加密随机数发送给服务端，服务端使用私钥解密，得到随机数，此时双方都有随机数，然后通过算法生成对称加密密钥（B），后续中间人与服务端通信就用这个对称加密密钥来加密数据了。 后续的通信过程中，中间人用对称加密密钥（A）解密客户端的 HTTPS 请求的数据，然后用对称加密密钥（B）加密 HTTPS 请求后，转发给服务端，接着服务端发送 HTTPS 响应数据给中间人，中间人用对称加密密钥（B）解密 HTTPS 响应数据，然后再用对称加密密钥（A）加密后，转发给客户端。 从客户端的角度看，其实并不知道网络中存在中间人服务器这个角色。那么中间人就可以解开浏览器发起的 HTTPS 请求里的数据，也可以解开服务端响应给浏览器的 HTTPS 响应数据。相当于，中间人能够\u0026quot;偷看\u0026quot;浏览器与服务端之间的 HTTPS 请求和响应的数据。\n但是要发生这种场景是有前提的，前提是用户点击接受了中间人服务器的证书。\n中间人服务器与客户端在 TLS 握手过程中，实际上发送了自己伪造的证书给浏览器，而这个伪造的证书是能被浏览器（客户端）识别出是非法的，于是就会提醒用户该证书存在问题。\n如果用户执意点击「继续浏览此网站」，相当于用户接受了中间人伪造的证书，那么后续整个 HTTPS 通信都能被中间人监听了。\n所以，这其实并不能说 HTTPS 不够安全，毕竟浏览器都已经提示证书有问题了，如果用户坚决要访问，那不能怪 HTTPS，得怪自己手贱。\n另外，如果你的电脑中毒了，被恶意导入了中间人的根证书，那么在验证中间人的证书的时候，由于你操作系统信任了中间人的根证书，那么等同于中间人的证书是合法的，这种情况下，浏览器是不会弹出证书存在问题的风险提醒的。\n这其实也不关 HTTPS 的事情，是你电脑中毒了才导致 HTTPS 数据被中间人劫持的。\n所以，HTTPS 协议本身到目前为止还是没有任何漏洞的，即使你成功进行中间人攻击，本质上是利用了客户端的漏洞（用户点击继续访问或者被恶意导入伪造的根证书），并不是 HTTPS 不够安全。\n为什么抓包工具能截取 HTTPS 数据？\n很多抓包工具 之所以可以明文看到 HTTPS 数据，工作原理与中间人一致的。\n对于 HTTPS 连接来说，中间人要满足以下两点，才能实现真正的明文代理：\n中间人，作为客户端与真实服务端建立连接这一步不会有问题，因为服务端不会校验客户端的身份； 中间人，作为服务端与真实客户端建立连接，这里会有客户端信任服务端的问题，也就是服务端必须有对应域名的私钥； 中间人要拿到私钥只能通过如下方式：\n去网站服务端拿到私钥； 去 CA 处拿域名签发私钥； 自己签发受浏览器信任的证书； 不用解释，抓包工具只能使用第三种方式取得中间人的身份。\n因此使用抓包工具进行 HTTPS 抓包的时候，抓包工具会生成根证书，导入到客户端系统的 受信任的根证书列表 中，这里的根证书实际上起认证中心（CA）的作用。\n随后抓包工具使用该根证书签发域名的证书，因为根证书受信任，域名的证书同样会被浏览器信任。也就是抓包工具给自己创建了一个认证中心 CA，客户端拿着中间人（抓包工具）签发的证书去中间人（抓包工具）自己的 CA 做认证，这个证书当然被认为是有效的。\n如何避免被中间人抓取数据？\n我们要保证自己电脑的安全，不要被病毒乘虚而入，而且也不要点击任何证书非法的网站，这样 HTTPS 数据就不会被中间人截取到了。\n当然，我们还可以通过 HTTPS 双向认证来避免这种问题。\n一般我们的 HTTPS 是单向认证，客户端只会验证了服务端的身份，但是服务端并不会验证客户端的身份。\n如果用了双向认证方式，不仅客户端会验证服务端的身份，而且服务端也会验证客户端的身份。服务端一旦验证到请求自己的客户端为不可信任的，服务端就拒绝继续通信，客户端如果发现服务端为不可信任的，那么也中止通信。\nHTTP/1.1、HTTP/2、HTTP/3 演变 # HTTP/1.1 相比 HTTP/1.0 提高了什么性能？ # HTTP/1.1 相比 HTTP/1.0 性能上的改进：\n使用长连接的方式改善了 HTTP/1.0 短连接造成的性能开销。 支持管道（pipeline）网络传输，只要第一个请求发出去了，不必等其回来，就可以发第二个请求出去，可以减少整体的响应时间。 但 HTTP/1.1 还是有性能瓶颈：\n请求 / 响应头部（Header）未经压缩就发送，首部信息越多延迟越大。只能压缩 Body 的部分； 发送冗长的首部。每次互相发送相同的首部造成的浪费较多； 服务器是按请求的顺序响应的，如果服务器响应慢，会招致客户端一直请求不到数据，也就是队头阻塞； 没有请求优先级控制； 请求只能从客户端开始，服务器只能被动响应。 HTTP/2 做了什么优化？ # HTTP/2 协议是基于 HTTPS 的，所以 HTTP/2 的安全性也是有保障的。\n那 HTTP/2 相比 HTTP/1.1 性能上的改进：\n头部压缩 二进制格式 并发传输 服务器主动推送资源 1. 头部压缩\nHTTP/2 会压缩头（Header）如果你同时发出多个请求，他们的头是一样的或是相似的，那么，协议会帮你消除重复的部分。\n这就是所谓的 HPACK 算法：在客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索引号，以后就不发送同样字段了，只发送索引号，这样就提高速度了。\n2. 二进制格式\nHTTP/2 不再像 HTTP/1.1 里的纯文本形式的报文，而是全面采用了二进制格式，头信息和数据体都是二进制，并且统称为帧（frame）：头信息帧（Headers Frame）和数据帧（Data Frame）。\n这样虽然对人不友好，但是对计算机非常友好，因为计算机只懂二进制，那么收到报文后，无需再将明文的报文转成二进制，而是直接解析二进制报文，这增加了数据传输的效率。\n比如状态码 200，在 HTTP/1.1 是用 \u0026lsquo;2\u0026rsquo;\u0026lsquo;0\u0026rsquo;\u0026lsquo;0\u0026rsquo; 三个字符来表示（二进制：00110010 00110000 00110000），共用了 3 个字节，如下图\n在 HTTP/2 对于状态码 200 的二进制编码是 10001000，只用了 1 字节就能表示，相比于 HTTP/1.1 节省了 2 个字节，如下图：\nHeader: :status: 200 OK 的编码内容为：1000 1000，那么表达的含义是什么呢？\n最前面的 1 标识该 Header 是静态表中已经存在的 KV。 在静态表理，\u0026quot;:status: 200 ok\u0026quot;静态表编码是 8，二进制即是 1000。 因此，整体加起来就是 1000 1000。\n3. 并发传输\n我们都知道 HTTP/1.1 的实现是基于请求 - 响应模型的。同一个连接中，HTTP 完成一个事务（请求与响应），才能处理下一个事务，也就是说在发出请求等待响应的过程中，是没办法做其他事情的，如果响应迟迟不来，那么后续的请求是无法发送的，也造成了队头阻塞的问题。\n而 HTTP/2 就很牛逼了，引出了 Stream 概念，多个 Stream 复用在一条 TCP 连接。\n从上图可以看到，1 个 TCP 连接包含多个 Stream，Stream 里可以包含 1 个或多个 Message，Message 对应 HTTP/1 中的请求或响应，由 HTTP 头部和包体构成。Message 里包含一条或者多个 Frame，Frame 是 HTTP/2 最小单位，以二进制压缩格式存放 HTTP/1 中的内容（头部和包体）。\n针对不同的 HTTP 请求用独一无二的 Stream ID 来区分，接收端可以通过 Stream ID 有序组装成 HTTP 消息，不同 Stream 的帧是可以乱序发送的，因此可以并发不同的 Stream，也就是 HTTP/2 可以并行交错地发送请求和响应。\n比如下图，服务端并行交错地发送了两个响应：Stream 1 和 Stream 3，这两个 Stream 都是跑在一个 TCP 连接上，客户端收到后，会根据相同的 Stream ID 有序组装成 HTTP 消息。\n4、服务器推送\nHTTP/2 还在一定程度上改善了传统的「请求 - 应答」工作模式，服务端不再是被动地响应，可以主动向客户端发送消息。\n客户端和服务器双方都可以建立 Stream，Stream ID 也是有区别的，客户端建立的 Stream 必须是奇数号，而服务器建立的 Stream 必须是偶数号。\n比如下图，Stream 1 是客户端向服务端请求的资源，属于客户端建立的 Stream，所以该 Stream 的 ID 是奇数（数字 1）；Stream 2 和 4 都是服务端主动向客户端推送的资源，属于服务端建立的 Stream，所以这两个 Stream 的 ID 是偶数（数字 2 和 4）。\n再比如，客户端通过 HTTP/1.1 请求从服务器那获取到了 HTML 文件，而 HTML 可能还需要依赖 CSS 来渲染页面，这时客户端还要再发起获取 CSS 文件的请求，需要两次消息往返，如下图左边部分：\n如上图右边部分，在 HTTP/2 中，客户端在访问 HTML 时，服务器可以直接主动推送 CSS 文件，减少了消息传递的次数。\nHTTP/2 有什么缺陷？\nHTTP/2 通过 Stream 的并发能力，解决了 HTTP/1 队头阻塞的问题，看似很完美了，但是 HTTP/2 还是存在\u0026quot;队头阻塞\u0026quot;的问题，只不过问题不是在 HTTP 这一层面，而是在 TCP 这一层。\nHTTP/2 是基于 TCP 协议来传输数据的，TCP 是字节流协议，TCP 层必须保证收到的字节数据是完整且连续的，这样内核才会将缓冲区里的数据返回给 HTTP 应用，那么当「前 1 个字节数据」没有到达时，后收到的字节数据只能存放在内核缓冲区里，只有等到这 1 个字节数据到达时，HTTP/2 应用层才能从内核中拿到数据，这就是 HTTP/2 队头阻塞问题。\n举个例子，如下图：\n图中发送方发送了很多个 packet，每个 packet 都有自己的序号，你可以认为是 TCP 的序列号，其中 packet 3 在网络中丢失了，即使 packet 4-6 被接收方收到后，由于内核中的 TCP 数据不是连续的，于是接收方的应用层就无法从内核中读取到，只有等到 packet 3 重传后，接收方的应用层才可以从内核中读取到数据，这就是 HTTP/2 的队头阻塞问题，是在 TCP 层面发生的。\n所以，一旦发生了丢包现象，就会触发 TCP 的重传机制，这样在一个 TCP 连接中的所有的 HTTP 请求都必须等待这个丢了的包被重传回来。\nHTTP/3 做了哪些优化？ # 前面我们知道了 HTTP/1.1 和 HTTP/2 都有队头阻塞的问题：\nHTTP/1.1 中的管道（pipeline）虽然解决了请求的队头阻塞，但是没有解决响应的队头阻塞，因为服务端需要按顺序响应收到的请求，如果服务端处理某个请求消耗的时间比较长，那么只能等响应完这个请求后，才能处理下一个请求，这属于 HTTP 层队头阻塞。 HTTP/2 虽然通过多个请求复用一个 TCP 连接解决了 HTTP 的队头阻塞，但是一旦发生丢包，就会阻塞住所有的 HTTP 请求，这属于 TCP 层队头阻塞。 HTTP/2 队头阻塞的问题是因为 TCP，所以 HTTP/3 把 HTTP 下层的 TCP 协议改成了 UDP！\nUDP 发送是不管顺序，也不管丢包的，所以不会出现像 HTTP/2 队头阻塞的问题。大家都知道 UDP 是不可靠传输的，但基于 UDP 的 QUIC 协议 可以实现类似 TCP 的可靠性传输。\nQUIC 有以下 3 个特点。\n无队头阻塞 更快的连接建立 连接迁移 1、无队头阻塞\nQUIC 协议也有类似 HTTP/2 Stream 与多路复用的概念，也是可以在同一条连接上并发传输多个 Stream，Stream 可以认为就是一条 HTTP 请求。\nQUIC 有自己的一套机制可以保证传输的可靠性的。当某个流发生丢包时，只会阻塞这个流，其他流不会受到影响，因此不存在队头阻塞问题。这与 HTTP/2 不同，HTTP/2 只要某个流中的数据包丢失了，其他流也会因此受影响。\n所以，QUIC 连接上的多个 Stream 之间并没有依赖，都是独立的，某个流发生丢包了，只会影响该流，其他流不受影响。\n2、更快的连接建立\n对于 HTTP/1 和 HTTP/2 协议，TCP 和 TLS 是分层的，分别属于内核实现的传输层、openssl 库实现的表示层，因此它们难以合并在一起，需要分批次来握手，先 TCP 握手，再 TLS 握手。\nHTTP/3 在传输数据前虽然需要 QUIC 协议握手，但这个握手过程只需要 1 RTT，握手的目的是为确认双方的「连接 ID」，连接迁移就是基于连接 ID 实现的。\n但是 HTTP/3 的 QUIC 协议并不是与 TLS 分层，而是 QUIC 内部包含了 TLS，它在自己的帧会携带 TLS 里的\u0026quot;记录\u0026rdquo;，再加上 QUIC 使用的是 TLS/1.3，因此仅需 1 个 RTT 就可以「同时」完成建立连接与密钥协商，如下图：\n甚至，在第二次连接的时候，应用数据包可以和 QUIC 握手信息（连接信息 + TLS 信息）一起发送，达到 0-RTT 的效果。\n如下图右边部分，HTTP/3 当会话恢复时，有效负载数据与第一个数据包一起发送，可以做到 0-RTT（下图的右下角）：\n3、连接迁移\n基于 TCP 传输协议的 HTTP 协议，由于是通过四元组（源 IP、源端口、目的 IP、目的端口）确定一条 TCP 连接。\n那么当移动设备的网络从 4G 切换到 WIFI 时，意味着 IP 地址变化了，那么就必须要断开连接，然后重新建立连接。而建立连接的过程包含 TCP 三次握手和 TLS 四次握手的时延，以及 TCP 慢启动的减速过程，给用户的感觉就是网络突然卡顿了一下，因此连接的迁移成本是很高的。\n而 QUIC 协议没有用四元组的方式来\u0026quot;绑定\u0026quot;连接，而是通过连接 ID 来标记通信的两个端点，客户端和服务器可以各自选择一组 ID 来标记自己，因此即使移动设备的网络变化后，导致 IP 地址变化了，只要仍保有上下文信息（比如连接 ID、TLS 密钥等），就可以\u0026quot;无缝\u0026quot;地复用原连接，消除重连的成本，没有丝毫卡顿感，达到了连接迁移的功能。\n所以，QUIC 是一个在 UDP 之上的伪 TCP + TLS + HTTP/2 的多路复用的协议。\nQUIC 是新协议，对于很多网络设备，根本不知道什么是 QUIC，只会当做 UDP，这样会出现新的问题，因为有的网络设备是会丢掉 UDP 包的，而 QUIC 是基于 UDP 实现的，那么如果网络设备无法识别这个是 QUIC 包，那么就会当作 UDP 包，然后被丢弃。\nHTTP/3 现在普及的进度非常的缓慢，不知道未来 UDP 是否能够逆袭 TCP。\n参考资料：\n[1] 上野 宣。图解 HTTP.人民邮电出版社。\n[2] 罗剑锋。透视 HTTP 协议。极客时间。\n[3] 陈皓.HTTP 的前世今。酷壳 CoolShell.https://coolshell.cn/articles/19840.html\n[4] 阮一峰.HTTP 协议入门。阮一峰的网络日志.http://www.ruanyifeng.com/blog/2016/08/http.html\n[5] 小林coding。\n","date":"31 October 2023","permalink":"/posts/reviews/network/http123-https/","section":"博客","summary":"HTTP # HTTP 基本概念 # HTTP 是什么？ # HTTP 是超文本传输协议，也就是HyperText Transfer Protocol。","title":"HTTP-1 HTTPS HTTP-2 HTTP-3"},{"content":"","date":"31 October 2023","permalink":"/tags/websocket/","section":"Tags","summary":"","title":"Websocket"},{"content":"","date":"31 October 2023","permalink":"/tags/langium/","section":"Tags","summary":"","title":"Langium"},{"content":"Usage # 1 Langium download # https://www.npmjs.com/package/langium demo: https://langium.org/docs/getting-started/ note1：第一次按以上流程创建 DSL，HELLO-WORLD 项目在/Users/{用户名}/目录下 note2: VScode 下安装 code 指令。 Shift+Command+P调起命令窗口，输入shell Command，下方出现 Install 'code' command in PATH 选项，点击以安装 vscode extension: langium 2 Langium Concepts # 1. The Grammar Language\ndocument: https://langium.org/docs/grammar-language/ Language Declaration: Langium 语法文件以声明语言名称的标题开头 Terminal Rules: Langium 解析器内置流基于Javascript Regular Expressions的 lexer，也允许使用EBNF表达式。但是建议使用 javascript 正则表达式，因为在 langium 内部将 EBNF 转换成了正则表达式 Parser Rules: Parser Rules 向 parser 指示哪些令牌序列是有效的 The Entry Rule: 解析步骤起点的 Parser Rules，从关键字 entry 开始，并匹配其他 Parser Rules 2. 目录结构\n以 https://langium.org/docs/getting-started/的demo为例：\nsrc/language-server/hello-world.langium: 语法规则文件 document: https://langium.org/tutorials/writing_a_grammar/ src/language-server/hello-world-validator.ts: 合法性检验，当用户输入相同函数名、关键字输入错误\u0026hellip;等一系列不符合语法规则时，编辑器能给出相应的错误提醒 document: https://langium.org/tutorials/validation/ src/cli/index.ts: 语言的命令行界面的一般布局，并允许注册特定命令 document: https://langium.org/tutorials/customizing_cli/ src/language-server/main-browser.ts: 为的语言服务器创建一个新的入口点等 document: https://langium.org/tutorials/langium_and_monaco/ src/static/index.html,src/static/styles.css: 静态页面，在与 Monaco Editor 集成，在 web 上运行会用到。 document: https://langium.org/tutorials/langium_and_monaco/ src/web/index.ts: 网络的生成器端点 document: https://langium.org/tutorials/generation_in_the_web/ 3. Extension\n基于 Langium 的语言构建 VSIX 扩展（VSCode 扩展）\ndocument: https://langium.org/tutorials/building_an_extension/ 4. running Langium in the web\n在网络中将 Langium 与 Monaco Editor 集成，无需后端\ndocument: https://langium.org/tutorials/generation_in_the_web/ Resource # document：https://langium.org/docs/ github：https://github.com/langium/langium VS Code extension API：https://code.visualstudio.com/api/language-extensions/overview Typescript document：https://typescript.bootcss.com/basic-types.html ","date":"31 October 2023","permalink":"/posts/language/dsl/langium/","section":"博客","summary":"langium相关","title":"langium简介"},{"content":"","date":"31 October 2023","permalink":"/tags/vscode/","section":"Tags","summary":"","title":"Vscode"},{"content":"Resource # Protocol specification https://microsoft.github.io/language-server-protocol/ Syntax highlight guide https://code.visualstudio.com/api/language-extensions/syntax-highlight-guide Language Server Tools https://langserver.org/ Language Server Protocol https://microsoft.github.io/language-server-protocol/ VSCode Syntax Highlight Guide https://code.visualstudio.com/api/language-extensions/syntax-highlight-guide Scope Lists https://macromates.com/manual/en/language_grammars https://www.apeth.com/nonblog/stories/textmatebundle.html ","date":"31 October 2023","permalink":"/posts/language/dsl/vscode-language-server/","section":"博客","summary":"Resource # Protocol specification https://microsoft.","title":"vscode-language-server"},{"content":"","date":"31 October 2023","permalink":"/tags/xtext/","section":"Tags","summary":"","title":"Xtext"},{"content":"Usage # 项目创建 # XText 开发一个新的语言\n定义 xtext 文件 dsl.xtext 包括语法定义，语义（Cross-Reference）定义 生成模型代码 XText 根据 dsl.xtext 在 src-gen 目录下，生成 AST 节点模型类 parser，semantic analysis 等阶段需要的 类，如 GrammarAccess，Scope 等 定义 GenerateDsl.mwe2 定义生成流程 generateXtendStub = false 禁用 xtend 模板文件生成 编写 Language Implementation 编写 IDE Features 项目初始化\n使用 Eclipse 开发 Xtext 应用能够得到最大化的支持，包括 xtext，xtext 语言支持，Editor 支持，自动生成 Artifact 等 由于 Eclipse 一些使用上的原因，建议将 Xtext 当做一个纯 Java 框架进行使用，通过 Gradle 自动根据 xtext 生成源代码，这样能够使用 IDEA 进行开发。 目录结构\nxxx.dsl 定义 DSL 的核心处理类，包括 Format，Scope，Validation，Code Generation xxx.dsl.ide 定义与 IDE 相关的处理类，包括 Hover，QuickFix 等 与 Language Server 相关的业务逻辑 xxx.dsl.tests 测试类 Concepts # Xtend # 一种类似 Java 的语言，包含一些语法糖，如在 Code Generation 使用模板语言定义代码生成过程 Xtext 文档和教程中大量使用该语言，但由于 xtent 在 IDEA 中没有支持，建议只使用 Java 进行开发 Inject # Xtext 框架使用 com.google.inject 库进行依赖注入，注册不同的语言服务（如 GrammarAccess，Formatter） Grammar Language # https://www.eclipse.org/Xtext/documentation/301_grammarlanguage.html 名为 xtext 的 DSL，用于定义语言语法，以及与语义对象的映射关系 语法对象：抽象语法树的节点，与源代码对应，Parsing 过程将字符串转换为多个语法对象构造的树 语义对象：语义分析处理的对象，比如类型对象，模块对象等 Xtext 框架设计原则是，用 xtext 定义语法以及语义对象，在解析过程中，同时构造出语义对象。其他的语言服务将处理这些语义对象 语义对象使用 EMF Ecore 模型作为规范，见 https://www.eclipse.org/Xtext/documentation/308_emf_integration.html#model-metamodel Module # https://www.eclipse.org/Xtext/documentation/302_configuration.html 如 DslRuntimeModule 类似于 依赖注入中的 Module，提供当前 DSL 各种语言服务的类 要使用 DSL 的语言功能（如编译器或 Language Server），通过 Injector 获取 Module，然后调用对应的方法使用 Language Implementation # https://www.eclipse.org/Xtext/documentation/303_runtime_concepts.html\n实现语言的各种功能\nCodeGeneration\n用于将 AST 解释执行，或翻译为其他代码 如 Model -\u0026gt; Java, YAML 等 实现 IGenerator2 接口 关注对象 当前待生成的资源（语法树子树根节点） 输出管理（输出的内容，输出的文件路径） 引用其他的对象 Validation\n静态分析实现 Lint，检测模型是否满足约束 静态分析输出 Errors 与 Warnings，通过 Resource.getErrors() 与 Resource.getWarnings() 获取 类别 Automatic Validation Lexer/Parser: 语法校验 Linker：交叉引用校验 利用 Scope（符号表）等信息，执行校验 可能会跨多个模块 Serializer：Concrete Syntax Validation collapsed:: true 具体的语法验证，当验证通过，说明模型可以被正确序列化 TODO: 使用场景 用于模型序列化后，再反序列化回来？ Custom Validation 实现 AbstractDslValidator Linking\n实现交叉引用 需要完成两步 在 xtext grammar 文件中，定义交叉引用 通过 Scoping API 声明 Linking 的语义 Lazy Link Xtext 建议使用 Lazy Link 通过创建 Proxy 对象实现，当实际访问该 Proxy 对象时，才进行 resolve Scoping 通过 Scoping API 定义如何通过引用找到引用的对象 Tips # 安装过程可以记录的点\n安装 Eclipse \u0026amp; XText 下载 Eclipse https://www.eclipse.org/Xtext/download.html 选择 Eclipse IDE for Java Developer 安装 在 Installer 中，右上角设置 - Adavanced Mode - 连接图标 - 设置代理 - 切换回 Easy Mode 代理选择全局模式 安装完成后，可设置 Eclipse HTTP 代理 (似乎没用) 在 Eclipse 中添加 Xtext update URL Help -\u0026gt; Install New Software \u0026hellip; -\u0026gt; 复制下面链接 -\u0026gt; Add \u0026hellip; https://download.eclipse.org/modeling/tmf/xtext/updates/composite/releases/ Or https://mirrors.tuna.tsinghua.edu.cn/eclipse/modeling/tmf/xtext/updates/composite/releases/ 添加完成后，选择 xtext，拉取 metadata，拉取完成后，Software sites 出现多个 URL，同样替换源 添加下列 URL https://mirrors.ustc.edu.cn/eclipse/modeling/tmf/xtext/updates/releases/ 替换源 Help -\u0026gt; Install New Software.. -\u0026gt; Available Software sites 将 https://download.eclipse.org 全部替换为 https://mirrors.tuna.tsinghua.edu.cn/eclipse 需要全部替换，否则每次 fetching children 都可能从某个 eclipse.org 中拉取，影响速度 Eclipse 设置代理 需要使用 http 代理，socks5 代理有些问题 Window -\u0026gt; Preference -\u0026gt; Network 设置 http 代理，选择 Active Provider 为 manual 另一个方法是，修改 eclipse.ini 位于 ~/eclipse\\java-2023-03\\eclipse VSCode 插件 grammarcraft.xtend-lang grammarcraft.xtext-lang Resource # Document https://www.eclipse.org/Xtext/documentation/301_grammarlanguage.html Runtime concepts https://www.eclipse.org/Xtext/documentation/303_runtime_concepts.html Book https://github.com/varmaprr/books/blob/master/Implementing%20Domain%20Specific%20Languages%20with%20Xtext%20and%20Xtend%20-%20Second%20Edition.pdf ","date":"31 October 2023","permalink":"/posts/language/dsl/xtext/","section":"博客","summary":"Usage # 项目创建 # XText 开发一个新的语言","title":"xtext简介"},{"content":"Resources # 小林coding 3.1 HTTP 常见面试题 一次完整的HTTP请求过程 # 当我们在web浏览器的地址栏中输入：www.baidu.com，具体发生了什么？\n对www.baidu.com这个网址进行DNS域名解析，得到对应的IP地址 根据这个IP，找到对应的服务器，发起TCP的三次握手 建立TCP连接后发起HTTP请求 服务器响应HTTP请求，浏览器得到html代码 浏览器解析html代码，并请求html代码中的资源（如js、css、图片等）（先得到html代码，才能去找这些资源） 浏览器对页面进行渲染呈现给用户 服务器关闭关闭TCP连接 1.DNS怎么找到域名的？\nDNS域名解析采用的是递归查询的方式，过程是，先去找DNS缓存-\u0026gt;缓存找不到就去找根域名服务器-\u0026gt;根域名又会去找下一级，这样递归查找之后，找到了，给我们的web浏览器\n2.为什么HTTP协议要基于TCP来实现？\nTCP是一个端到端的可靠的面相连接的协议，HTTP基于传输层TCP协议不用担心数据传输的各种问题（当发生错误时，会重传）\n3.最后一步浏览器是如何对页面进行渲染的？\na）解析html文件构成 DOM树 b）解析CSS文件构成渲染树 c）边解析，边渲染 d）JS 单线程运行，JS有可能修改DOM结构，意味着JS执行完成前，后续所有资源的下载是没有必要的，所以JS是单线程，会阻塞后续资源下载\nDNS解析（域名解析服务器） # 首先会搜索浏览器自身的DNS缓存（缓存时间比较短，大概只有1分钟，且只能容纳1000条缓存）\n如果浏览器自身的缓存里面没有找到，那么浏览器会搜索系统自身的DNS缓存\n如果还没有找到，那么尝试从 hosts文件 里面去找\n在前面三个过程都没获取到的情况下，就递归地去域名服务器去查找，具体过程如下\nDNS优化两个方面：DNS缓存、DNS负载均衡\nTCP连接建立（三次握手） # 拿到域名对应的IP地址之后，User-Agent（一般指浏览器）会以一个随机端口（1024\u0026lt;端口\u0026lt;65535）向服务器的WEB程序（常用的有httpd，nginx）等的80端口。这个连接请求（原始的http请求经过 TCP/IP 4层模型的层层封包）到达服务器端后（这中间有各种路由设备，局域网内除外），进入到网卡，然后是进入到内核的TCP/IP协议栈（用于识别连接请求，解封包，一层一层的剥开），还有可能要经过Netfilter防火墙（属于内核的模块）的过滤，最终达到WEB程序，最终建立了 TCP/IP 的连接。\nTCP 是面向连接的协议，所以使用 TCP 前必须先建立连接，而建立连接是通过三次握手来进行的。三次握手的过程如下图：\n发起HTTP请求(建立连接后) # HTTP请求报文由四部分组成：请求行、请求头、空格、请求正文\n请求行：用于描述客户端的请求方式（GET/POST等），请求的资源名称(URL)以及使用的HTTP协议的版本号 请求头：用于描述客户端请求哪台主机及其端口，以及客户端的一些环境信息等 空行：空行就是\\r\\n (POST请求时候有) 请求正文：当使用POST等方法时，通常需要客户端向服务器传递数据。这些数据就储存在请求正文中（GET方式是保存在url地址后面，不会放到这里） GET请求\n下面是浏览器对 http://localhost:8081/test?name=XXG\u0026amp;age=23的GET 请求时发送给服务器的数据：\n可以看出请求包含请求行和请求头两部分。其中请求行中包含 method（例如 GET、POST）、URI（通一资源标志符）和协议版本三部分，三个部分之间以空格分开。请求行和每个请求头各占一行，以换行符 CRLF（即 \\r\\n）分割。\nPOST请求\n下面是浏览器对 http://localhost:8081/test 的 POST 请求时发送给服务器的数据，消息体中带上参数 name=XXG\u0026amp;age=23\n可以看出，上面的请求包含四个部分：请求行、请求头、空格、消息体，比之前的 GET 请求多了一个请求消息，其中 请求头和消息体之间用一个空行分割。POST 请求的参数不在 URL 中，而是在消息体中，请求头中多了一项 Content-Length 用于表示消息体的字节数，这样服务器才能知道请求是否发送结束。这也就是 GET 请求和 POST 请求的主要区别。\n那么起始行中的请求方法有哪些种呢？ GET: 完整请求一个资源 （常用） HEAD: 仅请求响应首部 POST：提交表单（常用） PUT: (webdav) 上传文件（但是浏览器不支持该方法） DELETE：(webdav) 删除 OPTIONS：返回请求的资源所支持的方法的方法 TRACE: 追求一个资源请求中间所经过的代理（该方法不能由浏览器发出）\n那什么是URL、URI、URN？ URI Uniform Resource Identifier 统一资源标识符 URL Uniform Resource Locator 统一资源定位符 URN Uniform Resource Name 统一资源名称 URL和URN 都属于 URI，为了方便就把URL和URI暂时都通指一个东西\n服务器响应http请求，浏览器得到html代码 # HTTP响应也由四部分组成：状态行，响应头，空格，消息体\n状态行包括：协议版本、状态码、状态码描述 状态码：状态码用于表示服务器对请求的处理结果 1xx：指示信息——表示请求已经接受，继续处理 2xx：成功——表示请求已经被成功接收、理解、接受。 3xx：重定向——要完成请求必须进行更进一步的操作 4xx：客户端错误——请求有语法错误或请求无法实现 5xx：服务器端错误——服务器未能实现合法的请求。 200（没有问题） 302（要你去找别人） 304（要你去拿缓存） 307（要你去拿缓存） 403（有这个资源，但是没有访问权限） 404（服务器没有这个资源） 500（服务器这边有问题）\n响应头：响应头用于描述服务器的基本信息，以及客户端如何处理数据 空格：CRLF（即 \\r\\n）分割 消息体：服务器返回给客户端的数据 响应格式如下图\n上面的 HTTP 响应中，响应头中的 Content-Length 同样用于表示消息体的字节数。Content-Type 表示消息体的类型，通常浏览网页其类型是HTML，当然还会有其他类型，比如图片、视频等。\n浏览器解析html代码，并请求html代码中的资源 # 浏览器拿到html文件后，就开始解析其中的html代码，遇到js/css/image等静态资源时，就向服务器端去请求下载（会使用多线程下载，每个浏览器的线程数不一样），这是时候就用上 keep-alive特性了，建立一次HTTP连接，可以请求多个资源，下载资源的顺序就是按照代码里面的顺序，但是由于每个资源大小不一样，而浏览器又是多线程请求请求资源，所以这里显示的顺序并不一定是代码里面的顺序。\n浏览器对页面进行渲染呈现给用户 # 最后，浏览器利用自己内部的工作机制，把请求的静态资源和html代码进行渲染，渲染之后呈现给用户，浏览器是一个边解析边渲染的过程。首先浏览器解析HTML文件构建DOM树，然后解析CSS文件构建渲染树，等到渲染树构建完成后，浏览器开始布局渲染树并将其绘制到屏幕上。这个过程比较复杂，涉及到两个概念: reflow(回流)和repain(重绘)。\nDOM节点中的各个元素都是以盒模型的形式存在，这些都需要浏览器去计算其位置和大小等，这个过程称为relow； 当盒模型的位置,大小以及其他属性，如颜色,字体,等确定下来之后，浏览器便开始绘制内容，这个过程称为repain。 页面在首次加载时必然会经历reflow和repain。reflow和repain过程是非常消耗性能的，尤其是在移动设备上，它会破坏用户体验，有时会造成页面卡顿。所以我们应该尽可能少的减少reflow和repain。JS的解析是由浏览器中的JS解析引擎完成的。JS是单线程运行，JS有可能修改DOM结构，意味着JS执行完成前，后续所有资源的下载是没有必要的，所以JS是单线程，会阻塞后续资源下载。\n服务器关闭关闭TCP连接 # 一般情况下，一旦Web服务器向浏览器发送了请求数据，它就要关闭TCP连接，然后如果浏览器或者服务器在其头信息加入了这行代码：\nConnection:keep-alive TCP连接在发送后将仍然保持打开状态，于是，浏览器可以继续通过相同的连接发送请求。保持连接节省了为每个请求建立新连接所需的时间，还节约了网络带宽。\n自此一次完整的HTTP事务宣告完成.\n","date":"30 October 2023","permalink":"/posts/reviews/network/http-process/","section":"博客","summary":"Resources # 小林coding 3.","title":"一次完整的HTTP请求过程"},{"content":"","date":"30 October 2023","permalink":"/tags/akka/","section":"Tags","summary":"","title":"Akka"},{"content":"Akka 是一个用于在 JVM 上构建高并发、分布式和可容错的事件驱动应用程序的运行时工具包。Akka 既可以用于 Java，也可以用于 Scala。本指南通过描述 Java 版本的Hello World示例来介绍 Akka。\nActors 是 Akka 的执行单元。Actor 模型是一种抽象，它让编写正确的并发、并行和分布式系统更加容易。Hello World示例说明了 Akka 的基础知识。在 30 分钟内，你应该能够下载并运行示例，并使用本指南了解示例是如何构造的。这会让你初步了解 Akka 的魅力，希望这能够让你拥有深入了解 Akka 的兴趣！\n在体验过这个示例之后，想深入了解 Akka，阅读「 Getting Started Guide」是一个很好的选择。\n下载示例 # Java 版本的Hello World示例是一个包含 Maven 和 Gradle 构建文件的压缩项目。你可以在 Linux、MacOS 或 Windows 上运行它。唯一的先决条件是安装 Java 8 和 Maven 或 Gradle。\n下载和解压示例：\n在「 Lightbend Tech Hub」上通过点击CREATE A PROJECT FOR ME下载压缩文件。 将 ZIP 文件解压缩到方便的位置： 在 Linux 和 OSX 系统上，打开终端并使用命令unzip akka-quickstart-java.zip。 在 Windows 上，使用文件资源管理器等工具提取项目。 运行示例 # 确保你已经安装了构建工具，然后打开终端窗口，并从项目目录中键入以下命令以运行Hello World：\n// Maven $ mvn compile exec:exec // Grade $ gradle run 输出应该如下所示（一直向右滚动以查看 Actor 输出）：\n// Maven Scanning for projects... [INFO] [INFO] ------------------------\u0026lt; hello-akka-java:app \u0026gt;------------------------- [INFO] Building app 1.0 [INFO] --------------------------------[ jar ]--------------------------------- [INFO] [INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ app --- [WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent! [INFO] [INFO] --- exec-maven-plugin:1.6.0:exec (default-cli) @ app --- [2019-10-12 09:20:30,248] [INFO] [akka.event.slf4j.Slf4jLogger] [helloakka-akka.actor.default-dispatcher-3] [] - Slf4jLogger started SLF4J: A number (1) of logging calls during the initialization phase have been intercepted and are SLF4J: now being replayed. These are subject to the filtering rules of the underlying logging system. SLF4J: See also http://www.slf4j.org/codes.html#replay \u0026gt;\u0026gt;\u0026gt; Press ENTER to exit \u0026lt;\u0026lt;\u0026lt; [2019-10-12 09:20:30,288] [INFO] [com.lightbend.akka.sample.Greeter] [helloakka-akka.actor.default-dispatcher-6] [akka://helloakka/user/greeter] - Hello Charles! [2019-10-12 09:20:30,290] [INFO] [com.lightbend.akka.sample.GreeterBot] [helloakka-akka.actor.default-dispatcher-3] [akka://helloakka/user/Charles] - Greeting 1 for Charles [2019-10-12 09:20:30,291] [INFO] [com.lightbend.akka.sample.Greeter] [helloakka-akka.actor.default-dispatcher-6] [akka://helloakka/user/greeter] - Hello Charles! [2019-10-12 09:20:30,291] [INFO] [com.lightbend.akka.sample.GreeterBot] [helloakka-akka.actor.default-dispatcher-3] [akka://helloakka/user/Charles] - Greeting 2 for Charles [2019-10-12 09:20:30,291] [INFO] [com.lightbend.akka.sample.Greeter] [helloakka-akka.actor.default-dispatcher-6] [akka://helloakka/user/greeter] - Hello Charles! [2019-10-12 09:20:30,291] [INFO] [com.lightbend.akka.sample.GreeterBot] [helloakka-akka.actor.default-dispatcher-3] [akka://helloakka/user/Charles] - Greeting 3 for Charles // Grade :run [2019-10-12 09:47:16,399] [INFO] [akka.event.slf4j.Slf4jLogger] [helloakka-akka.actor.default-dispatcher-3] [] - Slf4jLogger started SLF4J: A number (1) of logging calls during the initialization phase have been intercepted and are SLF4J: now being replayed. These are subject to the filtering rules of the underlying logging system. SLF4J: See also http://www.slf4j.org/codes.html#replay \u0026gt;\u0026gt;\u0026gt; Press ENTER to exit \u0026lt;\u0026lt;\u0026lt; [2019-10-12 09:47:16,437] [INFO] [com.lightbend.akka.sample.Greeter] [helloakka-akka.actor.default-dispatcher-6] [akka://helloakka/user/greeter] - Hello Charles! [2019-10-12 09:47:16,439] [INFO] [com.lightbend.akka.sample.GreeterBot] [helloakka-akka.actor.default-dispatcher-3] [akka://helloakka/user/Charles] - Greeting 1 for Charles [2019-10-12 09:47:16,440] [INFO] [com.lightbend.akka.sample.Greeter] [helloakka-akka.actor.default-dispatcher-6] [akka://helloakka/user/greeter] - Hello Charles! [2019-10-12 09:47:16,440] [INFO] [com.lightbend.akka.sample.GreeterBot] [helloakka-akka.actor.default-dispatcher-3] [akka://helloakka/user/Charles] - Greeting 2 for Charles [2019-10-12 09:47:16,440] [INFO] [com.lightbend.akka.sample.Greeter] [helloakka-akka.actor.default-dispatcher-6] [akka://helloakka/user/greeter] - Hello Charles! [2019-10-12 09:47:16,440] [INFO] [com.lightbend.akka.sample.GreeterBot] [helloakka-akka.actor.default-dispatcher-3] [akka://helloakka/user/Charles] - Greeting 3 for Charles \u0026lt;=========----\u0026gt; 75% EXECUTING [27s] \u0026gt; :run 恭喜你，你刚刚运行了你的第一个 Akka 应用程序。\nHello World 都做了什么？ # 示例包含了三个Actor：\nGreeter： 接收命令来Greet其他人，并使用Greeted来确认收到了消息。 GreeterBot：接收到从其他Greeter回复的问候，并发送多个额外的问候消息，并收集回复信息直到达到指定的数量。 GreeterMain：引导一切的守护Actor 使用 Actor 模型的好处 # Akka 的以下特性使你能够以直观的方式解决困难的并发性和可伸缩性挑战：\n事件驱动模型：Event-driven model，Actor 通过响应消息来执行工作。Actor 之间的通信是异步的，允许 Actor 发送消息并继续自己的工作，而不是阻塞等待响应。 强隔离原则：Strong isolation principles，与 Java 中的常规对象不同，Actor 在调用的方法方面，没有一个公共 API。相反，它的公共 API 是通过 Actor 处理的消息来定义的。这可以防止 Actor 之间共享状态；观察另一个 Actor 状态的唯一方法是向其发送请求状态的消息。 位置透明：Location transparency，系统通过工厂方法构造 Actor 并返回对实例的引用。因为位置无关紧要，所以 Actor 实例可以启动、停止、移动和重新启动，以向上和向下扩展以及从意外故障中恢复。 轻量级：Lightweight，每个实例只消耗几百个字节，这实际上允许数百万并发 Actor 存在于一个应用程序中。 让我们看看在Hello World示例中使用 Actor 和消息一起工作的一些最佳实践。\n定义 Actor 和消息 # 每个Actor定义它可以接收的消息类型T。类(classes)和对象(objects)由于不可变并支持模式匹配，可以当做非常特别的消息类型。我们在Actor中会用到这些特性接收匹配的消息。\nHello World的 Actor 使用三种不同的消息：\nGreet：向Greeter执行问候的指令； Greeted：Greeter用来确认问候发生时回复的消息； SayHello：GreeterMain开始执行问候进程的指令； 在定义 Actor 及其消息时，请记住以下建议：\n因为消息是 Actor 的公共 API，所以定义具有良好名称、丰富语义和特定于域的含义的消息是一个很好的实践，即使它们只是包装你的数据类型，这将使基于 Actor 的系统更容易使用、理解和调试。 消息应该是不可变的，因为它们在不同的线程之间共享。 将 Actor 的关联消息作为静态类放在 Actor 的类中是一个很好的实践，这使得理解 Actor 期望和处理的消息类型更加容易。 通过静态工厂方法获得Actor的初始行为是一个很好的实践 让我们来看看Greeter，GreeterBot和GreeterMain的实现是如何证明上述的这些实践建议的。\n让我们看看 Actor 如何实现Greeter和Printer来演示这些最佳实践。\nGreeter Actor # 下面的代码段来自于Greeter.java，其实现了Greeter Actor：\npublic class Greeter extends AbstractBehavior\u0026lt;Greeter.Greet\u0026gt; { public static final class Greet { public final String whom; public final ActorRef\u0026lt;Greeted\u0026gt; replyTo; public Greet(String whom, ActorRef\u0026lt;Greeted\u0026gt; replyTo) { this.whom = whom; this.replyTo = replyTo; } } public static final class Greeted { public final String whom; public final ActorRef\u0026lt;Greet\u0026gt; from; public Greeted(String whom, ActorRef\u0026lt;Greet\u0026gt; from) { this.whom = whom; this.from = from; } } public static Behavior\u0026lt;Greet\u0026gt; create() { return Behaviors.setup(Greeter::new); } private Greeter(ActorContext\u0026lt;Greet\u0026gt; context) { super(context); } @Override public Receive\u0026lt;Greet\u0026gt; createReceive() { return newReceiveBuilder().onMessage(Greet.class, this::onGreet).build(); } private Behavior\u0026lt;Greet\u0026gt; onGreet(Greet command) { getContext().getLog().info(\u0026#34;Hello {}!\u0026#34;, command.whom); command.replyTo.tell(new Greeted(command.whom, getContext().getSelf())); return this; } } 上面这个代码片段定义了两种消息类型，一种被Actor用来问候其他人，另外一种被Actor用来确认问候已经完成。Greet类型不仅包含了被问候人的信息，还持有了ActorRef，是由消息发送者提供的以便于GreeterActor可以发回确认信息。\nActor的行为被定义为Greeter继承自AbstractBehavior，带有newReceiveBuilder的工厂行为。处理下一条信息然后可能导致与处理当前这条信息的行为不同。只要当前实例是可变的就可以通过修改当前实例来更新状态。在当前这个例子中，我们不需要更新任何状态，所以我们直接返回this而不包含任何字段更新，这也意味着，处理下一条消息的行为与当前相同。\n被当前行为处理的消息类型被声明为类Greet。通常，一个actor处理超过一种具体的消息类型，这样会有一个所有消息类型可以实现的公共的接口。\n在最后一行我们能看到GreeterActor使用tell方法发送消息到另外一个Actor。这是一个不会阻塞调用者线程的异步操作。\n因为replyTo地址通过类型ActorRef\u0026lt;Greeted\u0026gt;进行声明，编译器仅允许我们发送这种类型的消息，使用其他消息类型会导致编译错误。\n这个Actor可以接收的消息类型和回复的消息类型定义了我们所说的协议。当前用例是一个简单的请求-回复协议，但Actor可以定义任意我们需要的复杂协议。协议与其行为被恰当的包装在了一个范围——Greeter类\nGreeter bot actor # package $package$; import akka.actor.typed.Behavior; import akka.actor.typed.javadsl.*; public class GreeterBot extends AbstractBehavior\u0026lt;Greeter.Greeted\u0026gt; { public static Behavior\u0026lt;Greeter.Greeted\u0026gt; create(int max) { return Behaviors.setup(context -\u0026gt; new GreeterBot(context, max)); } private final int max; private int greetingCounter; private GreeterBot(ActorContext\u0026lt;Greeter.Greeted\u0026gt; context, int max) { super(context); this.max = max; } @Override public Receive\u0026lt;Greeter.Greeted\u0026gt; createReceive() { return newReceiveBuilder().onMessage(Greeter.Greeted.class, this::onGreeted).build(); } private Behavior\u0026lt;Greeter.Greeted\u0026gt; onGreeted(Greeter.Greeted message) { greetingCounter++; getContext().getLog().info(\u0026#34;Greeting {} for {}\u0026#34;, greetingCounter, message.whom); if (greetingCounter == max) { return Behaviors.stopped(); } else { message.from.tell(new Greeter.Greet(message.whom, getContext().getSelf())); return this; } } } 注意这个Actor如何使用实例变量管理计数器。不需要诸如synchronized或AtomicInteger这样的并发保护，因为一个Actor实例一次只处理一条消息。\nGreeter main actor # 第三个Actor产生了Greeter和GreeterBot，并启动他们的交互，创建Actor以及spawn做了什么将在下面讨论。\npackage $package$; import akka.actor.typed.ActorRef; import akka.actor.typed.Behavior; import akka.actor.typed.javadsl.*; public class GreeterMain extends AbstractBehavior\u0026lt;GreeterMain.SayHello\u0026gt; { public static class SayHello { public final String name; public SayHello(String name) { this.name = name; } } private final ActorRef\u0026lt;Greeter.Greet\u0026gt; greeter; public static Behavior\u0026lt;SayHello\u0026gt; create() { return Behaviors.setup(GreeterMain::new); } private GreeterMain(ActorContext\u0026lt;SayHello\u0026gt; context) { super(context); //#create-actors greeter = context.spawn(Greeter.create(), \u0026#34;greeter\u0026#34;); //#create-actors } @Override public Receive\u0026lt;SayHello\u0026gt; createReceive() { return newReceiveBuilder().onMessage(SayHello.class, this::onSayHello).build(); } private Behavior\u0026lt;SayHello\u0026gt; onSayHello(SayHello command) { //#create-actors ActorRef\u0026lt;Greeter.Greeted\u0026gt; replyTo = getContext().spawn(GreeterBot.create(3), command.name); greeter.tell(new Greeter.Greet(command.name, replyTo)); //#create-actors return this; } } 创建 Actor # 到目前为止，我们已经研究了 Actor 的定义和他们的消息。现在，让我们更深入地了解位置透明（location transparency）的好处，看看如何创建 Actor 实例。\n位置透明的好处 # 在 Akka 中，不能使用new关键字创建 Actor 的实例。相反，你应该使用工厂方法创建 Actor 实例。工厂不返回 Actor 实例，而是返回指向 Actor 实例的引用akka.actor.ActorRef。在分布式系统中，这种间接创建实例的方法增加了很多好处和灵活性。\n在 Akka 中位置无关紧要。位置透明性意味着，无论是在正在运行 Actor 的进程内，还是运行在远程计算机上，ActorRef都可以保持相同语义。如果需要，运行时可以通过更改 Actor 的位置或整个应用程序拓扑来优化系统。这就启用了故障管理的“让它崩溃（let it crash）”模型，在该模型中，系统可以通过销毁有问题的 Actor 和重新启动健康的 Actor 来自我修复。\nAkka ActorSystem # ActorSystem是Akka的初始接入点。通常每个应用使用一个AcotrSystem来创建。ActorSystem有一个名称和一个守护Actor。应用的启动通常在守护Actor中完成。\n当前这个ActorSystem的守护Actor是GreeterMain。\nfinal ActorSystem\u0026lt;GreeterMain.SayHello\u0026gt; greeterMain = ActorSystem.create(GreeterMain.create(), \u0026#34;helloakka\u0026#34;); 它使用Behaviors.setup来启动应用\npackage $package$; import akka.actor.typed.ActorRef; import akka.actor.typed.Behavior; import akka.actor.typed.javadsl.*; public class GreeterMain extends AbstractBehavior\u0026lt;GreeterMain.SayHello\u0026gt; { public static class SayHello { public final String name; public SayHello(String name) { this.name = name; } } private final ActorRef\u0026lt;Greeter.Greet\u0026gt; greeter; public static Behavior\u0026lt;SayHello\u0026gt; create() { return Behaviors.setup(GreeterMain::new); } private GreeterMain(ActorContext\u0026lt;SayHello\u0026gt; context) { super(context); //#create-actors greeter = context.spawn(Greeter.create(), \u0026#34;greeter\u0026#34;); //#create-actors } @Override public Receive\u0026lt;SayHello\u0026gt; createReceive() { return newReceiveBuilder().onMessage(SayHello.class, this::onSayHello).build(); } private Behavior\u0026lt;SayHello\u0026gt; onSayHello(SayHello command) { //#create-actors ActorRef\u0026lt;Greeter.Greeted\u0026gt; replyTo = getContext().spawn(GreeterBot.create(3), command.name); greeter.tell(new Greeter.Greet(command.name, replyTo)); //#create-actors return this; } } 新建子actors # 其他actor的创建在ActorContext上使用spawn方法。GreeterMain在启动时使用这种方式创建一个Greeter，并且每收到一个SayHello消息创建一个新的GreeterBot。\ngreeter = context.spawn(Greeter.create(), \u0026#34;greeter\u0026#34;); ActorRef\u0026lt;Greeter.Greeted\u0026gt; replyTo = getContext().spawn(GreeterBot.create(3), command.name); greeter.tell(new Greeter.Greet(command.name, replyTo)); 异步通信 # Actor 是被动的和消息驱动的。Actor 在收到消息前什么都不做。Actor 使用异步消息进行通信。这样可以确保发送者不会一直等待接收者处理他们的消息。相反，发件人将邮件放在收件人的邮箱之后，就可以自由地进行其他工作。Actor 的邮箱本质上是一个具有排序语义的消息队列。从同一个 Actor 发送的多条消息的顺序被保留，但可以与另一个 Actor 发送的消息交错。\n你可能想知道 Actor 在不处理消息的时候在做什么，比如，做什么实际的工作？实际上，它处于挂起状态，在这种状态下，它不消耗除内存之外的任何资源。同样，这也展示了 Actor 的轻量级和高效性。\n给 Actor 发生消息 # 要将消息放入 Actor 的邮箱，我们需要使用ActorRef的tell方法。例如，Hello World的主函数main向Greeter Actor 发送如下消息：\ngreeterMain.tell(new GreeterMain.SayHello(\u0026#34;Charles\u0026#34;)); Greeter Actor 也回复确认消息：\ncommand.replyTo.tell(new Greeted(command.whom, getContext().getSelf())); 我们已经研究了如何创建 Actor 和发送消息。现在，让我们来回顾一下Main类的全部内容。\nMain class # Hello World中的AkkaQuickstart对象创建了带有守护者的ActorSystem，守护者是顶层的负责启动应用的actor。守护者通常使用包含初始启动的Behaviors.setup进行定义。\npackage $package$; import akka.actor.typed.ActorSystem; import java.io.IOException; public class AkkaQuickstart { public static void main(String[] args) { //#actor-system final ActorSystem\u0026lt;GreeterMain.SayHello\u0026gt; greeterMain = ActorSystem.create(GreeterMain.create(), \u0026#34;helloakka\u0026#34;); //#actor-system //#main-send-messages greeterMain.tell(new GreeterMain.SayHello(\u0026#34;Charles\u0026#34;)); //#main-send-messages try { System.out.println(\u0026#34;\u0026gt;\u0026gt;\u0026gt; Press ENTER to exit \u0026lt;\u0026lt;\u0026lt;\u0026#34;); System.in.read(); } catch (IOException ignored) { } finally { greeterMain.terminate(); } } } 完整代码 # 下面是创建示例应用程序的三个类Greeter，GreeterBot，GreeterMain和AkkaQuickstart的完整源代码：\nGreater.java # package $package$; import akka.actor.typed.ActorRef; import akka.actor.typed.Behavior; import akka.actor.typed.javadsl.*; import java.util.Objects; // #greeter public class Greeter extends AbstractBehavior\u0026lt;Greeter.Greet\u0026gt; { public static final class Greet { public final String whom; public final ActorRef\u0026lt;Greeted\u0026gt; replyTo; public Greet(String whom, ActorRef\u0026lt;Greeted\u0026gt; replyTo) { this.whom = whom; this.replyTo = replyTo; } } public static final class Greeted { public final String whom; public final ActorRef\u0026lt;Greet\u0026gt; from; public Greeted(String whom, ActorRef\u0026lt;Greet\u0026gt; from) { this.whom = whom; this.from = from; } // #greeter @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Greeted greeted = (Greeted) o; return Objects.equals(whom, greeted.whom) \u0026amp;\u0026amp; Objects.equals(from, greeted.from); } @Override public int hashCode() { return Objects.hash(whom, from); } @Override public String toString() { return \u0026#34;Greeted{\u0026#34; + \u0026#34;whom=\u0026#39;\u0026#34; + whom + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, from=\u0026#34; + from + \u0026#39;}\u0026#39;; } // #greeter } public static Behavior\u0026lt;Greet\u0026gt; create() { return Behaviors.setup(Greeter::new); } private Greeter(ActorContext\u0026lt;Greet\u0026gt; context) { super(context); } @Override public Receive\u0026lt;Greet\u0026gt; createReceive() { return newReceiveBuilder().onMessage(Greet.class, this::onGreet).build(); } private Behavior\u0026lt;Greet\u0026gt; onGreet(Greet command) { getContext().getLog().info(\u0026#34;Hello {}!\u0026#34;, command.whom); //#greeter-send-message command.replyTo.tell(new Greeted(command.whom, getContext().getSelf())); //#greeter-send-message return this; } } // #greeter GreeterBot.java # package $package$; import akka.actor.typed.Behavior; import akka.actor.typed.javadsl.*; public class GreeterBot extends AbstractBehavior\u0026lt;Greeter.Greeted\u0026gt; { public static Behavior\u0026lt;Greeter.Greeted\u0026gt; create(int max) { return Behaviors.setup(context -\u0026gt; new GreeterBot(context, max)); } private final int max; private int greetingCounter; private GreeterBot(ActorContext\u0026lt;Greeter.Greeted\u0026gt; context, int max) { super(context); this.max = max; } @Override public Receive\u0026lt;Greeter.Greeted\u0026gt; createReceive() { return newReceiveBuilder().onMessage(Greeter.Greeted.class, this::onGreeted).build(); } private Behavior\u0026lt;Greeter.Greeted\u0026gt; onGreeted(Greeter.Greeted message) { greetingCounter++; getContext().getLog().info(\u0026#34;Greeting {} for {}\u0026#34;, greetingCounter, message.whom); if (greetingCounter == max) { return Behaviors.stopped(); } else { message.from.tell(new Greeter.Greet(message.whom, getContext().getSelf())); return this; } } } GreeterMain.java # package $package$; import akka.actor.typed.ActorRef; import akka.actor.typed.Behavior; import akka.actor.typed.javadsl.*; public class GreeterMain extends AbstractBehavior\u0026lt;GreeterMain.SayHello\u0026gt; { public static class SayHello { public final String name; public SayHello(String name) { this.name = name; } } private final ActorRef\u0026lt;Greeter.Greet\u0026gt; greeter; public static Behavior\u0026lt;SayHello\u0026gt; create() { return Behaviors.setup(GreeterMain::new); } private GreeterMain(ActorContext\u0026lt;SayHello\u0026gt; context) { super(context); //#create-actors greeter = context.spawn(Greeter.create(), \u0026#34;greeter\u0026#34;); //#create-actors } @Override public Receive\u0026lt;SayHello\u0026gt; createReceive() { return newReceiveBuilder().onMessage(SayHello.class, this::onSayHello).build(); } private Behavior\u0026lt;SayHello\u0026gt; onSayHello(SayHello command) { //#create-actors ActorRef\u0026lt;Greeter.Greeted\u0026gt; replyTo = getContext().spawn(GreeterBot.create(3), command.name); greeter.tell(new Greeter.Greet(command.name, replyTo)); //#create-actors return this; } } AkkaQuickstart.java # package $package$; import akka.actor.typed.ActorSystem; import java.io.IOException; public class AkkaQuickstart { public static void main(String[] args) { //#actor-system final ActorSystem\u0026lt;GreeterMain.SayHello\u0026gt; greeterMain = ActorSystem.create(GreeterMain.create(), \u0026#34;helloakka\u0026#34;); //#actor-system //#main-send-messages greeterMain.tell(new GreeterMain.SayHello(\u0026#34;Charles\u0026#34;)); //#main-send-messages try { System.out.println(\u0026#34;\u0026gt;\u0026gt;\u0026gt; Press ENTER to exit \u0026lt;\u0026lt;\u0026lt;\u0026#34;); System.in.read(); } catch (IOException ignored) { } finally { greeterMain.terminate(); } } } 作为另一个最佳实践，我们应该提供一些单元测试。\n测试 Actor # Hello World示例中的测试展示了 JUnit 框架的使用。虽然测试的覆盖范围不完整，但它简单地展示了测试 Actor 代码是多么的容易，并提供了一些基本概念。\npackage $package$; import akka.actor.testkit.typed.javadsl.TestKitJunitResource; import akka.actor.testkit.typed.javadsl.TestProbe; import akka.actor.typed.ActorRef; import org.junit.ClassRule; import org.junit.Test; //#definition public class AkkaQuickstartTest { @ClassRule public static final TestKitJunitResource testKit = new TestKitJunitResource(); //#definition //#test @Test public void testGreeterActorSendingOfGreeting() { TestProbe\u0026lt;Greeter.Greeted\u0026gt; testProbe = testKit.createTestProbe(); ActorRef\u0026lt;Greeter.Greet\u0026gt; underTest = testKit.spawn(Greeter.create(), \u0026#34;greeter\u0026#34;); underTest.tell(new Greeter.Greet(\u0026#34;Charles\u0026#34;, testProbe.getRef())); testProbe.expectMessage(new Greeter.Greeted(\u0026#34;Charles\u0026#34;, underTest)); } //#test } 测试类定义 # public class AkkaQuickstartTest { @ClassRule public static final TestKitJunitResource testKit = new TestKitJunitResource(); 使用TestKitJunitResource JUnit规则包含对JUnit的支持。自动创建并清理ActorTestKit。通过 完整文档查看如何直接使用testkit\n测试方法 # 这个测试使用TestProbe 来检查并确认是否得到期望的行为，源码片段如下：\n@Test public void testGreeterActorSendingOfGreeting() { TestProbe\u0026lt;Greeter.Greeted\u0026gt; testProbe = testKit.createTestProbe(); ActorRef\u0026lt;Greeter.Greet\u0026gt; underTest = testKit.spawn(Greeter.create(), \u0026#34;greeter\u0026#34;); underTest.tell(new Greeter.Greet(\u0026#34;Charles\u0026#34;, testProbe.getRef())); testProbe.expectMessage(new Greeter.Greeted(\u0026#34;Charles\u0026#34;, underTest)); } 一旦我们有了TestProbe 的引用，我们将它传递给Greeter作为Greet消息的一部分。然后确认Greeter响应问候发生。\n完整测试程序 # 这里是完整的测试代码：\npackage $package$; import akka.actor.testkit.typed.javadsl.TestKitJunitResource; import akka.actor.testkit.typed.javadsl.TestProbe; import akka.actor.typed.ActorRef; import org.junit.ClassRule; import org.junit.Test; //#definition public class AkkaQuickstartTest { @ClassRule public static final TestKitJunitResource testKit = new TestKitJunitResource(); //#definition //#test @Test public void testGreeterActorSendingOfGreeting() { TestProbe\u0026lt;Greeter.Greeted\u0026gt; testProbe = testKit.createTestProbe(); ActorRef\u0026lt;Greeter.Greet\u0026gt; underTest = testKit.spawn(Greeter.create(), \u0026#34;greeter\u0026#34;); underTest.tell(new Greeter.Greet(\u0026#34;Charles\u0026#34;, testProbe.getRef())); testProbe.expectMessage(new Greeter.Greeted(\u0026#34;Charles\u0026#34;, underTest)); } //#test } 示例代码只涉及了ActorTestKit功能的一小部分，在「 这里」可以找到更完整的概述。\n运行应用程序 # 你可以通过命令行或者 IDE 来运行Hello World应用程序。在本指南的最后一个主题，我们描述了如何在 IntelliJ IDEA 中运行该示例。但是，在我们再次运行应用程序之前，让我们先快速的查看构建文件。\nMaven POM 文件 \u0026lt;!-- #build-sample --\u0026gt; \u0026lt;project\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;hello-akka-java\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;app\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0\u0026lt;/version\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;akka.version\u0026gt;$akka_version$\u0026lt;/akka.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.typesafe.akka\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;akka-actor-typed_2.13\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;\\${akka.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;ch.qos.logback\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;logback-classic\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.2.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.typesafe.akka\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;akka-actor-testkit-typed_2.13\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;\\${akka.version}\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;junit\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.13.1\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.5.1\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;source\u0026gt;1.8\u0026lt;/source\u0026gt; \u0026lt;target\u0026gt;1.8\u0026lt;/target\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.codehaus.mojo\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;exec-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.6.0\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;executable\u0026gt;java\u0026lt;/executable\u0026gt; \u0026lt;arguments\u0026gt; \u0026lt;argument\u0026gt;-classpath\u0026lt;/argument\u0026gt; \u0026lt;classpath /\u0026gt; \u0026lt;argument\u0026gt;$package$.AkkaQuickstart\u0026lt;/argument\u0026gt; \u0026lt;/arguments\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; Grade 构建文件 apply plugin: \u0026#39;java\u0026#39; apply plugin: \u0026#39;idea\u0026#39; apply plugin: \u0026#39;application\u0026#39; repositories { mavenCentral() mavenLocal() } dependencies { implementation \u0026#39;com.typesafe.akka:akka-actor-typed_2.13:$akka_version$\u0026#39; implementation \u0026#39;ch.qos.logback:logback-classic:1.2.3\u0026#39; testImplementation \u0026#39;com.typesafe.akka:akka-actor-testkit-typed_2.13:$akka_version$\u0026#39; testImplementation \u0026#39;junit:junit:4.13.1\u0026#39; } mainClassName = \u0026#34;$package$.AkkaQuickstart\u0026#34; run { standardInput = System.in } 注意：有些依赖有后缀_2.13，这个后缀是编译依赖的scala版本。所有依赖必须使用相同的scala版本编译。所以你不能在单个项目中使用akka-actors_2.13和akka-testkit_2.12，因为它们有不同的scala版本。\n运行项目 # 和前面一样，从控制台运行应用程序：\n// Maven $ mvn compile exec:exec // Grade $ gradle run 输出应该如下所示（一直向右滚动以查看 Actor 输出）：\n// Maven Scanning for projects... [INFO] [INFO] ------------------------\u0026lt; hello-akka-java:app \u0026gt;------------------------- [INFO] Building app 1.0 [INFO] --------------------------------[ jar ]--------------------------------- [INFO] [INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ app --- [WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent! [INFO] [INFO] --- exec-maven-plugin:1.6.0:exec (default-cli) @ app --- [2019-10-12 09:20:30,248] [INFO] [akka.event.slf4j.Slf4jLogger] [helloakka-akka.actor.default-dispatcher-3] [] - Slf4jLogger started SLF4J: A number (1) of logging calls during the initialization phase have been intercepted and are SLF4J: now being replayed. These are subject to the filtering rules of the underlying logging system. SLF4J: See also http://www.slf4j.org/codes.html#replay \u0026gt;\u0026gt;\u0026gt; Press ENTER to exit \u0026lt;\u0026lt;\u0026lt; [2019-10-12 09:20:30,288] [INFO] [com.lightbend.akka.sample.Greeter] [helloakka-akka.actor.default-dispatcher-6] [akka://helloakka/user/greeter] - Hello Charles! [2019-10-12 09:20:30,290] [INFO] [com.lightbend.akka.sample.GreeterBot] [helloakka-akka.actor.default-dispatcher-3] [akka://helloakka/user/Charles] - Greeting 1 for Charles [2019-10-12 09:20:30,291] [INFO] [com.lightbend.akka.sample.Greeter] [helloakka-akka.actor.default-dispatcher-6] [akka://helloakka/user/greeter] - Hello Charles! [2019-10-12 09:20:30,291] [INFO] [com.lightbend.akka.sample.GreeterBot] [helloakka-akka.actor.default-dispatcher-3] [akka://helloakka/user/Charles] - Greeting 2 for Charles [2019-10-12 09:20:30,291] [INFO] [com.lightbend.akka.sample.Greeter] [helloakka-akka.actor.default-dispatcher-6] [akka://helloakka/user/greeter] - Hello Charles! [2019-10-12 09:20:30,291] [INFO] [com.lightbend.akka.sample.GreeterBot] [helloakka-akka.actor.default-dispatcher-3] [akka://helloakka/user/Charles] - Greeting 3 for Charles // Grade :run [2019-10-12 09:47:16,399] [INFO] [akka.event.slf4j.Slf4jLogger] [helloakka-akka.actor.default-dispatcher-3] [] - Slf4jLogger started SLF4J: A number (1) of logging calls during the initialization phase have been intercepted and are SLF4J: now being replayed. These are subject to the filtering rules of the underlying logging system. SLF4J: See also http://www.slf4j.org/codes.html#replay \u0026gt;\u0026gt;\u0026gt; Press ENTER to exit \u0026lt;\u0026lt;\u0026lt; [2019-10-12 09:47:16,437] [INFO] [com.lightbend.akka.sample.Greeter] [helloakka-akka.actor.default-dispatcher-6] [akka://helloakka/user/greeter] - Hello Charles! [2019-10-12 09:47:16,439] [INFO] [com.lightbend.akka.sample.GreeterBot] [helloakka-akka.actor.default-dispatcher-3] [akka://helloakka/user/Charles] - Greeting 1 for Charles [2019-10-12 09:47:16,440] [INFO] [com.lightbend.akka.sample.Greeter] [helloakka-akka.actor.default-dispatcher-6] [akka://helloakka/user/greeter] - Hello Charles! [2019-10-12 09:47:16,440] [INFO] [com.lightbend.akka.sample.GreeterBot] [helloakka-akka.actor.default-dispatcher-3] [akka://helloakka/user/Charles] - Greeting 2 for Charles [2019-10-12 09:47:16,440] [INFO] [com.lightbend.akka.sample.Greeter] [helloakka-akka.actor.default-dispatcher-6] [akka://helloakka/user/greeter] - Hello Charles! [2019-10-12 09:47:16,440] [INFO] [com.lightbend.akka.sample.GreeterBot] [helloakka-akka.actor.default-dispatcher-3] [akka://helloakka/user/Charles] - Greeting 3 for Charles \u0026lt;=========----\u0026gt; 75% EXECUTING [27s] \u0026gt; :run 还记得我们实现 Greeter Actor 使用 Akka 的 Logger 吗？这就是为什么我们记录东西时会有很多额外的信息。例如，日志输出包含诸如何时和从哪个 Actor 记录日志之类的信息。\n请注意应用程序一直执行，直到你按下回车键或使用其他方式中断。\n为了执行单元测试，我们输入test命令：\n// Maven $ mvn test // Grade $ gradle test 下一步 # 如果你使用 IntelliJ，请尝试将示例项目与 IntelliJ IDEA 集成。\n想要继续了解更多有关 Akka 和 Actor Systems 的信息，请参阅「 Getting Started Guide」，欢迎你加入我们！\nIntelliJ IDEA # JetBrains 的 IntelliJ 是 Java/Scala 社区中领先的 IDE 之一，它对 Akka 有着极好的支持。本节将指导你完成示例项目的设置、测试和运行。\n设置项目 # 设置项目很简单。打开 IntelliJ 并选择File -\u0026gt; Open...并指向你安装示例项目的目录。\n检查项目代码 # 如果我们打开文件src/main/java/com/lightbend/akka/sample/HelloAkka.java，我们将看到许多行以//# ...开头，作为文档的注释。为了从源代码中去掉这些行，我们可以在 IntelliJ 中使用出色的查找/替换功能。选择Edit -\u0026gt; Find -\u0026gt; Replace in Path...，选中Regex框并添加[//#].*正则表达式，然后单击查找窗口中的Replace in Find Window...。选择想要替换的内容，并对所有文件重复此操作。\n测试和运行 # 对于测试，我们只需右键单击文件src/test/java/com/lightbend/akka/sample/HelloAkkaTest.java，然后选择Run 'HelloAkkaTest'。\n类似地运行应用程序，我们右击文件src/main/java/com/lightbend/akka/sample/HelloAkka.java，并选择Run 'HelloAkka.main()'。\n有关更多详细信息，请参阅「 运行应用程序」部分。\n想要进一步了解 IntelliJ IDEA，可以参阅「 史上最简单的 IntelliJ IDEA 教程」系列文章。\n","date":"30 October 2023","permalink":"/posts/architecture/iot/akka-java/","section":"博客","summary":"Akka 是一个用于在 JVM 上构建高并发、分布式和可容错的事件驱动应用程序的运行时工具包。Akka 既可以用于 Java，也可以用于 Scala。本指南通过描述 Java 版本的\u003ccode\u003eHello World\u003c/code\u003e示例来介绍 Akka。","title":"快速入门 Akka Java 指南"},{"content":"我们试图建立一个通用的术语来定义一个坚实的基础以对并发、分布式系统这些 Akka 的目标问题展开交流。请注意，对于这些术语并没有一个统一的定义。我们只是为了寻找一些可行的定义以便在整个文档中进行引用。\n并发 vs. 并行 # 并发与并行是相关的概念，但有一些不同点。并发意味着有两个或更多任务正在进行，即使不是在同时执行。这种例子可以通过时间切片来实现，其中一部分任务被顺序执行，并与其他部分任务混合。并行在另一方面来说意味着执行会真正的同时发生。\n异步 vs. 同步 # 一个方法调用，如果调用者直到方法返回一个值或抛出异常才能继续前进，则被认为这是同步调用。另一方面，一个异步调用允许调用者继续前进有限个步骤，而方法的完成则会通过一些额外的机制来告知调用者(比如注册回调、Future或通过消息)。\n无阻塞 vs. 阻塞 # 我们所说的阻塞是指一个线程的延迟会无限期的延迟其他线程。一个很好的例子是一个资源可以通过互斥量被一个线程独占使用。如果一个线程无限期的持有该资源(比如意外进入无线循环)，其他线程则无法继续前进。相反，无阻塞则意外着没有线程能够无限期的延迟其他线程。\n总是应该更优先选择使用无阻塞操作，因为阻塞操作将导致系统的整体进度无法保证。\n死锁 vs. 饥饿 vs. 活锁 # 当多个参与者都在相互等待对方到达一个特殊状态以便程序能够继续进行，则会发生死锁。因为在其他参与者到达一个确定状态(\u0026ldquo;Catch-22\u0026rdquo; 问题)之前没有一个能够继续进行下去，从而导致所有受影响的子系统失速。死锁与阻塞紧密相关，因为这必然是由于一个线程无期限的延迟了其他线程的进展。\n在死锁的情况下，没有参与者可以继续前进，相反则会发生饥饿，当一些参与者能够继续前进时，仍有一些不能前进。一个经典的场景是源生调度算法总是会选择高优先级的任务而非低优先级的那些。如果进入的高优先级任务的数量总是保持很高，这时没有任何低优先级的任务能够完成。\n活锁和死锁类似，也是没有参与者能够继续前进。但与之前被冻结在一个状态等待其他参与者继续前进不同的是，所有的参与者在连续不断的改变他们的状态。比如一个场景，有两个参与者拥有两个完全一致的可用资源。他们互相都在尝试获取资源，但同时也检查对方是否需要该资源。如果一个资源整备其他参与者访问，则该参与者会尝试请求资源的其他实例。不幸的情况是两个参与者可能会在两个资源之间来回跳，从来没有获得，而总是让给对方。\n竟态条件 # 如果一组事件的顺序会被外部的不确定因素影响，我们称之为竟态条件。竟态条件经常会发生在多个线程拥有共享的可变状态，而线程对状态的修改操作会被无法预期的行为打乱。虽然通常的情况是这样，但共享状态并非必须要有竟态条件。比如一个例子，一个客户端发送无序的 P1、P2 给服务端。由于包可能通过不同的网络路由传输，服务端可能会先收到 P2 然后才是 P1。如果消息中并不包含服务端可以识别发送顺序的信息，因此基于包的具体含义可能会引起静态条件。\n无阻塞保证(Progress Conditions) # 阻塞由于多种原因是不被提倡的，包括死锁的危险和被缩减的系统吞吐量。\n无需等待(Wait-freedom) # 一个方法的每次调用都保证会在有限个步骤后结束，则称该方法为\u0026quot;wait-free\u0026quot;。如果一个方法是有界的(bounded)\u0026ldquo;wait-free\u0026rdquo;，则步骤的数量拥有一个有限的上界(upper bound)。\n从这个定义中可以知道无等待方法从不会阻塞，因此死锁也不会发生。另外，因为每个参与者都会在有限个步骤后(即调用结束后)继续前进，无等待方法也不会出现饥饿。\n无锁(Lock-freedom) # 无锁相比无等待是一个较弱的特性。在无锁(lock-free)调用中，有些方法通常无限制的在有限个步骤后结束。这个定义意味着无锁调用永远不会发生死锁。另一方面，一些调用在有限个步骤后结束的保证并不足以保证它们最终都会结束。换句话说就是无锁并不能足以保证饥饿的发生。\n无阻碍(Obstruction-freedom) # 无阻碍是这里讨论的最弱的无阻塞保证。如果一个方法在隔离执行后(其他线程执行任何步骤，比如被挂起)有一个时间点，并在有限数量步骤后结束，则称该方法为无阻碍。所有无锁对象都是无障碍的，但相反则不能成立。\n乐观并发控制(Optimistic concurrency control , OCC) 方法通常是无阻碍的。OCC 方式是指每个参与者都尝试在共享对象上执行它们的操作，但是如果一个参与者检测到冲突，它会回滚修改，并根据一些调度规则再次尝试。如果一个时间点只有一个参与者在尝试，操作则会成功。\n","date":"28 October 2023","permalink":"/posts/architecture/iot/concept/","section":"博客","summary":"试图建立一个通用的术语来定义一个坚实的基础以对并发、分布式系统这些 Akka 的目标问题展开交流。请注意，对于这些术语并没有一个统一的定义。我们只是为了寻找一些可行的定义以便在整个文档中进行引用。","title":"术语与概念"},{"content":"在不同领域工作的组织机构正在独立发现构建相同软件的模式。这些系统更强大、更有弹性、更灵活，能够更好地满足现代需求。\n之所以会发生这些变化，是因为近年来应用需求发生了巨大变化。仅在几年前，一个大型应用程序需要数十台服务器、数秒的响应时间、数小时的离线维护和数千兆字节的数据。如今，应用程序部署在从移动设备到运行数千个多核处理器的云计算集群等各种设备上。用户期望毫秒级的响应时间和 100% 的正常运行时间。数据的单位是 Petabytes。昨天的软件架构根本无法满足今天的需求。\n我们认为需要一种连贯一致的系统架构方法，而且我们相信所有必要的方面都已得到单独认可：我们希望系统具有响应性、复原性、弹性和消息驱动性。我们称之为反应式系统。\n作为反应式系统构建的系统更加灵活、松散耦合和可扩展。这使它们更易于开发和变更。它们对故障的容忍度更高，当故障发生时，它们会以优雅而非灾难的方式应对。反应式系统反应灵敏，能为用户提供有效的互动反馈。\nReactive Systems # Responsive：如果可能，系统应及时作出反应。响应性是可用性和实用性的基石，但不仅如此，响应性还意味着可以快速发现问题并有效处理。响应式系统的重点是提供快速、一致的响应时间，建立可靠的上限，从而提供一致的服务质量。这种一致的行为反过来又简化了错误处理，建立了终端用户的信心，并鼓励了进一步的互动。\nResilient： 面对故障，系统仍能保持响应。这不仅适用于高可用性的关键任务系统，任何不具备弹性的系统在发生故障后都会反应迟钝。弹性是通过复制、遏制、隔离和授权来实现的。故障被控制在每个组件内，组件之间相互隔离，从而确保系统的部分组件可以在不影响整个系统的情况下发生故障并恢复。每个组件的恢复都委托给另一个（外部）组件，并在必要时通过复制确保高可用性。一个组件的客户端不承担处理其故障的负担。\nElastic：系统在不同的工作量下保持响应。反应型系统可以通过增加或减少分配用于服务这些输入的资源，对输入率的变化做出反应。这意味着设计不存在争用点或中心瓶颈，从而能够分片或复制组件，并在它们之间分配输入。反应式系统通过提供相关的实时性能测量，支持预测式和反应式扩展算法。它们在商品硬件和软件平台上以具有成本效益的方式实现了弹性。\nMessage Driven：反应式系统依靠异步消息传递来建立组件之间的边界，以确保松散耦合、隔离和位置透明。这种边界还提供了将故障作为消息委派的方法。采用显式消息传递可通过塑造和监控系统中的消息队列，并在必要时施加反向压力，实现负载管理、弹性和流量控制。将透明消息传递作为一种通信手段，可使故障管理在整个集群或单个主机内使用相同的结构和语义。无阻塞通信允许接收方仅在活动时消耗资源，从而减少系统开销。\n大型系统由较小的系统组成，因此依赖于其组成部分的反应特性。这意味着，反应式系统采用的设计原则使这些特性适用于所有规模的系统，从而使它们具有可组合性。世界上最大的系统都依赖于基于这些特性的架构，每天满足数十亿人的需求。现在是时候从一开始就有意识地应用这些设计原则了，而不是每次都要重新发现它们。\nResources # http://reactivemanifesto.org/ ","date":"28 October 2023","permalink":"/posts/architecture/iot/reactive-manifesto/","section":"博客","summary":"响应式系统具有响应性、复原性、弹性和消息驱动性。","title":"响应式宣言(Reactive Manifesto)"},{"content":"Akka可以在这里找到指定版本的下载链接： http://akka.io/downloads。\n所有的示例代码都是编译通过的，如果想要查看源码，请查看 Akka 文档在 Github 上子项目，包含 Java 和 Scala 两个部分。本教程主要是 Akka 的 Java 部分。\nActor System # 使用 Akka 可以让你从为 Actor 系统创建基础设施和编写控制基本行为所需的初级（low-level）代码中解脱出来。为了理解这一点，让我们看看你在代码中创建的 Actor 与 Akka 在内部为你创建和管理的 Actor 之间的关系，Actor 的生命周期和失败处理。\n在你的项目中添加如下依赖：\n\u0026lt;!-- Maven --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.typesafe.akka\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;akka-actor_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.5.19\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Gradle --\u0026gt; dependencies { compile group: \u0026#39;com.typesafe.akka\u0026#39;, name: \u0026#39;akka-actor_2.11\u0026#39;, version: \u0026#39;2.5.19\u0026#39; } \u0026lt;!-- sbt --\u0026gt; libraryDependencies += \u0026#34;com.typesafe.akka\u0026#34; %% \u0026#34;akka-actor\u0026#34; % \u0026#34;2.5.19\u0026#34; Akka 的 Actor 层级 # Akka 的 Actor 总是属于父 Actor。通常，你可以通过调用getContext().actorOf()来创建 Actor。与创建一个\u0026quot;独立的\u0026quot; Actor 不同，这会将新 Actor 作为一个子节点注入到已经存在的树中：创建 Actor 的 Actor 成为新创建的子 Actor 的父级。你可能会问，你创造的第一个 Actor 的父节点是谁？\n如下图所示，所有的 Actor 都有一个共同的父节点，即用户守护者。可以使用system.actorOf()在当前 Actor 下创建新的 Actor 实例。正如我们在「 快速入门 Akka Java 指南」中介绍的那样，创建 Actor 将返回一个有效的 URL 引用。例如，如果我们用system.actorOf(…, \u0026quot;someActor\u0026quot;)创建一个名为someActor的 Actor，它的引用将包括路径/user/someActor。\n事实上，在你在代码中创建 Actor 之前，Akka 已经在系统中创建了三个 Actor 。这些内置的 Actor 的名字包含guardian，因为他们守护他们所在路径下的每一个子 Actor。守护者 Actor 包括：\n/，根守护者（root guardian）。这是系统中所有 Actor 的父 Actor，也是系统本身终止时要停止的最后一个 Actor。\n/user，守护者（guardian）。这是用户创建的所有 Actor 的父 Actor。不要让用户名混淆，它与最终用户和用户处理无关。使用 Akka 库创建的每个 Actor 都将有一个事先准备的固定路径/user/。\n/system，系统守护者（system guardian）。这是除上述三个 Actor 外，系统创建的所有 Actor 的父 Actor，\n在Hello World示例中，我们已经看到system.actorOf()如何直接在/user下创建 Actor。我们称之为顶级 Actor，尽管实际上它只是在用户定义的层次结构的顶部。你的ActorSystem中通常只有一个（或极少数）顶级 Actor。我们通过从现有的 Actor 调用context.actorOf()来创建子 Actor 或非顶级 Actor。context.actorOf()方法具有与system.actorOf()相同的签名，后者是其对应的顶级。\n查看 Actor 层次结构的最简单方法是打印ActorRef实例。在这个小实验中，我们创建了一个 Actor，打印了它的引用，创建了这个 Actor 的一个子 Actor，并打印了这个子 Actor 的引用。我们从Hello World项目开始，如果你还没有下载它，请从「 Lightbend Tech Hub」下载 QuickStart 项目。\n在你的Hello World项目中，导航到com.example包并创建一个新的名为ActorHierarchyExperiments.java的 Java 文件。将下面代码段中的代码复制并粘贴到此新源文件中。保存文件并运行sbt \u0026quot;runMain com.example.ActorHierarchyExperiments\u0026quot;来观察输出。\npackage com.example; import akka.actor.AbstractActor; import akka.actor.AbstractActor.Receive; import akka.actor.ActorRef; import akka.actor.ActorSystem; import akka.actor.Props; public class ActorHierarchyExperiments { public static void main(String[] args) throws java.io.IOException { ActorSystem system = ActorSystem.create(\u0026#34;testSystem\u0026#34;); ActorRef firstRef = system.actorOf(PrintMyActorRefActor.props(), \u0026#34;first-actor\u0026#34;); System.out.println(\u0026#34;First: \u0026#34; + firstRef); firstRef.tell(\u0026#34;printit\u0026#34;, ActorRef.noSender()); System.out.println(\u0026#34;\u0026gt;\u0026gt;\u0026gt; Press ENTER to exit \u0026lt;\u0026lt;\u0026lt;\u0026#34;); try { System.in.read(); } finally { system.terminate(); } } } class PrintMyActorRefActor extends AbstractActor { static Props props() { return Props.create(PrintMyActorRefActor.class, PrintMyActorRefActor::new); } @Override public Receive createReceive() { return receiveBuilder() .matchEquals(\u0026#34;printit\u0026#34;, p -\u0026gt; { ActorRef secondRef = getContext().actorOf(Props.empty(), \u0026#34;second-actor\u0026#34;); System.out.println(\u0026#34;Second: \u0026#34; + secondRef); }) .build(); } } 注意信息要求第一个 Actor 完成工作的方式。我们使用父 Actor 的引用firstRef.tell(\u0026quot;printit\u0026quot;, ActorRef.noSender())发送消息。当代码执行时，输出包括第一个 Actor 的引用，以及匹配printit模式时创建的子 Actor 的引用。你的输出应该与下面的内容相似：\nFirst: Actor[akka://testSystem/user/first-actor#1053618476] Second: Actor[akka://testSystem/user/first-actor/second-actor#-1544706041] 注意 Actor 引用的结构：\n两条路径都以akka://testSystem/开头。因为所有 Actor 的引用都是有效的 URL，所以akka://是协议字段的值。 接下来，就像在万维网（World Wide Web）上一样，URL 标识系统。在本例中，系统名为testSystem，但它可以是任何其他名称。如果启用了多个系统之间的远程通信，则 URL 的这一部分包括主机名，以便其他系统可以在网络上找到它。 因为第二个 Actor 的引用包含路径/first-actor/，这个标识它为第一个 Actor 的子 Actor。 Actor 引用的最后一部分，即#1053618476或#-1544706041是一个在大多数情况下可以忽略的唯一标识符。 既然你了解了 Actor 层次结构的样子，你可能会想：为什么我们需要这个层次结构？它是用来干什么的？\n层次结构的一个重要作用是安全地管理 Actor 的生命周期。接下来，我们来考虑一下，这些知识如何帮助我们编写更好的代码。\nActor 的生命周期 # Actor 在被创建时就会出现，然后在用户请求时被停止。每当一个 Actor 被停止时，它的所有子 Actor 也会被递归地停止。这种行为大大简化了资源清理，并有助于避免诸如由打开的套接字和文件引起的资源泄漏。事实上，在处理初级多线程代码时，一个通常被忽视的困难是各种并发资源的生命周期管理。\n要停止 Actor，建议的模式是调用 Actor 内部的getContext().stop(getSelf())来停止自身，通常是对某些用户定义的停止消息的响应，或者当 Actor 完成其任务时。从技术上讲，通过调用getContext().stop(actorRef)是可以停止另一个 Actor 的，但通过这种方式停止任意的 Actor 被认为是一种糟糕的做法：停止 Actor 的一个比较好的方法是，尝试向他们发送一个\u0026quot;毒丸（PoisonPill）\u0026ldquo;或自定义的停止消息。\nAkka Actor 的 API 暴露了许多生命周期的钩子，你可以在 Actor 的实现中覆盖这些钩子。最常用的是preStart()和postStop()方法。\npreStart()在 Actor 启动之后但在处理其第一条消息之前调用。 postStop()在 Actor 停止之前调用，在此时之后将不再处理任何消息。 让我们在一个简单的实验中使用生命周期中的preStart()和postStop()钩子来观察停止一个 Actor 时的行为。首先，将以下两个 Actor 类添加到项目中：\nclass StartStopActor1 extends AbstractActor { static Props props() { return Props.create(StartStopActor1.class, StartStopActor1::new); } @Override public void preStart() { System.out.println(\u0026#34;first started\u0026#34;); getContext().actorOf(StartStopActor2.props(), \u0026#34;second\u0026#34;); } @Override public void postStop() { System.out.println(\u0026#34;first stopped\u0026#34;); } @Override public Receive createReceive() { return receiveBuilder() .matchEquals(\u0026#34;stop\u0026#34;, s -\u0026gt; { getContext().stop(getSelf()); }) .build(); } } class StartStopActor2 extends AbstractActor { static Props props() { return Props.create(StartStopActor2.class, StartStopActor2::new); } @Override public void preStart() { System.out.println(\u0026#34;second started\u0026#34;); } @Override public void postStop() { System.out.println(\u0026#34;second stopped\u0026#34;); } // Actor.emptyBehavior is a useful placeholder when we don\u0026#39;t // want to handle any messages in the actor. @Override public Receive createReceive() { return receiveBuilder() .build(); } } 接下来，创建一个主函数来启动 Actors，然后向他们发送stop消息：\nActorRef first = system.actorOf(StartStopActor1.props(), \u0026#34;first\u0026#34;); first.tell(\u0026#34;stop\u0026#34;, ActorRef.noSender()); 你可以再次使用sbt命令来启动这个项目，其输出结果应该如下面这样：\nfirst started second started second stopped first stopped 当我们停止first Actor 时，它会在停止自身之前，先停止了它的子 Actor second。这个顺序是严格的，在调用父 Actor 的postStop()钩子之前，会先调用所有子 Actor 的postStop()钩子。\n在 Akka 参考手册的「 Actor Lifecycle」部分提供了关于 Actor 的全套生命周期钩子的详细信息。\n失败处理 # 父 Actor 和子 Actor 在他们的生命周期中是相互联系的。当一个 Actor 失败（抛出一个异常或从接收中冒出一个未处理的异常）时，它将暂时挂起。如前所述，失败信息被传播到父 Actor，然后父 Actor 决定如何处理由子 Actor 引起的异常。这样，父 Actor 就可以作为子 Actor 的监督者（supervisors）。默认的监督策略是停止并重新启动子 Actor。如果不更改默认策略，所有失败都会导致重新启动。\n让我们在一个简单的实验中观察默认策略。将下面的类添加到项目中，就像之前的类一样：\nclass SupervisingActor extends AbstractActor { static Props props() { return Props.create(SupervisingActor.class, SupervisingActor::new); } ActorRef child = getContext().actorOf(SupervisedActor.props(), \u0026#34;supervised-actor\u0026#34;); @Override public Receive createReceive() { return receiveBuilder() .matchEquals(\u0026#34;failChild\u0026#34;, f -\u0026gt; { child.tell(\u0026#34;fail\u0026#34;, getSelf()); }) .build(); } } class SupervisedActor extends AbstractActor { static Props props() { return Props.create(SupervisedActor.class, SupervisedActor::new); } @Override public void preStart() { System.out.println(\u0026#34;supervised actor started\u0026#34;); } @Override public void postStop() { System.out.println(\u0026#34;supervised actor stopped\u0026#34;); } @Override public Receive createReceive() { return receiveBuilder() .matchEquals(\u0026#34;fail\u0026#34;, f -\u0026gt; { System.out.println(\u0026#34;supervised actor fails now\u0026#34;); throw new Exception(\u0026#34;I failed!\u0026#34;); }) .build(); } } 然后运行：\nActorRef supervisingActor = system.actorOf(SupervisingActor.props(), \u0026#34;supervising-actor\u0026#34;); supervisingActor.tell(\u0026#34;failChild\u0026#34;, ActorRef.noSender()); 你应该看到类似如下的输出结果：\nsupervised actor started supervised actor fails now supervised actor stopped supervised actor started [ERROR] [03/29/2017 10:47:14.150] [testSystem-akka.actor.default-dispatcher-2] [akka://testSystem/user/supervising-actor/supervised-actor] I failed! java.lang.Exception: I failed! at tutorial_1.SupervisedActor$$anonfun$receive$4.applyOrElse(ActorHierarchyExperiments.scala:57) at akka.actor.Actor$class.aroundReceive(Actor.scala:513) at tutorial_1.SupervisedActor.aroundReceive(ActorHierarchyExperiments.scala:47) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:519) at akka.actor.ActorCell.invoke(ActorCell.scala:488) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257) at akka.dispatch.Mailbox.run(Mailbox.scala:224) at akka.dispatch.Mailbox.exec(Mailbox.scala:234) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) 我们看到失败后，被监督的 Actor 停止并立即重新启动。我们还看到一个日志条目，报告处理的异常，在本例中是我们的测试异常。在这个例子中，我们使用了preStart()和postStop()钩子，这是重启后和重启前默认调用的钩子，因此我们无法区分 Actor 内部是第一次启动还是重启。这通常是正确的做法，重新启动的目的是将 Actor 设置为已知的良好状态，这通常意味着一个干净的开始阶段。实际上，在重新启动时，调用的是preRestart()和postRestart()方法，但如果不重写这两个方法，则默认分别委托给postStop()和preStart()。你可以尝试重写这些附加方法，并查看输出是如何变化的。\n无论如何，我们还是建议查看「 监督和监控」，以了解更深入的细节。\nResources # Gitter Chat，Akka 在线交流平台； Akka Forums，Akka 论坛； Akka in GitHub，Akka 开源项目仓库； Akka Official Website，Akka 官网； Akka Java API，Akka 应用程序编程接口。 快速入门指南 快速入门 Akka Java 指南 快速入门 Akka Scala 指南 gitbook https://zhangyi.gitbooks.io/akka-in-action/content/actor_system.html https://guobinhit.github.io/akka-guide/ ","date":"27 October 2023","permalink":"/posts/architecture/iot/akka/","section":"博客","summary":"Akka 是一个开源项目，基于 Apache 2 License。AKKA框架是用Scala写的，主要用于高并发与分布式应用，目前已经得到广泛地运用，例如Spark、Spray等框架在底层都使用了AKKA进行并发处理。","title":"akka框架"},{"content":"一、公式使用参考 # 1．如何插入公式 # $ \\LaTeX $ 的数学公式有两种：行中公式和独立公式。行中公式放在文中与其它文字混编，独立公式单独成行。\n行中公式可以用如下方法表示： $ 数学公式 $\n独立公式可以用如下方法表示： $$ 数学公式 $$\n自动编号的公式可以用如下方法表示： 若需要手动编号，参见“ 大括号和行标的使用”。 \\begin{equation} 数学公式 \\label{eq:当前公式名} \\end{equation}\n自动编号后的公式可在全文任意处使用 \\eqref{eq:公式名} 语句引用。\n例子： $ J_\\alpha(x) = \\sum_{m=0}^\\infty \\frac{(-1)^m}{m! \\Gamma (m + \\alpha + 1)} {\\left({ \\frac{x}{2} }\\right)}^{2m + \\alpha} \\text {，行内公式示例} $ 显示：$ J_\\alpha(x) = \\sum_{m=0}^\\infty \\frac{(-1)^m}{m! \\Gamma (m + \\alpha + 1)} {\\left({ \\frac{x}{2} }\\right)}^{2m + \\alpha} \\text {，行内公式示例} $\n例子：\n$$ J_\\alpha(x) = \\sum_{m=0}^\\infty \\frac{(-1)^m}{m! \\Gamma (m + \\alpha + 1)} {\\left({ \\frac{x}{2} }\\right)}^{2m + \\alpha} \\text {，独立公式示例} $$ 显示：$$ J_\\alpha(x) = \\sum_{m=0}^\\infty \\frac{(-1)^m}{m! \\Gamma (m + \\alpha + 1)} {\\left({ \\frac{x}{2} }\\right)}^{2m + \\alpha} \\text {，独立公式示例} $$\n例子：\n$$ 在公式 \\eqref{eq:sample} 中，我们看到了这个被自动编号的公式。$$ \\begin{equation} E=mc^2 \\text{，自动编号公式示例} \\label{eq:sample} \\end{equation} 显示： $$ 在公式 \\eqref{eq:sample} 中，我们看到了这个被自动编号的公式。$$\n\\begin{equation} E=mc^2 \\text{，自动编号公式示例} \\label{eq:sample} \\end{equation}\n2．如何输入上下标 # ^ 表示上标, _ 表示下标。如果上下标的内容多于一个字符，需要用 {} 将这些内容括成一个整体。上下标可以嵌套，也可以同时使用。\n例子： $$ x^{y^z}=(1+{\\rm e}^x)^{-2xy^w} $$ 显示：$$ x^{y^z}=(1+{\\rm e}^x)^{-2xy^w} $$ 另外，如果要在左右两边都有上下标，可以使用 \\sideset 命令；也可以简单地在符号前面多打一个上下标，此时会以行内公式渲染。\n例子： $$ \\sideset{^1_2}{^3_4}\\bigotimes \\quad or \\quad {^1_2}\\bigotimes {^3_4} $$ 显示：$$\\sideset{^1_2}{^3_4}\\bigotimes \\quad or \\quad {^1_2}\\bigotimes {^3_4} $$ 3．如何输入括号和分隔符 # ()、[] 和 | 表示符号本身，使用 \\{\\} 来表示 {} 。当要显示大号的括号或分隔符时，要用 \\left 和 \\right 命令。\n一些特殊的括号：\n输入 显示 输入 显示 \\langle $\\langle$ \\rangle $\\rangle$ \\lceil $\\lceil$ \\rceil $\\rceil$ \\lfloor $\\lfloor$ \\rfloor $\\rfloor$ \\lbrace $\\lbrace$ \\rbrace $\\rbrace$ \\lvert $\\lvert$ \\rvert $\\rvert$ \\lVert $\\lVert$ \\rVert $\\rVert$ @lymd 有时，我们需要在行内使用两个竖杠表示向量间的某种空间距离，可以这样写 \\lVert \\boldsymbol{X}_i - \\boldsymbol{S}_j \\rVert^2 → $\\lVert \\boldsymbol{X}_i - \\boldsymbol{S}_j \\rVert^2$ 例子： $$ f(x,y,z) = 3y^2z \\left( 3+\\frac{7x+5}{1+y^2} \\right) $$ 显示：$$ f(x,y,z) = 3y^2z \\left( 3+\\frac{7x+5}{1+y^2} \\right) $$ 有时要用 \\left. 或 \\right. 进行匹配而不显示本身。\n例子： $$ \\left. \\frac{{\\rm d}u}{{\\rm d}x} \\right| _{x=0} $$ 显示：$$ \\left. \\frac{{\\rm d}u}{{\\rm d}x} \\right| _{x=0} $$ 4．如何输入分数 # 通常使用 \\frac {分子} {分母} 来生成一个分数，分数可多层嵌套。如果分式较为复杂，亦可使用 分子 \\over 分母 此时分数仅有一层。\n例子： $$ \\frac{a-1}{b-1} \\quad or \\quad {a+1 \\over b+1} $$ 显示：$$ \\frac{a-1}{b-1} \\quad or \\quad {a+1 \\over b+1} $$ 当分式 仅有两个字符时 可直接输入 \\frac ab 来快速生成一个 $\\large\\frac ab$ 。\n例子： $$ \\frac 12,\\frac 1a,\\frac a2 \\quad \\mid \\quad \\text{2 letters only:} \\quad \\frac 12a \\,, k\\frac q{r^2} $$ 显示：$$ \\frac 12,\\frac 1a,\\frac a2 \\quad \\mid \\quad \\text{2 letters only:} \\quad \\frac 12a ,, k\\frac q{r^2} $$ 5．如何输入开方 # 使用 \\sqrt [根指数，省略时为2] {被开方数} 命令输入开方。\n例子： $$ \\sqrt{2} \\quad or \\quad \\sqrt[n]{3} $$ 显示：$$ \\sqrt{2} \\quad or \\quad \\sqrt[n]{3} $$ 6．如何输入省略号 # 数学公式中常见的省略号有两种，\\ldots 表示与 文本底线 对齐的省略号，\\cdots 表示与 文本中线 对齐的省略号。\n例子： $$ f(x_1,x_2,\\underbrace{\\ldots}_{\\rm ldots} ,x_n) = x_1^2 + x_2^2 + \\underbrace{\\cdots}_{\\rm cdots} + x_n^2 $$ 显示：$$ f(x_1,x_2,\\underbrace{\\ldots}{\\rm ldots} ,x_n) = x_1^2 + x_2^2 + \\underbrace{\\cdots}{\\rm cdots} + x_n^2 $$ 7．如何输入向量 # 使用 \\vec{向量} 来自动产生一个向量。也可以使用 \\overrightarrow 等命令自定义字母上方的符号。\n例子： $$ \\vec{a} \\cdot \\vec{b}=0 $$ 显示：$$ \\vec{a} \\cdot \\vec{b}=0 $$\n例子：\n$$ xy \\text{ with arrows:} \\quad \\overleftarrow{xy} \\; \\mid \\; \\overleftrightarrow{xy} \\; \\mid \\; \\overrightarrow{xy} $$ 显示：$$ xy \\text{ with arrows:} \\quad \\overleftarrow{xy} ; \\mid ; \\overleftrightarrow{xy} ; \\mid ; \\overrightarrow{xy} $$ 8．如何输入积分 # 使用 \\int_积分下限^积分上限 {被积表达式} 来输入一个积分。\n例子：\n$$ \\int_0^1 {x^2} \\,{\\rm d}x $$ 显示：$$ \\int_0^1 {x^2} ,{\\rm d}x $$\n本例中 \\, 和 {\\rm d} 部分可省略，但加入能使式子更美观，详见“ 在字符间加入空格”及“ 如何进行字体转换”。\n9．如何输入极限运算 # 使用 \\lim_{变量 \\to 表达式} 表达式 来输入一个极限。如有需求，可以更改 \\to 符号至任意符号。\n例子：\n$$ \\lim_{n \\to \\infty} \\frac{1}{n(n+1)} \\quad and \\quad \\lim_{x\\leftarrow{示例}} \\frac{1}{n(n+1)} $$ 显示：$$ \\lim_{n \\to \\infty} \\frac{1}{n(n+1)} \\quad and \\quad \\lim_{x\\leftarrow{示例}} \\frac{1}{n(n+1)} $$\n10．如何输入累加、累乘运算 # 使用 \\sum_{下标表达式}^{上标表达式} {累加表达式} 来输入一个累加。与之类似，使用 \\prod \\bigcup \\bigcap 来分别输入累乘、并集和交集，更多符号可参考“ 其它特殊字符”。 此类符号在行内显示时上下标表达式将会移至右上角和右下角，如 $\\sum_{i=1}^n \\frac{1}{i^2}$。\n例子： $$ \\sum_{i=1}^n \\frac{1}{i^2} \\quad and \\quad \\prod_{i=1}^n \\frac{1}{i^2} \\quad and \\quad \\bigcup_{i=1}^{2} \\Bbb{R} $$ 显示：$$ \\sum_{i=1}^n \\frac{1}{i^2} \\quad and \\quad \\prod_{i=1}^n \\frac{1}{i^2} \\quad and \\quad \\bigcup_{i=1}^{2} \\Bbb{R} $$ 11．如何输入希腊字母 # 输入 \\小写希腊字母英文全称 和 \\首字母大写希腊字母英文全称 来分别输入小写和大写希腊字母。 对于大写希腊字母与现有字母相同的，直接输入大写字母即可。\n输入 显示 输入 显示 输入 显示 输入 显示 \\alpha $\\alpha$ A $A$ \\beta $\\beta$ B $B$ \\gamma $\\gamma$ \\Gamma $\\Gamma$ \\delta $\\delta$ \\Delta $\\Delta$ \\epsilon $\\epsilon$ E $E$ \\zeta $\\zeta$ Z $Z$ \\eta $\\eta$ H $H$ \\theta $\\theta$ \\Theta $\\Theta$ \\iota $\\iota$ I $I$ \\kappa $\\kappa$ K $K$ \\lambda $\\lambda$ \\Lambda $\\Lambda$ \\mu $\\mu$ M $M$ \\nu $\\nu$ N $N$ \\xi $\\xi$ \\Xi $\\Xi$ o $o$ O $O$ \\pi $\\pi$ \\Pi $\\Pi$ \\rho $\\rho$ P $P$ \\sigma $\\sigma$ \\Sigma $\\Sigma$ \\tau $\\tau$ T $T$ \\upsilon $\\upsilon$ \\Upsilon $\\Upsilon$ \\phi $\\phi$ \\Phi $\\Phi$ \\chi $\\chi$ X $X$ \\psi $\\psi$ \\Psi $\\Psi$ \\omega $\\omega$ \\Omega $\\Omega$ 部分字母有变量专用形式，以 \\var- 开头。\n小写形式 大写形式 变量形式 显示 \\epsilon E \\varepsilon $\\epsilon \\mid E \\mid \\varepsilon$ \\theta \\Theta \\vartheta $\\theta \\mid \\Theta \\mid \\vartheta$ \\rho P \\varrho $\\rho \\mid P \\mid \\varrho$ \\sigma \\Sigma \\varsigma $\\sigma \\mid \\Sigma \\mid \\varsigma$ \\phi \\Phi \\varphi $\\phi \\mid \\Phi \\mid \\varphi$ 12．如何输入其它特殊字符 # **完整的 $\\LaTeX$ 可用符号列表可以在 这份文档 中查阅（极长，共 348 页），大部分常用符号可以参阅 这份精简版文档 查询。**需要注意的是，$\\LaTeX$ 符号并不保证在 MathJax v2.2 中可用，即在 Cmd Markdown 编辑阅读器中可能并不支持所输入的特定命令。\n若需要显示更大或更小的字符，在符号前插入 \\large 或 \\small 命令。 MathJax 针对任意元素均提供从小至大 \\tiny \\Tiny \\scriptsize \\small *默认值 \\normalsize \\large \\Large \\LARGE \\huge \\Huge 共十种渲染大小，详见 官方文档。\n若找不到需要的符号，推荐使用 $\\large\\rm{Detexify}$ 来画出想要的符号 (1)．关系运算符 # 输入 显示 输入 显示 输入 显示 输入 显示 \\pm $\\pm$ \\times $\\times$ \\div $\\div$ \\mid $\\mid$ \\nmid $\\nmid$ \\cdot $\\cdot$ \\circ $\\circ$ \\ast $\\ast$ \\bigodot $\\bigodot$ \\bigotimes $\\bigotimes$ \\bigoplus $\\bigoplus$ \\leq $\\leq$ \\geq $\\geq$ \\neq $\\neq$ \\approx $\\approx$ \\equiv $\\equiv$ \\sum $\\sum$ \\prod $\\prod$ \\coprod $\\coprod$ \\backslash $\\backslash$ (2)．集合运算符 # 输入 显示 输入 显示 输入 显示 \\emptyset $\\emptyset$ \\in $\\in$ \\notin $\\notin$ \\subset $\\subset$ \\supset $\\supset$ \\subseteq $\\subseteq$ \\supseteq $\\supseteq$ \\cap $\\cap$ \\cup $\\cup$ \\vee $\\vee$ \\wedge $\\wedge$ \\uplus $\\uplus$ \\top $\\top$ \\bot $\\bot$ \\complement $\\complement$ (3)．对数运算符 # 输入 显示 输入 显示 输入 显示 \\log $\\log$ \\lg $\\lg$ \\ln $\\ln$ (4)．三角运算符 # 输入 显示 输入 显示 输入 显示 \\backsim $\\backsim$ \\cong $\\cong$ \\angle A $\\angle A$ \\sin $\\sin$ \\cos $\\cos$ \\tan $\\tan$ \\csc $\\csc$ \\sec $\\sec$ \\cot $\\cot$ (5)．微积分运算符 # 输入 显示 输入 显示 输入 显示 \\int $\\int$ \\iint $\\iint$ \\iiint $\\iiint$ \\partial $\\partial$ \\oint $\\oint$ \\prime $\\prime$ \\lim $\\lim$ \\infty $\\infty$ \\nabla $\\nabla$ (6)．逻辑运算符 # 输入 显示 输入 显示 输入 显示 \\because $\\because$ \\therefore $\\therefore$ \\neg $\\neg$ \\forall $\\forall$ \\exists $\\exists$ \\not\\subset $\\not\\subset$ \\not\u0026lt; $\\not\u0026lt;$ \\not\u0026gt; $\\not\u0026gt;$ \\not= $\\not=$ (7)．戴帽符号 # 输入 显示 输入 显示 输入 显示 \\hat{xy} $\\hat{xy}$ \\widehat{xyz} $\\widehat{xyz}$ \\bar{y} $\\bar{y}$ \\tilde{xy} $\\tilde{xy}$ \\widetilde{xyz} $\\widetilde{xyz}$ \\acute{y} $\\acute{y}$ \\breve{y} $\\breve{y}$ \\check{y} $\\check{y}$ \\grave{y} $\\grave{y}$ \\dot{x} $\\dot{x}$ \\ddot{x} $\\ddot{x}$ \\dddot{x} $\\dddot{x}$ 若需要在特定文字顶部\\底部放置内容，可使用 \\overset{顶部内容}{正常内容} 和 \\underset{底部内容}{正常内容} 命令。\n例子： $$ \\verb+\\overset{above}{level}+ \\qquad \\overset{xx}{ABC} \\;\\; \\mid \\quad \\overset{x^2}{\\longmapsto}\\ \\, \\mid \\quad \\overset{\\bullet\\circ\\circ\\bullet}{T} $$ 显示： $$ \\verb+\\overset{above}{level}+ \\qquad \\overset{xx}{ABC} ;; \\mid \\quad \\overset{x^2}{\\longmapsto}\\ , \\mid \\quad \\overset{\\bullet\\circ\\circ\\bullet}{T} $$\n例子：\n$$ \\verb+\\underset{below}{level}+ \\qquad \\underset{xx}{ABC} \\;\\; \\mid \\quad \\underset{x^2}{\\longmapsto}\\ \\, \\mid \\quad \\underset{\\bullet\\circ\\circ\\bullet}{T} $$ 显示： $$ \\verb+ \\underset{below}{level}+ \\qquad \\underset{xx}{ABC} ;; \\mid \\quad \\underset{x^2}{\\longmapsto}\\ , \\mid \\quad \\underset{\\bullet\\circ\\circ\\bullet}{T} $$ 此命令可叠加嵌套使用，生成类似化学反应式的多重条件符号， 如 \\overset{H_2}{\\underset{1300℃}{\\Longleftrightarrow}}： $$ \\rm{SrO+V^{\u0026rsquo;\u0026rsquo;}{Sr} \\overset{H_2}{\\underset{1300℃}{\\Longleftrightarrow}} Sr^{\\times}{Sr}+2e^{\u0026rsquo;}+\\frac 12O_2(g)} $$ 和 \\overset{Surface/bulk}{\\underset{diffusion}{\\longleftrightarrow}}： $$ \\rm{2OH^{\\bullet}{O(STN)}+2O^{\\times}{O(YSZ)} ; \\overset{Surface/bulk}{\\underset{diffusion}{\\longleftrightarrow}} ;; 2OH^{\\bullet}{O(YSZ)}+2O^{\\times}{O(STN)}} $$\n一般建议在书写化学方程式时声明 \\require{AMDcd} 语句，使用 MathJax 内置的交换图表功能，具体例子可 参见下文。\n(8)．连线符号 # 其它可用的文字修饰符可参见官方文档 \u0026ldquo;Additional decorations\u0026rdquo;。\n输入 显示 \\fbox{a+b+c+d} 高级框选需 声明 enclose 标签 $\\fbox{a+b+c+d}$ \\overleftarrow{a+b+c+d} $\\overleftarrow{a+b+c+d}$ \\overrightarrow{a+b+c+d} $\\overrightarrow{a+b+c+d}$ \\overleftrightarrow{a+b+c+d} $\\overleftrightarrow{a+b+c+d}$ \\underleftarrow{a+b+c+d} $\\underleftarrow{a+b+c+d}$ \\underrightarrow{a+b+c+d} $\\underrightarrow{a+b+c+d}$ \\underleftrightarrow{a+b+c+d} $\\underleftrightarrow{a+b+c+d}$ \\overline{a+b+c+d} $\\overline{a+b+c+d}$ \\underline{a+b+c+d} $\\underline{a+b+c+d}$ \\overbrace{a+b+c+d}^{Sample} $\\overbrace{a+b+c+d}^{Sample}$ \\underbrace{a+b+c+d}_{Sample} $\\underbrace{a+b+c+d}_{Sample}$ \\overbrace{a+\\underbrace{b+c}_{1.0}+d}^{2.0} $\\overbrace{a+\\underbrace{b+c}_{1.0}+d}^{2.0}$ \\underbrace{a\\cdot a\\cdots a}_{b\\text{ times}} $\\underbrace{a\\cdot a\\cdots a}_{b\\text{ times}}$ (9)．箭头符号 # 推荐使用符号：\n输入 显示 输入 显示 输入 显示 \\to $\\to$ \\mapsto $\\mapsto$ \\underrightarrow{1℃/min} $\\underrightarrow{1℃/min}$ \\implies $\\implies$ \\iff $\\iff$ \\impliedby $\\impliedby$ 其它可用符号：\n输入 显示 输入 显示 \\uparrow $\\uparrow$ \\Uparrow $\\Uparrow$ \\downarrow $\\downarrow$ \\Downarrow $\\Downarrow$ \\leftarrow $\\leftarrow$ \\Leftarrow $\\Leftarrow$ \\rightarrow $\\rightarrow$ \\Rightarrow $\\Rightarrow$ \\leftrightarrow $\\leftrightarrow$ \\Leftrightarrow $\\Leftrightarrow$ \\longleftarrow $\\longleftarrow$ \\Longleftarrow $\\Longleftarrow$ \\longrightarrow $\\longrightarrow$ \\Longrightarrow $\\Longrightarrow$ \\longleftrightarrow $\\longleftrightarrow$ \\Longleftrightarrow $\\Longleftrightarrow$ 13．如何进行字体转换 # 若要对公式的某一部分字符进行字体转换，可以用 {\\字体 {需转换的部分字符}} 命令，其中 \\字体 部分可以参照下表选择合适的字体。一般情况下，公式默认为斜体字 $italic$ 。\n示例中 全部大写 的字体仅大写可用。\n输入 全字母可用 显示 输入 仅大写可用 显示 \\rm 罗马体 $\\rm{Sample}$ \\mathcal 花体（数学符号等） $\\mathcal{SAMPLE}$ \\it 斜体 $\\it{Sample}$ \\mathbb 黑板粗体（定义域等） $\\mathbb{SAMPLE}$ \\bf 粗体 $\\bf{Sample}$ \\mit 数学斜体 $\\mit{SAMPLE}$ \\sf 等线体 $\\sf{Sample}$ \\scr 手写体 $\\scr{SAMPLE}$ \\tt 打字机体 $\\tt{Sample}$ \\frak 旧德式字体 $\\frak{Sample}$ @lymd \\boldsymbol{\\alpha} 用来表示向量或者矩阵的加粗斜体，如向量 $\\boldsymbol{\\vec\\alpha}$。\n转换字体十分常用，例如在积分中：\n例子： \\begin{array}{cc} \\mathrm{Bad} \u0026amp; \\mathrm{Better} \\\\ \\hline \\\\ \\int_0^1 x^2 dx \u0026amp; \\int_0^1 x^2 \\,{\\rm d}x \\end{array} 显示： \\begin{array}{cc} \\mathrm{Bad} \u0026amp; \\mathrm{Better} \\ \\hline \\ \\int_0^1 x^2 dx \u0026amp; \\int_0^1 x^2 ,{\\rm d}x \\end{array} 注意比较两个式子间 $dx$ 与 ${\\rm d} x$ 的不同。 使用 \\operatorname 命令也可以达到相同的效果，详见“ 定义新的运算符”。\n14．如何高亮一行公式 # 使用 \\bbox[底色, (可选)边距, (可选)边框 border: 框宽度 框类型 框颜色] 命令来高亮一行公式。 底色和框颜色支持详见“ 更改文字颜色”，边距及框宽度支持 绝对像素 px 或 相对大小 em，框类型支持 实线 solid 或 虚线 dashed。\n例子： $$ \\bbox[yellow]{ e^x=\\lim_{n\\to\\infty} \\left( 1+\\frac{x}{n} \\right)^n \\qquad (1) } $$ 显示： $$ \\bbox[yellow]{ e^x=\\lim_{n\\to\\infty} \\left( 1+\\frac{x}{n} \\right)^n \\qquad (1) } $$ 例子： $$ \\bbox[#9ff, 5px]{ % 此处向外添加 5 像素的边距 e^x=\\lim_{n\\to\\infty} \\left( 1+\\frac{x}{n} \\right)^n \\qquad (1) } $$ 显示： $$ \\bbox[#9ff, 5px]{ e^x=\\lim_{n\\to\\infty} \\left( 1+\\frac{x}{n} \\right)^n \\qquad (1) } $$ 例子： $$ % 此处使用 0.5 倍行高作为边距，附加 2 像素的实线边框（Ctrl+Alt+Y 可见） \\bbox[#2f3542, 0.5em, border:2px solid #f1f2f6]{ \\color{#f1f2f6}{e^x=\\lim_{n\\to\\infty} \\left( 1+\\frac{x}{n} \\right)^n \\qquad (1)} } $$ 显示： $$ \\bbox[#2f3542, 0.5em, border:2px solid #f1f2f6]{ \\color{#f1f2f6}{e^x=\\lim_{n\\to\\infty} \\left( 1+\\frac{x}{n} \\right)^n \\qquad (1)} } $$ 15．大括号和行标的使用 # 在 \\left 和 \\right 之后加上要使用的括号来创建自动匹配高度的圆括号 ( )，方括号 [ ] 和花括号 \\{ \\}。 在每个公式末尾前使用 \\tag {行标} 来实现行标。\n例子： $$ f\\left( \\left[ \\frac{ 1+\\left\\{x,y\\right\\} }{ \\left( \\frac xy + \\frac yx \\right) (u+1) }+a \\right]^{3/2} \\right) \\tag {行标} $$ 显示： $$ f\\left( \\left[ \\frac{ 1+\\left{x,y\\right} }{ \\left( \\frac xy + \\frac yx \\right) (u+1) }+a \\right]^{3/2} \\right) \\tag {行标} $$ 如果你需要在不同的行显示对应括号，可以在每一行对应处使用 \\left. 或 \\right. 来放一个“不存在的括号”。\n例子： $$ \\begin{align*} a=\u0026amp;\\left(1+2+3+ \\cdots \\right. \\\\ \u0026amp;\\cdots+\\left. \\infty-2+\\infty-1+\\infty\\right) \\end{align*} $$ 显示： $$ \\begin{align*} a=\u0026amp;\\left(1+2+3+ \\cdots \\right. \\ \u0026amp;\\cdots+\\left. \\infty-2+\\infty-1+\\infty\\right) \\end{align*} $$ 如果你需要将大括号里面显示的分隔符也变大，可以使用 \\middle 命令，此处分别使用单竖线 | 和双竖线 \\\\| 。\n例子： $$ \\left\\langle q \\; \\middle| \\frac{\\frac xy}{\\frac uv} \\middle\\| p \\right\\rangle $$ 显示： $$ \\left\\langle\nq ; \\middle| \\frac{\\frac xy}{\\frac uv} \\middle| p \\right\\rangle $$ 16．其它命令 # (1)．定义新的运算符 \\operatorname # 当需要使用的运算符不在 MathJax 的内置库中时，程序可能会报错或产生错误的渲染结果。此时可以使用 \\operatorname 命令定义一个新的运算符号。\n反例： \\begin{array}{c|c} \\mathrm{Error} \u0026amp; \\text{Wrong rendering} \\\\ \\hline \\\\ \\arsinh(x) \u0026amp; arsinh(x) \\\\ \\Res_{z=1} \u0026amp; Res_{z=1}{\\frac{1}{z^2-z}=1} \\\\ \\end{array} 显示： \\begin{array}{c|c} \\mathrm{Error} \u0026amp; \\text{Wrong rendering} \\ \\hline \\ \\arsinh(x) \u0026amp; arsinh(x) \\ \\Res_{z=1} \u0026amp; Res_{z=1}{\\frac{1}{z^2-z}=1} \\ \\end{array} 使用 \\operatorname{运算符}{式子} 来生成一个普通运算，或使用 \\operatorname*{运算符}_{下标}^{上标}{式子} 来生成一个含上下标的自定义运算。\n例子： \\begin{array}{c|c} \\text{Normal Operator} \u0026amp; \\text{Operator with label above and below} \\\\ \\hline \\\\ \\scriptsize\\text{\\operatorname{arsinh}{x}} \u0026amp; \\scriptsize\\text{\\operatorname*{Res}_{z=1}{\\frac{1}{z^2-z}=1}} \\\\ \\operatorname{arsinh}{x} \u0026amp; \\operatorname*{Res}_{z=1}{\\frac{1}{z^2-z}=1} \\\\ \\end{array} 显示： \\begin{array}{c|c} \\text{Normal Operator} \u0026amp; \\text{Operator with label above and below} \\ \\scriptsize\\text{\\operatorname{arsinh}{x}} \u0026amp; \\scriptsize\\text{\\operatorname*{Res}{z=1}{\\frac{1}{z^2-z}=1}} \\[2ex] \\hline \\ \\operatorname{arsinh}{x} \u0026amp; \\operatorname*{Res}{z=1}{\\frac{1}{z^2-z}=1} \\end{array} 查询 关于此命令的定义和 关于此命令的讨论来进一步了解此命令。\n(2)．添加注释文字 \\text # 在 \\text {文字} 中仍可以使用 $公式$ 插入其它公式。\n例子： $$ f(n)= \\begin{cases} n/2, \u0026amp; \\text {if $n$ is even} \\\\ 3n+1, \u0026amp; \\text{if $n$ is odd} \\end{cases} $$ 显示： $$ f(n)= \\begin{cases} n/2, \u0026amp; \\text {if $n$ is even} \\ 3n+1, \u0026amp; \\text{if $n$ is odd} \\end{cases} $$ (3)．在字符间加入空格 # 有四种宽度的空格可以使用： \\,、\\;、\\quad 和 \\qquad，灵活使用 \\text{n个空格} 也可以在任意位置实现空格。 同时存在一种负空格 \\! 用来减小字符间距，一般在物理单位中使用。 反复使用 \\! 命令能够实现不同元素的叠加渲染，如$\\wedge!!!!!!!!;\\bigcirc$ 和 $ }!!!!!\\div $\n例子： \\begin{array}{c|c} \\text{Spaces} \u0026amp; \\text{Negative Space in Units} \\\\ \\hline \\\\ \\overbrace{a \\! b}^{\\text{\\!}} \\mid \\underbrace{ab}_{\\rm{default}} \\mid \\overbrace{a \\, b}^{\\text{\\,}} \\mid \\underbrace{a \\; b}_{\\text{\\;}} \\mid \\overbrace{a \\quad b}^{\\text{\\quad}} \\mid \\underbrace{a \\qquad b}_{\\text{\\qquad}} \u0026amp; \\mathrm{N}\\!\\cdot\\!\\mathrm{m} \\mid \\mathrm{s}\\!\\cdot\\!\\mathrm{A} \\mid \\mathrm{kg}\\!\\cdot\\!\\mathrm{m}^2 \\\\ \\end{array} 显示： \\begin{array}{c|c} \\text{Spaces} \u0026amp; \\text{Negative Space in Units} \\ \\hline \\ \\overbrace{a ! b}^{\\text{!}} \\mid \\underbrace{ab}{\\rm{default}} \\mid \\overbrace{a , b}^{\\text{,}} \\mid \\underbrace{a ; b}{\\text{;}} \\mid \\overbrace{a \\quad b}^{\\text{\\quad}} \\mid \\underbrace{a \\qquad b}_{\\text{\\qquad}} \u0026amp; \\mathrm{N}!\\cdot!\\mathrm{m} \\mid \\mathrm{s}!\\cdot!\\mathrm{A} \\mid \\mathrm{kg}!\\cdot!\\mathrm{m}^2 \\ \\end{array} 一些常见的公式单位可表达如下：\n例子： $$ \\mu_0=4\\pi\\times10^{-7} \\ \\left.\\mathrm{\\mathrm{T}\\!\\cdot\\!\\mathrm{m}}\\middle/\\mathrm{A}\\right. $$ $$ 180^\\circ=\\pi \\ \\mathrm{rad} $$ $$ \\mathrm{N_A} = 6.022\\times10^{23} \\ \\mathrm{mol}^{-1} $$ 显示： $$ \\mu_0=4\\pi\\times10^{-7} \\ \\left.\\mathrm{\\mathrm{T}!\\cdot!\\mathrm{m}}\\middle/\\mathrm{A}\\right. $$ $$ 180^\\circ=\\pi \\ \\mathrm{rad} $$ $$ \\mathrm{N_A} = 6.022\\times10^{23} \\ \\mathrm{mol}^{-1} $$ (4)．更改文字颜色 \\color # 使用 \\color{颜色}{文字} 来更改特定的文字颜色。\n更改文字颜色需要浏览器支持 ，如果浏览器不知道你所需的颜色，那么文字将被渲染为黑色。对于较旧的浏览器（HTML4 \u0026amp; CSS2），以下颜色是被支持的：\n输入 显示 输入 显示 black $\\color{black}{text}$ grey $\\color{grey}{text}$ silver $\\color{silver}{text}$ white $\\color{white}{text}$ maroon $\\color{maroon}{text}$ red $\\color{red}{text}$ yellow $\\color{yellow}{text}$ lime $\\color{lime}{text}$ olive $\\color{olive}{text}$ green $\\color{green}{text}$ teal $\\color{teal}{text}$ auqa $\\color{auqa}{text}$ blue $\\color{blue}{text}$ navy $\\color{navy}{text}$ purple $\\color{purple}{text}$ fuchsia $\\color{fuchsia}{text}$ 对于较新的浏览器（HTML5 \u0026amp; CSS3），HEX 颜色将被支持：\n输入 \\color {#rgb} {text} 来自定义更多的颜色，其中 #rgb 或 #rrggbb 的 r g b 可输入 0-9 和 a-f 来表示红色、绿色和蓝色的纯度（饱和度）。\n例子： \\begin{array}{|rrrrrrrr|}\\hline \\verb+#000+ \u0026amp; \\color{#000}{text} \u0026amp; \u0026amp; \u0026amp; \\verb+#00F+ \u0026amp; \\color{#00F}{text} \u0026amp; \u0026amp; \\\\ \u0026amp; \u0026amp; \\verb+#0F0+ \u0026amp; \\color{#0F0}{text} \u0026amp; \u0026amp; \u0026amp; \\verb+#0FF+ \u0026amp; \\color{#0FF}{text} \\\\ \\verb+#F00+ \u0026amp; \\color{#F00}{text} \u0026amp; \u0026amp; \u0026amp; \\verb+#F0F+ \u0026amp; \\color{#F0F}{text} \u0026amp; \u0026amp; \\\\ \u0026amp; \u0026amp; \\verb+#FF0+ \u0026amp; \\color{#FF0}{text} \u0026amp; \u0026amp; \u0026amp; \\verb+#FFF+ \u0026amp; \\color{#FFF}{text} \\\\ \\hline\\end{array} 显示： \\begin{array}{|rrrrrrrr|}\\hline \\verb+#000+ \u0026amp; \\color{#000}{text} \u0026amp; \u0026amp; \u0026amp; \\verb+#00F+ \u0026amp; \\color{#00F}{text} \u0026amp; \u0026amp; \\ \u0026amp; \u0026amp; \\verb+#0F0+ \u0026amp; \\color{#0F0}{text} \u0026amp; \u0026amp; \u0026amp; \\verb+#0FF+ \u0026amp; \\color{#0FF}{text} \\ \\verb+#F00+ \u0026amp; \\color{#F00}{text} \u0026amp; \u0026amp; \u0026amp; \\verb+#F0F+ \u0026amp; \\color{#F0F}{text} \u0026amp; \u0026amp; \\ \u0026amp; \u0026amp; \\verb+#FF0+ \u0026amp; \\color{#FF0}{text} \u0026amp; \u0026amp; \u0026amp; \\verb+#FFF+ \u0026amp; \\color{#FFF}{text} \\ \\hline\\end{array} 例子： \\begin{array}{|rrrrrrrr|}\\hline \\verb+#000+ \u0026amp; \\color{#000}{text} \u0026amp; \\verb+#005+ \u0026amp; \\color{#005}{text} \u0026amp; \\verb+#00A+ \u0026amp; \\color{#00A}{text} \u0026amp; \\verb+#00F+ \u0026amp; \\color{#00F}{text} \\\\ \\verb+#500+ \u0026amp; \\color{#500}{text} \u0026amp; \\verb+#505+ \u0026amp; \\color{#505}{text} \u0026amp; \\verb+#50A+ \u0026amp; \\color{#50A}{text} \u0026amp; \\verb+#50F+ \u0026amp; \\color{#50F}{text} \\\\ \\verb+#A00+ \u0026amp; \\color{#A00}{text} \u0026amp; \\verb+#A05+ \u0026amp; \\color{#A05}{text} \u0026amp; \\verb+#A0A+ \u0026amp; \\color{#A0A}{text} \u0026amp; \\verb+#A0F+ \u0026amp; \\color{#A0F}{text} \\\\ \\verb+#F00+ \u0026amp; \\color{#F00}{text} \u0026amp; \\verb+#F05+ \u0026amp; \\color{#F05}{text} \u0026amp; \\verb+#F0A+ \u0026amp; \\color{#F0A}{text} \u0026amp; \\verb+#F0F+ \u0026amp; \\color{#F0F}{text} \\\\ \\hline \\verb+#080+ \u0026amp; \\color{#080}{text} \u0026amp; \\verb+#085+ \u0026amp; \\color{#085}{text} \u0026amp; \\verb+#08A+ \u0026amp; \\color{#08A}{text} \u0026amp; \\verb+#08F+ \u0026amp; \\color{#08F}{text} \\\\ \\verb+#580+ \u0026amp; \\color{#580}{text} \u0026amp; \\verb+#585+ \u0026amp; \\color{#585}{text} \u0026amp; \\verb+#58A+ \u0026amp; \\color{#58A}{text} \u0026amp; \\verb+#58F+ \u0026amp; \\color{#58F}{text} \\\\ \\verb+#A80+ \u0026amp; \\color{#A80}{text} \u0026amp; \\verb+#A85+ \u0026amp; \\color{#A85}{text} \u0026amp; \\verb+#A8A+ \u0026amp; \\color{#A8A}{text} \u0026amp; \\verb+#A8F+ \u0026amp; \\color{#A8F}{text} \\\\ \\verb+#F80+ \u0026amp; \\color{#F80}{text} \u0026amp; \\verb+#F85+ \u0026amp; \\color{#F85}{text} \u0026amp; \\verb+#F8A+ \u0026amp; \\color{#F8A}{text} \u0026amp; \\verb+#F8F+ \u0026amp; \\color{#F8F}{text} \\\\ \\hline \\verb+#0F0+ \u0026amp; \\color{#0F0}{text} \u0026amp; \\verb+#0F5+ \u0026amp; \\color{#0F5}{text} \u0026amp; \\verb+#0FA+ \u0026amp; \\color{#0FA}{text} \u0026amp; \\verb+#0FF+ \u0026amp; \\color{#0FF}{text} \\\\ \\verb+#5F0+ \u0026amp; \\color{#5F0}{text} \u0026amp; \\verb+#5F5+ \u0026amp; \\color{#5F5}{text} \u0026amp; \\verb+#5FA+ \u0026amp; \\color{#5FA}{text} \u0026amp; \\verb+#5FF+ \u0026amp; \\color{#5FF}{text} \\\\ \\verb+#AF0+ \u0026amp; \\color{#AF0}{text} \u0026amp; \\verb+#AF5+ \u0026amp; \\color{#AF5}{text} \u0026amp; \\verb+#AFA+ \u0026amp; \\color{#AFA}{text} \u0026amp; \\verb+#AFF+ \u0026amp; \\color{#AFF}{text} \\\\ \\verb+#FF0+ \u0026amp; \\color{#FF0}{text} \u0026amp; \\verb+#FF5+ \u0026amp; \\color{#FF5}{text} \u0026amp; \\verb+#FFA+ \u0026amp; \\color{#FFA}{text} \u0026amp; \\verb+#FFF+ \u0026amp; \\color{#FFF}{text} \\\\ \\hline\\end{array} 显示： \\begin{array}{|rrrrrrrr|}\\hline \\verb+#000+ \u0026amp; \\color{#000}{text} \u0026amp; \\verb+#005+ \u0026amp; \\color{#005}{text} \u0026amp; \\verb+#00A+ \u0026amp; \\color{#00A}{text} \u0026amp; \\verb+#00F+ \u0026amp; \\color{#00F}{text} \\ \\verb+#500+ \u0026amp; \\color{#500}{text} \u0026amp; \\verb+#505+ \u0026amp; \\color{#505}{text} \u0026amp; \\verb+#50A+ \u0026amp; \\color{#50A}{text} \u0026amp; \\verb+#50F+ \u0026amp; \\color{#50F}{text} \\ \\verb+#A00+ \u0026amp; \\color{#A00}{text} \u0026amp; \\verb+#A05+ \u0026amp; \\color{#A05}{text} \u0026amp; \\verb+#A0A+ \u0026amp; \\color{#A0A}{text} \u0026amp; \\verb+#A0F+ \u0026amp; \\color{#A0F}{text} \\ \\verb+#F00+ \u0026amp; \\color{#F00}{text} \u0026amp; \\verb+#F05+ \u0026amp; \\color{#F05}{text} \u0026amp; \\verb+#F0A+ \u0026amp; \\color{#F0A}{text} \u0026amp; \\verb+#F0F+ \u0026amp; \\color{#F0F}{text} \\ \\hline \\verb+#080+ \u0026amp; \\color{#080}{text} \u0026amp; \\verb+#085+ \u0026amp; \\color{#085}{text} \u0026amp; \\verb+#08A+ \u0026amp; \\color{#08A}{text} \u0026amp; \\verb+#08F+ \u0026amp; \\color{#08F}{text} \\ \\verb+#580+ \u0026amp; \\color{#580}{text} \u0026amp; \\verb+#585+ \u0026amp; \\color{#585}{text} \u0026amp; \\verb+#58A+ \u0026amp; \\color{#58A}{text} \u0026amp; \\verb+#58F+ \u0026amp; \\color{#58F}{text} \\ \\verb+#A80+ \u0026amp; \\color{#A80}{text} \u0026amp; \\verb+#A85+ \u0026amp; \\color{#A85}{text} \u0026amp; \\verb+#A8A+ \u0026amp; \\color{#A8A}{text} \u0026amp; \\verb+#A8F+ \u0026amp; \\color{#A8F}{text} \\ \\verb+#F80+ \u0026amp; \\color{#F80}{text} \u0026amp; \\verb+#F85+ \u0026amp; \\color{#F85}{text} \u0026amp; \\verb+#F8A+ \u0026amp; \\color{#F8A}{text} \u0026amp; \\verb+#F8F+ \u0026amp; \\color{#F8F}{text} \\ \\hline \\verb+#0F0+ \u0026amp; \\color{#0F0}{text} \u0026amp; \\verb+#0F5+ \u0026amp; \\color{#0F5}{text} \u0026amp; \\verb+#0FA+ \u0026amp; \\color{#0FA}{text} \u0026amp; \\verb+#0FF+ \u0026amp; \\color{#0FF}{text} \\ \\verb+#5F0+ \u0026amp; \\color{#5F0}{text} \u0026amp; \\verb+#5F5+ \u0026amp; \\color{#5F5}{text} \u0026amp; \\verb+#5FA+ \u0026amp; \\color{#5FA}{text} \u0026amp; \\verb+#5FF+ \u0026amp; \\color{#5FF}{text} \\ \\verb+#AF0+ \u0026amp; \\color{#AF0}{text} \u0026amp; \\verb+#AF5+ \u0026amp; \\color{#AF5}{text} \u0026amp; \\verb+#AFA+ \u0026amp; \\color{#AFA}{text} \u0026amp; \\verb+#AFF+ \u0026amp; \\color{#AFF}{text} \\ \\verb+#FF0+ \u0026amp; \\color{#FF0}{text} \u0026amp; \\verb+#FF5+ \u0026amp; \\color{#FF5}{text} \u0026amp; \\verb+#FFA+ \u0026amp; \\color{#FFA}{text} \u0026amp; \\verb+#FFF+ \u0026amp; \\color{#FFF}{text} \\ \\hline\\end{array} (5)．添加删除线 # 使用删除线功能必须声明 $$ 符号。\n在公式内使用 \\require{cancel} 来允许片段删除线的显示。 声明片段删除线后，使用 \\cancel{字符}、\\bcancel{字符}、\\xcancel{字符} 和 \\cancelto{字符} 来实现各种片段删除线效果。\n例子： $$ \\require{cancel} \\begin{array}{rl} \\verb|y+\\cancel{x}| \u0026amp; y+\\cancel{x} \\\\ \\verb|\\cancel{y+x}| \u0026amp; \\cancel{y+x} \\\\ \\verb|y+\\bcancel{x}| \u0026amp; y+\\bcancel{x} \\\\ \\verb|y+\\xcancel{x}| \u0026amp; y+\\xcancel{x} \\\\ \\verb|y+\\cancelto{0}{x}| \u0026amp; y+\\cancelto{0}{x} \\\\ \\verb+\\frac{1\\cancel9}{\\cancel95} = \\frac15+\u0026amp; \\frac{1\\cancel9}{\\cancel95} = \\frac15 \\\\ \\end{array} $$ 显示： $$ \\require{cancel} \\begin{array}{rl} \\verb|y+\\cancel{x}| \u0026amp; y+\\cancel{x} \\ \\verb|\\cancel{y+x}| \u0026amp; \\cancel{y+x} \\ \\verb|y+\\bcancel{x}| \u0026amp; y+\\bcancel{x} \\ \\verb|y+\\xcancel{x}| \u0026amp; y+\\xcancel{x} \\ \\verb|y+\\cancelto{0}{x}| \u0026amp; y+\\cancelto{0}{x} \\ \\verb+\\frac{1\\cancel9}{\\cancel95} = \\frac15+\u0026amp; \\frac{1\\cancel9}{\\cancel95} = \\frac15 \\ \\end{array} $$ 使用 \\require{enclose} 来允许整段删除线的显示。 声明整段删除线后，使用 \\enclose{删除线效果}{字符} 来实现各种整段删除线效果。 其中，删除线效果有 horizontalstrike、verticalstrike、updiagonalstrike 和 downdiagonalstrike，可叠加使用。\n例子： $$ \\require{enclose} \\begin{array}{rl} \\verb|\\enclose{horizontalstrike}{x+y}| \u0026amp; \\enclose{horizontalstrike}{x+y} \\\\ \\verb|\\enclose{verticalstrike}{\\frac xy}| \u0026amp; \\enclose{verticalstrike}{\\frac xy} \\\\ \\verb|\\enclose{updiagonalstrike}{x+y}| \u0026amp; \\enclose{updiagonalstrike}{x+y} \\\\ \\verb|\\enclose{downdiagonalstrike}{x+y}| \u0026amp; \\enclose{downdiagonalstrike}{x+y} \\\\ \\verb|\\enclose{horizontalstrike,updiagonalstrike}{x+y}| \u0026amp; \\enclose{horizontalstrike,updiagonalstrike}{x+y} \\\\ \\end{array} $$ 显示： $$ \\require{enclose} \\begin{array}{rl} \\verb|\\enclose{horizontalstrike}{x+y}| \u0026amp; \\enclose{horizontalstrike}{x+y} \\ \\verb|\\enclose{verticalstrike}{\\frac xy}| \u0026amp; \\enclose{verticalstrike}{\\frac xy} \\ \\verb|\\enclose{updiagonalstrike}{x+y}| \u0026amp; \\enclose{updiagonalstrike}{x+y} \\ \\verb|\\enclose{downdiagonalstrike}{x+y}| \u0026amp; \\enclose{downdiagonalstrike}{x+y} \\ \\verb|\\enclose{horizontalstrike,updiagonalstrike}{x+y}| \u0026amp; \\enclose{horizontalstrike,updiagonalstrike}{x+y} \\ \\end{array} $$ 此外， \\enclose 命令还可以产生包围的边框和圆等，参见 MathML Menclose Documentation 以查看更多效果。\n例子： 分别使用 circle 和 roundedbox 包围的公式 $$ \\require{enclose} \\begin{array}{c} \\enclose{circle}{f(\\top),\\, f^2(\\top),\\, f^3(\\top) \\,\\cdots\\, f^n(\\top)} \\\\ \\enclose{roundedbox}{f(\\bot),\\, f^2(\\bot),\\, f^3(\\bot) \\,\\cdots\\, f^n(\\bot)} \\\\ \\end{array} $$ 使用 box 框住所有公式 $$ \\require{enclose} \\enclose{box}{ \\begin{array}{c} f(\\top),\\, f^2(\\top),\\, f^3(\\top) \\,\\cdots\\, f^n(\\top) \\\\ f(\\bot),\\, f^2(\\bot),\\, f^3(\\bot) \\,\\cdots\\, f^n(\\bot) \\\\ \\end{array} } $$ 显示： 分别使用 circle 和 roundedbox 包围的公式 使用 box 框住所有公式 $$ \\require{enclose} \\begin{array}{c} \\enclose{circle}{f(\\top),, f^2(\\top),, f^3(\\top) ,\\cdots, f^n(\\top)} \\ \\enclose{roundedbox}{f(\\bot),, f^2(\\bot),, f^3(\\bot) ,\\cdots, f^n(\\bot)} \\ \\end{array} $$ $$ \\require{enclose} \\enclose{box}{ \\begin{array}{c} f(\\top),, f^2(\\top),, f^3(\\top) ,\\cdots, f^n(\\top) \\ f(\\bot),, f^2(\\bot),, f^3(\\bot) ,\\cdots, f^n(\\bot) \\ \\end{array} } $$ 此例语法可参见“ 如何输入一个数组或表格”。\n二、矩阵使用参考 # 1．如何输入无框矩阵 # 在开头使用 \\begin{matrix}，在结尾使用 \\end{matrix}，在中间插入矩阵元素，每个元素之间插入 \u0026amp; ，并在每行结尾处使用 \\\\ 。 使用矩阵时必须声明 $ 或 $$ 符号。\n例子： $$ \\begin{matrix} 1 \u0026amp; x \u0026amp; x^2 \\\\ 1 \u0026amp; y \u0026amp; y^2 \\\\ 1 \u0026amp; z \u0026amp; z^2 \\\\ \\end{matrix} $$ 显示： $$ \\begin{matrix} 1 \u0026amp; x \u0026amp; x^2 \\ 1 \u0026amp; y \u0026amp; y^2 \\ 1 \u0026amp; z \u0026amp; z^2 \\ \\end{matrix} $$ 2．如何输入边框矩阵 # 在开头将 matrix 替换为 pmatrix bmatrix Bmatrix vmatrix Vmatrix 。\n例子： $ \\begin{matrix} 1 \u0026amp; 2 \\\\ 3 \u0026amp; 4 \\\\ \\end{matrix} $ $ \\begin{pmatrix} 1 \u0026amp; 2 \\\\ 3 \u0026amp; 4 \\\\ \\end{pmatrix} $ $ \\begin{bmatrix} 1 \u0026amp; 2 \\\\ 3 \u0026amp; 4 \\\\ \\end{bmatrix} $ $ \\begin{Bmatrix} 1 \u0026amp; 2 \\\\ 3 \u0026amp; 4 \\\\ \\end{Bmatrix} $ $ \\begin{vmatrix} 1 \u0026amp; 2 \\\\ 3 \u0026amp; 4 \\\\ \\end{vmatrix} $ $ \\begin{Vmatrix} 1 \u0026amp; 2 \\\\ 3 \u0026amp; 4 \\\\ \\end{Vmatrix} $ 显示： matrix pmatrix bmatrix Bmatrix vmatrix Vmatrix $ \\begin{matrix} 1 \u0026amp; 2 \\ 3 \u0026amp; 4 \\ \\end{matrix} $ $ \\begin{pmatrix} 1 \u0026amp; 2 \\ 3 \u0026amp; 4 \\ \\end{pmatrix} $ $ \\begin{bmatrix} 1 \u0026amp; 2 \\ 3 \u0026amp; 4 \\ \\end{bmatrix} $ $ \\begin{Bmatrix} 1 \u0026amp; 2 \\ 3 \u0026amp; 4 \\ \\end{Bmatrix} $ $ \\begin{vmatrix} 1 \u0026amp; 2 \\ 3 \u0026amp; 4 \\ \\end{vmatrix} $ $ \\begin{Vmatrix} 1 \u0026amp; 2 \\ 3 \u0026amp; 4 \\ \\end{Vmatrix} $ 3．如何输入带省略符号的矩阵 # 使用 \\cdots $\\cdots$ , \\ddots $\\ddots$ , \\vdots $\\vdots$ 来输入省略符号。\n例子： $$ \\begin{pmatrix} 1 \u0026amp; a_1 \u0026amp; a_1^2 \u0026amp; \\cdots \u0026amp; a_1^n \\\\ 1 \u0026amp; a_2 \u0026amp; a_2^2 \u0026amp; \\cdots \u0026amp; a_2^n \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 1 \u0026amp; a_m \u0026amp; a_m^2 \u0026amp; \\cdots \u0026amp; a_m^n \\\\ \\end{pmatrix} $$ 显示： $$ \\begin{pmatrix} 1 \u0026amp; a_1 \u0026amp; a_1^2 \u0026amp; \\cdots \u0026amp; a_1^n \\ 1 \u0026amp; a_2 \u0026amp; a_2^2 \u0026amp; \\cdots \u0026amp; a_2^n \\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ 1 \u0026amp; a_m \u0026amp; a_m^2 \u0026amp; \\cdots \u0026amp; a_m^n \\ \\end{pmatrix} $$ ##4．如何输入带分割符号的矩阵\n详见\u0026quot; 数组使用参考\u0026quot;。\n例子： $$ \\left[ \\begin{array}{cc|c} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 4 \u0026amp; 5 \u0026amp; 6 \\\\ \\end{array} \\right] $$ 显示： $$ \\left[ \\begin{array}{cc|c} 1 \u0026amp; 2 \u0026amp; 3 \\ 4 \u0026amp; 5 \u0026amp; 6 \\ \\end{array} \\right] $$ 其中 cc|c 代表在一个三列矩阵中的第二和第三列之间插入分割线。\n5．如何输入行中矩阵 # 若想在一行内显示矩阵， 使用\\bigl(\\begin{smallmatrix} ... \\end{smallmatrix}\\bigr)。\n例子： 这是一个行中矩阵的示例 $\\bigl(\\begin{smallmatrix} a \u0026amp; b \\\\ c \u0026amp; d \\end{smallmatrix}\\bigr)$ 。 显示：这是一个行中矩阵的示例 $\\bigl(\\begin{smallmatrix} a \u0026amp; b \\ c \u0026amp; d \\end{smallmatrix}\\bigr)$ 。 三、方程式序列使用参考 # 1．如何输入一个方程式序列 # 人们经常想要一列整齐且居中的方程式序列。使用 \\begin{align}…\\end{align} 来创造一列方程式，其中在每行结尾处使用 \\\\ 。使用方程式序列无需声明公式符号 $ 或 $$ 。\n请注意 {align} 语句是自动编号的，使用 {align*} 声明不自动编号。\n例子： \\begin{align} \\sqrt{37} \u0026amp; = \\sqrt{\\frac{73^2-1}{12^2}} \\\\ \u0026amp; = \\sqrt{\\frac{73^2}{12^2}\\cdot\\frac{73^2-1}{73^2}} \\\\ \u0026amp; = \\sqrt{\\frac{73^2}{12^2}}\\sqrt{\\frac{73^2-1}{73^2}} \\\\ \u0026amp; = \\frac{73}{12}\\sqrt{1-\\frac{1}{73^2}} \\\\ \u0026amp; \\approx \\frac{73}{12}\\left(1-\\frac{1}{2\\cdot73^2}\\right) \\\\ \\end{align} 显示：\n\\begin{align} \\sqrt{37} \u0026amp; = \\sqrt{\\frac{73^2-1}{12^2}} \\ \u0026amp; = \\sqrt{\\frac{73^2}{12^2}\\cdot\\frac{73^2-1}{73^2}} \\ \u0026amp; = \\sqrt{\\frac{73^2}{12^2}}\\sqrt{\\frac{73^2-1}{73^2}} \\ \u0026amp; = \\frac{73}{12}\\sqrt{1-\\frac{1}{73^2}} \\ \u0026amp; \\approx \\frac{73}{12}\\left(1-\\frac{1}{2\\cdot73^2}\\right) \\ \\end{align}\n本例中每行公式的编号续自“ 如何插入公式”中的自动编号公式\\eqref{eq:sample} 。\n2．在一个方程式序列的每一行中注明原因 # 在 {align} 中后添加 \u0026amp; 符号来自动对齐后面的内容，可灵活组合 \\text 和 \\tag 语句。\\tag 语句编号优先级高于自动编号。\n例子： \\begin{align} v + w \u0026amp; = 0 \u0026amp; \\text{Given} \\tag 1 \\\\ -w \u0026amp; = -w + 0 \u0026amp; \\text{additive identity} \\tag 2 \\\\ -w + 0 \u0026amp; = -w + (v + w) \u0026amp; \\text{equations $(1)$ and $(2)$} \\\\ \\end{align} 显示： \\begin{align} v + w \u0026amp; = 0 \u0026amp; \\text{Given} \\tag 1 \\ -w \u0026amp; = -w + 0 \u0026amp; \\text{additive identity} \\tag 2 \\ -w + 0 \u0026amp; = -w + (v + w) \u0026amp; \\text{equations $(1)$ and $(2)$} \\ \\end{align} 本例中第一、第二行的自动编号被 \\tag 语句覆盖，第三行的编号为自动编号。\n@joyphys 如何引用 \\tag 标记的公式？ 使用 \\tag{yourtag} 来标记公式，然后在 \\tag 之后加上 \\label{yourlabel} 四、条件表达式使用参考 # 1．如何输入一个条件表达式 # 使用 \\begin{cases}…\\end{cases} 来创造一组条件表达式，在每一行条件中插入 \u0026amp; 来指定需要对齐的内容，并在每一行结尾处使用 \\\\。\n例子： $$ f(n) = \\begin{cases} n/2, \u0026amp; \\text{if $n$ is even} \\\\ 3n+1, \u0026amp; \\text{if $n$ is odd} \\\\ \\end{cases} $$ 显示： $$ f(n) = \\begin{cases} n/2, \u0026amp; \\text{if $n$ is even} \\ 3n+1, \u0026amp; \\text{if $n$ is odd} \\ \\end{cases} $$ @Sherlockk 用 markdown+math 编辑时 \\text 内需用 \\(equation\\)\n2．如何输入一个左侧对齐的条件表达式 # 若想让文字在左侧对齐显示，则有如下方式：\n例子： $$ \\left. \\begin{array}{l} \\text{if $n$ is even:} \u0026amp; n/2 \\\\ \\text{if $n$ is odd:} \u0026amp; 3n+1 \\\\ \\end{array} \\right\\} =f(n) $$ 显示： $$ \\left. \\begin{array}{l} \\text{if $n$ is even:} \u0026amp; n/2 \\ \\text{if $n$ is odd:} \u0026amp; 3n+1 \\ \\end{array} \\right} =f(n) $$ 3．如何使条件表达式适配行高 # 在一些情况下，条件表达式中某些行的行高为非标准高度，此时使用 \\\\[2ex] 语句代替该行末尾的 \\\\ 来让编辑器适配。\n例子： 不适配[2ex] $$ f(n) = \\begin{cases} \\frac{n}{2}, \u0026amp; \\text{if $n$ is even} \\\\ 3n+1, \u0026amp; \\text{if $n$ is odd} \\\\ \\end{cases} $$ 适配[2ex] $$ f(n) = \\begin{cases} \\frac{n}{2}, \u0026amp; \\text{if $n$ is even} \\\\[2ex] 3n+1, \u0026amp; \\text{if $n$ is odd} \\\\ \\end{cases} $$ 显示： 不适配[2ex] 适配[2ex] $$ f(n) = \\begin{cases} \\frac{n}{2}, \u0026amp; \\text{if $n$ is even} \\ 3n+1, \u0026amp; \\text{if $n$ is odd} \\ \\end{cases} $$ $$ f(n) = \\begin{cases} \\frac{n}{2}, \u0026amp; \\text{if $n$ is even} \\[2ex] 3n+1, \u0026amp; \\text{if $n$ is odd} \\ \\end{cases} $$ 一个 [ex] 指一个 \u0026ldquo;X-Height\u0026rdquo;，即 x 字母高度。可以根据情况指定多个 [ex]，如 [3ex]、[4ex] 等。 其实可以在任意换行处使用 \\\\[2ex] 语句，只要你觉得合适。\n五、数组与表格使用参考 # 1．如何输入一个数组或表格 # 通常，一个格式化后的表格比单纯的文字或排版后的文字更具有可读性。 数组和表格均以 \\begin{array} 开头，并在其后定义列数及每一列的文本对齐属性，c l r 分别代表居中、左对齐及右对齐。若需要插入垂直分割线，在定义式中插入 | ，若要插入水平分割线，在下一行输入前插入 \\hline 。 与矩阵相似，每行元素间均须要插入 \u0026amp; ，每行元素以 \\\\ 结尾，最后以 \\ end{array} 结束数组。 使用单个数组或表格时无需声明 $ 或 $$ 符号。\n例子： \\begin{array}{c|lcr} n \u0026amp; \\text{左对齐} \u0026amp; \\text{居中对齐} \u0026amp; \\text{右对齐} \\\\ \\hline 1 \u0026amp; 0.24 \u0026amp; 1 \u0026amp; 125 \\\\ 2 \u0026amp; -1 \u0026amp; 189 \u0026amp; -8 \\\\ 3 \u0026amp; -20 \u0026amp; 2000 \u0026amp; 1+10i \\\\ \\end{array} 显示： \\begin{array}{c|lcr} n \u0026amp; \\text{左对齐} \u0026amp; \\text{居中对齐} \u0026amp; \\text{右对齐} \\ \\hline 1 \u0026amp; 0.24 \u0026amp; 1 \u0026amp; 125 \\ 2 \u0026amp; -1 \u0026amp; 189 \u0026amp; -8 \\ 3 \u0026amp; -20 \u0026amp; 2000 \u0026amp; 1+10i \\ \\end{array} 2．如何输入一个嵌套的数组或表格 # 多个数组\\表格可 互相嵌套 并组成一组数组或表格。 使用嵌套前必须声明 $$ 符号。\n例子： $$ \\begin{array}{c} % 总表格 \\begin{array}{cc} % 第一行内分成两列 \\begin{array}{c|cccc} % 第一列\u0026#34;最小值\u0026#34;数组 \\text{min} \u0026amp; 0 \u0026amp; 1 \u0026amp; 2 \u0026amp; 3 \\\\ \\hline 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 1 \u0026amp; 0 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \\\\ 2 \u0026amp; 0 \u0026amp; 1 \u0026amp; 2 \u0026amp; 2 \\\\ 3 \u0026amp; 0 \u0026amp; 1 \u0026amp; 2 \u0026amp; 3 \\\\ \\end{array} \u0026amp; \\begin{array}{c|cccc} % 第二列\u0026#34;最大值\u0026#34;数组 \\text{max} \u0026amp; 0 \u0026amp; 1 \u0026amp; 2 \u0026amp; 3 \\\\ \\hline 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 2 \u0026amp; 2 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 3 \u0026amp; 3 \u0026amp; 3 \u0026amp; 3 \\\\ \\end{array} \\end{array} % 第一行表格组结束 \\\\ \\begin{array}{c|cccc} % 第二行 Delta 值数组 \\Delta \u0026amp; 0 \u0026amp; 1 \u0026amp; 2 \u0026amp; 3 \\\\ \\hline 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 1 \u0026amp; 0 \u0026amp; 1 \u0026amp; 2 \\\\ 2 \u0026amp; 2 \u0026amp; 1 \u0026amp; 0 \u0026amp; 1 \\\\ 3 \u0026amp; 3 \u0026amp; 2 \u0026amp; 1 \u0026amp; 0 \\\\ \\end{array} % 第二行表格结束 \\end{array} % 总表格结束 $$ 显示： $$ \\begin{array}{c} % 总表格 \\begin{array}{cc} % 第一行内分成两列 \\begin{array}{c|cccc} % 第一列\u0026quot;最小值\u0026quot;数组 \\text{min} \u0026amp; 0 \u0026amp; 1 \u0026amp; 2 \u0026amp; 3 \\ \\hline 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\ 1 \u0026amp; 0 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \\ 2 \u0026amp; 0 \u0026amp; 1 \u0026amp; 2 \u0026amp; 2 \\ 3 \u0026amp; 0 \u0026amp; 1 \u0026amp; 2 \u0026amp; 3 \\ \\end{array} \u0026amp; \\begin{array}{c|cccc} % 第二列\u0026quot;最大值\u0026quot;数组 \\text{max} \u0026amp; 0 \u0026amp; 1 \u0026amp; 2 \u0026amp; 3 \\ \\hline 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 2 \u0026amp; 3 \\ 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 2 \u0026amp; 3 \\ 2 \u0026amp; 2 \u0026amp; 2 \u0026amp; 2 \u0026amp; 3 \\ 3 \u0026amp; 3 \u0026amp; 3 \u0026amp; 3 \u0026amp; 3 \\ \\end{array} \\end{array} % 第一行表格组结束 \\ \\begin{array}{c|cccc} % 第二行 Delta 值数组 \\Delta \u0026amp; 0 \u0026amp; 1 \u0026amp; 2 \u0026amp; 3 \\ \\hline 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 2 \u0026amp; 3 \\ 1 \u0026amp; 1 \u0026amp; 0 \u0026amp; 1 \u0026amp; 2 \\ 2 \u0026amp; 2 \u0026amp; 1 \u0026amp; 0 \u0026amp; 1 \\ 3 \u0026amp; 3 \u0026amp; 2 \u0026amp; 1 \u0026amp; 0 \\ \\end{array} % 第二行表格结束 \\end{array} % 总表格结束 $$ ##3．如何输入一个方程组\n可以使用 \\begin{array} … \\end{array} 和 \\left\\{ … \\right. 来创建一个方程组：\n例子： $$ \\left\\{ \\begin{array}{c} a_1x+b_1y+c_1z=d_1 \\\\ a_2x+b_2y+c_2z=d_2 \\\\ a_3x+b_3y+c_3z=d_3 \\\\ \\end{array} \\right. $$ 显示： $$ \\left{ \\begin{array}{c} a_1x+b_1y+c_1z=d_1 \\ a_2x+b_2y+c_2z=d_2 \\ a_3x+b_3y+c_3z=d_3 \\ \\end{array} \\right. $$ 或使用条件表达式组 \\begin{cases} … \\end{cases} 来实现相同效果：\n例子： \\begin{cases} a_1x+b_1y+c_1z=d_1 \\\\ a_2x+b_2y+c_2z=d_2 \\\\ a_3x+b_3y+c_3z=d_3 \\\\ \\end{cases} 显示： \\begin{cases} a_1x+b_1y+c_1z=d_1 \\ a_2x+b_2y+c_2z=d_2 \\ a_3x+b_3y+c_3z=d_3 \\ \\end{cases} 六、连分数使用参考 # 1．如何输入一个连分式 # 就像输入分式时使用 \\frac 一样，使用 \\cfrac 来创建一个连分数。\n例子： $$ x = a_0 + \\cfrac{1^2}{a_1 + \\cfrac{2^2}{a_2 + \\cfrac{3^2}{a_3 + \\cfrac{4^4}{a_4 + \\cdots } } } } $$ 显示： $$ x = a_0 + \\cfrac{1^2}{a_1 + \\cfrac{2^2}{a_2 + \\cfrac{3^2}{a_3 + \\cfrac{4^4}{a_4 + \\cdots } } } } $$ 不要使用普通的 \\frac 或 \\over 来生成连分数，这样会看起来很恶心。\n反例： $$ x = a_0 + \\frac{1^2}{a_1 + \\frac{2^2}{a_2 + \\frac{3^2}{a_3 + \\frac{4^4}{a_4 + \\cdots } } } } $$ 显示： $$ x = a_0 + \\frac{1^2}{a_1 + \\frac{2^2}{a_2 + \\frac{3^2}{a_3 + \\frac{4^4}{a_4 + \\cdots } } } } $$ 当然，你可以使用 \\frac 来表达连分数的紧缩记法。\n例子： $$ x = a_0 + \\frac{1^2}{a_1 +} \\frac{2^2}{a_2 +} \\frac{3^2}{a_3 +} \\frac{4^4}{a_4 +} \\cdots $$ 显示： $$ x = a_0 + \\frac{1^2}{a_1 +} \\frac{2^2}{a_2 +} \\frac{3^2}{a_3 +} \\frac{4^4}{a_4 +} \\cdots $$ 连分数通常都太大以至于不易排版，所以建议在连分数前后声明 $$ 符号，或使用像 [a0,a1,a2,a3,…] 一样的紧缩记法。\n七、交换图表使用参考 # 1．如何输入一个交换图表 # 推荐使用 Cmd Markdown 自带的各种图功能，详见 Cmd Markdown 高阶语法手册。\n使用一行 \\require{AMScd} 语句来允许交换图表的显示。 声明交换图表后，语法与矩阵相似，在开头使用 \\begin{CD}，在结尾使用 \\ end{CD}，在中间插入图表元素，每个元素之间插入 \u0026amp; ，并在每行结尾处使用 \\\\。\n例子： $$ \\require{AMScd} \\begin{CD} A @\u0026gt;a\u0026gt;\u0026gt; B \\\\ @V b V V\\# @VV c V \\\\ C @\u0026gt;\u0026gt;d\u0026gt; D \\\\ \\end{CD} $$ 显示： $$ \\require{AMScd} \\begin{CD} A @\u0026gt;a\u0026raquo; B \\ @V b V V# @VV c V \\ C @\u0026raquo;d\u0026gt; D \\ \\end{CD} $$ 其中，@\u0026gt;\u0026gt;\u0026gt; 代表右箭头、@\u0026lt;\u0026lt;\u0026lt; 代表左箭头、@VVV 代表下箭头、@AAA 代表上箭头、@= 代表水平双实线、@| 代表竖直双实线、@.代表没有箭头。 在 @\u0026gt;\u0026gt;\u0026gt; 的 \u0026gt;\u0026gt;\u0026gt; 之间任意插入文字即代表该箭头的注释文字。\n例子： $$ \\require{AMDcd} \\begin{CD} A @\u0026gt;\u0026gt;\u0026gt; B @\u0026gt;{\\text{very long label}}\u0026gt;\u0026gt; C \\\\ @. @AAA @| \\\\ D @= E @\u0026lt;\u0026lt;\u0026lt; F \\\\ \\end{CD} $$ 显示： $$ \\require{AMDcd} \\begin{CD} A @\u0026raquo;\u0026gt; B @\u0026gt;{\\text{very long label}}\u0026raquo; C \\ @. @AAA @| \\ D @= E @\u0026laquo;\u0026lt; F \\ \\end{CD} $$ 在本例中，very long label 自动延长了它所在箭头以及对应箭头的长度，因而交换图表十分适合进行化学反应式的书写。\n例子： $$ \\require{AMDcd} \\begin{CD} \\rm{RCOHR^{\u0026#39;}SO_3Na} @\u0026gt;{\\large\\text{Hydrolysis, $\\Delta$, Dil.HCl}}\u0026gt;\u0026gt; \\rm{(RCOR^{\u0026#39;})+NaCl+SO_2+ H_2O} \\end{CD} $$ 显示： $$ \\require{AMDcd} \\begin{CD} \\rm{RCOHR^{\u0026rsquo;}SO_3Na} @\u0026gt;{\\large\\text{Hydrolysis, $\\Delta$, Dil.HCl}}\u0026raquo; \\rm{(RCOR^{\u0026rsquo;})+NaCl+SO_2+ H_2O} \\end{CD} $$ 八、一些特殊的注意事项 # !! 本段内容为个人翻译，可能有不准确之处 !! These are issues that won\u0026rsquo;t affect the correctness of formulas, but might make them look significantly better or worse. Beginners should feel free to ignore this advice; someone else will correct it for them, or more likely nobody will care.\n现在指出的小问题并不会影响公式的正确显示，但能让它们看起来明显更好看。初学者可无视这些建议，自然会有强迫症患者替你们改掉它的，或者更可能地，不会有人在意这些细节。\nDon\u0026rsquo;t use \\frac in exponents or limits of integrals; it looks bad and can be confusing, which is why it is rarely done in professional mathematical typesetting. Write the fraction horizontally, with a slash:\n在以 $e$ 为底的指数函数、极限和积分中尽量不要使用 \\frac 符号——它会使整段函数看起来很奇怪并可能产生歧义，因此它在专业数学排版中几乎从不出现。可试着横着写这些分式，中间使用斜线间隔 / （用斜线代替分数线）。\n例子： \\begin{array}{cc} \\mathrm{Bad} \u0026amp; \\mathrm{Better} \\\\ \\hline \\\\ \\large e^{i\\frac{\\pi}2} \\quad e^{\\frac{i\\pi}2}\u0026amp; \\large e^{i\\pi/2} \\\\[2ex] \\int_{-\\frac\\pi2}^\\frac\\pi2 \\sin x\\,dx \u0026amp; \\int_{-\\pi/2}^{\\pi/2}\\sin x\\,dx \\\\ \\end{array} 显示： \\begin{array}{cc} \\mathrm{Bad} \u0026amp; \\mathrm{Better} \\ \\hline \\ \\large e^{i\\frac{\\pi}2} \\quad e^{\\frac{i\\pi}2}\u0026amp; \\large e^{i\\pi/2} \\[2ex] \\int_{-\\frac\\pi2}^\\frac\\pi2 \\sin x,dx \u0026amp; \\int_{-\\pi/2}^{\\pi/2}\\sin x,dx \\ \\end{array} The | symbol has the wrong spacing when it is used as a divider, for example in set comprehensions. Use \\mid instead:\n使用 | 符号作为分隔符时会产生错误的间距，因此在需要分隔时最好使用 \\mid 来代替它。\n例子: \\begin{array}{cc} \\mathrm{Bad} \u0026amp; \\mathrm{Better} \\\\ \\hline \\\\ \\{x|x^2\\in\\Bbb Z\\} \u0026amp; \\{x\\mid x^2\\in\\Bbb Z\\} \\\\ \\end{array} 显示： \\begin{array}{cc} \\mathrm{Bad} \u0026amp; \\mathrm{Better} \\ \\hline \\ {x|x^2\\in\\Bbb Z} \u0026amp; {x\\mid x^2\\in\\Bbb Z} \\ \\end{array} For double and triple integrals, don\u0026rsquo;t use \\int\\int or \\int\\int\\int. Instead use the special forms \\iint and \\iiint:\n使用多重积分符号时，不要多次使用 \\int 来声明，直接使用 \\iint 来表示二重积分，使用 \\iiint 来表示三重积分。 个人补充：在表示面积分和体积分时下标建议使用 \\boldsymbol{S} 和 \\boldsymbol{V} 符号；对于多维函数的超体积，可使用 \\idotsint，如下面的例子所示。\n例子： \\begin{array}{cc} \\mathrm{Bad} \u0026amp; \\mathrm{Better} \\\\ \\hline \\\\ \\int\\int_S f(x)\\,dy\\,dx \u0026amp; \\iint_{\\boldsymbol{S}} f(x)\\,{\\rm d}y\\,{\\rm d}x \\\\ \\int\\int\\int_V f(x)\\,dz\\,dy\\,dx \u0026amp; \\iiint_{\\boldsymbol{V}} f(x)\\,{\\rm d}z\\,{\\rm d}y\\,{\\rm d}x \\\\[3ex] \\hline \\\\ \\text{多重积分示例} \u0026amp; \\idotsint_{\\boldsymbol{D}} f(x_1,x_2,\\,\\cdots\\, ,x_n)\\,{\\rm d}x_1\\cdots{\\rm d}x_n \\end{array} 显示： $$ \\require{AMSmath} \\begin{array}{cc} \\mathrm{Bad} \u0026amp; \\mathrm{Better} \\ \\hline \\ \\int\\int_S f(x),dy,dx \u0026amp; \\iint_{\\boldsymbol{S}} f(x),{\\rm d}y,{\\rm d}x \\ \\int\\int\\int_V f(x),dz,dy,dx \u0026amp; \\iiint_{\\boldsymbol{V}} f(x),{\\rm d}z,{\\rm d}y,{\\rm d}x \\[3ex] \\hline \\ \\text{多重积分示例} \u0026amp; \\idotsint_{\\boldsymbol{D}} f(x_1,x_2,,\\cdots, ,x_n),{\\rm d}x_1\\cdots{\\rm d}x_n \\end{array} $$ Use \\,, to insert a thin space before differentials; without this $\\TeX$ will mash them together:\n使用多重积分时，在被积变量后加入 \\, （或在微分符号 ${\\rm d}$ 之前）来插入一个小的间距，否则各种被积变量将会挤成一团。注意比较 ${\\rm d}z{\\rm d} y{\\rm d} x$ 的不同。\n例子： \\begin{array}{cc} \\mathrm{Bad} \u0026amp; \\mathrm{Better} \\\\ \\hline \\\\ \\iiint_V f(x){\\rm d}z {\\rm d}y {\\rm d}x \u0026amp; \\iiint_{\\boldsymbol{V}} f(x)\\,{\\rm d}z\\,{\\rm d}y\\,{\\rm d}x \\\\ \\end{array} 显示： \\begin{array}{cc} \\mathrm{Bad} \u0026amp; \\mathrm{Better} \\ \\hline \\ \\iiint_V f(x){\\rm d}z {\\rm d}y {\\rm d}x \u0026amp; \\iiint_{\\boldsymbol{V}} f(x),{\\rm d}z,{\\rm d}y,{\\rm d}x \\ \\end{array} 感谢您花费时间阅读这份指导手册，本手册内容可能有疏漏之处，欢迎更改指正。 更多语法请参见： Cmd Markdown 简明语法手册， Cmd Markdown 高阶语法手册。 祝您记录、阅读、分享愉快！\nDrafted \u0026amp; Translated by Eric P. 2015-10-02\n","date":"23 October 2023","permalink":"/posts/skills/markdown%E5%85%AC%E5%BC%8F%E8%AF%AD%E6%B3%95/","section":"博客","summary":"一、公式使用参考 # 1．如何插入公式 # $ \\LaTeX $ 的数学公式有两种：行中公式和独立公式。行中公式放在文中与其它文字混编，独立公式单独成行。","title":"Markdown公式语法.md"},{"content":"机器学习是什么 # 机器学习就是让机器能自动找到一个函数function\n语音识别（Speech Recognition）：输入是音频，输出是音频对应的文字 图像分类：输入是图片，输出是类别（比如猫、狗） AlphaGo下围棋：输入是当前棋盘的状态，输出是下一步落棋的位置 对话/问答系统 机器能够找到哪些函数 # 为解决不同的问题、完成不同的任务，需要找到不同的函数，那机器学习能找到哪些函数呢？\n回归（Regression）：输出是一个连续的数值、标量，比如PM2.5预测 分类（Classification）：输出是一个离散的值。 二分类（Binary Classification）的输出就是0或1、Yes或No、\u0026hellip;，比如文本情感分析的输出可以是正面和负面 多分类（Multi-Category Classification）的输出就是[1,2,3,\u0026hellip;,N]，比如图像分类里判断一张图片是猫还是狗还是杯子 生成（Generation）：很多教科书吧机器学习划分为回归问题和分类问题，但其实不止这两种问题，比如生成（Generation）。生成是指让机器学习如何创造/生成，比如生成文本、图片等。 如何告诉机器我们希望找到什么函数 # 我们该如何为机器提供学习资料？\n有监督学习（Supervised Learning）：可以把有监督学习种的“监督”理解为标签（Label），即数据集种不仅包括特征还包括标签。有了标签，我们就可以评价一个函数的好坏，进而优化这个函数。使用Loss判断函数的好坏，Loss越小，函数越好。 强化学习（Reinforcement Learning）：原始的AlphaGo是先通过有监督学习优化到一定程度，然后用强化学习继续优化。新版本的AlphaGo是完全通过强化学习实现的，优于原始的AlphaGo。 无监督学习（Unsupervised Learning）：只给机器提供数据特征，但不提供数据标签。那机器能学到什么呢？ 下面以让机器学习下围棋为例：有监督学习VS强化学习\n有监督学习：函数的输入（数据特征）就是期盼状态，函数的输出（数据标签）就是下一步落棋的位置。此时，我们需要为机器提供的数据就类似棋谱（如果现在棋局是这样，那下一步怎么落棋最好），但其实人类不一定知道怎么落棋最好。 强化学习：让机器跟自己、别人下棋，把结果（赢或输）作为Reward，引导机器学习如何下棋。如果它赢了，那它就知道这一盘里有几步棋下得好，但不知道是哪几步；如果它输了，它就知道这一盘里有几步棋下得不好，但不知道是哪几步。 机器如何找出我们想找到的函数 # 我们要给定函数形式/范围（模型），比如假定函数是线性模型、神经网络等等。模型就是一个函数集，模型的参数确定以后，才得到一个函数。 找到更好的函数： 使用梯度下降（Gradient Descent），找到更好的函数。 前沿研究 # AI的可解释性（Explainable AI）：比如，机器为什么认为这张图片里有一只猫？ 对抗攻击（Adversarial Attack）：对输入故意添加一些人无法察觉的细微的干扰，导致模型以高置信度给出一个错误的输出。 模型压缩（Network Compression）： 把模型压缩以减少模型对计算资源消耗。 异常检测（Anomaly Detection）：使机器知道它遇到了自己不知道的东西。 迁移学习（Transfer Learning/Domain Adversarial Learning）： 一个模型已经学到了一些知识，将这些知识应用到另一个任务中。 元学习（Meta Learning）： 让机器学习如何学习。机器学习是我们教机器学习某种知识，元学习是我们教机器如何学习。 终身学习（Life-Long Learning）：让机器终身学习，学习完任务1、再继续学任务2、…… 机器学习的三个步骤 # 确定模型（Model）/函数集（Function Set） 确定如何评价函数的好坏 确定如何找到最好的函数 ","date":"23 October 2023","permalink":"/posts/ai/li-hongyis-notes/%E4%B8%80%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/","section":"博客","summary":"机器学习是什么 # 机器学习就是让机器能自动找到一个函数function","title":"一、机器学习概论"},{"content":"7.1 CNN入门详解 # 卷积神经网络（CNN）常常被用来做图像处理，当然也可以用一般的神经网络，那它们各自有什么优缺点呢？\nFNN用于图片处理的缺点 # 使用一般的全连接前馈神经网络（FNN）处理图片时的缺点：\n需要很多的参数： 假设有一张尺寸100×100的图片（尺寸已经算很小了），那输入层就有100×100×3=30K个像素，假设第一个隐藏层有1K个神经元（一个神经元包含30K个参数），这就已经需要30M个参数了…… 该架构中每个神经元就是一个分类器，这是没必要的： 第一个隐藏层作为最基础的pattern分类器（比如判断有无绿色、边缘等），第二个隐藏层基于第一个隐藏层继续做pattern分类（比如木头、肉类），以此类推…… 按照人类的直观理解，我们不是像全连接神经网络一样去处理图片的。具体来看，有哪些方面呢？\n图片的一些性质 # 结合全连接前馈神经网络的缺点和人类对图片的直观理解，可以得到下述图片的3个性质。\n性质1：Some patterns are much smaller than the whole image. # 在识别某个模式（pattern）时，一个神经元并不需要图片的所有像素点。对于一张人类全身照的图片，我们只需要看到头部而非整张图片就可以判断它是一个人脸。所以我们应该是可以用少量参数去识别这些pattern的。\n性质2：The same patterns appear in different regions. # 比如说人脸可以在图片的中间区域，也可以在图片的某个角落区域。所以识别不同区域中的相同pattern的多个分类器（或detector）应该用同一组参数或者共享参数。\n性质3：Subsampling the pixels will not change the object # CNN架构说明 # 2014年在ECCV上提出，针对上述的图片的3个性质，确定了CNN的架构如下。\n如上图所示，图片经过卷积层然后进行最大池化（max pooling），这个步骤可以进行多次；然后将数据展开（Flatten），然后将数据传进全连接前馈网络得到最后的图片分类结果。\n如上图所示，卷积是针对了图片的性质1和性质2，最大池化是针对了图片的性质3。\n卷积(Convolution) ★ # 假设有一张6×6的二值图，即一个6×6的矩阵。\n卷积核（Filter） # 神经元就是一个计算/函数，卷积核其实就是神经元。如下图所示，1个卷积层可以有多个卷积核，矩阵里元素的值就是需要通过学习得到的参数。因为这里的输入是一个矩阵，所以卷积核也是1个矩阵（卷积核的通道数等于输入的通道数）。假设卷积核大小是3×3，这对应了图片的性质1，即用小的卷积核识别一个小的pattern。\n怎么做卷积 # 如下图所示\n卷积区域： 根据该卷积核的大小（以3×3为例），选择图片中相同大小的区域进行卷积。 卷积的计算方法： 从图片中扫描得到的3×3矩阵和卷积核的3×3矩阵，这2个矩阵相同位置的元素相乘可以得到9个值并求和（也就是内积）得到1个值，这就是1次卷积操作。 卷积顺序和方向： 卷积核按照从左到右、从上到下的顺序，从图片左上角开始移动，移动步长（stride）可以设置（以1为例）。在扫描到的每个区域中，都进行1次卷积。1个卷积核移动结束后，则得到1个新的矩阵（大小为4×4），即1个卷积核的输出是1个矩阵。 卷积层有多个卷积核，每个卷积核都按照该方式进行卷积得到多个矩阵，这些矩阵合起来就形成了1个卷积层的特征图（Feature Map），这个特征图也就是卷积层的输出。 卷积层特征图的通道数等于该卷积层中卷积核的数量，即某卷积层有多少个卷积核，那该卷积层的特征图就有多少个通道。 ","date":"23 October 2023","permalink":"/posts/ai/li-hongyis-notes/%E4%B8%83convolutional-neural-network/","section":"博客","summary":"7.1 CNN入门详解 # 卷积神经网络（CNN）常常被用来做图像处理，当然也可以用一般的神经网络，那它们各自有什么优缺点呢？","title":"七、Convolutional Neural Network"},{"content":"梯度下降伪代码 # 梯度下降可以优化损失函数的值，使其尽量小，即可找到最好（在数据集上拟合效果最好）的模型参数。现在假设模型 $f$ 中只有一个参数 $w$ ，则损失函数为 $L(f) = L(w)$ ，梯度下降算法如下（若模型有多个参数，按相同方法更新各方法）\n初始化参数：随机选取一个 $w_0$ （$w_0$ 并不一定是随机选取） 计算梯度 $\\frac{dL(f)}{dw}$ ，如果小于0，此时 $w$ 增大则 $L(f)$ 会减小；如果大于0，此时 $w$ 增大则 $L(w)$ 会减小。如果模型有多个参数，则计算损失函数在各个参数方向上的偏导数。 更新模型参数 $w_1=w_0-lr\\frac{dL(f)}{dw}$ ，$w$ 的变化量取决于梯度和学习率（Learning Rate）的大小：梯度绝对值或学习率越大，则 $w$ 变化量越大。如果模型有多个参数，则用上一步计算出的偏导数对应更新各参数。 重复第2步和第3步。经过多次参数更新/迭代（iteration），可以使损失函数的值达到局部最小（即局部最优，Local Optimal），但不一定是全局最优。 自适应学习率（Adaptive Learning Rate） # 梯度下降的过程中，固定学习率并不合理。学习率太大，可能导致loss不减小反而增大；学习率太小，loss会减小得很慢。基本原则是随着参数迭代更新，学习率应该越来越小，比如 $\\eta_t = \\frac{\\eta}{\\sqrt{t+1}}$ 。更好的办法是每个参数都有各自的学习率，比如Adagrad。\nAdagrad # Adaptive Gradient Descent，自适应梯度下降。2011年提出，核心是每个参数（parameter）有不同的学习率。每次迭代中，学习率要除以它对应参数的之前梯度的均方根（RMS），即 $w_{t+1} = w_t-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}{(g_t)^2}}}g_t$ ，其中 $t$ 是迭代次数，$w$ 是参数，$g$ 是梯度，$\\eta$ 是初始学习率。随着参数迭代，$t$ 越来越大，$\\sqrt{\\sum_{i=0}^{t}{(g_t)^2}}$ 也越来越大，因此学习率的变化趋势是越来越小。\nAdagrad的矛盾（Contradiction） # 一般的梯度下降方法中 $w_{t+1}=w_t-\\eta_tg_t$ ，其中 $\\eta_t$ 是常数，梯度越大时，则参数更新的步幅越大，这是由 $g_t$ 项决定的。在Adagrad中，$\\eta$ 是常量，梯度 $g_t$ 越大时，会使得参数更新的步幅越大，但 $\\sqrt{\\sum_{i=0}^{t}{(g_t)^2}}$ 越大会使得参数更新的步幅越小，这是一个矛盾吗？\n为什么要除以之前的梯度的均方根？\n一种直观的解释：增强参数更新步幅变化的惯性。与之前梯度相比如果现在的梯度更大，则现在梯度除以之前的梯度会使参数更新的步幅更大；如果现在的梯度更小，则会使步幅更新的步幅更小。这样就相当于增强了参数更新步幅变化的惯性，即如果参数更新的步幅突然变大或变小，就扩大这个趋势。 同时考虑一次梯度和二次梯度：在Adagrad中，之前梯度的均方根是用来通过一次梯度估计二次梯度（虽然可以直接使用二次梯度，但其很难计算） 只考虑一个参数：当参数只有一个或只考虑一个参数时，梯度越大，离最优点就越远，参数更新的步幅应该越大。 考虑多个参数：当参数由多个参数时，上述内容不一定成立。如果参数1的梯度比参数2的梯度大，但如果损失函数关于参数1的曲线比关于参数2的曲线更陡峭（即二次梯度更大），那参数1离最优点的距离可能比参数2更近。所以当参数有多个或者考虑多个参数时，我们既要考虑一次梯度又要考虑二次梯度。结论是一次梯度越大、二次梯度越小，离最优点就越远，参数更新的步幅应该越大。 SGD # Stochastic Gradient Descent，随机梯度下降，1847年提出，可以让训练过程更快。普通梯度下降中需要计算所有样本的Loss，二SGD只计算一个样本的Loss，然后进行梯度下降。\n梯度下降的数学理论 # 初始化一组参数后，我们找到领域中另一个使损失函数最小的一组参数并更新参数（然后不断重复这一步骤）。 在极小的邻域中，可以利用泰勒级数将损失函数简化，然后求其最小值，损失函数简化后，要使其最小即是让其中两个向量的内积最小，由此可以得出新的一组参数的值（具体过程略），这就是梯度下降算法。 学习率的作用是限制邻域大小，学习率太大可能使邻域太大，导致损失函数展开程泰勒级数时的误差较大。 当然也可以将损失函数展开成2次（比如牛顿迭代式），但这并不实用，因为要计算二次微分，甚至可能要求除海森矩阵（Hessian Matrix）逆矩阵等等，这些在做深度学习时是不实用的。 梯度下降的局限性 # 梯度下降过程中，每次参数更新不一定都会使损失函数的值更小。求出的知识局部最小值（Local Minima）甚至是鞍点（Saddle Point），不一定是全局最优解。\n","date":"23 October 2023","permalink":"/posts/ai/li-hongyis-notes/%E4%B8%89%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/","section":"博客","summary":"梯度下降伪代码 # 梯度下降可以优化损失函数的值，使其尽量小，即可找到最好（在数据集上拟合效果最好）的模型参数。现在假设模型 $f$ 中只有一个参数 $w$ ，则损失函数为 $L(f) = L(w)$ ，梯度下降算法如下（若模型有多个参数，按相同方法更新各方法）","title":"三、梯度下降"},{"content":"2.1 线性回归模型 # 回归模型应用案例 # 股票市场预测（Stock Market Forecast）：预测某个公司明天的股票情况 自动驾驶车（Self-Driving Car）：预测方向盘的转动角度 推荐系统（Recommendation）：预测某用户购买某商品的可能性 线性回归模型（Linear Regression Model） # 形式如下： $y= f(x) = w \\cdot x + b $\ny是输出，$\\widehat{y}$ 是真实值/标签（label) w是权重（weight） b是偏置（bias） x是输入（input），也可叫做特征（feature）。数据集中一般包含多个object，每个object一般包含多个component。此时，上标是object的索引，下标是component的索引 损失函数（Loss Function）如果不考虑模型的好坏，衡量一个函数的好坏，其实是衡量模型参数的好坏。以线性模型为例，就是衡量参数和的好坏。如 $ L(f) = L(w,b) = \\sum_{n=1} ^{10}{ \\widehat{y} - (b + w \\cdot x_n)} $ ，把所有的样本误差的平方和作为损失函数 输入：一个函数 输出：多么地不好（how bad it is）。损失函数值越大，则这个函数越差、与数据集中内容月不相符 梯度下降（Gradient Descent） # 梯度下降可以优化损失函数的值，使其尽量小，即可找到最好（在数据集上模拟效果最好）的模型参数。 现在假设模型$f$中只有一个参数 $w$，则损失函数为$L(f) = L(w)$ ，梯度下降算法如下\n初始化参数：随机选取一个 $ w_0 $（并不一定是随机选取），令 $ w = w_0 $ 计算梯度 $\\frac{dL(f)}{dw}|_{w=w_0}$ ，如果小于0，此时w增大则L（f）减小，如果大于0，此时w减小则L（w）会减小。如果模型中有多个参数，则计算损失函数在各个参数方向上的偏导数 更新模型参数，$w_1 = w_0 - lr \\frac{dL(f)}{dw}|_{w=w_0}$ ，w的变化量取决于梯度和学习率（Learning Rate）的大小：梯度绝对值或学习率越大，则w变化量越大。如果模型有多个参数，则用上一步计算出的偏导数对应更新各参数。 重复第2步和第3步，经过多次参数更新/迭代（iteration），可以使损失函数的值达到局部最小（即局部最优，Local Optimal），但不一定是全局最优。 现在假设模型$f$中只有两个参数 $w$，则损失函数为$L(f) = L(w)$ ，梯度下降算法如下（若模型中有多个参数，按相同方法更新各参数）\n初始化参数：随机选取初始值$w_0$和$b_0$（并不一定是随机选取），令$w = w_0$，$b=b_0$ 计算梯度 $ \\frac{dL(f)}{dw}|{w=w_0,b=b_0} $， $ \\frac{dL(f)}{db}|{w=w_0,b=b_0} $ 更新模型参数，$ w_1 = w_0 - lr \\frac{dL(f)}{dw}|{w=w_0,b=b_0} $ ， $ b_1 = b_0 - lr \\frac{dL(f)}{db}|{w=w_0,b=b_0} $ 。 重复第2步和第3步，经过多次参数更新/迭代（iteration），可以使损失函数的值达到局部最小（即局部最优，Local Optimal），但不一定是全局最优。 2.2 如何选择模型、减小误差 # 模型选择（How to select model） # 模型越复杂，一般在训练集上的误差（Error）越小。因为更复杂的模型（函数集）包含更多的函数。比如二次模型包含一次模型 模型越复杂，其在测试集上的误差（Error）不一定越小，因为模型过于复杂时，越容易被数据影响，可能导致过拟合。 误差（Error） # 误差的来源 # 暂时称通过机器学习得到的函数为人工函数，它其实是对“上帝函数”的估计（Estimator），和“上帝函数”之间是有误差的。\n误差来源于两方面：一是Bias，二是Variance，需要权衡（trade-off）两者以使总误差最小。\n如上图所示，Bias是指人工函数（的期望）和上帝函数之间的距离，Variance是指人工函数的离散程度（或者说是不稳定程度）\n如上图所示，横轴是模型的复杂程度（1次幂、2次幂、……），纵轴是误差大小。模型越复杂，Bias越小，Variance越大。\nVariance # 使用相同模型在不同数据上拟合得到的函数是不同的，这些函数之间的离散程度就是Variance。以射箭为例，Variance衡量的就是射得稳不稳。模型越复杂，Variance越大。因为模型越简单，越不容易被数据影响（对数据不敏感，感知数据变化的能力较差），那Variance就越小。\nBias # 使用相同模型在不同数据上拟合得到的函数是不同的，取这些函数的“期望”，该期望与“真理”的差距就是Bias。以射箭为例，Bias衡量的就是射得准不准（这里的“准”的含义有待商榷）。**模型越简单，Bias越大。**因为模型就是个函数集（Function Set）。模型越简单，则其包含的函数就越少、包含“上帝函数”的几率就越小，甚至可能不包括上帝函数。\n在函数集很小的情况下，即使是其中最好的函数，它与“上帝函数”的差距也还是很大的。\n2.3 欠拟合与过拟合 # 欠拟合（Underfitting） # Bias较大、Variance较小。如果模型在训练集上的误差很大，则此时Bias是大的，情况为欠拟合。\nBias大时如何处理：使用更复杂的模型，比如添加考虑更多维度的输入、把线性模型换成非线性模型。\n过拟合（Overfitting） # Bias较小、Variance较大。如果模型在训练集上的误差很小，但是在测试集上的误差很大，则此时Variance是大的，情况为过拟合。\nVariance大时如何处理：\n使用更复杂的数据集：比如添加数据（很有效，但不一定做得到）、数据增强等方法。 使用更简单的模型（不是根本方法），可能是模型过于复杂导致了过拟合，因此可以简化模型缓解过拟合。 正则化（Regularization）：正则化可能会使Bias增大，所以需要调整正则化的参数。如$L_{new} = L_{old} + \\lambda \\sum{w_i^2}$ ，其中$\\lambda$ 是一个常数。加上正则项 $\\lambda \\sum{w_i^2}$ 的目的是让函数参数尽可能地接近0，使函数变得平滑。 平滑（Smooth） # 平滑是指输入变化影响输出变化的程度（输出对输入的敏感程度）。假设输入变化，如果函数越不平滑，则输出变化程度越大。函数参数越接近0，这个函数就越平滑（smooth）。\n我们为什么喜欢一个平滑的函数？适度平滑的函数可以缓解函数输入中包含的噪声对函数输出的影响。如果输入中包含一些噪声/干扰（noise），那平滑函数的输出受输入中包含的噪声干扰的程度更小。 我们为什么不喜欢过于平滑的函数？函数过于平滑，就无法有效地提取数据的特征，这不是我们想要的函数。假设有一个极限平滑的函数，即该函数的输出不受输入的影响，那当然不是个好的函数。 2.4交叉验证 # 在机器学习中，同城不能将全部数据用于模型训练，否则将没有数据集可以用来评估模型\nThe Validation Set Approach # 将数据集划分成训练集（Training Set）和测试集（Test Set）两部分。\n缺点：这种方法的缺点是依赖于训练集和测试集的划分方法，并且只用了部分数据进行模型的训练。\nLOOCV（Leave One Out Cross Validation） # 假设数据集中有N个数据，取其中1个数据作为测试集，将剩下的N-1个数据作为训练集，这样重复N次就得到N个模型以及N个误差值，最终使用这N个误差值的平均值评估该模型。\n优点：该方法不受训练集和测试集划分方法的影响，因为每个数据都单独做过测试集；同时该方法用了N-1个数据训练模型，也几乎用到了所有的数据，保证了模型的Bias更小。\n缺点：该方法的缺点是计算量过大，是The Validation Set Approach耗时的N-1倍。\nK折交叉验证（K-fold Cross Validation） # 该方法是LOOCV的折中，即将数据集分成K份。\n如何选取K的值：K的选取是一个Bias和Variance的trade-off。一般选择K=5或10。K越大，每次训练时训练集的数据量就越大，则Bias越小；但每次训练时的训练集之间的相关性越大（考虑最极端的情况K=N，也就是LOOCV，每次训练使用的数据几乎是一样的），这种大相关性会导致最终的误差具有更大的Variance。\n","date":"23 October 2023","permalink":"/posts/ai/li-hongyis-notes/%E4%BA%8C%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/","section":"博客","summary":"2.1 线性回归模型 # 回归模型应用案例 # 股票市场预测（Stock Market Forecast）：预测某个公司明天的股票情况 自动驾驶车（Self-Driving Car）：预测方向盘的转动角度 推荐系统（Recommendation）：预测某用户购买某商品的可能性 线性回归模型（Linear Regression Model） # 形式如下： $y= f(x) = w \\cdot x + b $","title":"二、回归模型"},{"content":"5.1引言 # 深度学习的历史 # 1958年：心理学家Rosenblatt提出感知机（Perceptron） 它是一个线性模型。 1969年：有人说感知机是线性模型，具有局限性。 1980年代：多层感知机（Multi-layer Perceptron） 和当今的神经网络是没有本质差别的。 1986年：Hinton提出反向传播算法（Backpropagation） 但是超过3个隐藏层的神经网络，还是训练不出好的结果。 1989年：有人提出一个隐藏层就可以得到任何函数，为什么要多层？ 多层感知机慢慢淡出大家的视野。 2006年：受限玻尔兹曼机初始化（RBM Initialization） Hinton提出用受限玻尔兹曼机做初始化，很多人觉得这是个大突破，但实际上用处并不大。 至少让多层感知机回到大家的视野。 2009年：GPU 2011年：神经网络用于语音识别 2012年：神经网络技术赢得ILSVRC（ImageNet Large Scale Visual Recognition Challenge） 深度学习的三个步骤 # 和机器学习一样：\n确定模型（Model）/函数集（Function Set），在深度学习中就是定义一个神经网络。 不同的连接会构成多样的网络结构。 确定如何评价函数的好坏 如果是多分类，那和Classification一章中一样，计算每个样本预测结果与Ground Truth的交叉熵，然后求和，即为Loss。 确定如何找到最好的函数 还是Gradient Descent。 神经网络模型对应的函数比较复杂，而反向传播算法（Backpropagation）是一个很有效的计算神经网络梯度的方法。 神经网络的结构 # 输入层（Input Layer）：实际上就是输入，并不是真正的“层”。 隐藏层（Hidden Layers）：输入层和输出层之间的层，Deep指有很多隐藏层，多少层才算Deep并没有统一标准。可以看成特征提取器（Feature Extractor），作用是代替特征工程（Feature Engineering）。 输出层（Output Layer）：最后一层，可以看成分类器 全连接前反馈神经网络 # 即Fully Connected Feedforward Neural Network，FFN。\n全连接是指每个神经元与上一层的所有神经元相连。 前馈神经网络（FNN，Feedforward Neural Network）是指各神经元分层排列，每个神经元只与前一层的神经元相连，接收前一层的输出，并输出给下一层，各层间没有反馈。 一些网络 # 其中Residual Net并不是一般的全连接前馈神经网络\n网络结构 提出年份 层数 ImageNet错误率 AlexNet 2012 8 16.4% 2014 19 7.3% VGGNet 2014 22 6.7% Residual Net 2015 152 3.57% 机器学习和深度学习面对的不同问题 # 在机器学习中，人类需要手工做特征工程（Feature Engineering），人类需要思考如何提取特征。 有了深度学习以后，人类可以不做特征工程，但也遇到了新的问题：人类需要设计合适的网络结构。 这两个问题哪个更容易呢？可能后者更容易些，比如在图像识别、语音识别任务中，人类可能并不知道自己是如何识别图像和语音的，就无法通过符号主义进行特征工程。\n关于深度学习的一些疑问 # 虽然深度学习的的准确度很高，但是它使用的参数更多，参数多、准确度高也是很正常的事，所以有什么特别之处呢？ 只用一个神经元足够多的隐藏层，这个模型就包括了任意函数，那为什么不这么做而非要深度呢？为什么要是Deep而不是Fat呢？ 如何设计神经网络的结构？ 多少层？每一层有多少个神经元？ 只能凭经验（实验结果）和直觉，当然可以让机器自己去找网络结构，即网络架构搜索（NAS，Network Architecture Search）。 必须用全连接前馈神经网络吗？ 不是。比如卷积神经网络（Convolutional Neural Networks, CNN）。 5.2 神经网络为什么要是深度的 # 深度的原因 # 矮胖的神经网络和高瘦的神经网络，假设它们参数量相同，哪一个更好呢？ 2011年有一个实验，证明在参数量相当的情况下，高瘦的神经网络（即深度神经网络）的准确度更高，因为深度可以实现模块化。 只用一个神经元足够多的隐藏层，这个模型就包括了任意函数，那为什么不这么做呢？ 这样确实可以包括任意函数，但实现的效率不高。 相关网址，也可以通过谷歌等找找其它答案。 “深度”的好处 # 模块化（Modularization） # 就像写程序一样，我们不能把所有代码写在main函数里，而需要通过定义函数等方式将程序模块化。 如下图所示，假如要做一个图片的四分类，两个维度分别是头发长短和性别，如果使用矮胖的神经网络会遇到一个问题，就是短头发的女生样本和长头发的男生样本会比较少，那这两个类别的分类器就会比较差。\n如下图所示，我们可以先定义各属性的分类器（Classifiers for the attributes），即先定义性别和头发长短的分类器，然后再做四分类。这样第一层分类器就不会遇到样本少的问题，第二层的分类器也容易训练，整体上也需要更少的训练集。\n在深度神经网络中，每层网络都可以作为下一层网络使用的一个模块，并且这个模块化是通过机器学习自动得到的。 常有“人工智能=机器学习+大数据”的说法，但实际上“深度”使得需要的数据更少，如果数据集无限大，根本就不需要机器学习，只要去数据库里拿就好了。深度学习也并不是通过大量参数暴力拟合出一个模型，反而是在通过模块化有效利用数据。 这里只是一个图像分类的例子，“深度”产生的模块化在语音识别任务中也有体现，与逻辑电路也有相似的问题和结论，具体可以看** 李宏毅视频**。\n端到端学习（End-to-end Learning） # 深度神经网络模型就像是把一个个函数串接在一起，每个函数负责某个功能，每个函数负责什么功能是通过机器学习根据数据自动确定的。** 李宏毅视频**中有讲这一点在语音识别、CV任务中的体现。\n处理复杂任务 # 有时类似的输入要输出差别很大的结果，比如白色的狗和北极熊看起来差不多，但分类结果非常不同；有时差别很大的输入要输出相同的结果，比如火车正面和侧面的图片都应该被分类成火车。 只有一个隐藏层的网络是无法处理这种任务的。** 李宏毅视频**中有讲这一点在语音识别、CV任务中的体现。\n其它 # Do deep nets really need to be deep? ** 李宏毅视频**里也还有很多关于“深度”的探讨。\n5.3 神经网络中的反向传播算法 # 链式法则（Chain Rule） # $\\begin{cases} z=h(y)\\y=g(x)\\end{cases} \\rightarrow \\frac{dz}{dx}=\\frac{dz}{dy} \\frac{dy}{dx}$ $\\begin{cases} z=k(x,y)\\x=g(s)\\y=h(s)\\end{cases} \\rightarrow \\frac{dz}{ds}=\\frac{dz}{dx} \\frac{dx}{ds}+\\frac{dz}{dy} \\frac{dy}{ds}$ 反向传播算法（Backpropagation） # 变量定义 # 如下图所示，设神经网络输入为 $x_n$ ，该输入对应的 label 是 $\\hat y^n$ ，神经网络的参数是 $\\theta$ ，神经网络的输出是 $\\hat y^n$ 。整个神经网络的Loss为 $L(\\theta)=\\sum^N_{n=1}{C^n(\\theta)}$ 。假设 $\\theta$ 中有一个参数 $w$ ，那 $\\frac{\\partial{L(\\theta)}}{\\partial{w}}=\\sum^N_{n=1}{\\frac{\\partial{C^n(\\theta)}}{\\partial{w}}}$ 。\n一个神经元的情况 # 如下图所示， $z=x_1w_1+x_2w_2+b$ ，根据链式法则可知 $\\frac{\\partial{C}}{\\partial{w}}=\\frac{\\partial{z}}{\\partial{w}}\\frac{\\partial{C}}{\\partial{z}}$ ，其中为所有参数 $w$ 计算 $\\frac{\\partial{z}}{\\partial{w}}$ 是 Forward Pass，为所有激活函数的输入 z 计算 $\\frac{\\partial{C}}{\\partial{z}}$ 是 Backward Pass。\nForward Pass # Forward Pass 是为所有参数 $w$ 计算 $\\frac{\\partial{z}}{\\partial{w}}$ ，它的方向是从钱往后算的，所以叫做 Forward Pass。一一个神经元为例，因为 $z=x_1w_1+x_2w_2+b$ ，所以 $\\frac{\\partial{z}}{\\partial{w_1} }= x_1 $ ， $\\frac{\\partial{z}}{\\partial{w_2} }= x_2 $ ，如下图所示。\n规律是：该权重乘以的那个输入的值。所以当有多个神经元时，如下图所示。\nBackward Pass # Backward Pass 是为所有激活函数的输入 z 计算 $\\frac{\\partial{C}}{\\partial{z}}$ ，它的方向是从后往前算的，要先算出输出层的 $\\frac{\\partial{C}}{\\partial{z\u0026rsquo;}}$ ，再往前计算它神经元的 $\\frac{\\partial{C}}{\\partial{z}}$ ，所以叫做 Backword Pass。\n如上图所示，令 $a=\\sigma(z)$ ，根据链式法则，可知 $\\frac{\\partial{C}}{\\partial{z}}=\\frac{\\partial{C}}{\\partial{a}}$ ，其中 $\\frac{\\partial{C}}{\\partial{a}}=\\sigma\u0026rsquo;(z)$ 是一个常数，因为在 Forward Pass 时 z 的值就已经确定了，而 $\\frac{\\partial{C}}{\\partial{a}}=\\frac{\\partial{z}\u0026rsquo;}{\\partial{a}}\\frac{\\partial{C}}{\\partial{z}\u0026rsquo;}+\\frac{\\partial{z}\u0026rsquo;\u0026rsquo;}{\\partial{a}}\\frac{\\partial{C}}{\\partial{z}\u0026rsquo;\u0026rsquo;}=w_3\\frac{\\partial{C}}{\\partial{z}\u0026rsquo;}+w_4\\frac{\\partial{C}}{\\partial{z}\u0026rsquo;\u0026rsquo;}$ ，所以 $ \\frac{\\partial{C}}{\\partial{z}}=\\sigma\u0026rsquo;(z)[w_3\\frac{\\partial{C}}{\\partial{z}\u0026rsquo;}+w_4\\frac{\\partial{C}}{\\partial{z}\u0026rsquo;\u0026rsquo;}]$ 。\n对于式子 $\\frac{\\partial{C}}{\\partial{z}}=\\sigma\u0026rsquo;(z)[w_3\\frac{\\partial{C}}{\\partial{z}\u0026rsquo;}+w_4\\frac{\\partial{C}}{\\partial{z}\u0026rsquo;\u0026rsquo;}]$ ，我们可以发现两点：\n1、$\\frac{\\partial{C}}{\\partial{z}}$ 的计算式是递归的，因为在计算 $\\frac{\\partial{C}}{\\partial{z}}$ 的时候需要计算 $\\frac{\\partial{C}}{\\partial{z}\u0026rsquo;}$ 和 $\\frac{\\partial{C}}{\\partial{z}\u0026rsquo;\u0026rsquo;}$ 。如下图所示，输出层的 $\\frac{\\partial{C}}{\\partial{z}\u0026rsquo;}$ 和 $\\frac{\\partial{C}}{\\partial{z}\u0026rsquo;\u0026rsquo;}$ 是容易计算的。\n2、$\\frac{\\partial{C}}{\\partial{z}}$ 的计算式 $\\frac{\\partial{C}}{\\partial{z}}=\\sigma\u0026rsquo;(z)[w_3\\frac{\\partial{C}}{\\partial{z}\u0026rsquo;}+w_4\\frac{\\partial{C}}{\\partial{z}\u0026rsquo;\u0026rsquo;}]$ 是一个神经元的形式。如下图所示，只不过没有嵌套 sigmoid 函数，而是乘一个常数 $\\sigma\u0026rsquo;(z)$ ，每一个 $\\frac{\\partial{C}}{\\partial{z}}$ 都是一个神经元的形式，所以可以通过神经网络计算 $\\frac{\\partial{C}}{\\partial{z}}$ 。\n总结 # 通过Forward Pass ，为所有参数 $w$ 计算 $\\frac{\\partial{z}}{\\partial{w}}$ ； 通过 Backward Pass ，为所有激活函数的输入 z 计算 $\\frac{\\partial{C}}{\\partial{z}}$ ； 最后通过 $\\frac{\\partial{C}}{\\partial{w}}=\\frac{\\partial{z}}{\\partial{w}}\\frac{\\partial{C}}{\\partial{z}}$ ，也就求出了梯度。 ","date":"23 October 2023","permalink":"/posts/ai/li-hongyis-notes/%E4%BA%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/","section":"博客","summary":"5.1引言 # 深度学习的历史 # 1958年：心理学家Rosenblatt提出感知机（Perceptron） 它是一个线性模型。 1969年：有人说感知机是线性模型，具有局限性。 1980年代：多层感知机（Multi-layer Perceptron） 和当今的神经网络是没有本质差别的。 1986年：Hinton提出反向传播算法（Backpropagation） 但是超过3个隐藏层的神经网络，还是训练不出好的结果。 1989年：有人提出一个隐藏层就可以得到任何函数，为什么要多层？ 多层感知机慢慢淡出大家的视野。 2006年：受限玻尔兹曼机初始化（RBM Initialization） Hinton提出用受限玻尔兹曼机做初始化，很多人觉得这是个大突破，但实际上用处并不大。 至少让多层感知机回到大家的视野。 2009年：GPU 2011年：神经网络用于语音识别 2012年：神经网络技术赢得ILSVRC（ImageNet Large Scale Visual Recognition Challenge） 深度学习的三个步骤 # 和机器学习一样：","title":"五、深度学习"},{"content":"6.1 神经网络训练问题与解决方案 # 明确问题类型及其对应方法 # 在深度学习中，一般有两种问题：\n在训练集上性能不好 在测试集上性能不好。 当一个方法被提出时，它往往是针对这两个问题其中之一的，比如dropout方法是用来处理在测试集上性能不好的情况。\n处理神经网络在训练集上性能不好的情况和方法 # 修改神经网络架构，比如换成更好的激活函数： sigmoid函数会导致梯度消失，可以换成ReLU、Leaky ReLU、Parametric ReLU、Maxout 调整学习率： 比如RMSProp、Momentum、Adam 处理神经网络在测试集上性能不好的情况和方法 # Early Stopping、Regularization，这两个是比较传统的方法，不只适用于深度学习 Dropout，比较有深度学习的特色 一些性能优化方法的简介 # 下面3点都是在增加模型的随机性，鼓励模型做更多的exploration。\nShuffling： 输入数据的顺序不要固定，mini-batch每次要重新生成 Dropout： 鼓励每个神经元都学到东西，也可以广义地理解为增加随机性 Gradient noise： 2015年提出，计算完梯度后，加上Gaussian noise。 随着迭代次数增加，noise应该逐渐变小。 下面3点是关于学习率调整的技巧\nwarm up： 开始时学习率较小，等稳定之后学习率变大 Curriculum learning： 2009年提出，先使用简单的数据训练模型（一方面此时模型比较弱，另一方面在clean data中更容易提取到核心特征），然后再用难的数据训练模型。 这样可以提高模型的鲁棒性。 Fine-tuning 下面3点是关于数据预处理的技巧，避免模型学习到太极端的参数\nNormalization： 有Batch Normalization、Instance Normalization、Group Normalization、Layer Normalization、Positional Normalization Regularization 6.2 神经网络精度低不一定是因为过拟合 # 相比于决策树等方法，神经网络更不容易过拟合：K近邻、决策树等方法在训练集上更容易得到100%等很高的正确率，神经网络一般不能，训练神经网络首先遇到的问题一般是在训练集上的精度不高。 不要总是把精度低归咎于过拟合：如果模型在训练集上精度高，对于K近邻、决策树等方法我们可以直接判断为过拟合，但对于神经网络来说我们还需要检查神经网络在测试集上的精度。如果神经网络在训练集上精度高但在测试集上精度低，这才说明神经网络过拟合了。 如果56层的神经网络和20层的神经网络相比，56层网络在测试集上的精度低于20层网络，这还不能判断为56层网络包含了过多参数导致过拟合。一般来讲，56层网络优于20层网络，但如果我们发现56层网络在训练集上的精度本来就低于20层网络，那原因可能有很多而非过拟合，比如56层网络没训练好导致一个不好的局部最优、虽然56层网络的参数多但结构有问题等等。 感兴趣可以看看ResNet论文** Deep Residual Learning for Image Recognition**，这篇论文可能与该问题有关。 6.3 常用激活函数（训练集） # 梯度消失（Vanishing Gradient Problem） # 定义：1980年代常用的激活函数是sigmoid函数。以MNIST手写数字识别为例，在使用sigmoid函数时会发现随着神经网络层数增加，识别准确率逐渐下降，这个现象的原因并不是过拟合（原因见上文），而是梯度消失。\n如上图所示，当神经网络层数很多时，靠近输入层的参数的梯度会很小，靠近输出层的参数的梯度会很大。当每个参数的学习率相同时，靠近输入层的参数会更新得很慢，靠近输出层的几层参数会更新得很快。所以，当靠近输入层的参数几乎还是随机数时，靠近输出层的参数已经收敛了。\n原因： 按照反向传播的式子，这确实是会发生的。直观感觉上，sigmoid函数输入的范围是无穷大，但输出的范围是[0,1]，也就是说sigmoid函数减弱了输入变化导致输出变化的幅度。那为什么靠近输出层的参数的梯度更大呢？sigmoid函数是一层层叠起来的，不断地减弱靠近输入层的参数的变化导致输出变化的幅度，所以更靠后的参数的梯度越大。 解决方法： Hinton提出无监督逐层训练方法以解决这个问题，其基本思想是每次训练一层隐节点。 后来Hinton等人提出修改激活函数，比如换成ReLU。 ReLU（Rectified Linear Unit） # 定义： 当输入小于等于0时，输出为0；当输入大于0时，输出等于输入，如下图所示。\n优点： 相比于sigmoid函数，它有以下优点\n运算更快 更符合生物学 等同于无穷多个bias不同的sigmoid函数叠加起来 可以解决梯度消失问题 如何解决梯度消失问题？ 当ReLU输出为0时该激活函数对神经网络不起作用，所以在神经网络中生效的激活函数都是输出等于输入，所以就不会出现sigmoid函数导致的减弱输入变化导致输出变化的幅度的情况。 ReLU会使整个神经网络变成线性的吗？ 可知有效的激活函数都是线性的，但整个神经网络还是非线性的。当输入改变很小、不改变每个激活函数的Operation Region（操作区域，大概意思就是输入范围）时，整个神经网络是线性的；当输入改变很大、改变了Operation Region时，整个神经网络就是非线性的。==目前我是凭直觉理解这一点，还未细究== ReLU可以做微分吗？ 不用处理输入为0的情况，当输入小于0时，微分就是0，当输入大于0时微分就是1。 Leaky ReLU # 当输入小于等于0时，输出为输入的0.01倍；当输入大于0时，输出等于输入。\nParametric ReLU # 当输入小于等于0时，输出为输入的 $\\alpha$ 倍；当输入大于0时，输出等于输入。\n其中 $\\alpha$ 是通过梯度下降学习到的参数\nMaxout # 通过学习得到一个激活函数，人为将每层输出的多个值分组，然后输出每组值中的最大值。（跟maxpooling一模一样），其实ReLU是Maxout的一个特例。\nMaxout比ReLU包含了更多的函数\nMaxout 可以得到任意的分段线性凸函数（piecewise linear convex），有几个分段取决于每组里有几个值\n如何训练Maxout # Maxout只是选择输出哪一个线性函数的值而已，因此Maxout激活函数还是线性的。 因为在多个值中只选择最大值进行输出，所以会形成一个比较瘦长/窄深的神经网络。 在多个值中只选择最大值进行输出，这并不会导致一些参数无法被训练：因为输入不同导致一组值中的最大值不同，所以各个参数都可能被训练到。 当输入不同时，形成的也是不同结构的神经网络。\n6.4 学习率调整方法（训练集） # Adagrad # Adaptive Gradient Descent，自适应梯度下降，解决不同参数应该使用不同的更新速率的问题。Adagrad自适应地为各个参数分配不同学习率的算法。2011年提出，核心是每个参数（parameter）有不同的学习率。每次迭代中，学习率要除以它对应参数的之前梯度的均方根（RMS），即 $w_{t+1} = w_t-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}{(g_t)^2}}}g_t$ 。\nRMSProp # 背景 # RMSProp是Adagrad的升级版，在2013年Hinton在Coursera提出。 在训练神经网络时，损失函数不一定是凸函数（局部最小值即为全局最小值），可能是各种各样的函数，有时需要较大的学习率，有时需要较小的学习率，而Adagrad并不能实现这种效果，因此产生了RMSProp。\n定义 # $w_{t+1}=w_{t}-\\frac{\\eta}{\\sigma_t}g_t$ （$\\sigma_0=g_0 , \\sigma_t=\\sqrt{\\alpha(\\sigma_{t+1})^2+(1-\\alpha)(g_t)^2}$），其中 $w$ 是某个参数， $\\eta$ 是学习率，$g$ 是梯度， $\\alpha$ 代表旧的梯度的重要性，值越小则旧的梯度越不重要。\n神经网络中很难找到最优的参数吗？ # 面临的问题有plateau、saddle point和local minima。\n2007年有人（名字读音好像是young la ken）指出神经网络的error surface是很平滑的，没有很多局部最优。假设有1000个参数，一个参数处于局部最优的概率是 $p$ ，则整个神经网络处于局部最优的概率是 $p^{1000}$ ，这个值是很小的。\nMomentum # 如何处理停滞期、鞍点、局部最小值等问题？ # 考虑现实世界中物体具有惯性、动量（Momentum）的特点，尽可能避免“小球”陷入error surface上的这几种位置。\n定义 # 1986年提出。如下图所示，不仅考虑当前的梯度，还考虑上一次的移动方向：$v_t = \\lambda v_{t-1}-\\eta g_t$ ， $v_0=0$，其中 $t$ 是迭代次数，$v$ 指移动方向（movement），类似物理里的速度，$g$ 是梯度（gradient），$\\lambda$ 用来控制惯性的重要性，值越大代表惯性越重要，$\\eta$ 是学习率\nAdam # RMSProp+Momentum+Bias Correction，2015年提出\nAdam VS SGDM # 目前常用的就是Adam和SGDM。\nAdam训练速度快，large generalization gap（在训练集和验证集上的性能差异大），但不稳定； SGDM更稳定，little generalization gap，更加converge（收敛）。 SGDM适用于计算机视觉，Adam适用于NLP、Speech Synthesis、GAN、Reinforcement Learning。 ####　SWATS\n2017年提出，尝试把Adam和SGDM结合，其实就是前一段时间用Adam，后一段时间用SGDM，但在切换时需要解决一些问题。\n尝试改进Adam # AMSGrad Adam的问题: Non-informative gradients contribute more than informative gradients. 在Adam中，之前所有的梯度都会对第 $t$ 步的movement产生影响。然而较早阶段(比如第1、2步)的梯度信息是相对无效的，较晚阶段（比如 $t-1$ 、$t-2$ 步)的梯度信息是相对有效的。在Adam中，可能发生较早阶段梯度相对于较晚阶段梯度比重更大的问题。 提出AMSGrad: 2018年提出 AdaBound 2019年提出，目的也是改进Adam。 Adam需要warm up吗？需要。 warm up：开始时学习率小，后面学习率大。 因为实验结果说明在刚开始的几次（大概是10次）迭代中，参数值的分布比较散乱（distort），因此梯度值就比较散乱，导致梯度下降不稳定。 RAdam 2020年提出 Lookahead 2019年提出，像一个wrapper一样套在优化器外面，适用于Adam、SGDM等任何优化器。 迭代几次后会回头检查一下。 Nadam 2016年提出，把NAG的概念应用到Adam上。 AdamW 2017年提出，这个优化器还是有重要应用的（训练出了某个BERT模型）。 尝试改进SGDM # LR range test 2017年提出 Cyclical LR 2017年提出 SGDR 2017年提出，模拟Cosine但并不是Cosine One-cycle LR 2017年提出，warm-up+annealing+fine-tuning SGDW 2017年提出 改进Momentum # 背景 # 如果梯度指出要停下来，但动量说要继续走，这样可能导致坏的结果。\nNAG（Nesterov accelerated gradient） 1983年提出，会预测下一步。 6.5 处理测试集性能不好的方法 # 6.5.1 Early Stopping # 如果学习率调整得较好，随着迭代次数增加，神经网络在训练集上的loss会越来越小，但因为验证集（Validation set）和训练集不完全一样，所以神经网络在验证集上的loss可能不降反升，所以我们应该在神经网络在验证集上loss最小时停止训练。\n6.5.2 Regularization # 正则化，L2 regularization如下图所示（$||\\theta||^2$ ，L2范式的平方），一般不考虑bias项。\nL2 regularization 的偏微分，每次都会使 $w_t$ 接近0，因为 $1-\\eta\\lambda$ 接近1但是小于1，比如0.99，每次成0.99，都会向0接近。\n当然，regularization 也可以有其他形式，比如是 $w_i$ 绝对值的集合，成为L1 regularization，具体如下图。但是，这个方法每次使得梯度减少的速度都一样，都是 1 或 -1 乘上 $\\eta\\lambda$ 。\n6.5.3 Dropout # dropout是指在 深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。注意是暂时，对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。\ndropout 在test集合上不使用，只用在Validation set上，并且在test集合上的时候，需要把weight 乘上 $(1-p)%$ 。\n直观的解释：一个团队中，每个人都希望自己的同伴会做这个工作，最后什么都没有做；但是如果你知道你的同伴会dropdout，你就会做的更好，去carry他；不过当testing的时候，没有人会去dropout，所以最后的结果会更好。\n为什么要乘 $(1-p)%$ ？直观的解释如下图。\ndropout是一个整体\n做dropout 的时候是训练了一堆 neural structual\n那这些值的 average 与一开始算的 $\\hat y$ 是否相等呢？\n其实是一样的。\n","date":"23 October 2023","permalink":"/posts/ai/li-hongyis-notes/%E5%85%ADtips-for-training-dnn/","section":"博客","summary":"6.1 神经网络训练问题与解决方案 # 明确问题类型及其对应方法 # 在深度学习中，一般有两种问题：","title":"六、Tips for Training DNN"},{"content":"4.1 分类简介及其与回归的区别 # 分类模型应用案例（Classification Cases） # 信用评分（Credit Scoring） 输入：收入、储蓄、职业、年龄、信用历史等等 输出：是否贷款 医疗诊断（Medical Diagnosis） 输入：现在症状、年龄、性别、病史 输出：哪种疾病 手写文字识别（Handwritten Character Recognition） 输入：文字图片 输出：是哪一个汉字 人脸识别（Face Recognition） 输入：面部图片 输出：是哪个人 把分类当成回归去做 # 不行\n假设有两个类别，其中类别1的标签为1，类别2的标签为-1，那0就是分界线，大于0就是类别1，小于0就是类别2。但是回归模型会惩罚哪些太正确的样本，如果结果远远大于1，它的分类应该是类别1还是类别2？这时为了降低整体误差，需要调整已经找到的回归函数，就会导致结果的不准确。 假设有多个类别，类别1的标签是1，类别2的标签是2，类别3的标签是3。这样的话，标签间具有2和3相近、3大于2这种本来不存在的数字关系。 理想替代方案（Ideal Alternatives） # 模型：模型可以根据特征判断类型，输入是特征，输出是类别 损失函数：预测错误的次数，即$L(f)=\\sum_n{\\sigma(f(x_n) \\neq \\hat{y_n}) }$ 。这个函数不可微。 如何找到最好的函数，比如感知机（Perceptron）、支持向量机（SVM） 4.2 分类模型指概率生成模型 # 贝叶斯公式 # $P(A \\cap B) = P(A)P(B|A) = P(B)P(A|B)$ $P(A|B) = \\frac{P(A)P(B|A)}{P(B)}$ 全概率公式 # $P(B)=\\sum_{i=1}^{n}{P(A_i)P(B|A_i)}$\n概率生成模型（Probalitity Genetative Model） # 理论与定义 # 假设有两个类别的$C_1和C_2$，要判断对象$x$属于哪个类别，这样把分类问题变成了概率计算问题。\n根据贝叶斯公式（Bayes\u0026rsquo; theorem）和全概率公式（Total Probability Theorem）可以知道，$x$属于类别$C_1$的概率为$P(C_1|x)= \\frac{P(x|C_1)P(C_1)}{P(x)}=\\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}$ ，如果$P(C_1|x)\u0026gt;0.5$ 则类别为$C_1$ ，否则类别为$C_2$。 概率生成模型的意思就是可以通过这个模型生成一个$$x$$。具体来讲就是，根据$P(x)=P(x|C_1)P(C_1)+P(x|C_2)P(C_2)$ 计算出$P(x)$，就可以知道 $x$ 的分布进而生成 $x$ 。如果想要计算出$P(x)$，就要根据训练集估计出$P(C_1)$、$P(x|C_1)$、$P(C_2)$、$P(x|C_2)$这四个值。更直观一点地讲，每个类别就是一个多元正态分布，其中多元是因为每个样本有多个维度的特征。 可以根据数据集中属于两个类别的对象的数量计算 $P(C_1)$ 和 $P(C_2)$ 这两个先验概率（Prior Probability）。如果有2个样本属于类别$C_1$ ，4个样本属于类别$C_2$ ，那$P(C_1)= \\frac{1}{3}$、$P(C_2)= \\frac{2}{3}$。 要计算后验概率（Posterior Probability）$P(x|C_1)$ 和 $P(x|C_2)$，可以假设训练集中的各类别样本的特征分别是从某个多元正太分布（多元对应特征的多维）中取样得到的，或者说是假设训练集中各类别样本的特征分别符合某多元正态分布。该正太分布的输入是一个样本的特征 $x$，输出为样本 $x$ 是从这个正太分布取样得到（或者说该样本属于某类别）的概率密度，然后通过积分就可以求得 $P(x|C_1)$ 和 $P(x|C_2)$ 。 正太分布公式为 $f_{\\mu,\\sum{(x)}}=\\frac{1}{(2\\pi)^\\frac{D}{2}} \\frac{1}{|\\sum|^\\frac{1}{2}} {e^{-\\frac{1}{2}(x-\\mu)^T\\sum^{-1}{x-\\mu}}}$ 。正太分布有2个参数，即均值 $\\mu$ （代表正太分布的中心位置）和协方差矩阵（Covariance Matrix）$\\sum$ （代表正态分布的离散程度），计算出均值 $\\mu$ 和协方差 $\\sum$ 即可得到该正态分布。公式中的 $D$ 为多维特征的维度。 实际上从任何一个正态分布中取样都有可能得到训练集中的特征，只是概率不同而已。通过极大值似然估计（Maximum Likelihood Estimate，MLE），我们可以找到取样得到训练集特征的概率最大的那个正态分布，假设其均值和协方差矩阵为 $ \\mu^* $ 和 $ \\sum^* $ 。 根据某正态分布的均值 $\\mu$ 和协方差 $\\sum$ ，可以计算出从该正态分布取样得到训练集的概率。 $ L(\\mu,\\sum) = f_{\\mu,\\sum}{x_1} f_{\\mu,\\sum}{x_2}f_{\\mu,\\sum}{x_3}\u0026hellip;f_{\\mu,\\sum}{x_N} $ ，这就是似然函数（Likelihood Function），其中$N$ 是训练集中某个类别样本的数量。 $\\mu^,\\sum^=\\arg\\max_{\\mu,\\sum}{L(\\mu,\\sum)}$，当然可以求导。直觉：$\\mu^=\\frac{1}{N}\\sum_{i=1}^{N}{x_i}$，$\\sum^ = \\frac{1}{N}\\sum_{i=1}^{N}(x_i-\\mu^*)^2T$ 协方差矩阵共享 # 每个类别的特征符合一个多元正态分布，每个多元正态分布也有不同的均值和协方差矩阵。让每个类别对应的多元正态分布共享一个协方差矩阵（各个协方差矩阵的加权平均和），公式为 $\\sum = \\frac{N_1}{N_1+N_2}\\sum_1+\\frac{N_2}{N_1+N_2}\\sum_2$，可以减少模型参数，缓解过拟合\n极大似然估计 # 极大似然估计指已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，然后通过若干次试验，观察其结果，利用结果推出参数的大概值。一般说来，在一次试验中如果事件A发生了，则认为此时的参数值会使得 $P(A|\\theta)$ 最大，极大似然估计法就是要这样估计出的参数值，使所选取的样本在被选的总体中出现的可能性为最大。\n求极大似然函数估计值的一般步骤：\n写出似然函数 对似然函数取对数，并整理 求导数 解似然函数 当共享协方差矩阵时，此时似然函数是$L(\\mu_1,\\mu_2,\\sum)=f_{\\mu_1,\\sum}(x_1)f_{\\mu_1,\\sum}(x_2)\u0026hellip;f_{\\mu_1,\\sum}{(x_{N1}) \\times f_{\\mu_2,\\sum}(x_{N1+1})f_{\\mu_2,\\sum}(x_{N1+2})\u0026hellip;f_{\\mu_2,\\sum}(x_{N1+N2})}$ ，其中 $N_1$ 为训练集中类别 $C_1$ 的样本数、$N_2$ 为训练集中类别 $C_2$ 的样本数。当只有两个类别、两个特征时，如果共享协方差矩阵，那最终得到的两个类别的分界线是直线（横纵轴是两个特征），这一点可以在下文解释。\n除了正态分布，还可以用其它的概率模型。 比如对于二值特征，可以使用伯努利分布（Bernouli Distribution）。 朴素贝叶斯分类：如果假设样本各个维度的数据是互相独立的，那这就是朴素贝叶斯分类器（Naive Bayes Classfier）。 Sigmoid 函数 # 由上我们可知，$P(C_1|x)= \\frac{P(x|C_1)P(C_1)}{P(x)}=\\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}=\\frac{1} {1+\\frac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1)}}$ ，令 $z=\\ln{\\frac{P(x|C_1)P(C_1)}{P(x|C_2)P(C_2)}}$ ，则 $P(C_1|x) =\\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}=\\frac{1} {1+\\frac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1)}}=\\frac{1}{1+e^{-z}} = \\delta(z)$ ，这就是Sigmoid函数。\n如果共享协方差矩阵，经过运算可以得到 $z=w_T+b$ 的形式，其中常量 $w_T = (\\mu_1-\\mu_2)^T\\sum^{-1}$，常量 $b=-\\frac{1}{2}(\\mu_1)^T(\\sum_1)^{-1}\\mu_1+\\frac{1}{2}(\\mu_2)^T(\\sum_2)^{-1}\\mu_2+\\ln{\\frac{N_1}{N_2}}$ ，即形如 $P(C_1|x) = \\delta(w\\cdot x+b)$ 。\n4.3 分类模型之逻辑回归 # 逻辑回归 # 假设训练集如下图所示，有2个类别 $C_1$ 和 $C_2$ ，下图表格中的每列为一个样本。\n$x_1$ $x_2$ $x_3$ \u0026hellip; $x_N$ $C_1$ $C_1$ $C_2$ \u0026hellip; $C_1$ $\\hat{y_1} =1$ $\\hat{y_2} =1$ $\\hat{y_3} = 0$ \u0026hellip; $\\hat{y_n} =1$ 例如，第一列表示样本 $x_1$ 的类别为 $C_1$ ，所以它标签是 $\\hat{y_1}$ 是1。\n模型定义 # 在分类（Classification）一节中，我们要找到一个模型 $P_{w,b}(C_1|x)$ ，如果 $P_{w,b}(C_1|x)\\geq0.5$ ，则 $x$ 属于类别 $C_1$ ，否则属于类别 $C_2$ 。可知 $P_{w,b}(C_1|x) = \\sigma(z)$ ，其中 $\\sigma(z)=\\frac{1}{1+e^{-z}}$ （Sigmoid Fuction），$z=w \\cdot x+b=\\sum^{N}{i=1}{w_ix_i+b} $ 。最终我们找到了模型 $f{w,b(x)}=\\sigma(\\sum^N_{i=1}{w_ix_i+b})$。 最终我们找到了模型 $f_{w,b(x)=\\sigma(\\sum^N_{i=1}{w_ix_i+b})}$ ，这其实就是逻辑回归（Logistic Regression）。\n损失函数 # 从模型 $ f_{w,b}(x)=P_{w,b}(C_1|x) $ 中取样得到训练集的概率为： $ L(w,b)=f_{w,b}(x_1)f_{w,b}(x_2)(1-f_{w,b}(x_3))\u0026hellip;f_{w,b}(x_N) $ （似然函数）。\n我们要求出 $w^,b^=\\arg\\max_{w,b}L(w,b)$，等同于 $w^,b^=\\arg\\min_{w,b}-\\ln{L(w,b)}$ （对数似然方程，Log-likelihood Equation）。\n而 $ -\\ln{L(w,b)=-\\ln{f_{w,b}{x_1}} -\\ln{f_{w,b}{x_2}}} -\\ln{(1-f_{w,b}{x_3})}\u0026hellip;$ ，其中 $ \\ln{f_{w,b}(x_N)=\\hat{y^N}\\ln{f_{w,b}{(x^N)}} + (1-\\hat{y^N})\\ln{(1-f_{w,b}{(x^N)})}} $ ，所以 $ -\\ln{L(w,b)=\\sum^N_{n=1}{-[\\hat y^N\\ln{f_{w,b}(x^N)}+(1-\\hat y^n)\\ln(1-f_{w,b}(x^N))]}} $ ，式中N用来选择某个样本。\n假设有两个伯努利分布 $p$ 和 $q$ ，在 $p$ 中有 $p(x=1)=\\hat {y^N}$ ，$p(x=0)=1-\\hat{y^N}$ ，在 $q$ 中有 $q(x=1)=f(x_N)$ ，$q(x=0)=1-f(x_N)$ ，则 $p$ 和 $q$ 的交叉熵（Cross Entropy，代表两个分布有多接近，两个分布一摸一样时交叉熵为0），为 $H(p,q)=-\\sum_x{p(x)\\ln(q(x))}$ 。所以损失函数 $L(f)=\\sum^N_{n=1}{C(f(x_n),\\hat{y^N})}$ ，其中 $C(f(x_N),\\hat{y^N})=-[\\hat y^N\\ln{f_{w,b}(x^N)}+(1-\\hat y^n)\\ln(1-f_{w,b}(x^N))]$ ，即损失函数为所有样本的 $f(x_N)$ 与 $\\hat{y_N}$ 的交叉熵之和，式中 $N$ 用来选择某个样本。\n梯度 # $ \\frac{-\\ln{L(w,b)}}{\\sigma_{w_i}} = \\sum^N_{n=1}{-(\\hat{y^n}-f_{w,b}{(x^n)})x_i^n} $ ，其中 $i$ 用来选择数据的某个维度， $n$ 用来选择某个样本， $ N $ 为数据集中样本个数。该式表明，预测值与label相差越大时，参数更新的步幅越大，这符合常理。\n逻辑回归 VS 线性回归 # 模型 # 逻辑函数模型比线性回归模型多了一个sigmoid函数。逻辑函数输出是[0,1]，而线性回归的输出是任意值。\n损失函数 # 逻辑回归模型使用的数据集中label的值必须是0或1，而线性回归模型训练集中label的值是真实值。\n图中的 $\\frac{1}{2}$ 是为了方便求导 。这里有一个问题，为什么逻辑回归模型中不适用Square Error呢？这个问题的答案见下文\n梯度 # 逻辑回归模型和线性回归模型的梯度公式一样\n为什么逻辑回归模型中不使用Square Error # 由上图可知，当label的值为1时，不管预测值是0还是1，梯度都为0，当label值为0时也是这样。\n如下图所示，如果在逻辑回归中使用Square Error，当梯度接近0时，我们无法判断目前与最优解的距离，也就无法调节学习率；并且在大多数时候梯度都是接近0的，收敛速度会很慢。\n判别模型 VS 生成模型 # 形式对比 # 逻辑回归是一个判别模型（Discriminative Model），用正态分布描述后验概率（Posterior Probability）则是生成模型（Generative Model）。如果生成模型中公用协方差矩阵，那两个模型/函数集其实是一样的，都是 $ P(C_1|x)=\\sigma(w \\cdot x+b) $ 。因为做了不同的假，即使是使用同一个数据集、同一个模型，找到的函数是不一样的。\n优劣对比 # 如果现在数据很少，当假设了概率分布以后，就可以需要更少的数据用于训练，受数据影响较小；而判别模型就只根据数据来学习，易受数据影响，需要更多数据 当假设了概率分布后，生成模型受数据影响小，对噪声的鲁棒性更强 对于生成模型来讲，先验的和基于类别的概率（Prors and class-dependent probabilities），即 $ P(C_1) $ 和 $ P(C_2) $ ，可以从不同的来源估计得到。以语音识别为例，如果用生成模型，可能并不需要声音的数据，网上的文本也可以用来估计某段文本出现的概率 多分类（Multi-class Classification） # 以3个类别 $C_1、C2和 C3$ 为例，分别对应参数$w_1、b_1、W_2、b_2、W_3、b_3$，即$z_1=w_1 \\cdot x+b_1、z_2=w_2 \\cdot x+b_2、z_3=w_3 \\cdot x+b_3$\nSoftmax # 使用softmax（$y_i=\\frac{e_{z_i}}{\\sum^c_{j=1}{e_{z_j}}}$）\nsoftmax公式中为什么要用$e$？这是由原因的/可解释的，可以看下PRML，也可以搜一下最大熵\n最大熵（Maximum Entropy）其实也是一种分类器，和逻辑回归一样，只是从信息论的角度来看\n损失函数 # 计算预测值$y$和$\\hat y$都是一个向量，即$-\\sum^3_{i=1}{{\\hat y}_i\\ln{y_i}}$\n这时需要使用one-hot编码：如果$x\\in C_1$，则$y=\\begin{bmatrix} 1\\ 0\\ 0\\ \\end{bmatrix}$；如果$x\\in C_1$，则$y=\\begin{bmatrix} 0\\ 1\\ 0\\ \\end{bmatrix}$；如果$x\\in C_1$，则$y=\\begin{bmatrix} 0\\ 0\\ 1\\ \\end{bmatrix}$。\n梯度 # 和逻辑回归的思路一样。\n逻辑回归的局限性 # 如下图所示，假如有2个类别，数据集中有4个样本，每个样本有2维特征，将这4个样本画在图上。\n如下图所示，假如用逻辑回归做分类，即$y=\\sigma(z)=\\sigma(w_1\\cdot x_1+w_2\\cdot x_2+b)$，我们找不到一个可以把“蓝色”样本和“红色”样本间隔开的函数。\n假如一定要用逻辑回归，那我们可以怎么办呢？我们可以尝试特征变换（Feature Transformation）。\n特征变换（Feature Transformation） # 在上面的例子中，我们并不能找到一个能将蓝色样本和红色样本间隔开的函数。如下图所示，我们可以把原始的数据/特征转换到另外一个空间，在这个新的特征空间中，找到一个函数将“蓝色”样本和“红色”样本间隔开。比如把原始的两维特征变换为$\\begin{bmatrix} 0\\ 0\\ \\end{bmatrix} 和 \\begin{bmatrix} 1\\ 1\\ \\end{bmatrix}$ 的距离，在这个新的特征空间，“蓝色”样本和“红色”样本是可分的。\n但有一个问题是，我们并不一定知道怎么进行特征变换。或者说我们想让机器自己学会特征变换，这可以通过级联逻辑回归模型实现，即把多个逻辑回归模型连接起来，如下图所示。下图中有3个逻辑回归模型，根据颜色称它们为小蓝、小绿和小红。小蓝和小绿的作用是分别将原始的2维特征变换为新的特征$x_1\u0026rsquo;和x_2\u0026rsquo;$，小红的作用是在新的特征空间$\\begin{bmatrix} x_1\u0026rsquo;\\ x_2\u0026rsquo;\\ \\end{bmatrix}$上将样本分类。\n如下图所示，举一个例子。小蓝的功能是（下图左上角），离$(1,0)$越远、离$(0,1)$越近，则$x_1\u0026rsquo;$越大；小蓝的功能是（下图左上角），离$(1,0)$越远、离$(0,1)$越近，则$x_1\u0026rsquo;$越小。小蓝和小绿将特征映射到新的特征空间$\\begin{bmatrix} x_1\u0026rsquo;\\ x_2\u0026rsquo;\\ \\end{bmatrix}$中，结果见下图右下角，然后小红就能找到一个函数将“蓝色”样本和“红色”样本间隔开。\n神经网络（Neural Network） # 假如把上例中的一个逻辑回归叫做神经元（Neuron），那我们就形成了一个神经网络。\nROC # 在信号检测理论中，接收者操作特征曲线(receiver operating characteristic curve，或者叫ROC曲线)是 坐标图式的分析工具，用于 (1) 选择最佳的 信号侦测模型、舍弃次佳的模型。 (2) 在同一模型中设定最佳阈值。在做决策时，ROC分析能不受成本／效益的影响，给出客观中立的建议。\nROC曲线首先是由二战中的电子工程师和雷达工程师发明的，用来侦测战场上的敌军载具(飞机、船舰)，也就是信号检测理论。之后很快就被引入了心理学来进行信号的知觉检测。数十年来，ROC分析被用于医学、无线电、生物学、 犯罪心理学领域中，而且最近在机器学习(machine learning)和数据挖掘(data mining)领域也得到了很好的发展。\n术语\n阳性(P, positive) 阴性(N, Negative) 真阳性 (TP, true positive) 正确的肯定。又称：命中 (hit) 真阴性 (TN, true negative) 正确的否定。又称：正确拒绝 (correct rejection) 伪阳性 (FP, false positive) 错误的肯定，又称：假警报 (false alarm)，第一型错误伪阴性 (FN, false negative) 错误的否定，又称：未命中 (miss)，第二型错误 真阳性率 (TPR, true positive rate) 又称：命中率 (hit rate)、 敏感度(sensitivity) TPR = TP / P = TP / (TP+FN) 伪阳性率(FPR, false positive rate) 又称：错误命中率，假警报率 (false alarm rate) FPR = FP / N = FP / (FP + TN) 准确度 (ACC, accuracy) ACC = (TP + TN) / (P + N) 即：(真阳性+真阴性) / 总样本数真阴性率 (TNR) 又称：特异度 (SPC, specificity) SPC = TN / N = TN / (FP + TN) = 1 - FPR 阳性预测值 (PPV) PPV = TP / (TP + FP) 阴性预测值 (NPV) NPV = TN / (TN + FN) 假发现率 (FDR) FDR = FP / (FP + TP) ","date":"23 October 2023","permalink":"/posts/ai/li-hongyis-notes/%E5%9B%9B%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/","section":"博客","summary":"4.1 分类简介及其与回归的区别 # 分类模型应用案例（Classification Cases） # 信用评分（Credit Scoring） 输入：收入、储蓄、职业、年龄、信用历史等等 输出：是否贷款 医疗诊断（Medical Diagnosis） 输入：现在症状、年龄、性别、病史 输出：哪种疾病 手写文字识别（Handwritten Character Recognition） 输入：文字图片 输出：是哪一个汉字 人脸识别（Face Recognition） 输入：面部图片 输出：是哪个人 把分类当成回归去做 # 不行","title":"四、分类模型"},{"content":"","date":"23 October 2023","permalink":"/posts/ai/li-hongyis-notes/","section":"博客","summary":"李宏毅机器学习\u0026amp;深度学习笔记","title":"李宏毅机器学习\u0026深度学习笔记"},{"content":"cluster 模块 # cluster模块用于组建 Node.js 应用的集群。\ncluster.isMaster属性表示当前进程是否为主进程。\nconst cluster = require(\u0026#39;cluster\u0026#39;); const isMaster = cluster.isMaster; cluster.fork()方法复制当前进程。\nconst cluster = require(\u0026#39;cluster\u0026#39;); const { cpus } = require(\u0026#39;os\u0026#39;); const numWorkers = cpus().length; const isMaster = cluster.isMaster; if (isMaster) { process.stdout.write(\u0026#39;master process\u0026#39;); const workers = []; for(let i = 0; i \u0026lt; numWorkers; i++) { workers.push(cluster.fork()); } } else { process.stdout.write(\u0026#39;worker process\u0026#39;); } 上面代码按照 CPU 内核的数目新建 Worker 进程。\n主线程可以负责监听整个集群的运行情况。我们使用cluster.on()方法监听集群的事件。online事件在 Worker 进程上线时触发，exit事件在 Worker 进程下线时触发。\nif (isMaster) { log(`Forking ${numWorkers} workers`); const workers = []; for(let i = 0; i \u0026lt; numWorkers; i++) { workers.push(cluster.fork()); } cluster.on(\u0026#39;online\u0026#39;, (worker) =\u0026gt; log(`Worker ${worker.process.pid} is online`)); cluster.on(\u0026#39;exit\u0026#39;, (worker, exitCode) =\u0026gt; { log(`Worker ${worker.process.id} exited with code ${exitCode}`); log(`Starting a new worker`); cluster.fork(); }) } 上面代码中，主进程如果发现一个 Worker 进程下线，就再新建一个 Worker 进程。\n至于每个 Worker 进程可以指定运行的任务。\nif (isMaster) { // ... } else { // 应用的内容 } 参考链接 # How to scale your Node.js server using clustering, Michele Riva ","date":"23 October 2023","permalink":"/posts/language/nodejs/stdlib/cluster/","section":"博客","summary":"cluster 模块 # cluster模块用于组建 Node.","title":"cluster.md"},{"content":"events 模块 # Node 通过 events 模块提供事件，形成模块之间的通信机制，消除模块与模块的强耦合。\n概述 # events模块提供一个构造函数，可以用来生成事件发生器的实例。\nconst EventEmitter = require(\u0026#39;events\u0026#39;); const emitter = new EventEmitter(); 上面代码中，emitter就是事件发生器的实例。\nemit()方法用于触发事件。\nemitter.emit(\u0026#39;user-registered\u0026#39;, user); 上面代码触发了user-registered事件，第二个参数user是事件传递的数据。\non()方法用于监听事件。\nemitter.on(\u0026#39;user-registered\u0026#39;, (user) =\u0026gt; { console.log(\u0026#39;event has occured\u0026#39;); }); 注意，emit()方法和on()方法都是同步的，请看下面的例子。\nemitter.on(\u0026#39;someEvent\u0026#39;, function () { console.log(\u0026#39;event has occured\u0026#39;); }); function f() { console.log(\u0026#39;start\u0026#39;); emitter.emit(\u0026#39;someEvent\u0026#39;); console.log(\u0026#39;end\u0026#39;); } f() // start // event has occured // end 上面代码中，emit()方法执行以后，on()方法会立刻执行。只有等到on()方法执行结束以后，emit()后面的方法才会继续执行。\n事件接口的部署 # 事件接口可以部署在任意对象上，使得这些对象也能订阅和发布消息。\nvar EventEmitter = require(\u0026#39;events\u0026#39;).EventEmitter; function Dog(name) { this.name = name; } Dog.prototype.__proto__ = EventEmitter.prototype; // 另一种写法 // Dog.prototype = Object.create(EventEmitter.prototype); var simon = new Dog(\u0026#39;simon\u0026#39;); simon.on(\u0026#39;bark\u0026#39;, function () { console.log(this.name + \u0026#39; barked\u0026#39;); }); setInterval(function () { simon.emit(\u0026#39;bark\u0026#39;); }, 500); 上面代码新建了一个构造函数Dog，然后让其继承EventEmitter，因此Dog就拥有了EventEmitter的接口。最后，为Dog的实例指定bark事件的监听函数，再使用EventEmitter的emit方法，触发bark事件。\nNode 内置模块util的inherits方法，提供了另一种继承 Event Emitter 接口的方法。\nvar util = require(\u0026#39;util\u0026#39;); var EventEmitter = require(\u0026#39;events\u0026#39;).EventEmitter; var Radio = function (station) { var self = this; setTimeout(function() { self.emit(\u0026#39;open\u0026#39;, station); }, 0); setTimeout(function() { self.emit(\u0026#39;close\u0026#39;, station); }, 5000); this.on(\u0026#39;newListener\u0026#39;, function(listener) { console.log(\u0026#39;Event Listener: \u0026#39; + listener); }); }; util.inherits(Radio, EventEmitter); module.exports = Radio; 上面代码中，Radio是一个构造函数，它的实例继承了EventEmitter接口。下面是使用这个模块的例子。\nvar Radio = require(\u0026#39;./radio.js\u0026#39;); var station = { freq: \u0026#39;80.16\u0026#39;, name: \u0026#39;Rock N Roll Radio\u0026#39;, }; var radio = new Radio(station); radio.on(\u0026#39;open\u0026#39;, function(station) { console.log(\u0026#39;\u0026#34;%s\u0026#34; FM %s 打开\u0026#39;, station.name, station.freq); console.log(\u0026#39;♬ ♫♬\u0026#39;); }); radio.on(\u0026#39;close\u0026#39;, function(station) { console.log(\u0026#39;\u0026#34;%s\u0026#34; FM %s 关闭\u0026#39;, station.name, station.freq); }); Event Emitter 的实例方法 # Event Emitter 的实例方法如下。\nemitter.on(name, f) 对事件name指定监听函数f emitter.addListener(name, f) addListener是on方法的别名 emitter.once(name, f) 与on方法类似，但是监听函数f是一次性的，使用后自动移除 emitter.listeners(name) 返回一个数组，成员是事件name所有监听函数 emitter.removeListener(name, f) 移除事件name的监听函数f emitter.removeAllListeners(name) 移除事件name的所有监听函数 emit() # EventEmitter实例对象的emit方法，用来触发事件。它的第一个参数是事件名称，其余参数都会依次传入回调函数。\nvar EventEmitter = require(\u0026#39;events\u0026#39;).EventEmitter; var myEmitter = new EventEmitter(); var connection = function (id) { console.log(\u0026#39;client id: \u0026#39; + id); }; myEmitter.on(\u0026#39;connection\u0026#39;, connection); myEmitter.emit(\u0026#39;connection\u0026#39;, 6); // client id: 6 once() # 该方法类似于on方法，但是回调函数只触发一次。\nvar EventEmitter = require(\u0026#39;events\u0026#39;).EventEmitter; var myEmitter = new EventEmitter; myEmitter.once(\u0026#39;message\u0026#39;, function(msg){ console.log(\u0026#39;message: \u0026#39; + msg); }); myEmitter.emit(\u0026#39;message\u0026#39;, \u0026#39;this is the first message\u0026#39;); myEmitter.emit(\u0026#39;message\u0026#39;, \u0026#39;this is the second message\u0026#39;); myEmitter.emit(\u0026#39;message\u0026#39;, \u0026#39;welcome to nodejs\u0026#39;); 上面代码触发了三次message事件，但是回调函数只会在第一次调用时运行。\n下面代码指定，一旦服务器连通，只调用一次的回调函数。\n{% highlight javascript %}\nserver.once(\u0026lsquo;connection\u0026rsquo;, function (stream) { console.log(\u0026lsquo;Ah, we have our first user!\u0026rsquo;); });\n{% endhighlight %}\n该方法返回一个EventEmitter对象，因此可以链式加载监听函数。\nremoveListener() # 该方法用于移除回调函数。它接受两个参数，第一个是事件名称，第二个是回调函数名称。这就是说，不能用于移除匿名函数。\nvar EventEmitter = require(\u0026#39;events\u0026#39;).EventEmitter; var emitter = new EventEmitter; emitter.on(\u0026#39;message\u0026#39;, console.log); setInterval(function(){ emitter.emit(\u0026#39;message\u0026#39;, \u0026#39;foo bar\u0026#39;); }, 300); setTimeout(function(){ emitter.removeListener(\u0026#39;message\u0026#39;, console.log); }, 1000); 上面代码每300毫秒触发一次message事件，直到1000毫秒后取消监听。\n另一个例子是使用removeListener方法模拟once方法。\nvar EventEmitter = require(\u0026#39;events\u0026#39;).EventEmitter; var emitter = new EventEmitter; function onlyOnce () { console.log(\u0026#34;You\u0026#39;ll never see this again\u0026#34;); emitter.removeListener(\u0026#34;firstConnection\u0026#34;, onlyOnce); } emitter.on(\u0026#34;firstConnection\u0026#34;, onlyOnce); setMaxListeners() # Node默认允许同一个事件最多可以指定10个回调函数。\nemitter.on(\u0026#39;someEvent\u0026#39;, function () { console.log(\u0026#39;event 1\u0026#39;); }); emitter.on(\u0026#39;someEvent\u0026#39;, function () { console.log(\u0026#39;event 2\u0026#39;); }); emitter.on(\u0026#39;someEvent\u0026#39;, function () { console.log(\u0026#39;event 3\u0026#39;); }); 超过10个回调函数，会发出一个警告。这个门槛值可以通过setMaxListeners方法改变。\nemitter.setMaxListeners(20); removeAllListeners() # 该方法用于移除某个事件的所有回调函数。\nvar EventEmitter = require(\u0026#39;events\u0026#39;).EventEmitter; var emitter = new EventEmitter; // some code here emitter.removeAllListeners(\u0026#34;firstConnection\u0026#34;); 如果不带参数，则表示移除所有事件的所有回调函数。\n{% highlight javascript %}\nemitter.removeAllListeners();\n{% endhighlight %}\nlisteners() # listeners方法接受一个事件名称作为参数，返回该事件所有回调函数组成的数组。\nvar EventEmitter = require(\u0026#39;events\u0026#39;).EventEmitter; var ee = new EventEmitter; function onlyOnce () { console.log(ee.listeners(\u0026#34;firstConnection\u0026#34;)); ee.removeListener(\u0026#34;firstConnection\u0026#34;, onlyOnce); console.log(ee.listeners(\u0026#34;firstConnection\u0026#34;)); } ee.on(\u0026#34;firstConnection\u0026#34;, onlyOnce) ee.emit(\u0026#34;firstConnection\u0026#34;); ee.emit(\u0026#34;firstConnection\u0026#34;); // [ [Function: onlyOnce] ] // [] 上面代码显示两次回调函数组成的数组，第一次只有一个回调函数onlyOnce，第二次是一个空数组，因为removeListener方法取消了回调函数。\n错误捕获 # 事件处理过程中抛出的错误，可以使用try...catch捕获。\nvar EventEmitter = require(\u0026#39;events\u0026#39;).EventEmitter; var emitter = new EventEmitter(); emitter.on(\u0026#39;beep\u0026#39;, function () { console.log(\u0026#39;beep\u0026#39;); }); emitter.on(\u0026#39;beep\u0026#39;, function () { throw Error(\u0026#39;oops!\u0026#39;); }); emitter.on(\u0026#39;beep\u0026#39;, function () { console.log(\u0026#39;beep again\u0026#39;); }); console.log(\u0026#39;before emit\u0026#39;); try { emitter.emit(\u0026#39;beep\u0026#39;); } catch(err) { console.error(\u0026#39;caught while emitting:\u0026#39;, err.message); } console.log(\u0026#39;after emit\u0026#39;); 上面的代码，beep一共绑定了三个监听函数。其中，第二个监听函数会抛出错误。执行上面的代码，会得到下面的结果。\nbefore emit beep caught while emitting: oops! after emit 可以看到，第二个监听函数抛出的错误被try...catch代码块捕获了。一旦被捕获，该事件后面的监听函数都不会再执行了。\n如果不使用try...catch，可以让进程监听uncaughtException事件。\nprocess.on(\u0026#39;uncaughtException\u0026#39;, function (err) { console.error(\u0026#39;uncaught exception:\u0026#39;, err.stack || err); // 关闭资源 closeEverything(function(err) { if (err) console.error(\u0026#39;Error while closing everything:\u0026#39;, err.stack || err); // 退出进程 process.exit(1); }); }); 事件类型 # Events模块默认支持两个事件。\nnewListener事件：添加新的回调函数时触发。 removeListener事件：移除回调时触发。 ee.on(\u0026#34;newListener\u0026#34;, function (evtName) { console.log(\u0026#34;New Listener: \u0026#34; + evtName); }); ee.on(\u0026#34;removeListener\u0026#34;, function (evtName) { console.log(\u0026#34;Removed Listener: \u0026#34; + evtName); }); function foo() {} ee.on(\u0026#34;save-user\u0026#34;, foo); ee.removeListener(\u0026#34;save-user\u0026#34;, foo); // New Listener: removeListener // New Listener: save-user // Removed Listener: save-user 上面代码会触发两次newListener事件，以及一次removeListener事件。\n参考链接 # Hage Yaapa, Node.js EventEmitter Tutorial ","date":"23 October 2023","permalink":"/posts/language/nodejs/stdlib/events/","section":"博客","summary":"events 模块 # Node 通过 events 模块提供事件，形成模块之间的通信机制，消除模块与模块的强耦合。","title":"events.md"},{"content":"http 模块 # http模块用于 HTTP 通信。\nhttp.Server # http.Server属性指向一个类，表示 Web 服务器实例。\n这个类继承了net.Server，而net.Server继承了 EventEmitter 接口，因此可以使用server.on()方法监听事情。最重要的一个事件是request，表示收到 HTTP 请求。\nserver.on(\u0026#39;request\u0026#39;, (request, response) =\u0026gt; { response.end(\u0026#39;Hello, world!\u0026#39;); }); server.listen()方法用于启动 Web 服务。这个方法需要指定监听的端口，以及一个回调函数（启动后要做什么）。\nserver.listen(PORT, () =\u0026gt; { console.log(`starting server at port ${PORT}`); }); http.createServer() # http.createServer()方法用于创建一个 Web 服务器，它的返回值就是一个http.Server实例。\nconst { createServer } = require(\u0026#39;http\u0026#39;); // 指定端口 const PORT = process.env.PORT || 8080; const server = createServer(); server.on(\u0026#39;request\u0026#39;, (request, response) =\u0026gt; { response.end(\u0026#39;Hello, world!\u0026#39;); }); server.listen(PORT, () =\u0026gt; { console.log(`starting server at port ${PORT}`); }); request事件的回调函数，可以作为createServer()方法的参数。因此，上面的代码也可以写成下面的样子。\nconst { createServer } = require(\u0026#39;http\u0026#39;); const PORT = process.env.PORT || 8080; const server = createServer((request, response) =\u0026gt; { response.end(\u0026#39;Hello, world!\u0026#39;); }).listen(PORT, () =\u0026gt; { console.log(`starting server at port ${PORT}`); }); response 对象 # HTTP 请求和回应都是数据流（stream），request是只读数据流，response是可写数据流。\nresponse.write()方法表示依次向 HTTP 回应写入内容。\nresponse.end()方法表示写入数据以后，关闭response数据流。\nconst { createServer } = require(\u0026#39;http\u0026#39;); createServer((request, response) =\u0026gt; { // 等同于 response.end(\u0026#39;Hello, world!\u0026#39;) response.write(\u0026#39;Hello\u0026#39;); response.write(\u0026#39;, \u0026#39;); response.write(\u0026#39;World!\u0026#39;); response.end(); }).listen(8080); 注意，response.end()是必需的，而且必须写在最后，否则 Node 不会关闭请求。\n如果要向客户端发送文件，也可以写成数据流。\nconst { createReadStream } = require(\u0026#39;fs\u0026#39;); const { createServer } = require(\u0026#39;http\u0026#39;); createServer((request, response) =\u0026gt; { createReadStream(__filename).pipe(response); }).listen(8080); 上面代码会将这个脚本代码，发送给客户端。这时不用写response.end()方法，因为pipe()方法会在发送结束后，自动关闭数据流。\nresponse.setHeader()方法用于设置返回的头信息。\nconst { createServer } = require(\u0026#39;http\u0026#39;); createServer((req, res) =\u0026gt; { res.setHeader(\u0026#39;content-type\u0026#39;, \u0026#39;application/json\u0026#39;); res.end(JSON.stringify({ foo: \u0026#39;bar\u0026#39; })); }).listen(8080); 下面是设置 Cookie 的例子。\nconst { createServer } = require(\u0026#39;http\u0026#39;); createServer((req, res) =\u0026gt; { res.setHeader( \u0026#39;Set-Cookie\u0026#39;, [\u0026#39;myCookie=myValue\u0026#39;], [\u0026#39;mySecondCookie=mySecondValue\u0026#39;] ); res.end(`Your cookies are: ${req.headers.cookie}`); }).listen(8080); response.writeHead()方法与response.setHeader()类似，但是优先级更高。它可以设置返回的 HTTP 状态码，第一个参数是状态码，第二个参数是状态说明。\nconst { createServer } = require(\u0026#39;http\u0026#39;); createServer((req, res) =\u0026gt; { res.writeHead(204, \u0026#39;My Custom Message\u0026#39;); res.end(); }).listen(8080); request 对象 # request.headers属性返回一个对象，包含了 HTTP 请求的头信息。该对象的键名是头信息的字段名，键值是头信息的字段值。\nconst { createServer } = require(\u0026#34;http\u0026#34;); createServer((request, response) =\u0026gt; { const languages = request.headers[\u0026#39;accept-language\u0026#39;]; response.end(languages); }).listen(8080); 上面代码返回 HTTP 请求的头信息accept-language。\nrequest.url属性返回浏览器请求的路径，可以基于这个属性做一个简单的路由。\nconst { createServer } = require(\u0026#39;http\u0026#39;); createServer((req, res) =\u0026gt; { switch (req.url) { case \u0026#39;/\u0026#39;: res.end(\u0026#39;You are on the main page!\u0026#39;); break; case \u0026#39;/about\u0026#39;: res.end(\u0026#39;You are on about page!\u0026#39;); break; default: res.statusCode = 404; res.end(\u0026#39;Page not found!\u0026#39;); } }).listen(8080); 获取查询字符串，可以用 Node 内置的url模块解析request.url属性。\nconst { createServer } = require(\u0026#39;http\u0026#39;); createServer((req, res) =\u0026gt; { const { query } = require(\u0026#39;url\u0026#39;).parse(req.url, true); if (query.name) { res.end(`You requested parameter name with value ${query.name}`); } else { res.end(\u0026#39;Hello!\u0026#39;); } }).listen(8080); request.method属性返回浏览器请求的方法。\nconst { createServer } = require(\u0026#39;http\u0026#39;); createServer((req, res) =\u0026gt; { if (req.method === \u0026#39;GET\u0026#39;) { return res.end(\u0026#39;List of data\u0026#39;); } else if (req.method === \u0026#39;POST\u0026#39;) { return res.end(\u0026#39;success\u0026#39;); } else { res.statusCode(400); return res.end(\u0026#39;Unsupported method\u0026#39;); } }).listen(8080); POST 请求发送的数据体是一个数据流（stream），可以用request.on()方法监听data事件，累加数据流的每一部分；监听end事件，得到数据流接收结束的通知。\nconst { createServer } = require(\u0026#39;http\u0026#39;); createServer((req, res) =\u0026gt; { if (req.method === \u0026#39;POST\u0026#39;) { let data = \u0026#39;\u0026#39;; req.on(\u0026#39;data\u0026#39;, chunk =\u0026gt; { data += chunk; }); req.on(\u0026#39;end\u0026#39;, () =\u0026gt; { try { const requestData = JSON.parse(data); requestData.ourMessage = \u0026#39;success\u0026#39;; res.setHeader(\u0026#39;Content-Type\u0026#39;, \u0026#39;application/json\u0026#39;); res.end(JSON.stringify(requestData)); } catch (e) { res.statusCode = 400; res.end(\u0026#39;Invalid JSON\u0026#39;); } }); } else { res.statusCode = 400; res.end(\u0026#39;Unsupported method, please POST a JSON object\u0026#39;); } }).listen(8080); 参考链接 # Node.js Fundamentals: Web Server Without Dependencies, Seva Zaikov ","date":"23 October 2023","permalink":"/posts/language/nodejs/stdlib/http/","section":"博客","summary":"http 模块 # http模块用于 HTTP 通信。","title":"http.md"},{"content":"","date":"23 October 2023","permalink":"/posts/language/nodejs/","section":"博客","summary":"","title":"nodejs"},{"content":"","date":"23 October 2023","permalink":"/posts/language/nodejs/stdlib/","section":"博客","summary":"","title":"nodejs/stdlib"},{"content":"os 模块 # cpus属性返回一个数组，每个成员对应一个 CPU 内核。下面代码可以获取本机的 CPU 内核数目。\nconst { cpus } = require(\u0026#39;os\u0026#39;); const numWorkers = cpus().length; ","date":"23 October 2023","permalink":"/posts/language/nodejs/stdlib/os/","section":"博客","summary":"os 模块 # cpus属性返回一个数组，每个成员对应一个 CPU 内核。下面代码可以获取本机的 CPU 内核数目。","title":"os.md"},{"content":"process 对象 # process对象是 Node 原生提供的对象，表示当前运行的 Node 进程。它不用引入模块，可以直接使用。\nprocess.argv # process.argv是一个数组，表示启动脚本时的命令行参数。\n它的前两项是固定的。\n第一项是 Node 可执行文件的路径 第二项是 JavaScript 脚本的路径 后面的数组成员都是命令行参数。\n$ node index.js --watch 上面这个命令执行后，在index.js脚本里面，process.argv数组共有三项。\nprocess.argv[0]：/path/to/node process.argv[1]：/path/to/index.js process.argv[2]：\u0026ndash;watch 如果只需要命令行参数，可以用解构赋值获取。\nconst [ , , ...args ] = process.argv; console.log(args[0]); // \u0026#34;--watch\u0026#34; 上面代码，args数组就是通过解构赋值，拿到的所有命令行参数。\n参考链接 # Extracting command line arguments from Node.js using destructuring, Nicholas C. Zakas ","date":"23 October 2023","permalink":"/posts/language/nodejs/stdlib/process/","section":"博客","summary":"process 对象 # process对象是 Node 原生提供的对象，表示当前运行的 Node 进程。它不用引入模块，可以直接使用。","title":"process.md"},{"content":"","date":"23 October 2023","permalink":"/posts/language/nodejs/npm/","section":"博客","summary":"","title":"nodejs/npm"},{"content":"npm update # 更新所有依赖项。\n$ npm update 更新单个依赖。\n$ npm update xml2js 同时更新 package.json 里面每个依赖的版本号。\n$ npm update --save ","date":"23 October 2023","permalink":"/posts/language/nodejs/npm/npm-update/","section":"博客","summary":"npm update # 更新所有依赖项。","title":"npm-update.md"},{"content":"npm version # npm version用来指定模块的版本，然后会将新的版本号写入package.json和package-lock.json。\n它的命令行用法如下。\nnpm version [ \u0026lt;newversion\u0026gt; | major | minor | patch | premajor | preminor | prepatch | prerelease | from-git ] 上面可以归纳为三种用法。\n（1）\u0026lt;newversion\u0026gt;：自己指定版本号。\n（2）七个版本关键字：patch，minor，major，prepatch，preminor，premajor，prerelease。这时原有版本号，会在相应的位置增加1。\nmajor：规则如下。\n（1）如果没有预发布号，则增加主版本号，并将次版本号和预发布号设为0。\n# 版本号从 3.1.0 变为 4.0.0 $ npm version major （2）如果有预发布号，且次版本号和补丁号都为0，则不升级主版本号，只去掉预发布号。\n# 版本号从 4.0.0 变为 5.0.0-0 $ npm version premajor # 版本号从 5.0.0-0 变为 5.0.0 $ npm version major （3）如果有预发布号，且次版本号和补丁号都不为0，则增加主版本号，将次版本号和补丁号都置为0，并去掉预发布号。\n# 版本号从 5.0.0-0 变为 5.1.0-0 $ npm version preminor : 5.0.0-0–\u0026gt; 5.1.0-0 # 版本号从 5.1.0-0 变为 6.0.0 $ npm version major minor：规则如下。\n（1）如果没有预发布号，则增加次版本号，并将补丁号设为0。\n# 版本号从 2.0.1 变为 2.1.0 $ npm version minor （2）如果有预发布号，且补丁号为0，则去掉预发布号，其他不变。\n# 版本号从 2.1.0 变为 3.0.0-0 $ npm version premajor # 版本号从 3.0.0-0 变为 3.0.0 $ npm version minor （3）如果有预发布号，且补丁号不为0，则去掉预发布号，增加次版本号，补丁号设为0。\n# 版本号从 3.0.0 变为 3.0.1-0 $ npm version prepatch # 版本号从 3.0.1-0 变为 3.1.0 $ npm version minor patch：如果预发布号，则去掉预发布后，其他保持不变；如果没有预发布号，则升级补丁号。\n# 版本号从 2.0.0-0 变为 2.0.0 $ npm version patch # 版本号从 2.0.0 变为 2.0.1 $ npm version patch premajor：增加主版本号，将次版本号和补丁号都设为0，增加预发布号为0。\n# 版本号从 1.1.0-0 变为 2.0.0-0 $ npm version premajor preminor：增加次版本号，补丁号设为0，预发布号设为0。\n# 版本号从 1.0.2-0 变为 1.1.0-0 $ npm version preminor prepatch：增加补丁号，同时预发布号设为0。\n# 版本号从 1.0.1-1 变为 1.0.2-0 $ npm version prepatch prerelease：如果没有预发布号，则增加补丁号，同时预发布号设为0；如果有预发布号，则预发布号增加1。\n# 版本号从 1.0.0 变为 1.0.1-0 $ npm version prerelease # 版本号从 1.0.1-0 变为 1.0.1-1 $ npm version prerelease （3）from-git：使用最新的 Git 标签，将其作为 npm 版本。\n这个命令如果是在一个 Git 仓库里面运行，它会创造一个新的提交和标签。如果不希望生成标签，可以使用命令行参数--no-git-tag-version。\n命令行参数-m或者--message可以指定提交信息。提交信息里面的%s参数会被替换成新的版本号。\n$ npm version patch -m \u0026#34;Upgrade to %s for reasons\u0026#34; \u0026ndash;pre-id # npm version命令的--pre-id参数，可以指定预发布号的前缀。\n$ npm version prerelease --pre-id rc 上面的命令会产生诸如1.0.0-rc.0的版本号。\n参考链接 # npm version 常用命令及用法示例 ","date":"23 October 2023","permalink":"/posts/language/nodejs/npm/npm-version/","section":"博客","summary":"npm version # npm version用来指定模块的版本，然后会将新的版本号写入package.","title":"npm-version.md"},{"content":"npm view 命令 # 查看某个模块发布过的所有版本。\n$ npm view [PackageName] versions ","date":"23 October 2023","permalink":"/posts/language/nodejs/npm/npm-view/","section":"博客","summary":"npm view 命令 # 查看某个模块发布过的所有版本。","title":"npm-view.md"},{"content":"npm whoami # npm whoami命令返回当前登录的 npm 用户名。\n$ npm whoami ","date":"23 October 2023","permalink":"/posts/language/nodejs/npm/npm-whoami/","section":"博客","summary":"npm whoami # npm whoami命令返回当前登录的 npm 用户名。","title":"npm-whoami.md"},{"content":"npx 使用教程 # npm 从5.2版开始，增加了 npx 命令，它有很多用处。\nNode 自带 npm 模块，所以可以直接使用 npx 命令。万一不能用，就要手动安装一下。\n$ npm install -g npx 调用项目安装的模块 # npx 想要解决的主要问题，就是调用项目内部安装的模块。比如，项目内部安装了测试工具 Mocha。\n$ npm install -D mocha 一般来说，调用 Mocha ，只能在项目脚本和 package.json 的 scripts字段里面， 如果想在命令行下调用，必须像下面这样。\n# 项目的根目录下执行 $ node-modules/.bin/mocha --version npx 就是想解决这个问题，让项目内部安装的模块用起来更方便，只要像下面这样调用就行了。\n$ npx mocha --version npx 的原理很简单，就是运行的时候，会到node_modules/.bin路径和环境变量$PATH里面，检查命令是否存在。\n由于 npx 会检查环境变量$PATH，所以系统命令也可以调用。\n# 等同于 ls $ npx ls 注意，Bash 内置的命令不在$PATH里面，所以不能用。比如，cd是 Bash 命令，因此就不能用npx cd。\n避免全局安装模块 # 除了调用项目内部模块，npx 还能避免全局安装的模块。比如，create-react-app这个模块是全局安装，npx 可以运行它，而且不进行全局安装。\n$ npx create-react-app my-react-app 上面代码运行时，npx 将create-react-app下载到一个临时目录，使用以后再删除。所以，以后再次执行上面的命令，会重新下载create-react-app。\n下载全局模块时，npx 允许指定版本。\n$ npx uglify-js@3.1.0 main.js -o ./dist/main.js 上面代码指定使用 3.1.0 版本的uglify-js压缩脚本。\n注意，只要 npx 后面的模块无法在本地发现，就会下载同名模块。比如，本地没有安装http-server模块，下面的命令会自动下载该模块，在当前目录启动一个 Web 服务。\n$ npx http-server --no-install 参数和--ignore-existing 参数 # 如果想让 npx 强制使用本地模块，不下载远程模块，可以使用--no-install参数。如果本地不存在该模块，就会报错。\n$ npx --no-install http-server 反过来，如果忽略本地的同名模块，强制安装使用远程模块，可以使用--ignore-existing参数。比如，本地已经全局安装了create-react-app，但还是想使用远程模块，就用这个参数。\n$ npx --ignore-existing create-react-app my-react-app 使用不同版本的 node # 利用 npx 可以下载模块这个特点，可以指定某个版本的 Node 运行脚本。它的窍门就是使用 npm 的 node 模块。\n$ npx node@0.12.8 -v v0.12.8 上面命令会使用 0.12.8 版本的 Node 执行脚本。原理是从 npm 下载这个版本的 node，使用后再删掉。\n某些场景下，这个方法用来切换 Node 版本，要比 nvm 那样的版本管理器方便一些。\n-p 参数 # -p参数用于指定 npx 所要安装的模块，所以上一节的命令可以写成下面这样。\n$ npx -p node@0.12.8 node -v v0.12.8 上面命令先指定安装node@0.12.8，然后再执行node -v命令。\n-p参数对于需要安装多个模块的场景很有用。\n$ npx -p lolcatjs -p cowsay [command] -c 参数 # 如果 npx 安装多个模块，默认情况下，所执行的命令之中，只有第一个可执行项会使用 npx 安装的模块，后面的可执行项还是会交给 Shell 解释。\n$ npx -p lolcatjs -p cowsay \u0026#39;cowsay hello | lolcatjs\u0026#39; # 报错 上面代码中，cowsay hello | lolcatjs执行时会报错，原因是第一项cowsay由 npx 解释，而第二项命令localcatjs由 Shell 解释，但是lolcatjs并没有全局安装，所以报错。\n-c参数可以将所有命令都用 npx 解释。有了它，下面代码就可以正常执行了。\n$ npx -p lolcatjs -p cowsay -c \u0026#39;cowsay hello | lolcatjs\u0026#39; -c参数的另一个作用，是将环境变量带入所要执行的命令。举例来说，npm 提供当前项目的一些环境变量，可以用下面的命令查看。\n$ npm run env | grep npm_ -c参数可以把这些 npm 的环境变量带入 npx 命令。\n$ npx -c \u0026#39;echo \u0026#34;$npm_package_name\u0026#34;\u0026#39; 上面代码会输出当前项目的项目名。\n执行 GitHub 源码 # npx 还可以执行 GitHub 上面的模块源码。\n# 执行 Gist 代码 $ npx https://gist.github.com/zkat/4bc19503fe9e9309e2bfaa2c58074d32 # 执行仓库代码 $ npx github:piuccio/cowsay hello 注意，远程代码必须是一个模块，即必须包含package.json和入口脚本。\n参考链接 # npx Speed Up Your npm Workflow With npx Introducing npx: an npm package runner ","date":"23 October 2023","permalink":"/posts/language/nodejs/npm/npx/","section":"博客","summary":"npx 使用教程 # npm 从5.","title":"npx.md"},{"content":"package.json # files 字段 # files字段是一个数组，里面指定了一组文件。当模块发布到 NPM 网站时，这组文件会被包括。这个字段是可选的，如果没有指定内容，那么发布时所有文件都会被包括在内。如果files字段包含目录名，该目录里面的所有文件都会被计入。\n{ \u0026#34;name\u0026#34;: \u0026#34;@adam_baldwin/wombats\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;main\u0026#34;: \u0026#34;index.js\u0026#34;, \u0026#34;files\u0026#34;: [ \u0026#34;index.js\u0026#34; ], ... } npm不会发布.gitignore里面列出的文件和目录。项目的根目录或子目录里面，还可以放置一个.npmignore文件，该文件会覆盖.gitignore，里面指定的文件和目录不会被发布。\n项目的根目录下，files字段优先级最高；子目录下，.npmignore优先。files字段指定的文件，不会被.npmignore或.gitignore排除。\n以下文件，发布的时候总是会包含。\npackage.json README CHANGES / CHANGELOG / HISTORY LICENSE / LICENCE NOTICE main字段里面的文件 README、CHANGES、LICENSE和NOTICE这四个文件名，可以采取任意的大小写组合。\n以下文件，发布的时候总是会被排除。\n.git CVS .svn .hg .lock-wscript .wafpickle-N .*.swp .DS_Store ._* npm-debug.log .npmrc node_modules config.gypi *.orig package-lock.json 基本上，npm 会发布files字段指定的文件和目录，以及那些总是会包含在内的文件（比如package.json），然后再除去那些被其他规则排除的文件和目录。\nnpm-packlist 模块会列出所有将要打包发布的文件和模块。npm pack命令则会将那些将要发布的内容打成一个tgz压缩包，放在项目的根目录下。\n","date":"23 October 2023","permalink":"/posts/language/nodejs/npm/package.json/","section":"博客","summary":"package.json # files 字段 # files字段是一个数组，里面指定了一组文件。当模块发布到 NPM 网站时，这组文件会被包括。这个字段是可选的，如果没有指定内容，那么发布时所有文件都会被包括在内。如果files字段包含目录名，该目录里面的所有文件都会被计入。","title":"package.json.md"},{"content":"发布 # 发布标签 # npm 支持为版本打上标签，这叫做发布标签（dist-tag）。如果不指定发布标签，默认就是latest。用户下载模块时，默认安装的就是latest标签指向的版本。\n新发布的版本，如果不希望用户默认安装，就需要自己指定标签。举例来说，某个模块的最新版本是4.6.12，但是有些用户还在使用老版本3.2.13。现在，你修正了一些老版本的 bug，发了一个新版本3.2.14。如果不指定发布标签，3.2.14的发布标签就是latest，因为它是最新发布的。\n这导致的后果就是，用户执行下面的命令，进行默认安装时，会出现非预期的结果。\n$ npm install \u0026lt;package\u0026gt; 执行上面命令时，用户会默认安装3.2.14，而不是4.6.12。因为latest标签指向3.2.14。\n解决方法就是，发布3.2.14的时候，为它打上一个发布标签。这样，3.2.14就不会占用latest标签。\n$ npm publish --tag=previous 执行上面的命令后，3.2.14的发布标签就是previous。\n安装时，必须指定这个标签，才能安装到3.2.14。\n$ npm install \u0026lt;package\u0026gt;@previous # 或者 $ npm install \u0026lt;package\u0026gt; --tag previous 上面的命令的两种语法都可以指定标签名。由于latest是默认标签，所以可以省略。\n$ npm install \u0026lt;package\u0026gt; # 等同于 $ npm install \u0026lt;package\u0026gt;@latest 一种常见的做法是，发布下一个大版本时，指定它的发布标签为next。\n# 发布 $ npm publish --tag=next # 安装 $ npm publish \u0026lt;package\u0026gt;@next 这样的话，用户默认安装的还是主流版本，但是愿意尝鲜的用户，可以使用新版本。\n等到新版本足够可靠以后，再把latest标签指定到新版本。\n$ npm dist-tag add \u0026lt;package\u0026gt;@5.0.1 latest 如果希望把默认的发布标签改掉，不再是latest，可以写在package.json里面。\n{ … \u0026#34;publishConfig\u0026#34;: { \u0026#34;tag\u0026#34;: \u0026#34;next\u0026#34; } } 上面的设置，可以使得发布新版本时，发布标签默认为next。\n常用的发布标签有stable、beta、dev等等。\nnpm dist-tag 命令 # npm dist-tag命令用来管理发布标签。\nnpm dist-tag ls用来列出所有的发布标签。如果不指定模块名，那么默认为当前模块。\n$ npm dist-tag ls [\u0026lt;pkg\u0026gt;] npm dist-tag add用来为一个版本指定发布标签。\n$ npm dist-tag add \u0026lt;pkg\u0026gt;@\u0026lt;version\u0026gt; [\u0026lt;tag\u0026gt;] npm dist-tag rm用来移除一个发布标签。\n$ npm dist-tag rm \u0026lt;pkg\u0026gt; \u0026lt;tag\u0026gt; .gitignore，.npmignore # 如果当前项目的根目录下有.gitignore文件，该文件里面的路径不会打包进入 npm 模块。\n如果有.npmignore文件，那么 npm 将忽略.gitignore文件，不将.npmignore文件里面的路径打包进入 npm 模块。如果有些文件不希望进入 npm 模块（比如测试用例），但是希望进入 Git 仓库，那么可以使用.npmignore。\n# .gitignore node_modules # .npmignore node_modules tests 此外，package.json的files字段，也可以用来排除进入 npm 模块的文件。\n{ \u0026#34;files\u0026#34;: [ \u0026#34;index.js\u0026#34; ] } npm 会最优先排除files字段里面的文件。另外，无论如何设置，package.json文件都会进入 npm 模块。\nnpm pack 命令 # npm pack命令用来打包当前项目，打包后的文件会在当前目录下生成，文件名为\u0026lt;name\u0026gt;-\u0026lt;version\u0026gt;.tgz。\n$ npm pack 如果多次运行该命令，每次生成的包将覆盖前一次的包。\nnpm pack可以接受路径作为参数，打包该路径下的模块。如果没有提供任何参数，将打包当前目录。\n$ npm pack foo/bar --dry-run参数会输出打包的内容，而不生成打包文件。\n$ npm pack --dry-run 参考链接 # One simple trick for JavaScript package maintainers, by Stephan Bönnemann ","date":"23 October 2023","permalink":"/posts/language/nodejs/npm/publish/","section":"博客","summary":"发布 # 发布标签 # npm 支持为版本打上标签，这叫做发布标签（dist-tag）。如果不指定发布标签，默认就是latest。用户下载模块时，默认安装的就是latest标签指向的版本。","title":"publish.md"},{"content":"脚本功能 # npm run # npm不仅可以用于模块管理，还可以用于执行脚本。package.json文件有一个scripts字段，可以用于指定脚本命令，供npm直接调用。\n{ \u0026#34;name\u0026#34;: \u0026#34;myproject\u0026#34;, \u0026#34;devDependencies\u0026#34;: { \u0026#34;jshint\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;browserify\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;mocha\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;scripts\u0026#34;: { \u0026#34;lint\u0026#34;: \u0026#34;jshint **.js\u0026#34;, \u0026#34;test\u0026#34;: \u0026#34;mocha test/\u0026#34; } } 上面代码中，scripts字段指定了两项命令lint和test。命令行输入npm run-script lint或者npm run lint，就会执行jshint **.js，输入npm run-script test或者npm run test，就会执行mocha test/。npm run是npm run-script的缩写，一般都使用前者，但是后者可以更好地反应这个命令的本质。\nnpm run命令会自动在环境变量$PATH添加node_modules/.bin目录，所以scripts字段里面调用命令时不用加上路径，这就避免了全局安装NPM模块。\nnpm run如果不加任何参数，直接运行，会列出package.json里面所有可以执行的脚本命令。\nnpm内置了两个命令简写，npm test等同于执行npm run test，npm start等同于执行npm run start。\nnpm run会创建一个Shell，执行指定的命令，并临时将node_modules/.bin加入PATH变量，这意味着本地模块可以直接运行。\n举例来说，你执行ESLint的安装命令。\n$ npm i eslint --save-dev 运行上面的命令以后，会产生两个结果。首先，ESLint被安装到当前目录的node_modules子目录；其次，node_modules/.bin目录会生成一个符号链接node_modules/.bin/eslint，指向ESLint模块的可执行脚本。\n然后，你就可以在package.json的script属性里面，不带路径的引用eslint这个脚本。\n{ \u0026#34;name\u0026#34;: \u0026#34;Test Project\u0026#34;, \u0026#34;devDependencies\u0026#34;: { \u0026#34;eslint\u0026#34;: \u0026#34;^1.10.3\u0026#34; }, \u0026#34;scripts\u0026#34;: { \u0026#34;lint\u0026#34;: \u0026#34;eslint .\u0026#34; } } 等到运行npm run lint的时候，它会自动执行./node_modules/.bin/eslint .。\n如果直接运行npm run不给出任何参数，就会列出scripts属性下所有命令。\n$ npm run Available scripts in the user-service package: lint jshint **.js test mocha test/ 下面是另一个package.json文件的例子。\n\u0026#34;scripts\u0026#34;: { \u0026#34;watch\u0026#34;: \u0026#34;watchify client/main.js -o public/app.js -v\u0026#34;, \u0026#34;build\u0026#34;: \u0026#34;browserify client/main.js -o public/app.js\u0026#34;, \u0026#34;start\u0026#34;: \u0026#34;npm run watch \u0026amp; nodemon server.js\u0026#34;, \u0026#34;test\u0026#34;: \u0026#34;node test/all.js\u0026#34; }, 上面代码在scripts项，定义了四个别名，每个别名都有对应的脚本命令。\n$ npm run watch $ npm run build $ npm run start $ npm run test 其中，start和test属于特殊命令，可以省略run。\n$ npm start $ npm test 如果希望一个操作的输出，是另一个操作的输入，可以借用Linux系统的管道命令，将两个操作连在一起。\n\u0026#34;build-js\u0026#34;: \u0026#34;browserify browser/main.js | uglifyjs -mc \u0026gt; static/bundle.js\u0026#34; 但是，更方便的写法是引用其他npm run命令。\n\u0026#34;build\u0026#34;: \u0026#34;npm run build-js \u0026amp;\u0026amp; npm run build-css\u0026#34; 上面的写法是先运行npm run build-js，然后再运行npm run build-css，两个命令中间用\u0026amp;\u0026amp;连接。如果希望两个命令同时平行执行，它们中间可以用\u0026amp;连接。\n下面是一个流操作的例子。\n\u0026#34;devDependencies\u0026#34;: { \u0026#34;autoprefixer\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;cssmin\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;scripts\u0026#34;: { \u0026#34;build:css\u0026#34;: \u0026#34;autoprefixer -b \u0026#39;last 2 versions\u0026#39; \u0026lt; assets/styles/main.css | cssmin \u0026gt; dist/main.css\u0026#34; } 写在scripts属性中的命令，也可以在node_modules/.bin目录中直接写成bash脚本。下面是一个bash脚本。\n#!/bin/bash cd site/main browserify browser/main.js | uglifyjs -mc \u0026gt; static/bundle.js 假定上面的脚本文件名为build.sh，并且权限为可执行，就可以在scripts属性中引用该文件。\n\u0026#34;build-js\u0026#34;: \u0026#34;bin/build.sh\u0026#34; 参数 # npm run命令还可以添加参数。\n\u0026#34;scripts\u0026#34;: { \u0026#34;test\u0026#34;: \u0026#34;mocha test/\u0026#34; } 上面代码指定npm test，实际运行mocha test/。如果要通过npm test命令，将参数传到mocha，则参数之前要加上两个连词线。\n$ npm run test -- anothertest.js # 等同于 $ mocha test/ anothertest.js 上面命令表示，mocha要运行所有test子目录的测试脚本，以及另外一个测试脚本anothertest.js。\nnpm run本身有一个参数-s，表示关闭npm本身的输出，只输出脚本产生的结果。\n// 输出npm命令头 $ npm run test // 不输出npm命令头 $ npm run -s test scripts脚本命令最佳实践 # scripts字段的脚本命令，有一些最佳实践，可以方便开发。首先，安装npm-run-all模块。\n$ npm install npm-run-all --save-dev 这个模块用于运行多个scripts脚本命令。\n# 继发执行 $ npm-run-all build:html build:js # 等同于 $ npm run build:html \u0026amp;\u0026amp; npm run build:js # 并行执行 $ npm-run-all --parallel watch:html watch:js # 等同于 $ npm run watch:html \u0026amp; npm run watch:js # 混合执行 $ npm-run-all clean lint --parallel watch:html watch:js # 等同于 $ npm-run-all clean lint $ npm-run-all --parallel watch:html watch:js # 通配符 $ npm-run-all --parallel watch:* （1）start脚本命令\nstart脚本命令，用于启动应用程序。\n\u0026#34;start\u0026#34;: \u0026#34;npm-run-all --parallel dev serve\u0026#34; 上面命令并行执行dev脚本命令和serve脚本命令，等同于下面的形式。\n$ npm run dev \u0026amp; npm run serve 如果start脚本没有配置，npm start命令默认执行下面的脚本，前提是模块的根目录存在一个server.js文件。\n$ node server.js （2）dev脚本命令\ndev脚本命令，规定开发阶段所要做的处理，比如构建网页资源。\n\u0026#34;dev\u0026#34;: \u0026#34;npm-run-all dev:*\u0026#34; 上面命令用于继发执行所有dev的子命令。\n\u0026#34;predev:sass\u0026#34;: \u0026#34;node-sass --source-map src/css/hoodie.css.map --output-style nested src/sass/base.scss src/css/hoodie.css\u0026#34; 上面命令将sass文件编译为css文件，并生成source map文件。\n\u0026#34;dev:sass\u0026#34;: \u0026#34;node-sass --source-map src/css/hoodie.css.map --watch --output-style nested src/sass/base.scss src/css/hoodie.css\u0026#34; 上面命令会监视sass文件的变动，只要有变动，就自动将其编译为css文件。\n\u0026#34;dev:autoprefix\u0026#34;: \u0026#34;postcss --use autoprefixer --autoprefixer.browsers \\\u0026#34;\u0026gt; 5%\\\u0026#34; --output src/css/hoodie.css src/css/hoodie.css\u0026#34; 上面命令为css文件加上浏览器前缀，限制条件是只考虑市场份额大于5%的浏览器。\n（3）serve脚本命令\nserve脚本命令用于启动服务。\n\u0026#34;serve\u0026#34;: \u0026#34;live-server dist/ --port=9090\u0026#34; 上面命令启动服务，用的是 live-server模块，将服务启动在9090端口，展示dist子目录。\nlive-server模块有三个功能。\n启动一个HTTP服务器，展示指定目录的index.html文件，通过该文件加载各种网络资源，这是file://协议做不到的。 添加自动刷新功能。只要指定目录之中，文件有任何变化，它就会刷新页面。 npm run serve命令执行以后，自动打开浏览器。、 以前，上面三个功能需要三个模块来完成：http-server、live-reload和opener，现在只要live-server一个模块就够了。\n（4）test脚本命令\ntest脚本命令用于执行测试。\n\u0026#34;test\u0026#34;: \u0026#34;npm-run-all test:*\u0026#34;, \u0026#34;test:lint\u0026#34;: \u0026#34;sass-lint --verbose --config .sass-lint.yml src/sass/*\u0026#34; 上面命令规定，执行测试时，运行lint脚本，检查脚本之中的语法错误。\n（5）prod脚本命令\nprod脚本命令，规定进入生产环境时需要做的处理。\n\u0026#34;prod\u0026#34;: \u0026#34;npm-run-all prod:*\u0026#34;, \u0026#34;prod:sass\u0026#34;: \u0026#34;node-sass --output-style compressed src/sass/base.scss src/css/prod/hoodie.min.css\u0026#34;, \u0026#34;prod:autoprefix\u0026#34;: \u0026#34;postcss --use autoprefixer --autoprefixer.browsers \u0026#34;\u0026gt; 5%\u0026#34; --output src/css/prod/hoodie.min.css src/css/prod/hoodie.min.css\u0026#34; 上面命令将sass文件转为css文件，并加上浏览器前缀。\n（6）help脚本命令\nhelp脚本命令用于展示帮助信息。\n\u0026#34;help\u0026#34;: \u0026#34;markdown-chalk --input DEVELOPMENT.md\u0026#34; 上面命令之中，markdown-chalk模块用于将指定的markdown文件，转为彩色文本显示在终端之中。\n（7）docs脚本命令\ndocs脚本命令用于生成文档。\n\u0026#34;docs\u0026#34;: \u0026#34;kss-node --source src/sass --homepage ../../styleguide.md\u0026#34; 上面命令使用kss-node模块，提供源码的注释生成markdown格式的文档。\npre- 和 post- 脚本 # npm run为每条命令提供了pre-和post-两个钩子（hook）。以npm run lint为例，执行这条命令之前，npm会先查看有没有定义prelint和postlint两个钩子，如果有的话，就会先执行npm run prelint，然后执行npm run lint，最后执行npm run postlint。\n{ \u0026#34;name\u0026#34;: \u0026#34;myproject\u0026#34;, \u0026#34;devDependencies\u0026#34;: { \u0026#34;eslint\u0026#34;: \u0026#34;latest\u0026#34; \u0026#34;karma\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;scripts\u0026#34;: { \u0026#34;lint\u0026#34;: \u0026#34;eslint --cache --ext .js --ext .jsx src\u0026#34;, \u0026#34;test\u0026#34;: \u0026#34;karma start --log-leve=error karma.config.js --single-run=true\u0026#34;, \u0026#34;pretest\u0026#34;: \u0026#34;npm run lint\u0026#34;, \u0026#34;posttest\u0026#34;: \u0026#34;echo \u0026#39;Finished running tests\u0026#39;\u0026#34; } } 上面代码是一个package.json文件的例子。如果执行npm test，会按下面的顺序执行相应的命令。\npretest test posttest 如果执行过程出错，就不会执行排在后面的脚本，即如果prelint脚本执行出错，就不会接着执行lint和postlint脚本。\n下面是一个例子。\n{ \u0026#34;test\u0026#34;: \u0026#34;karma start\u0026#34;, \u0026#34;test:lint\u0026#34;: \u0026#34;eslint . --ext .js --ext .jsx\u0026#34;, \u0026#34;pretest\u0026#34;: \u0026#34;npm run test:lint\u0026#34; } 上面代码中，在运行npm run test之前，会自动检查代码，即运行npm run test:lint命令。\n下面是一些常见的pre-和post-脚本。\nprepublish：发布一个模块前执行。 postpublish：发布一个模块后执行。 preinstall：用户执行npm install命令时，先执行该脚本。 postinstall：用户执行npm install命令时，安装结束后执行该脚本，通常用于将下载的源码编译成用户需要的格式，比如有些模块需要在用户机器上跟本地的C++模块一起编译。 preuninstall：卸载一个模块前执行。 postuninstall：卸载一个模块后执行。 preversion：更改模块版本前执行。 postversion：更改模块版本后执行。 pretest：运行npm test命令前执行。 posttest：运行npm test命令后执行。 prestop：运行npm stop命令前执行。 poststop：运行npm stop命令后执行。 prestart：运行npm start命令前执行。 poststart：运行npm start命令后执行。 prerestart：运行npm restart命令前执行。 postrestart：运行npm restart命令后执行。 对于最后一个npm restart命令，如果没有设置restart脚本，prerestart和postrestart会依次执行stop和start脚本。\n另外，不能在pre脚本之前再加pre，即prepretest脚本不起作用。\n注意，即使Npm可以自动运行pre和post脚本，也可以手动执行它们。\n$ npm run prepublish 下面是post install的例子。\n{ \u0026#34;postinstall\u0026#34;: \u0026#34;node lib/post_install.js\u0026#34; } 上面的这个命令，主要用于处理从Git仓库拉下来的源码。比如，有些源码是用TypeScript写的，可能需要转换一下。\n下面是publish钩子的一个例子。\n{ \u0026#34;dist:modules\u0026#34;: \u0026#34;babel ./src --out-dir ./dist-modules\u0026#34;, \u0026#34;gh-pages\u0026#34;: \u0026#34;webpack\u0026#34;, \u0026#34;gh-pages:deploy\u0026#34;: \u0026#34;gh-pages -d gh-pages\u0026#34;, \u0026#34;prepublish\u0026#34;: \u0026#34;npm run dist:modules\u0026#34;, \u0026#34;postpublish\u0026#34;: \u0026#34;npm run gh-pages \u0026amp;\u0026amp; npm run gh-pages:deploy\u0026#34; } 上面命令在运行npm run publish时，会先执行Babel编译，然后调用Webpack构建，最后发到Github Pages上面。\n以上都是npm相关操作的钩子，如果安装某些模块，还能支持Git相关的钩子。下面以 husky模块为例。\n$ npm install husky --save-dev 安装以后，就能在package.json添加precommit、prepush等钩子。\n{ \u0026#34;scripts\u0026#34;: { \u0026#34;lint\u0026#34;: \u0026#34;eslint yourJsFiles.js\u0026#34;, \u0026#34;precommit\u0026#34;: \u0026#34;npm run test \u0026amp;\u0026amp; npm run lint\u0026#34;, \u0026#34;prepush\u0026#34;: \u0026#34;npm run test \u0026amp;\u0026amp; npm run lint\u0026#34;, \u0026#34;...\u0026#34;: \u0026#34;...\u0026#34; } } 类似作用的模块还有pre-commit、precommit-hook等。\n内部变量 # scripts字段可以使用一些内部变量，主要是package.json的各种字段。\n比如，package.json的内容是{\u0026quot;name\u0026quot;:\u0026quot;foo\u0026quot;, \u0026quot;version\u0026quot;:\u0026quot;1.2.5\u0026quot;}，那么变量npm_package_name的值是foo，变量npm_package_version的值是1.2.5。\n{ \u0026#34;scripts\u0026#34;:{ \u0026#34;bundle\u0026#34;: \u0026#34;mkdir -p build/$npm_package_version/\u0026#34; } } 运行npm run bundle以后，将会生成build/1.2.5/子目录。\nconfig字段也可以用于设置内部字段。\n\u0026#34;name\u0026#34;: \u0026#34;fooproject\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;reporter\u0026#34;: \u0026#34;xunit\u0026#34; }, \u0026#34;scripts\u0026#34;: { \u0026#34;test\u0026#34;: \u0026#34;mocha test/ --reporter $npm_package_config_reporter\u0026#34; } 上面代码中，变量npm_package_config_reporter对应的就是reporter。\n通配符 # npm 的通配符的规则如下。\n* 匹配0个或多个字符 ? 匹配1个字符 [...] 匹配某个范围的字符。如果该范围的第一个字符是!或^，则匹配不在该范围的字符。 !(pattern|pattern|pattern) 匹配任何不符合给定的模式 ?(pattern|pattern|pattern) 匹配0个或1个给定的模式 +(pattern|pattern|pattern) 匹配1个或多个给定的模式 *(a|b|c) 匹配0个或多个给定的模式 @(pattern|pat*|pat?erN) 只匹配给定模式之一 ** 如果出现在路径部分，表示0个或多个子目录。 ","date":"23 October 2023","permalink":"/posts/language/nodejs/npm/scripts/","section":"博客","summary":"脚本功能 # npm run # npm不仅可以用于模块管理，还可以用于执行脚本。package.","title":"scripts.md"},{"content":"Yarn 的用法 # 简介 # Yarn 是 Facebook 联合其他大公司推出的模块管理器。相比 npm，它有两个显著特点。\n（1）安装速度较快。\nYarn 采用平行安装模式，而 npm 采用的是线性模式，只有前一个模块安装完，才会安装下一个。\n（2）默认开启“版本锁定”功能\nYarn 希望安装依赖时，所有依赖的版本在不同机器都保持相同。为了达到这个目的，第一次安装依赖时，它默认生成一个锁定文件yarn.lock，将这个文件放到代码库之中，下次安装时就能保证，总是安装相同版本的依赖。这与npm shrinkwrap命令生成的npm-shrinkwrap.json的作用相似，只不过 Yarn 默认就可以生成这个文件。\n全局参数 global # 如果要全局执行一个命令，必须加上global参数。目前，add、bin、ls和remove四个命令，支持global参数。\n$ yarn global add create-react-app --prefix /usr/local yarn install # yarn install命令用于安装一个模块。如果yarn.lock文件存在，会优先读取该文件，按照该文件指定的版本安装。\n使用--production参数或环境变量NODE_ENV等于production，将不会安装devDependencies字段指定的模块。\n$ yarn install --production 如果使用--no-lockfile参数，yarn install将不会读取或生成yarn.lock。\n$ yarn install --no-lockfile yarn add # yarn add命令允许新增安装一个模块。它默认会将该模块加入package.json文件的dependencies字段。如果想加入devDependencies字段，要使用--dev参数。\n$ yarn add package-name $ yarn add package-name@1.2.3 $ yarn add package-name@tag yarn licenses # yarn licenses命令有两个子命令。\nyarn licenses ls命令列出所有模块的许可证。\nyarn licenses generate-disclaimer命令将所有模块的许可证的条款，全部显示出来。\nyarn why # yarn why命令列出之所以安装某个模块的原因，即为什么安装了它。\n$ yarn why jest 你也可以用它分析某个目录或者某个文件。\n$ yarn why node_modules/once $ yarn why node_modules/once/once.js yarn upgrade # yarn upgrade命令会按照package.json里面指定的版本范围，更新依赖版本，重新生成yarn.lock。\n$ yarn upgrade 如果单独升级某个模块，yarn upgrade会将它升级到latest标签指定的版本，然后改写package.json。这意味着该命令可能会将一个1.x版本的模块，升级到2.x。\n$ yarn upgrade d3-scale 更新时指定版本范围或标签，也是允许的。\n$ yarn upgrade d3-scale@1.0.2 $ yarn upgrade react@next yarn generate-lock-entry # yarn generate-lock-entry命令依照package.json文件，生成yarn.lock文件。\n$ yarn generate-lock-entry yarn.lock 文件 # yarn.lock是一个锁文件，用来记录当前项目的依赖模块的精确版本。只要项目的根目录有这个文件，下次安装依赖的时候，总是会安装一模一样的node_modules目录，这个特点称为决定性（determinism）。\n如果当前项目没有这个文件，那么第一次运行yarn install或者yarn add [模块名]命令的时候，就会生成这个文件。以后，再运行yarn add命令，会更新这个文件。\n举例来说，yarn add supports-color命令会产生下面的yarn.lock文件。\nhas-flag@^1.0.0: version \u0026#34;1.0.0\u0026#34; resolved \u0026#34;https://registry.yarnpkg.com/has-flag/-/has-flag-1.0.0.tgz#9d9e793165ce017a00f00418c43f942a7b1d11fa\u0026#34; supports-color@^3.2.3: version \u0026#34;3.2.3\u0026#34; resolved \u0026#34;https://registry.yarnpkg.com/supports-color/-/supports-color-3.2.3.tgz#65ac0504b3954171d8a64946b2ae3cbb8a5f54f6\u0026#34; dependencies: has-flag \u0026#34;^1.0.0\u0026#34; 上面代码中，模块之间使用空行分隔。每个模块会指明当前安装的精确版本（version字段）和下载地址（resovled字段），以及依赖的模块（dependencies字段）。\n注意，从yarn.lock文件看不出来，哪个模块会安装在node_modules目录的顶层，必须结合package.json才能看出来，具体的算法由 Yarn 决定。这也意味着，不同版本的 Yarn 处理同样的yarn.lock文件，可能会得到不一样的node_modules目录，但是每个模块的版本肯定都是相同的。只有相同版本的 Yarn，才能保证一定会得到相同的node_modules目录。\n参考链接 # Yarn determinism, by Sebastian McKenzie Running Yarn offline, by Konstantin Raev Yarn CLI ","date":"23 October 2023","permalink":"/posts/language/nodejs/npm/yarn/","section":"博客","summary":"Yarn 的用法 # 简介 # Yarn 是 Facebook 联合其他大公司推出的模块管理器。相比 npm，它有两个显著特点。","title":"yarn.md"},{"content":"概述 # npm有两种含义。\n首先，npm 是一个网站，用来登记和管理 Node 的模块，网址为 npmjs.org。\n其次，npm 是一个命令行软件，用来在用户的电脑上安装和管理 Node 模块。\n安装 # npm不需要单独安装。安装 Node 的时候，会默认一起安装npm。\n但是，默认安装的npm可能不是最新版本。在 Node 安装成功后，最好用下面的命令，更新到最新版本。\n$ npm install npm@latest -g 然后，运行下面的命令，查看一下 npm 的版本。\n$ npm --version # 等同于 $ npm -v 下面三个命令，也可以用来获取帮助。\n# 查看 npm 命令列表 $ npm help # 查看各个命令的简单用法 $ npm -l # 查看 npm 的配置 $ npm config list -l npm init # npm init用来初始化生成一个新的package.json文件。它会向用户提问一系列问题，如果你觉得不用修改默认配置，一路回车就可以了。\n如果使用了-f（代表force）、-y（代表yes），则跳过提问阶段，直接生成一个新的package.json文件。\n$ npm init -y npm set # npm set用来设置环境变量。\n$ npm set init-author-name \u0026#39;Your name\u0026#39; $ npm set init-author-email \u0026#39;Your email\u0026#39; $ npm set init-author-url \u0026#39;http://yourdomain.com\u0026#39; $ npm set init-license \u0026#39;MIT\u0026#39; 上面命令等于为npm init设置了默认值，以后执行npm init的时候，package.json的作者姓名、邮件、主页、许可证字段就会自动写入预设的值。这些信息会存放在用户主目录的 ~/.npmrc文件，使得用户不用每个项目都输入。如果某个项目有不同的设置，可以针对该项目运行npm config。\n$ npm set save-exact true 上面命令设置加入模块时，package.json将记录模块的确切版本，而不是一个可选的版本范围。\nnpm config # $ npm config set prefix $dir 上面的命令将指定的$dir目录，设为模块的全局安装目录。如果当前有这个目录的写权限，那么运行npm install的时候，就不再需要sudo命令授权了。\n$ npm config set save-prefix \u0026#39;~\u0026#39; 上面的命令使得npm install --save和npm install --save-dev安装新模块时，允许的版本范围从克拉符号（^）改成波浪号（~），即从允许小版本升级，变成只允许补丁包的升级。\n$ npm config set init.author.name $name $ npm config set init.author.email $email 上面命令指定使用npm init时，生成的package.json文件的字段默认值。\nnpm info # npm info命令可以查看每个模块的具体信息。比如，查看underscore模块的信息。\n$ npm info underscore { name: \u0026#39;underscore\u0026#39;, description: \u0026#39;JavaScript\\\u0026#39;s functional programming helper library.\u0026#39;, \u0026#39;dist-tags\u0026#39;: { latest: \u0026#39;1.5.2\u0026#39;, stable: \u0026#39;1.5.2\u0026#39; }, repository: { type: \u0026#39;git\u0026#39;, url: \u0026#39;git://github.com/jashkenas/underscore.git\u0026#39; }, homepage: \u0026#39;http://underscorejs.org\u0026#39;, main: \u0026#39;underscore.js\u0026#39;, version: \u0026#39;1.5.2\u0026#39;, devDependencies: { phantomjs: \u0026#39;1.9.0-1\u0026#39; }, licenses: { type: \u0026#39;MIT\u0026#39;, url: \u0026#39;https://raw.github.com/jashkenas/underscore/master/LICENSE\u0026#39; }, files: [ \u0026#39;underscore.js\u0026#39;, \u0026#39;underscore-min.js\u0026#39;, \u0026#39;LICENSE\u0026#39; ], readmeFilename: \u0026#39;README.md\u0026#39;} 上面命令返回一个JavaScript对象，包含了underscore模块的详细信息。这个对象的每个成员，都可以直接从info命令查询。\n$ npm info underscore description JavaScript\u0026#39;s functional programming helper library. $ npm info underscore homepage http://underscorejs.org $ npm info underscore version 1.5.2 npm search # npm search命令用于搜索npm仓库，它后面可以跟字符串，也可以跟正则表达式。\n$ npm search \u0026lt;搜索词\u0026gt; 下面是一个例子。\n$ npm search node-gyp // NAME DESCRIPTION // autogypi Autogypi handles dependencies for node-gyp projects. // grunt-node-gyp Run node-gyp commands from Grunt. // gyp-io Temporary solution to let node-gyp run `rebuild` under… // ... npm list # npm list命令以树型结构列出当前项目安装的所有模块，以及它们依赖的模块。\n$ npm list 加上global参数，会列出全局安装的模块。\n$ npm list -global npm list命令也可以列出单个模块。\n$ npm list underscore npm link # 开发NPM模块的时候，有时我们会希望，边开发边试用，比如本地调试的时候，require('myModule')会自动加载本机开发中的模块。Node规定，使用一个模块时，需要将其安装到全局的或项目的node_modules目录之中。对于开发中的模块，解决方法就是在全局的node_modules目录之中，生成一个符号链接，指向模块的本地目录。\nnpm link就能起到这个作用，会自动建立这个符号链接。\n请设想这样一个场景，你开发了一个模块myModule，目录为src/myModule，你自己的项目myProject要用到这个模块，项目目录为src/myProject。首先，在模块目录（src/myModule）下运行npm link命令。\nsrc/myModule$ npm link 上面的命令会在NPM的全局模块目录内，生成一个符号链接文件，该文件的名字就是package.json文件中指定的模块名。\n/path/to/global/node_modules/myModule -\u0026gt; src/myModule 这个时候，已经可以全局调用myModule模块了。但是，如果我们要让这个模块安装在项目内，还要进行下面的步骤。\n切换到项目目录，再次运行npm link命令，并指定模块名。\nsrc/myProject$ npm link myModule 上面命令等同于生成了本地模块的符号链接。\nsrc/myProject/node_modules/myModule -\u0026gt; /path/to/global/node_modules/myModule 然后，就可以在你的项目中，加载该模块了。\nvar myModule = require(\u0026#39;myModule\u0026#39;); 这样一来，myModule的任何变化，都可以直接反映在myProject项目之中。但是，这样也出现了风险，任何在myProject目录中对myModule的修改，都会反映到模块的源码中。\n如果你的项目不再需要该模块，可以在项目目录内使用npm unlink命令，删除符号链接。\nsrc/myProject$ npm unlink myModule npm bin # npm bin命令显示相对于当前目录的，Node模块的可执行脚本所在的目录（即.bin目录）。\n# 项目根目录下执行 $ npm bin ./node_modules/.bin npm adduser # npm adduser用于在npmjs.com注册一个用户。\n$ npm adduser Username: YOUR_USER_NAME Password: YOUR_PASSWORD Email: YOUR_EMAIL@domain.com npm publish # npm publish用于将当前模块发布到npmjs.com。执行之前，需要向npmjs.com申请用户名。\n$ npm adduser 如果已经注册过，就使用下面的命令登录。\n$ npm login 登录以后，就可以使用npm publish命令发布。\n$ npm publish 如果当前模块是一个beta版，比如1.3.1-beta.3，那么发布的时候需要使用tag参数，将其发布到指定标签，默认的发布标签是latest。\n$ npm publish --tag beta 如果发布私有模块，模块初始化的时候，需要加上scope参数。只有npm的付费用户才能发布私有模块。\n$ npm init --scope=\u0026lt;yourscope\u0026gt; Scopes相当于npm模块的命名空间，scoped package的包名格式为：\n@scope/project-name 包名以@开头，介于@和/之间的为scope。\nScoped Package默认为私有模块，付费用户才能发布，不过发布时可以指定为公开模块，就不需要付费。\n$ npm publish --access=public 如果你的模块是用ES6写的，那么发布的时候，最好转成ES5。首先，需要安装Babel。\n$ npm install --save-dev babel-cli@6 babel-preset-es2015@6 然后，在package.json里面写入build脚本。\n\u0026#34;scripts\u0026#34;: { \u0026#34;build\u0026#34;: \u0026#34;babel source --presets babel-preset-es2015 --out-dir distribution\u0026#34;, \u0026#34;prepublish\u0026#34;: \u0026#34;npm run build\u0026#34; } 运行上面的脚本，会将source目录里面的ES6源码文件，转为distribution目录里面的ES5源码文件。然后，在项目根目录下面创建两个文件.npmignore和.gitignore，分别写入以下内容。\n// .npmignore source // .gitignore node_modules distribution npm deprecate # 如果想废弃某个版本的模块，可以使用npm deprecate命令。\n$ npm deprecate my-thing@\u0026#34;\u0026lt; 0.2.3\u0026#34; \u0026#34;critical bug fixed in v0.2.3\u0026#34; 运行上面的命令以后，小于0.2.3版本的模块的package.json都会写入一行警告，用户安装这些版本时，这行警告就会在命令行显示。\nnpm owner # 模块的维护者可以发布新版本。npm owner命令用于管理模块的维护者。\n# 列出指定模块的维护者 $ npm owner ls \u0026lt;package name\u0026gt; # 新增维护者 $ npm owner add \u0026lt;user\u0026gt; \u0026lt;package name\u0026gt; # 删除维护者 $ npm owner rm \u0026lt;user\u0026gt; \u0026lt;package name\u0026gt; 其他命令 # npm home，npm repo # npm home命令可以打开一个模块的主页，npm repo命令则是打开一个模块的代码仓库。\n$ npm home $package $ npm repo $package 这两个命令不需要模块先安装。\nnpm outdated # npm outdated命令检查当前项目所依赖的模块，是否已经有新版本。\n$ npm outdated Package Current Wanted Latest Location normalize.css 4.0.0 4.2.0 5.0.0 photo-gallery 它会输出当前版本（current version）、应当安装的版本（wanted version）和最新发布的版本（latest version）。\nnpm prune # npm prune检查当前项目的node_modules目录中，是否有package.json里面没有提到的模块，然后将所有这些模块输出在命令行。\n$ npm prune npm shrinkwrap # npm shrinkwrap的作用是锁定当前项目的以来模块的版本。\n$ npm shrinkwrap 运行该命令后，会在当前项目的根目录下生成一个npm-shrinkwrap.json文件，内容是node_modules目录下所有已经安装模块的版本。\n下次运行npm install命令时，npm发现当前目录下有npm-shrinkwrap.json文件，就会只安装里面提到的模块，且版本也会保持一致。\n命令行参数 # \u0026ndash;proxy # --proxy参数允许对npm命令设置代理。\n$ npm --proxy http://127.0.0.1:8100 install # or $ npm config set proxy http://127.0.0.1:8100 $ npm install 上面的命令将通过本机的8100端口代理npm的通信。\n如果设置了环境变量HTTP-PROXY或者http-proxy和HTTPS-PROXY或者https-proxy，那么代理设置将以这个环境变量为准。\n\u0026ndash;registry # --registry参数设置npm与之通信的远程主机，默认是https://registry.npmjs.org/。\n$ npm install --registry=https://registry.npm.taobao.org # or $ npm config set registry \u0026#34;https://registry.npm.taobao.org\u0026#34; $ npm install 上面的命令将远程主机设为npm的淘宝镜像。\n参考链接 # James Halliday, task automation with npm run: npm run命令（package.json文件的script属性）的用法 Keith Cirkel, How to Use npm as a Build Tool justjs, npm link: developing your own npm modules without tears hoodie-css, Development Environment Help Stephan Bönnemann, How to make use of npm’s package distribution tags to create release channels Alex Booker, How to Build and Publish ES6 npm Modules Today, with Babel ","date":"23 October 2023","permalink":"/posts/language/nodejs/npm/basic/","section":"博客","summary":"概述 # npm有两种含义。","title":"basic.md"},{"content":"安装 # npm install # 基本用法 # npm install命令用于安装模块。npm i是该命令的别名。\n$ npm install lodash 上面命令在当前目录中安装了lodash模块。\n默认安装的是最新版本（即latest标签指向的版本），但是你可以 semver 表达式指定安装的版本。\n# 等同于 npm install lodash $ npm install lodash@latest # 指定确定的版本 $ npm install lodash@4.17.4 # 指定版本范围 $ npm install sax@\u0026#34;\u0026gt;=4.15.0 \u0026lt;4.18.0\u0026#34; # 指定大版本 $ npm install lodash@^4.0.0 上面最后一行命令，指定安装最新的4.x版。\n默认情况下，npm install不会修改package.json。--save或-S参数，将模块写入package.json的dependencies字段，--save-dev或-D，将模块加入package.json的devDependencies字段。\n# 将模块写入 package.json 的 dependencies 字段 $ npm install lodash --save $ npm install lodash -S # 将模块写入 package.json 的 devDependencies 字段 $ npm install lodash --save-dev $ npm install lodash -D 上面命令将模块写入package.json的时候，如果npm install没有指定版本，npm 会在安装的版本号前面添加^。比如，假定 lodash 的最新版是4.17.4，那么执行npm install -S lodash以后，package.json将写入的版本范围如下。\n\u0026#34;dependencies\u0026#34;: { \u0026#34;lodash\u0026#34;: \u0026#34;^4.17.4\u0026#34;, ... }, 上面的^4.17.4，表示兼容 4.17.4 以后的 4.x 版。\n如果不希望出现这种默认行为，可以使用--save-exact指定只将当前确定的版本号，写入package.json。\n$ npm install lodash --save --save-exact # 或者 $ npm install lodash --save-dev --save-exact 执行上面的命令以后，package.json里面的版本号将是固定的。\n\u0026#34;dependencies\u0026#34;: { \u0026#34;lodash\u0026#34;: \u0026#34;4.17.4\u0026#34;, ... }, 注意，--save-exact单独使用是无效的，必须与--save或--save-dev一起使用。\nnpm install也支持直接输入 Github 代码库地址。\n$ npm install git://github.com/package/path.git $ npm install git://github.com/package/path.git#0.1.0 全局安装 # npm 还可以将模块安装在全局，供所有项目使用。注意，一般情况下，全局安装只适用于一些命令行工具。\n全局安装，要使用--global或-g参数。此时，加上--save、--save-exact、--save-dev都是无效的。\n$ npm install create-react-app --global # 或者 $ npm install create-react-app -g 上面命令全局安装create-react-app模块。\n安装机制与重装 # 本地安装后，模块将存放在当前项目的node_modules子目录，然后只有在项目目录之中，才能调用这个模块。全局安装后，模块将存放在全局目录之中（通常是/usr/local/lib/），所有项目都可以调用这个模块，但是不应该这样做。\n安装之前，npm install会先检查，node_modules目录之中是否已经存在指定模块。如果存在，就不再重新安装了，即使远程仓库已经有了一个新版本，也是如此。\n如果你希望，一个模块不管是否安装过，npm 都要强制重新安装，可以使用-f或--force参数。\n$ npm install lodash --force 如果所有模块都要强制重新安装，那就删除node_modules目录，重新执行npm install。\n$ rm -rf node_modules $ npm install 安装项目依赖 # 不使用任何参数时，只使用npm install，会默认安装package.json里面的dependencies字段和devDependencies字段列出的所有模块。\n$ npm install 如果使用--production参数，可以只安装dependencies字段的模块。\n$ npm install --production # 等同于 $ NODE_ENV=production npm install 避免安装权限 # 默认情况下，npm 全局模块都安装在系统目录（比如/usr/local/lib/），普通用户没有写入权限，需要用到sudo命令。这不是很方便，我们可以在没有 root 权限的情况下，安装全局模块。\n首先，在主目录下新建配置文件.npmrc，在该文件中将prefix变量定义到一个你的个人目录下面（假定该目录是~/my-npm-modules）。\nprefix = /home/yourUsername/my-npm-modules 此后，全局安装的模块都会安装在这个子目录中，npm也会到~/my-npm-modules/bin目录去寻找命令。\n最后，将这个路径在.bash_profile文件（或.bashrc文件）中加入PATH变量。\nexport PATH=~/my-npm-modules/bin:$PATH npm update # npm update命令可以更新本地安装的模块到最新版本（符合 semver 的设置），如果该模块没有安装，则会安装该模块。npm up和npm upgrade是该命令的缩写。\n# 升级当前项目的某个模块 $ npm update lodash # 升级全局安装的某个模块 $ npm update -g lodash 不使用任何参数时，将更新当前项目的所有dependencies字段里面的模块。如果有模块没有安装，也将一起安装。\n$ npm update --dev参数会连带安装和更新devDependencies字段里面的模块。\n$ npm update --dev 更新时，会先到远程仓库查询最新版本，然后查询本地版本。如果本地版本不存在，或者远程版本较新，就会安装最新版本。\n使用-S或--save参数，可以在安装的同时，更新package.json里面模块的版本号。\n// 更新之前的package.json dependencies: { dep1: \u0026#34;^1.1.1\u0026#34; } // 更新之后的package.json dependencies: { dep1: \u0026#34;^1.2.2\u0026#34; } 注意，从 npm v2.6.1 开始，npm update只更新顶层模块，而不更新依赖的依赖，以前版本是递归更新的。如果想取到老版本的效果，要使用下面的命令。\n$ npm --depth 9999 update 注意，如果已经安装的模块版本比 semver 指定的版本更加新时，npm update有降级效果。\nnpm uninstall # npm uninstall命令，用来卸载已安装的模块。npm remove、npm rm、npm r、npm un和npm unlink，都是该命令的别名。\n# 卸载项目模块 $ npm uninstall lodash # 卸载全局模块 $ npm uninstall lodash --global $ npm uninstall lodash -g 使用--save参数（或-S），该模块将会从package.json的dependencies字段中移除。使用--save-dev参数（或-D）时，该模块将会从package.json的devDependencies字段中移除。\n$ npm uninstall lodash --save $ npm uninstall lodash --save-dev package-lock.json # 从 npm 5.0 版本开始，npm 模块默认会锁版本。在npm install命令安装依赖时，会自动生成package-lock.json文件，如果该文件已存在，则会更新该文件。下面是package-lock.json的一个例子。\n{ \u0026#34;name\u0026#34;: \u0026#34;react-example\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;lockfileVersion\u0026#34;: 1, \u0026#34;dependencies\u0026#34;: { \u0026#34;has-flag\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;resolved\u0026#34;: \u0026#34;https://registry.npmjs.org/has-flag/-/has-flag-1.0.0.tgz\u0026#34;, \u0026#34;integrity\u0026#34;: \u0026#34;sha1-nZ55MWXOAXoA8AQYxD+UKnsdEfo=\u0026#34; }, \u0026#34;supports-color\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;3.2.3\u0026#34;, \u0026#34;resolved\u0026#34;: \u0026#34;https://registry.npmjs.org/supports-color/-/supports-color-3.2.3.tgz\u0026#34;, \u0026#34;integrity\u0026#34;: \u0026#34;sha1-ZawFBLOVQXHYpklGsq48u4pfVPY=\u0026#34; }, \u0026#34;duplexify\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;3.5.0\u0026#34;, \u0026#34;resolved\u0026#34;: \u0026#34;https://registry.npmjs.org/duplexify/-/duplexify-3.5.0.tgz\u0026#34;, \u0026#34;integrity\u0026#34;: \u0026#34;sha1-GqdzAC4VeEV+nZ1KULDMquvL1gQ=\u0026#34;, \u0026#34;dependencies\u0026#34;: { \u0026#34;end-of-stream\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;resolved\u0026#34;: \u0026#34;https://registry.npmjs.org/end-of-stream/-/end-of-stream-1.0.0.tgz\u0026#34;, \u0026#34;integrity\u0026#34;: \u0026#34;sha1-1FlucCc0qT5A6a+GQxnqvZn/Lw4=\u0026#34; } } } } } 这个文件不仅指定了每个模块的精确版本，而且还指定了node_modules目录的结构（即哪些模块要安装在目录的顶层）。也就是说，哪怕你使用不同版本的 npm，只要有这个文件，最后得到的总是同样的node_modules目录。可以理解成，这个文件是node_modules目录的一个快照。\n上面的例子中可以看到，每个目录不仅有精确版本（version字段），还有下载地址（resolved字段）、哈希值（integrity字段）和依赖模块（dependencies字段）。\npackage-lock.json文件的lockfileVersion字段目前固定为1，以前的npm-shrinkwrap.json文件对应的lockfileVersion为0。这表明，前者实际上是后者的升级版。之所以叫一个新名字，是因为想表明这是一种全新的锁版本设计。\npackage-lock.json与npm-shrinkwrap.json有一些区别。首先，npm 在任何情况下，都不会将package-lock.json加入发布的代码之中。然后，在一个有package-lock.json的目录之中，执行npm shrinkwrap命令时，npm 会自动将package-lock.json改名为npm-shrinkwrap.json。\n如果同一个目录之中，同时存在package-lock.json与npm-shrinkwrap.json两个文件。这时，npm 会忽略package-lock.json，只使用npm-shrinkwrap.json。\n","date":"23 October 2023","permalink":"/posts/language/nodejs/npm/install/","section":"博客","summary":"安装 # npm install # 基本用法 # npm install命令用于安装模块。npm i是该命令的别名。","title":"install.md"},{"content":"npm exec 命令 # npm exec用来执行某个 npm 模块的内部命令，不管该模块在本地还是在远程。它有一个别名x，即npm exec等同于npm x。\n该命令与npx命令的作用类似，但是使用上有所不同。\nnpx会将所有参数原样传入模块内部。\n$ npx foo@latest bar --param=@npmcli/foo # 等同于 $ foo bar --param=@npmcli/foo npm exec则需要使用--分隔符，指定所要执行的命令和它的参数。\n$ npm exec -- foo@latest bar --param=@npmcli/foo # 等同于 $ foo bar --param=@npmcli/foo npm exec也可以用--package参数指定模块。\n$ npm exec --package=foo -- bar --bar-argument # 等同于 $ npx --package=foo bar --bar-argument --call或-c参数用来指定执行的整个命令。\n$ npm exec -c \u0026#39;eslint \u0026amp;\u0026amp; say \u0026#34;hooray, lint passed\u0026#34;\u0026#39; # 等同于 $ npx -c \u0026#39;eslint \u0026amp;\u0026amp; say \u0026#34;hooray, lint passed\u0026#34;\u0026#39; ","date":"23 October 2023","permalink":"/posts/language/nodejs/npm/npm-exec/","section":"博客","summary":"npm exec 命令 # npm exec用来执行某个 npm 模块的内部命令，不管该模块在本地还是在远程。它有一个别名x，即npm exec等同于npm x。","title":"npm-exec.md"},{"content":"npm init 命令 # npm init命令的作用，是生成package.json文件。它的别名是create。\n新建一个目录，作为模块的开发目录。进入该目录，执行npm init，屏幕上会依次出现一些问题，要求用户回答。用户回答以后，就会生成package.json文件。\n$ npm init 如果觉得回答问题太麻烦，想使用package.json的默认值，那就使用--yes或-y参数。\n$ npm init --yes # 或者 $ npm init -y 如果想设置package.json的一些默认值（作者、Email、许可证），需要提前用npm set命令设置。\n$ npm set init-author-email \u0026#34;example-user@example.com\u0026#34; $ npm set init-author-name \u0026#34;example_user\u0026#34; $ npm set init-license \u0026#34;MIT\u0026#34; npm init也可以格式化创建项目。\n$ npm init foo # 等同于 $ npm exec create-foo 上面的npm init foo这条命令，会去执行create-foo这个模块（package.json的bin属性）。\n举例来说，如果要执行create-react-app创建一个 React 项目，可以执行下面的命令。\n$ npm init react-app ./my-react-app 这种用法时，往往使用npm init命令的别名npm create。\n$ npm create xyz // 等同于 $ npm init xyz // 等同于 $ npx create-xyz ","date":"23 October 2023","permalink":"/posts/language/nodejs/npm/npm-init/","section":"博客","summary":"npm init 命令 # npm init命令的作用，是生成package.","title":"npm-init.md"},{"content":"npm link 命令 # 有时候，我们在本地修改了一些模块，想先测试这些修改是否有效。那么，怎么才能让依赖于该模块的应用，能够加载这些本地模块呢？\n一种方法是使用npm install的--save参数。\n$ npm install --no-save \u0026lt;模块的本地路径h\u0026gt; 上面的命令会从本地目录安装指定模块，但是不写入package.json。这样就可以让应用加载本地模块。\n另一种则是使用npm link命令，在node_modules目录里面建立一个符号链接，链接到本地模块。\n它分成两步，第一步先在本地模块的目录里，执行npm link。\n$ npm link 第二步是到你的应用目录，执行npm link \u0026lt;本地模块名\u0026gt;，在node_modules目录里面产生本地模块的符号链接。\n$ npm link \u0026lt;本地模块名\u0026gt; 也可以将上面两步合二为一，在应用目录里面，直接链接本地模块的路径。\n$ npm link \u0026lt;本地模块的路径\u0026gt; 等到测试完毕，再用npm unlink命令，先在应用目录删除符号链接。\n$ npm unlink --no-save \u0026lt;本地模块名\u0026gt; 再到本地模块的目录里面，执行npm unlink。\n$ npm unlink ","date":"23 October 2023","permalink":"/posts/language/nodejs/npm/npm-link/","section":"博客","summary":"npm link 命令 # 有时候，我们在本地修改了一些模块，想先测试这些修改是否有效。那么，怎么才能让依赖于该模块的应用，能够加载这些本地模块呢？","title":"npm-link.md"},{"content":"npm tag # npm 允许为版本添加标签，方便用户安装特定版本。只要指定标签，就可以安装该标签下的最新版本。\n发布模块时，如果不指定 tag，默认使用latest。安装时也是如此，不指定版本时，npm 默认安装latest标签对应的最新版本。\n如果指定 tag，发布时会将该 tag 指向最新发布的版本。用户如果想安装该版本，需要在安装时指定 tag。\n$ npm publish --tag beta 上面示例中，新发布版本的标签是beta。\n下面的命令为已发布的版本指定标签。注意，这一步之前不需要先移除已经存在的标签。\n# 语法 $ npm dist-tag add [PackageName]@[Version] [Tag] # 例子 $ npm dist-tag add foo-package@0.0.0 latest 安装时指定标签。\n$ npm install \u0026lt;package\u0026gt;@\u0026lt;tag\u0026gt; # 或者 $ npm install --tag \u0026lt;tag\u0026gt; 下面的命令查看所有 tag 和对应的版本。\n$ npm dist-tag ls # 或者 $ npm view [模块名] dist-tags 删除已经发布的标签。\n$ npm dist-tag rm [PackageName] [Tag] ","date":"23 October 2023","permalink":"/posts/language/nodejs/npm/npm-tag/","section":"博客","summary":"npm tag # npm 允许为版本添加标签，方便用户安装特定版本。只要指定标签，就可以安装该标签下的最新版本。","title":"npm-tag.md"},{"content":"npm token # npm token命令用来管理认证令牌。\nnpm token list命令列出所有激活的认证令牌。\n$ npm token list 上面命令的返回结果，以表格形式显示。如果加上--json参数，则返回 JSON 格式；加上--parseable参数，返回 Tab 键分隔的数据。\nnpm token create命令生成一个新的认证令牌。\n$ npm token create --read-only参数用来生成只读令牌，默认是读写令牌。--cidr=\u0026lt;cidr-ranges\u0026gt;参数用来指定令牌生效的 IP 地址范围。\nnpm token revoke命令收回令牌。\n$ npm token revoke \u0026lt;token|id\u0026gt; ","date":"23 October 2023","permalink":"/posts/language/nodejs/npm/npm-token/","section":"博客","summary":"npm token # npm token命令用来管理认证令牌。","title":"npm-token.md"},{"content":"定时器 # process.nextTick()会立即执行回调函数。 微任务队列。\nsetImmediate()， clearImmediate() # setImmediate(). It can be passed to clearImmediate()\nsetImmediate()会在下一轮执行回调函数。\n定时器在 IO 操作的回调函数之前执行。\nTimer: setTimeout和setInterval回调函数。 I/O callbacks: 处理除了setTimeout、setInterval、setImmediate的回调函数。 Check: 处理setImmediate()指定的回调函数。 nextTickQueue: 处理process.nextTick()的回调函数，不是 event loop 的一部分。 ","date":"23 October 2023","permalink":"/posts/language/nodejs/timer/","section":"博客","summary":"定时器 # process.","title":"timer"},{"content":"fs 模块 # fs.createReadStream() # fs.createReadStream方法读取一个文件，以 stream 的形式返回。\nconst readStream = fs.createReadStream( inputFilePath, { encoding: \u0026#39;utf8\u0026#39;, highWaterMark: 1024 } ); 该方法的第一个参数是文件的路径，第二个参数是一个配置对象。\n配置对象的encoding属性，决定了fs.createReadStream方法的返回值。如果该属性为null，返回的是二进制的 buffer；如果为字符串（比如utf8），返回的是这种编码的字符串。\n配置对象的highWaterMark属性指定了每次返回的 buffer 或字符串的最大体积（单位字节）。\nstream 以事件的形式获取。\nreadStream.on(\u0026#39;data\u0026#39;, (chunk) =\u0026gt; { console.log(\u0026#39;\u0026gt;\u0026gt;\u0026gt; \u0026#39;+chunk); }); readStream.on(\u0026#39;end\u0026#39;, () =\u0026gt; { console.log(\u0026#39;### DONE ###\u0026#39;); }); Node v10 开始，Stream 有异步遍历器（asynchronous iteration）接口（即具有Symbol.asyncIterator属性），因此可以使用for-await-of读取。\nasync function main(inputFilePath) { const readStream = fs.createReadStream(inputFilePath, { encoding: \u0026#39;utf8\u0026#39;, highWaterMark: 1024 }); for await (const chunk of readStream) { console.log(\u0026#39;\u0026gt;\u0026gt;\u0026gt; \u0026#39;+chunk); } console.log(\u0026#39;### DONE ###\u0026#39;); } 参考链接 # Using async iteration natively in Node.js, by Axel Rauschmayer ","date":"23 October 2023","permalink":"/posts/language/nodejs/fs/","section":"博客","summary":"fs 模块 # fs.","title":"fs"},{"content":"Promise # UnhandledPromiseRejectionWarning # 如果 Promise 运行过程中抛出错误，或者状态变为rejected，但是没有相应的处理代码，那么 Node 会抛出一个警告UnhandledPromiseRejectionWarning。\nnew Promise(function (resolve, reject) { reject(\u0026#39;message\u0026#39;); console.log(\u0026#39;hello\u0026#39;); }) // hello // UnhandledPromiseRejectionWarning: message 上面代码中，Promise 变为rejected状态，但是没有处理的代码，导致抛出警告UnhandledPromiseRejectionWarning。由于抛出的是警告，而不是错误，所以不影响hello的输出。请跟下面的代码比较一下。\nnew Promise(function (resolve, reject) { throw new Error(\u0026#39;message\u0026#39;); console.log(\u0026#39;hello\u0026#39;); }) // UnhandledPromiseRejectionWarning: Error: message 上面代码中，Promise 内部抛出错误，没有得到处理，所以会有警告UnhandledPromiseRejectionWarning。Promise 内部的错误阻止 Promise 内部的代码继续向下运行，所以不会输出hello。\n消除这个警告的方法很简单，就是为 Promise 加上错误处理代码，即catch代码块。\nnew Promise(function (resolve, reject) { reject(\u0026#39;message\u0026#39;); console.log(\u0026#39;hello\u0026#39;); }).catch(function (err) { console.log (err) }) // hello // message 上面代码中，由于catch代码块的存在，所以不会抛出警告。\nasync 函数里面使用 Promise，也要放在try/catch代码块之中。\nconst p = new Promise(function (resolve, reject) { reject(\u0026#39;message\u0026#39;); }); (async function () { try { await p; } catch (err) { console.log(err); } })(); // message 上面代码中，如果 Promise 没有放在try/catch代码块，就会抛出警告UnhandledPromiseRejectionWarning。\nNode 还提供process对象的unhandledRejection事件，在抛出警告UnhandledPromiseRejectionWarning之前，就会触发这个事件。如果设置了这个事件的处理代码，就不会抛出警告UnhandledPromiseRejectionWarning。\nnew Promise(function (resolve, reject) { reject(\u0026#39;message\u0026#39;); }); process.on(\u0026#39;unhandledRejection\u0026#39;, function (err) { console.log(err); }); // message Node 开发团队打算废除UnhandledPromiseRejectionWarning，以后如果没有相应的处理代码，Promise 内部抛错或者变为rejected状态，会导致 Node 进程终止执行，并且有一个非零的返回码。所以应该养成习惯，只要有 Promise，就要部署失败情况下的代码。\n","date":"23 October 2023","permalink":"/posts/language/nodejs/promise/","section":"博客","summary":"Promise # UnhandledPromiseRejectionWarning # 如果 Promise 运行过程中抛出错误，或者状态变为rejected，但是没有相应的处理代码，那么 Node 会抛出一个警告UnhandledPromiseRejectionWarning。","title":"promise"},{"content":"Node 的 REPL 环境 # 简介 # REPL 是 read-eval-print-loop 的缩写，表示命令行下的 Node 引擎的一个互动式对话环境。用户在其中输入命令，就可以立刻看到结果。read 表示读取用户的输入，eval 表示执行，print 表示输出运行的结果，loop 表示重复执行这个过程。\n命令行下输入node，就可以进入 Node 的 REPL 环境。\n$ node \u0026gt; REPL 环境的提示符是一个大于号（\u0026gt;）。\n退出 REPL，可以在行首按下 Ctrl + d，或者连续两次按下 Ctrl + c。\nREPL 环境与 Node 脚本的执行环境基本相似，只有一些很小的差异。比如，REPL 环境不是通过脚本触发的，所以没有__dirname和__filename这两个内置变量。\nREPL 会自动加载 Node 的核心模块，比如 fs、http、os、path等，不必require就可以直接使用。\n$ node \u0026gt; fs.read [Function] 上面代码中，REPL 环境可以直接使用fs.read方法，不必先加载fs模块。\nnode命令的-e参数，实际上就是在 REPL 环境运行代码。\n$ node -e \u0026#34;console.log(os.platform())\u0026#34; darwin _变量 # REPL 环境下，有一个内置变量_，上一个表达式的值就存放在这个变量之中。\n\u0026gt; require(\u0026#39;./example\u0026#39;); { some: \u0026#39;some text\u0026#39; } \u0026gt; _.some \u0026#39;some text\u0026#39; 上面代码中，require()方法加载了一个脚本，这行表达式的值就自动存放在_里面。\nREPL 允许用户对_变量赋值。\n\u0026gt; _ = \u0026#39;something\u0026#39; \u0026#39;something\u0026#39; \u0026gt; _ \u0026#39;something\u0026#39; 上面代码中，我们将something赋值给_，这时_就是一个普通变量了。\n.editor 命令 # REPL 环境下，按下回车键，就会提交并执行当前的输入。这对输入多行的代码非常不方便，有两个办法可以输入多行代码。一个是按 Shift + 回车键，另一个是使用.editor命令，键入编辑模式。\n\u0026gt; .editor function sum(a, b) { return a + b; } sum(5, 4); 输入.editor命令后，就可以输入多行文本。输入完成后，按下 Ctrl + d 就会执行这些代码；按下 Ctrl + c 就会取消本次输入，回到输入.editor之前的状态。\n特殊命令 # 除了.editor命令以外，REPL 还提供其他一些命令。\n.break：按下 Shift + 回车进入多行文本输入的过程中，输入.break命令会取消本次输入，相当于按下 Ctrl + c。 .clear：重置 REPL 上下文为空，并清除当前输入的多行文本。 .exit：关闭当前的 I/O 读写，退出 REPL 环境。 .help：显示 REPL 环境的特殊命令列表。 .save：将当前的 REPL 对换保存成一个文件，比如.save ./file/to/save.js。 .load：加载一个文件进入当前的 REPL 对话，比如.load ./file/to/load.js。 repl 模块 # Node 提供repl模块，可以在脚本中唤起 REPL 环境。\n// repl.js const repl = require(\u0026#39;repl\u0026#39;); // Our own prompt repl.start(\u0026#39;Code:: \u0026#39;); 上面代码中，repl.start()方法用于唤起 REPL 环境，该方法的参数是自定义的 REPL 环境提示符。执行上面的脚本，就会出现这个提示符。\n$ node repl.js Code:: 这个模块允许开发者向 REPL 环境注入变量，因此可以做很多事情。\n// repl.js const repl = require(\u0026#39;repl\u0026#39;); const r = repl.start(\u0026#39;Code:: \u0026#39;); r.context.sum = (...args) =\u0026gt; args.reduce((a, b) =\u0026gt; a + b, 0); 上面代码向 REPL 环境输入了一个sum函数。\n$ node repl.js Code:: sum(1, 2, 3) 6 Code:: .exit 参考链接 # Node.js REPL in Depth, by Seva Zaikov ","date":"23 October 2023","permalink":"/posts/language/nodejs/repl/","section":"博客","summary":"Node 的 REPL 环境 # 简介 # REPL 是 read-eval-print-loop 的缩写，表示命令行下的 Node 引擎的一个互动式对话环境。用户在其中输入命令，就可以立刻看到结果。read 表示读取用户的输入，eval 表示执行，print 表示输出运行的结果，loop 表示重复执行这个过程。","title":"repl"},{"content":"","date":"23 October 2023","permalink":"/posts/language/nodejs/module/","section":"博客","summary":"","title":"nodejs/module"},{"content":"this 变量 # Node 应用的顶层变量是global，对应浏览器的window变量。\n顶层的 this # 在 REPL 环境，顶层的this就指向global。\n\u0026gt; global === this true 顶层变量是global和this的属性。\n\u0026gt; var foo = \u0026#34;bar\u0026#34;; \u0026gt; this.foo bar \u0026gt; global.foo bar 上面代码中，foo是一个顶层变量，自动生成了this.foo和global.foo两个属性。\n在模块环境，顶层的this指向当前模块，即module.exports，默认是一个空对象，与global不是同一个对象。\n// 模块环境 console.log(this) // {} console.log(this === global) // false 模块内部的顶层变量，不会自动成为global和this的属性。\n// 模块环境 var foo = \u0026#34;bar\u0026#34;; console.log(this.foo); // undefined console.log(global.foo); // undefined 上面代码中，顶层变量foo并不会生成this.foo和global.foo两个属性。这是因为foo是模块内部的变量，不是全局有效，因此不是global的属性，而this是当前的模块对象，this.foo代表模块实例的属性，这跟变量foo是两回事情。\n另外，如果声明变量的时候，不使用var命令，而是直接赋值，那么该变量在 REPL 环境下将成为global和this的属性，在模块环境将只成为 global 的属性。\n// REPL 环境 \u0026gt; foo = \u0026#34;bar\u0026#34;; \u0026gt; global.foo bar \u0026gt; this.foo bar // 模块环境 foo = \u0026#34;bar\u0026#34;; console.log(this.foo); // undefined console.log(global.foo); // bar 函数内部的 this # 直接执行一个函数（不使用new命令），函数内部的this指向global，REPL 环境和模块环境都是如此。\nfoo = \u0026#34;bar\u0026#34;; function testThis () { this.foo = \u0026#34;foo\u0026#34;; } console.log(global.foo); // bar testThis(); console.log(global.foo); // foo 如果是严格模式，函数内部的this返回undefined。\nfoo = \u0026#34;bar\u0026#34;; function testThis() { \u0026#34;use strict\u0026#34;; this.foo = \u0026#34;foo\u0026#34;; } console.log(this.foo); // \u0026#34;bar\u0026#34; testThis(); // TypeError: Cannot set property \u0026#39;foo\u0026#39; of undefined 如果使用new命令调用某个函数，该函数就变成了构造函数，函数内部的this指向新建的实例对象。\n","date":"23 October 2023","permalink":"/posts/language/nodejs/module/this/","section":"博客","summary":"this 变量 # Node 应用的顶层变量是global，对应浏览器的window变量。","title":"this.md"},{"content":"","date":"20 October 2023","permalink":"/tags/serverless/","section":"Tags","summary":"","title":"Serverless"},{"content":"","date":"20 October 2023","permalink":"/posts/architecture/serverless/","section":"博客","summary":"Serverless 是一种计算模型，它使得开发者能够在无需管理服务器和基础架构的情况下运行代码（或称函数）。使用无服务器计算，开发者可以将代码上传到云平台，平台会在需要时根据流量自动进行资源分配和处理。","title":"Serverless"},{"content":"","date":"20 October 2023","permalink":"/tags/tencentyun/","section":"Tags","summary":"","title":"Tencentyun"},{"content":"Resources # 使用控制台创建一个事件函数 使用 Serverless Cloud Framework 创建函数 API实践 # 想要了解各API的用法，可先在腾讯云官方文档寻找： 云函数 API 概览-API 中心-腾讯云 (tencent.com)\n腾讯云官方推荐 API Explorer调试API和生成代码，它提供了在线调用、签名验证、SDK 代码生成和快速检索接口等能力：\n总结使用API步骤：\n在 文档中找到想要的API，了解API的功能和用法； 在 API Explorer中下载代码或调试； 安装代码所需要的对应依赖，并将参数（如秘钥等）替换成自己腾讯云账号的，密钥可前往 官网控制台进行获取； 为了项目的可维护性，参数可设置成环境变量或配置文件，并根据实际项目需要修改代码。 重要API整理 # 公共参数 # 云函数 公共参数-调用方式-API 中心-腾讯云 (tencent.com)\n使用Postman或者Curl直接发送请求时需要带上这些公共参数，下面是使用Curl发送请求的一个示例（由API Explorer生成）：\ncurl -X POST https://scf.tencentcloudapi.com -H \u0026#34;Authorization: TC3-HMAC-SHA256 Credential=AKIDntdFc3Qo12WwlvVaRtFeoF2s75BvoPiwIL8UvOXenQ-Ll8Le5SfIpM4jjLXyUGXX/2023-05-30/scf/tc3_request, SignedHeaders=content-type;host, Signature=08e82782ba555a81a8864c0ad57e0defbd27d0034eb5c8b1e5ccf60202fc1c2b\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; -H \u0026#34;Host: scf.tencentcloudapi.com\u0026#34; -H \u0026#34;X-TC-Action: ListFunctions\u0026#34; -H \u0026#34;X-TC-Timestamp: 1685434170\u0026#34; -H \u0026#34;X-TC-Version: 2018-04-16\u0026#34; -H \u0026#34;X-TC-Language: zh-CN\u0026#34; -H \u0026#34;X-TC-Token: 8HVPWbJ4b60AFSi6IGd41mG7pXaNSxOa607adae527ed98cca05322893420aec9TaLLoQIydU0_J99sP40grvLXfat-hSb2ysPTMqHotagIIy8UJzTWestfbmhKd0nxBQuJhT6cM3ew1ii_Ug47velPmhuzQMIN9CvG0jw084q450d1mqEFSjdm2k64wpNgzTIFEn-633R2hPqlO_09jJY_AxdDsmpX80Z7PurEqGOHACiRFGCqVavLck2SVMVLkM7WBg_13IOkp12TSi3cOA\u0026#34; -d \u0026#39;{}\u0026#39; 而使用各种语言的SDK调用这些API则不需要管这些公共参数，仅需配置秘钥即可。\n下面是一些重要的API，腾讯云官方的API文档已经很详细易用，了解更具体的用法可查看它。\n函数相关API # 获取函数列表 # https://cloud.tencent.com/document/api/583/18582\n创建函数 # 函数需要打包成.zip文件并采用BASE64编码转换成字符串在code的zipFile参数中，或先上传到腾讯云CosBucket对象存储桶中。\nhttps://cloud.tencent.com/document/api/583/18586\n删除函数 # 需要函数名。\nhttps://cloud.tencent.com/document/api/583/18585\n运行函数 # 需要函数名。\nhttps://cloud.tencent.com/document/api/583/17243\n获取函数详细信息 # 需要函数名。\nhttps://cloud.tencent.com/document/api/583/18584\n触发器相关API # 设置函数触发方式 # 即创建触发器，需要触发器的名称、类型（ cos 、cmq、 timer 定时触发器、 ckafka、apigw API网关）、名字、[描述]( 云函数 触发器配置描述-触发器-文档中心-腾讯云 (tencent.com))。触发器描述中指定触发器的配置。\nhttps://cloud.tencent.com/document/api/583/18589\n删除触发器 # 需要函数名、触发器名称、类型。\nhttps://cloud.tencent.com/document/api/583/18588\n更新触发器状态 # 开关触发器，需要函数名、触发器类型、触发器名、开或关。apigw类触发器无法开关。\nhttps://cloud.tencent.com/document/api/583/89800\n获取函数触发器列表 # 需要函数名。\nhttps://cloud.tencent.com/document/api/583/44268\n","date":"20 October 2023","permalink":"/posts/architecture/serverless/tencentyun-serverless-api/","section":"博客","summary":"腾讯云官方推荐并提供了使用控制台和使用Serverless 组件（Tencent Serverless Cloud Framework）两种方式用于管理云函数，分别在浏览器中和本地CLI中进行开发","title":"Tencentyun Serverless Api"},{"content":"","date":"20 October 2023","permalink":"/tags/alicloud/","section":"Tags","summary":"","title":"Alicloud"},{"content":"Resources # https://www.npmjs.com/package/@alicloud/fc-open20210406/v/2.0.9 CreateService # 重要参数 # 参数名 描述 类型 是否必填 serviceName 服务的名称 string 是 description 服务的描述信息 string 否 internetAccess 是否允许公网访问 boolean 否 role 服务所使用的角色 string 是 vpcConfig 服务的VPC配置 dict 否 logConfig 服务的日志配置 dict 否 nasConfig 服务的NAS配置 dict 否 vpcConfig 字典包含以下键值：\n键名 描述 类型 是否必填 VpcId VPC的ID string 是 VSwitchIds VSwitch的ID列表 list 是 logConfig 字典包含以下键值：\n键名 描述 类型 是否必填 Project 日志服务的项目名称 string 是 Logstore 日志服务的日志库名称 string 是 nasConfig 字典包含以下键值：\n键名 描述 类型 是否必填 MountPoints NAS挂载点列表 list 是 UserId 用于访问NAS的用户ID string 是 GroupId 用于访问NAS的用户组ID string 是 GetService # 重要参数 # 参数名 描述 类型 是否必填 serviceName 服务的名称 string 是 CreateFunction # 重要参数 # 参数名 描述 类型 是否必填 serviceName 函数所属的服务名称 string 是 functionName 函数的名称 string 是 description 函数的描述信息 string 否 runtime 函数的运行环境 string 是 handler 函数的处理程序 string 是 initializer 函数的初始化程序 string 否 customContainerConfig 函数的自定义容器配置 dict 否 caPort 函数容器的端口号（仅适用于自定义容器） int 否 environmentVariables 函数的环境变量 dict 否 vpcConfig 函数的VPC配置 dict 否 logConfig 函数的日志配置 dict 否 nasConfig 函数的NAS配置 dict 否 mountPoints 函数的NAS挂载点列表 list[dict] 否 code 函数的代码 dict 是 layers 函数的Layer列表 list[string] 否 caEnable 是否开启自定义容器功能（仅适用于自定义容器） boolean 否 caFiles 自定义容器的CA证书文件列表（仅适用于自定义容器） list[dict] 否 runtime参数 # runtime参数用于指定函数的运行环境。它决定了函数在何种语言和框架下执行。以下是runtime参数的常见取值及其对应的运行环境：\nruntime取值 运行环境 python2.7 Python 2.7 python3.6 Python 3.6 python3.7 Python 3.7 python3.8 Python 3.8 python3.9 Python 3.9 nodejs6 Node.js 6.x nodejs8 Node.js 8.x nodejs10 Node.js 10.x nodejs12 Node.js 12.x nodejs14 Node.js 14.x nodejs16 Node.js 16.x java8 Java 8 java11 Java 11 php7.2 PHP 7.2 dotnetcore2.1 .NET Core 2.1 dotnetcore3.1 .NET Core 3.1 custom 自定义运行时环境 对于大多数常用的编程语言和框架，可以直接选择相应的runtime取值。\n对于自定义运行时环境，可以选择\u0026quot;custom\u0026quot;作为runtime的取值，并提供自定义的运行时环境。自定义运行时环境需要提供相应的配置和执行文件，以便在函数中正确运行。\ncode参数 # code参数用于指定函数的代码内容或代码存储位置。根据函数的具体需求和使用情况，code参数可以采用不同的设置方式。以下是对code参数的详细介绍：\n代码内容：可以直接将函数的代码内容作为字符串传递给code参数。这适用于函数代码比较简单且较短小的情况。将代码内容作为字符串传递给code参数时，通常需要指定runtime参数来指定函数的运行环境。\n代码存储位置：可以将函数的代码存储在指定的位置，例如对象存储（如OSS）或代码仓库（如GitHub），然后通过code参数提供相关的配置信息来引用这些存储位置的代码。具体的设置方式会根据不同的云服务提供商和平台而有所不同，通常需要提供存储位置的信息（如存储桶名称、对象键或代码仓库地址等）。\n对于直接传递代码内容的方式，阿里云提供了使用Base64编码的zip包方式。您可以将函数代码打包成zip包，并使用Base64编码将其转换为字符串，然后将该字符串作为zipFile字段的值传递给code参数。\n对于代码存储位置的情况，您需要提供相应的配置信息，例如存储位置的访问密钥、存储桶名称、对象键等。具体的设置方式会因云服务提供商和平台而有所不同。例如，在使用对象存储（如OSS）存储位置的情况下，可以提供如下配置：\ncode = { \u0026#34;ossBucketName\u0026#34;: \u0026#34;my-bucket\u0026#34;, \u0026#34;ossObjectName\u0026#34;: \u0026#34;my-function.zip\u0026#34;, \u0026#34;ossAccessKeyId\u0026#34;: \u0026#34;my-access-key-id\u0026#34;, \u0026#34;ossAccessKeySecret\u0026#34;: \u0026#34;my-access-key-secret\u0026#34; } CreateFunction(serviceName=\u0026#34;my-service\u0026#34;, functionName=\u0026#34;my-function\u0026#34;, code=code, ...) customContainerConfig参数 # 参数 描述 image 指定要使用的容器镜像的名称。 command 指定容器启动时要执行的命令。 args 指定传递给容器启动命令的额外参数。 accelerationType 是否开启镜像加速。 webServerMode 镜像运行是否为Web Server模式。取值为true表示需要在容器镜像中实现Web Server来监听端口并处理请求。取值为false表示需要容器运行后主动退出进程，并且ExitCode需要为0。 GetFunction # 重要参数 # 以下是GetFunction接口的参数列表，包括参数名、参数类型、是否必填和参数描述：\n参数名 参数类型 是否必填 参数描述 serviceName 字符串 是 函数所属的服务名称。 functionName 字符串 是 要获取的函数名称。 qualifier 字符串 否 函数版本或别名。如果未提供，则返回函数最新版本的信息。 CreateTrigger # 重要参数 # CreateTrigger函数是用于创建触发器的阿里云函数计算接口。通过该接口，您可以为函数关联不同类型的触发器，例如定时触发器、API网关触发器、对象存储（OSS）触发器等。\n以下是CreateTrigger函数的使用方式：\n首先，确保已经创建了需要触发的函数（使用CreateFunction函数创建）。\n构造CreateTrigger函数的请求参数，包括以下必填字段：\nserviceName：函数所属的服务名称。 functionName：要创建触发器的函数名称。 triggerName：触发器的名称，用于在函数中唯一标识触发器。 triggerType：触发器的类型，例如Timer（定时触发器）、OSS（对象存储触发器）、HTTP（HTTP触发器）等。 triggerConfig：触发器的配置信息，根据不同类型的触发器有所不同。 根据需要，可以设置CreateTrigger函数的其他可选参数，例如qualifier（函数版本或别名）等。\n调用CreateTrigger函数，传递请求参数，并获取返回结果。返回结果将包含触发器的相关信息，如触发器ID、触发器状态等。\n阿里云触发器类型 # CreateTrigger函数支持多种触发器类型，您可以根据具体的需求选择适合的触发器类型。以下是一些常见的触发器类型及其简要介绍：\n定时触发器（Timer）：根据预设的时间表达式，周期性地触发函数的执行，例如每小时、每天、每周等。可以设置函数在特定时间点或时间间隔内触发。\n对象存储触发器（OSS）：当阿里云对象存储（OSS）上的文件发生变化时触发函数执行，例如文件上传、删除等事件。\nAPI网关触发器（HTTP）：将函数作为后端服务与阿里云API网关结合，实现基于HTTP请求的触发，例如接收API请求并进行处理。\n日志服务触发器（Log）：当日志服务中有新的日志产生时触发函数执行，例如实时处理日志、触发告警等。\nTable Store触发器（TableStore）：当阿里云Table Store中的数据发生变化时触发函数执行，例如数据更新、插入等操作。\n参考网址： 阿里云触发器简介\nTimer触发器 # 以下是一个定时触发器配置信息的例子，包括时间表达式和其他可选参数：\n{ \u0026#34;serviceName\u0026#34;: \u0026#34;my-service\u0026#34;, \u0026#34;functionName\u0026#34;: \u0026#34;my-function\u0026#34;, \u0026#34;triggerName\u0026#34;: \u0026#34;my-timer-trigger\u0026#34;, \u0026#34;triggerType\u0026#34;: \u0026#34;Timer\u0026#34;, \u0026#34;triggerConfig\u0026#34;: { \u0026#34;cronExpression\u0026#34;: \u0026#34;0 0 9 * * ?\u0026#34;, // 每天上午9点触发 \u0026#34;enable\u0026#34;: true, \u0026#34;payload\u0026#34;: \u0026#34;{\\\u0026#34;key\\\u0026#34;: \\\u0026#34;value\\\u0026#34;}\u0026#34; }, \u0026#34;qualifier\u0026#34;: \u0026#34;LATEST\u0026#34; } 在上述例子中，配置信息的各个字段的含义如下：\nserviceName：函数所属的服务名称。 functionName：要关联定时触发器的函数名称。 triggerName：定时触发器的名称。 triggerType：指定为\u0026quot;Timer\u0026quot;，表示创建定时触发器。 triggerConfig：包含触发器的具体配置信息。 cronExpression：时间表达式，指定触发器的触发时间。在此例中，表示每天的上午9点触发。 enable：是否启用触发器。 payload：可选的触发器参数，作为函数调用时传递的输入参数。在此例中，使用JSON字符串作为示例。 qualifier：可选参数，指定函数版本或别名。在此例中，设置为\u0026quot;LATEST\u0026quot;，表示使用最新版本的函数。 HTTP触发器 # 以下是一个HTTP触发器的配置信息示例：\n{ \u0026#34;serviceName\u0026#34;: \u0026#34;my-service\u0026#34;, \u0026#34;functionName\u0026#34;: \u0026#34;my-function\u0026#34;, \u0026#34;triggerName\u0026#34;: \u0026#34;my-http-trigger\u0026#34;, \u0026#34;triggerType\u0026#34;: \u0026#34;HTTP\u0026#34;, \u0026#34;triggerConfig\u0026#34;: { \u0026#34;authType\u0026#34;: \u0026#34;ANONYMOUS\u0026#34;, \u0026#34;methods\u0026#34;: [\u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;], \u0026#34;url\u0026#34;: \u0026#34;/my-endpoint\u0026#34; }, \u0026#34;qualifier\u0026#34;: \u0026#34;LATEST\u0026#34; } 在上述例子中，配置信息的各个字段的含义如下：\nserviceName：函数所属的服务名称。 functionName：要关联HTTP触发器的函数名称。 triggerName：HTTP触发器的名称。 triggerType：指定为\u0026quot;HTTP\u0026quot;，表示创建HTTP触发器。 triggerConfig：包含触发器的具体配置信息。 authType：HTTP触发器的认证类型，可以选择\u0026quot;ANONYMOUS\u0026quot;（匿名访问）或\u0026quot;CUSTOM\u0026quot;（自定义认证）。 methods：允许的HTTP请求方法列表。在此例中，允许GET和POST请求。 url：定义触发器的URL路径。在此例中，设置为\u0026quot;/my-endpoint\u0026quot;。 qualifier：可选参数，指定函数版本或别名。在此例中，设置为\u0026quot;LATEST\u0026quot;，表示使用最新版本的函数。 GetTrigger # 重要参数 # 以下是GetTrigger接口的参数列表，包括参数名、参数类型、是否必填和参数描述：\n参数名 参数类型 是否必填 参数描述 serviceName 字符串 是 触发器所属的服务名称。 functionName 字符串 是 触发器所属的函数名称。 triggerName 字符串 是 要获取的触发器名称。 InvokeFunction # 重要参数 # 以下是InvokeFunction接口的参数列表，包括参数名、参数类型、是否必填和参数描述：\n参数名 参数类型 是否必填 参数描述 serviceName 字符串 是 函数所属的服务名称。 functionName 字符串 是 要调用的函数名称。 payload 字符串 否 调用函数时传递的输入参数。可以是字符串形式的JSON数据。 qualifier 字符串 否 函数版本或别名。如果未提供，则使用函数的最新版本。 X-Fc-Invocation-Type 字符串 否 函数调用的方式。可选值包括 \u0026ldquo;Sync\u0026rdquo;（同步）和 \u0026ldquo;Async\u0026rdquo;（异步）。默认为 \u0026ldquo;Sync\u0026rdquo;。 X-Fc-Log-Type 字符串 否 函数调用的日志类型。可选值包括 \u0026ldquo;None\u0026rdquo;（无日志）和 \u0026ldquo;Tail\u0026rdquo;（返回完整日志）。默认为 \u0026ldquo;None\u0026rdquo;。 ","date":"20 October 2023","permalink":"/posts/architecture/serverless/alicloud-fc-api/","section":"博客","summary":"函数计算（Function Compute）是一个事件驱动的全托管 Serverless 计算服务，您无需管理服务器等基础设施，只需编写代码并上传，函数计算会为您准备好计算资源，并以弹性、可靠的方式运行您的代码。","title":"Alicloud Function Compute API"},{"content":"Git 简介 # Git 是什么 # Git 是一个开源的分布式版本控制系统。\nGit 和其它版本控制系统（包括 Subversion 和近似工具）的主要差别在于 Git 对待数据的方式。 从概念上来说，其它大部分系统以文件变更列表的方式存储信息，而 Git 是把数据看作是对小型文件系统的一系列快照。\n什么是版本控制 # 版本控制是一种记录一个或若干文件内容变化，以便将来查阅特定版本修订情况的系统。\n集中化的版本控制系统 # 介绍分布式版本控制系统前，有必要先了解一下传统的集中式版本控制系统。\n集中化的版本控制系统，诸如 CVS，Subversion 等，都有一个单一的集中管理的服务器，保存所有文件的修订版本，而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新。\n这么做最显而易见的缺点是中央服务器的单点故障。如果宕机一小时，那么在这一小时内，谁都无法提交更新，也就无法协同工作。要是中央服务器的磁盘发生故障，碰巧没做备份，或者备份不够及时，就会有丢失数据的风险。最坏的情况是彻底丢失整个项目的所有历史更改记录。\n分布式版本控制系统 # 分布式版本控制系统的客户端并不只提取最新版本的文件快照，而是把代码仓库完整地镜像下来。这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜像出来的本地仓库恢复。因为每一次的提取操作，实际上都是一次对代码仓库的完整备份。\n为什么使用 Git # Git 是分布式的。这是 Git 和其它非分布式的版本控制系统（例如 svn，cvs 等），最核心的区别。分布式带来以下好处：\n工作时不需要联网 - 首先，分布式版本控制系统根本没有“中央服务器”，每个人的电脑上都是一个完整的版本库，这样，你工作的时候，就不需要联网了，因为版本库就在你自己的电脑上。既然每个人电脑上都有一个完整的版本库，那多个人如何协作呢？比方说你在自己电脑上改了文件 A，你的同事也在他的电脑上改了文件 A，这时，你们俩之间只需把各自的修改推送给对方，就可以互相看到对方的修改了。 更加安全 集中式版本控制系统，一旦中央服务器出了问题，所有人都无法工作。 分布式版本控制系统，每个人电脑中都有完整的版本库，所以某人的机器挂了，并不影响其它人。 Git 的工作原理 # 个人认为，对于 Git 这个版本工具，再不了解原理的情况下，直接去学习命令行，可能会一头雾水。所以，本文特意将原理放在命令使用章节之前讲解。\n版本库 # 当你一个项目到本地或创建一个 git 项目，项目目录下会有一个隐藏的 .git 子目录。这个目录是 git 用来跟踪管理版本库的，如果不熟悉其工作机制，千万不要手动修改。\nhooks 目录：包含客户端或服务端的钩子脚本（hook scripts） info 目录：包含一个全局性排除（global exclude）文件， 用以放置那些不希望被记录在 .gitignore 文件中的忽略模式（ignored patterns）。 objects 目录：存储所有数据内容。 refs 目录：存储指向数据（分支、远程仓库和标签等）的提交对象的指针 HEAD 文件：指向目前被检出的分支。 index 文件保存暂存区信息。 config 文件：包含项目特有的配置选项。 description 文件：description 文件仅供 GitWeb 程序使用，我们无需关心。 哈希值 # Git 中所有数据在存储前都计算校验和，然后以校验和来引用。 这意味着不可能在 Git 不知情时更改任何文件内容或目录内容。 这个功能构筑在 Git 底层，是 Git 的关键组件。 若你在传送过程中丢失信息或损坏文件，Git 就能发现。\nGit 计算校验和的使用 SHA-1 哈希算法。 这是一个由 40 个十六进制字符（0-9 和 a-f）组成字符串，基于 Git 中文件的内容或目录结构计算出来。 SHA-1 哈希值看起来是这样：\n24b9da6552252987aa493b52f8696cd6d3b00373 Git 中使用这种哈希值的情况很多，你将经常看到这种哈希值。 实际上，Git 数据库中保存的信息都是以文件内容的哈希值来索引，而不是文件名。\n文件状态 # 在 GIt 中，你的文件可能会处于三种状态之一：\n已修改（modified） - 已修改表示修改了文件，但还没保存到数据库中。 已暂存（staged） - 已暂存表示对一个已修改文件的当前版本做了标记，使之包含在下次提交的快照中。 已提交（committed） - 已提交表示数据已经安全的保存在本地数据库中。 工作区域 # 与文件状态对应的，不同状态的文件在 Git 中处于不同的工作区域。\n工作区（working） - 当你 git clone 一个项目到本地，相当于在本地克隆了项目的一个副本。工作区是对项目的某个版本独立提取出来的内容。 这些从 Git 仓库的压缩数据库中提取出来的文件，放在磁盘上供你使用或修改。 暂存区（staging） - 暂存区是一个文件，保存了下次将提交的文件列表信息，一般在 Git 仓库目录中。 有时候也被称作`‘索引’\u0026rsquo;，不过一般说法还是叫暂存区。 本地仓库（local） - 提交更新，找到暂存区域的文件，将快照永久性存储到 Git 本地仓库。 远程仓库（remote） - 以上几个工作区都是在本地。为了让别人可以看到你的修改，你需要将你的更新推送到远程仓库。同理，如果你想同步别人的修改，你需要从远程仓库拉取更新。 分支管理 # Git Flow # Git Flow 应该是目前流传最广的 Git 分支管理策略。Git Flow 围绕的核心点是版本发布（release），它适用于迭代版本较长的项目。\n详细内容，可以参考这篇文章： Git 在团队中的最佳实践\u0026ndash;如何正确使用 Git Flow\nGit Flow 常用分支：\nmaster - 主线分支 develop - 开发分支 feature - 特性分支 release - 发布分支 hotfix - 问题修复分支 Git Flow 工作流程\n2.1. 主干分支 # 主干分支有两个，它们是伴随着项目生命周期长期存在的分支。\nmaster - 这个分支对应发布到生产环境的代码。这个分支只允许从其他分支合入代码，不能在这个分支直接修改。所有在 master 分支上的 Commit 都应该打 Tag。 develop - 这个分支包含所有要发布到下一个 release 的代码，这个分支主要是从其他分支合入代码，比如 feature 分支。 2.2. feature 分支 # 这个分支主要是用来开发一个新的功能，一旦开发完成，我们合并回 develop 分支进入下一个 release。feature 分支开发结束后，必须合并回 develop 分支, 合并完分支后一般会删点这个 feature 分支，但是我们也可以保留。\n2.3. release 分支 # release 分支基于 develop 分支创建，创建后，我们可以在这个 release 分支上进行测试，修复 Bug 等工作。同时，其它开发人员可以基于它开发新的 feature (记住：一旦创建了 release 分支之后不要从 develop 分支上合并新的改动到 release 分支)。\n发布 release 分支时，合并 release 到 master 和 develop， 同时在 master 分支上打个 Tag 记住 release 版本号，然后可以删除 release 分支了。\n2.4. hotfix 分支 # 当出现线上 bug 时，也意味着 master 存在 Bug。这时，我们需要基于 master 创建一个 hotfix 分支，在此分支上完成 bug 修复。修复后，我们应该将此分支合并回 master 和 develop 分支，同时在 master 上打一个 tag。所以，hotfix 的改动会进入下一个 release。\n2.5. 如何应用 Git Flow # 在实际开发中，如何具体落地 Git Flow 流程呢？\ngit 提供了 git flow 命令来手动管理，但是比较麻烦，所以还是建议使用 Git Flow 的 GUI 工具。比如： SourceTree、 VScode 的 GitFlow 插件、Intellij 的 GitFlow 插件等。\n想了解更详细的 Git Flow 介绍，可以参考：\nA Successful Git Branching Model\nGit 在团队中的最佳实践\u0026ndash;如何正确使用 Git Flow\nGithub Flow # 对于简单且迭代频繁的项目来说，Git Flow 可能有些太复杂了。这时，可以考虑 Github Flow。\n在 Github Flow 策略中，所有分支都是基于 master 创建。在 Feature 或 Bugfix 分支中完成工作后，将其合入 master，然后继续迭代。\n想了解更详细的 Github Flow 介绍，可以参考： GitHub Flow\nGit Commit 规范 # Git 每次提交代码，都要写 Commit message（提交说明），否则就不允许提交。\n好的 Commit message 可以让人一眼就明白提交者修改了什么内容，有什么影响；而不好的 Commit message 写了和没写一样，甚至还可能误导别人。\n先来看下图中不好的 Commit message 范例，从提交信息完全看不出来修改了什么。\n再来一张较好的 Commit message 范例，每次提交的是什么内容，做了什么一目了然。\nCommit message 的作用 # 从前面，我们不难看出，完善的 Commit message 非常有利于项目维护。即时是个人维护的项目，时间久了，可能也会忘记当初自己改了什么。\nCommit message 的作用还不仅仅是理解历史信息，它的主要作用如下：\n（1）提供易于理解的历史信息，方便检索\n（2）可以过滤某些 commit（比如文档改动），便于快速查找信息。\n（3）可以直接从 commit 生成 Change log。\nCommit message 的规范 # 开源社区有很多 Commit message 的规范，个人推荐使用 Angular Git Commit 规范，这是目前使用最广的写法，比较合理和系统化，并且有配套的工具。\n它主要有以下组成部分：\n标题行：必填, 描述主要修改类型和内容 主题内容：描述为什么修改, 做了什么样的修改, 以及开发的思路等等 页脚注释：放 Breaking Changes 或 Closed Issues 常用的修改项\ntype：commit 的类型 feat：新特性 fix：修改问题 refactor：代码重构 docs：文档修改 style：代码格式修改, 注意不是 css 修改 test：测试用例修改 chore：其他修改, 比如构建流程, 依赖管理. scope：commit 影响的范围, 比如：route, component, utils, build\u0026hellip; subject：commit 的概述 body：commit 具体修改内容, 可以分为多行 footer：一些备注, 通常是 BREAKING CHANGE 或修复的 bug 的链接 生成 Change log # 如果你的所有 Commit 都符合 Angular Git Commit 规范，那么发布新版本时，就可以用脚本自动生成 Change log。\n生成的文档包括以下三个部分。\nNew features Bug fixes Breaking changes. 每个部分都会罗列相关的 commit ，并且有指向这些 commit 的链接。当然，生成的文档允许手动修改，所以发布前，你还可以添加其他内容。\nconventional-changelog 就是生成 Change log 的工具，运行下面的命令即可。\n$ npm install -g conventional-changelog $ cd my-project $ conventional-changelog -p angular -i CHANGELOG.md -w 上面命令不会覆盖以前的 Change log，只会在CHANGELOG.md的头部加上自从上次发布以来的变动。\n如果你想生成所有发布的 Change log，要改为运行下面的命令。\n$ conventional-changelog -p angular -i CHANGELOG.md -w -r 0 为了方便使用，可以将其写入package.json的scripts字段。\n{ \u0026#34;scripts\u0026#34;: { \u0026#34;changelog\u0026#34;: \u0026#34;conventional-changelog -p angular -i CHANGELOG.md -w -r 0\u0026#34; } } 以后，直接运行下面的命令即可。\n$ npm run changelog Git 奇技淫巧 # 生成 SSH 公钥 # 许多 Git 服务器都使用 SSH 公钥进行认证。 为了向 Git 服务器提供 SSH 公钥，如果某系统用户尚未拥有密钥，必须事先为其生成一份。 这个过程在所有操作系统上都是相似的。 首先，你需要确认自己是否已经拥有密钥。 默认情况下，用户的 SSH 密钥存储在其 ~/.ssh 目录下。 进入该目录并列出其中内容，你便可以快速确认自己是否已拥有密钥：\n$ cd ~/.ssh $ ls authorized_keys2 id_dsa known_hosts config id_dsa.pub 我们需要寻找一对以 id_dsa 或 id_rsa 命名的文件，其中一个带有 .pub 扩展名。 .pub 文件是你的公钥，另一个则是私钥。 如果找不到这样的文件（或者根本没有 .ssh 目录），你可以通过运行 ssh-keygen 程序来创建它们。在 Linux/Mac 系统中，ssh-keygen 随 SSH 软件包提供；在 Windows 上，该程序包含于 MSysGit 软件包中。\n$ ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/home/schacon/.ssh/id_rsa): Created directory \u0026#39;/home/schacon/.ssh\u0026#39;. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/schacon/.ssh/id_rsa. Your public key has been saved in /home/schacon/.ssh/id_rsa.pub. The key fingerprint is: d0:82:24:8e:d7:f1:bb:9b:33:53:96:93:49:da:9b:e3 schacon@mylaptop.local 首先 ssh-keygen 会确认密钥的存储位置（默认是 .ssh/id_rsa），然后它会要求你输入两次密钥口令。如果你不想在使用密钥时输入口令，将其留空即可。\n现在，进行了上述操作的用户需要将各自的公钥发送给任意一个 Git 服务器管理员（假设服务器正在使用基于公钥的 SSH 验证设置）。 他们所要做的就是复制各自的 .pub 文件内容，并将其通过邮件发送。 公钥看起来是这样的：\n$ cat ~/.ssh/id_rsa.pub ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAklOUpkDHrfHY17SbrmTIpNLTGK9Tjom/BWDSU GPl+nafzlHDTYW7hdI4yZ5ew18JH4JW9jbhUFrviQzM7xlELEVf4h9lFX5QVkbPppSwg0cda3 Pbv7kOdJ/MTyBlWXFCR+HAo3FXRitBqxiX1nKhXpHAZsMciLq8V6RjsNAQwdsdMFvSlVK/7XA t3FaoJoAsncM1Q9x5+3V0Ww68/eIFmb1zuUFljQJKprrX88XypNDvjYNby6vw/Pb0rwert/En mZ+AW4OZPnTPI89ZPmVMLuayrD2cE86Z/il8b+gw3r3+1nKatmIkjn2so1d01QraTlMqVSsbx NrRFi9wrf+M7Q== schacon@mylaptop.local 在你的 Github 账户中，依次点击 Settings \u0026gt; SSH and GPG keys \u0026gt; New SSH key\n然后，将上面生成的公钥内容粘贴到 Key 编辑框并保存。至此大功告成。\n后面，你在克隆你的 Github 项目时使用 SSH 方式即可。\n如果觉得我的讲解还不够细致，可以参考： adding-a-new-ssh-key-to-your-github-account\n使用 .gitignore 忽略不必提交内容 # .gitignore 文件可能从字面含义也不难猜出：这个文件里配置的文件或目录，会自动被 git 所忽略，不纳入版本控制。\n在日常开发中，我们的项目经常会产生一些临时文件，如编译 Java 产生的 *.class 文件，又或是 IDE 自动生成的隐藏目录（Intellij 的 .idea 目录、Eclipse 的 .settings 目录等）等等。这些文件或目录实在没必要纳入版本管理。在这种场景下，你就需要用到 .gitignore 配置来过滤这些文件或目录。\n.gitignore 配置的规则很简单，也没什么可说的，看几个例子，自然就明白了。\n【示例】一份 Java 的 .gitignore\n# Compiled class file *.class # Log file *.log # BlueJ files *.ctxt # Mobile Tools for Java (J2ME) .mtj.tmp/ # Package Files # *.jar *.war *.nar *.ear *.zip *.tar.gz *.rar # virtual machine crash logs, see http://www.java.com/en/download/help/error_hotspot.xml hs_err_pid* 【推荐】这里推荐一个 Github 的开源项目： gitignore，在这里，你可以找到很多常用的 .gitignore 模板，如：Java、Nodejs、C++ 的 .gitignore 模板等等。\n使用 .gitattributes 解决 LF 和 CRLF 问题 # 你有没有在和多人协同开发时遇到过以下烦恼？\n开发者们分别使用不同的操作系统进行开发，有的人用 Windows，有的人用 Linux/MacOS。众所周知，不同操作系统默认的文件结尾行是不同的：在 Windows 上默认的是回车换行（Carriage Return Line Feed, CRLF），然而，在 Linux/MacOS 上则是换行（Line Feed, LF）。这就可能导致这种情况：明明文件内容一模一样，但是版本比对时仍然存在版本差异。\n那么如何解决这个问题呢？Git 提供了 .gitattributes 配置文件，它允许使用者指定由 git 使用的文件和路径的属性。\n在 Git 库中，一个普通文本文件的行尾默认是 LF。对于工作目录，除了 text 属性之外，还可以设置 eol 属性或 core.eol 配置变量。\n.gitattributes 文件中，可以用 text 属性指定某类文件或目录下的文件，控制它的行结束标准化。当一个文本文件被标准化时，它的行尾将在存储库中转换为 LF。要控制工作目录中使用的行结束风格，请使用单个文件的eol属性和所有文本文件的 core.eol 配置变量。\n【示例】一份 .gitattributes 示例\n* text=auto eol=lf *.txt text *.java text *.scala text *.groovy text *.gradle text *.properties text # unix style *.sh text eol=lf # win style *.bat text eol=crlf # binary *.jar binary *.war binary *.zip binary *.tar binary *.tar.gz binary *.gz binary *.apk binary *.bin binary *.exe binary 【推荐】这里推荐一个 Github 的开源项目： gitignore，在这里，你可以找到很多常用的 .gitignore 模板，如：Java、Nodejs、C++ 的 .gitignore 模板等等。\n同时提交代码到不同的远程仓库 # 如果，你在不同的 Git 远程仓库中维护同一个项目，你可能会有这样的需求：能不能一次提交，同时 push 到多个远程仓库中呢？\n这个可以有，解决方案如下：\n比如，我有一个 blog 项目，同时维护在 Github 和 Gitee 上。\n（1）首先，在 Github 和 Gitee 上配置本地的 ssh 公钥（如果是 Gitlab，也同样如此），这样中央仓库就能识别本地。\n生成 SSH 公钥的方法，请参考上文的 “生成 SSH 公钥” 章节。\n（2）进入 git 项目的隐藏目录 .git，打开 config 文件，参考下面配置进行编辑：\n[core] repositoryformatversion = 0 filemode = false bare = false logallrefupdates = true symlinks = false ignorecase = true [remote \u0026#34;origin\u0026#34;] url = git@github.com:dunwu/blog.git url = git@gitee.com:turnon/blog.git fetch = +refs/heads/*:refs/remotes/origin/* [branch \u0026#34;master\u0026#34;] remote = origin merge = refs/heads/master [user] name = dunwu email = forbreak@163.com 重点在于 remote \u0026quot;origin\u0026quot;，同时配置了两个 url。配置后，一旦触发 push 远程仓库的动作，就会同时推送提交记录到配置的远程仓库。\nGithub Issue 和 Gitlab Issue # 开发软件，Bug 再所难免，出现问题不可怕，可怕的是放任不管；所以，优秀的软件项目，都应该管理好问题追踪。软件的使用者，在使用中，可能会遇到形形色色的问题，难以解决，需要向维护者寻求帮助。\n问题追踪如此重要，所以各种代码托管平台都会提供 Issue 维护机制，如 Github Issue 和 Gitlab Issue。\n如果一个项目的开源社区很活跃，在没有任何约束的前提下，提问肯定是五花八门的，让维护者难以招架。\n其实，提问也是一门艺术，如果提问者的问题长篇大幅，言不达意，让别人难以理解，就很难得到有效帮助。关于如何高效的提问，推荐参考 提问的智慧 这篇文章，作者整理的非常好。\n作为开发者，你不能期望所有提问者都是训练有素的提问者。所以，使用规范化的 Issue 模板来引导提问者提问，可以大大减轻开发者的负担。\nGithub Issue 模板 # 如何在 Github Issue 平台上创建 Issue 模板呢？方法如下：\n（1）在仓库根目录创建新目录 .github\n（2）在 .github 目录中添加 ISSUE_TEMPLATE 目录，在其中添加的 md 文件都会被 Github 自动识，并将其作为 issue 的默认模板。\n示例，下面是携程 apollo 的一个 Issue 模板，要求提问者填充 bug 描述、复现步骤、期望、截图、日志等细节。\n更多模板： Github issue_templates 模板\nGitlab Issue 模板 # 如何在 Gitlab Issue 平台上创建 Issue 模板呢？方法如下：\n（1）在仓库根目录创建新目录 .gitlab\n（2）在 .gitlab 目录中添加 issue_templates 目录，在其中添加的 md 文件都会被 Gitlab 自动识，并将其作为 issue 的默认模板。\n更多模板： Gitlab 官方 issue_templates 模板\nGit Hook # 在执行提交代码（git commit），推送代码（git push）等行为时，我们可能希望做一些代码检查性工作，例如：代码 lint 检查、代码格式化等。当检查发现代码存在问题时，就拒绝代码提交，从而保证项目质量。\nGit 提供了 Git Hook 机制，允许使用者在特定的重要动作发生时触发自定义脚本。有两类钩子：客户端钩子和服务器端钩子。客户端钩子由诸如提交和合并等操作所触发调用，而服务器端钩子作用于诸如接收被推送的提交这样的联网操作。钩子都被存储在 Git 项目目录下的 .git/hooks 子目录中。Git 在这个目录下放置了一些示例，这些示例的名字都是以 .sample 结尾，如果想启用它们，得先移除这个后缀。\n常用的客户端钩子：\npre-commit 钩子：在提交信息前运行。 它用于检查即将提交的快照，例如，检查是否有所遗漏，确保测试运行，以及核查代码。 如果该钩子以非零值退出，Git 将放弃此次提交，不过你可以用 git commit --no-verify 来绕过这个环节。 你可以利用该钩子，来检查代码风格是否一致（运行类似 lint 的程序）、尾随空白字符是否存在（自带的钩子就是这么做的），或新方法的文档是否适当。 prepare-commit-msg 钩子：在启动提交信息编辑器之前，默认信息被创建之后运行。 它允许你编辑提交者所看到的默认信息。 该钩子接收一些选项：存有当前提交信息的文件的路径、提交类型和修补提交的提交的 SHA-1 校验。 它对一般的提交来说并没有什么用；然而对那些会自动产生默认信息的提交，如提交信息模板、合并提交、压缩提交和修订提交等非常实用。 你可以结合提交模板来使用它，动态地插入信息。 commit-msg 钩子：接收一个参数，此参数即上文提到的，存有当前提交信息的临时文件的路径。 如果该钩子脚本以非零值退出，Git 将放弃提交，因此，可以用来在提交通过前验证项目状态或提交信息。 在本章的最后一节，我们将展示如何使用该钩子来核对提交信息是否遵循指定的模板。 post-commit 钩子：在整个提交过程完成后运行。它不接收任何参数，但你可以很容易地通过运行 git log -1 HEAD 来获得最后一次的提交信息。 该钩子一般用于通知之类的事情。 pre-push 钩子：会在 git push 运行期间， 更新了远程引用但尚未传送对象时被调用。 它接受远程分支的名字和位置作为参数，同时从标准输入中读取一系列待更新的引用。 你可以在推送开始之前，用它验证对引用的更新操作（一个非零的退出码将终止推送过程）。 Javascript 应用 Git Hook # 想在 JavaScript 应用中使用 Git Hook，推荐使用 husky ，可以很方便的编写钩子处理命令。\n使用方法很简单，先安装 husky\nnpm i -D husky 然后，在 package.json 中添加配置：\n\u0026#34;husky\u0026#34;: { \u0026#34;hooks\u0026#34;: { \u0026#34;pre-commit\u0026#34;: \u0026#34;lint-staged\u0026#34; } }, \u0026#34;lint-staged\u0026#34;: { \u0026#34;src/**/*.{js,vue}\u0026#34;: [ \u0026#34;eslint --fix\u0026#34;, \u0026#34;git add\u0026#34; ] }, 以上配置的作用是，当提交代码前（ pre-commit ），先执行 lint-staged；\nlint-staged 中执行的动作是，对 src 目录的所有 js、vue 文件进行 eslint 检查，并尝试修复。如果修复后没有问题，就 git add 添加修改后的文件；如果修复失败，则拒绝提交代码。\n参考资料 # 官方资源 Git 官网 Git Github Github 官方教程 模板 gitignore 模板 - .gitignore 文件模板 gitattributes 模板 - .gitattributes 文件模板 github-cheat-sheet - git 命令简略图表 Git 教程 Learn Git branching - 交互式教程 Git 官方推荐教程 - Scott Chacon 的 Git 书。 git-flight-rules git-tips Git 中文教程 廖雪峰的 Git 教程 有关 git 的学习资源 文章 Git Cookbook Git 奇技淫巧 Git 风格指南 Git 在团队中的最佳实践\u0026ndash;如何正确使用 Git Flow Commit message 和 Change log 编写指南 Git 工具 guis - Git 官网展示的客户端工具列表。 gogs - 极易搭建的自助 Git 服务。 gitflow - 应用 fit-flow 模型的工具。 firstaidgit.io 一个可搜索的最常被问到的 Git 的问题 git-extra-commands - 一堆有用的额外的 Git 脚本 git-extras - GIT 工具集 \u0026ndash; repo summary, repl, changelog population, author commit percentages and more git-fire - git-fire 是一个 Git 插件，用于帮助在紧急情况下添加所有当前文件, 做提交(committing), 和推(push)到一个新分支(阻止合并冲突)。 git-tips - Git 小提示 git-town - 通用，高级 Git 工作流支持！ GUI 客户端 GitKraken - 豪华的 Git 客户端 Windows, Mac \u0026amp; Linux git-cola - 另外一个 Git 客户端 Windows \u0026amp; OS X GitUp - 一个新的 Git 客户端，在处理 Git 的复杂性上有自己的特点 gitx-dev - 图形化的 Git 客户端 OS X Source Tree - 免费的图形化 Git 客户端 Windows \u0026amp; OS X Tower - 图形化 Git 客户端 OS X(付费) git cheat sheet github-git-cheat-sheet ","date":"19 October 2023","permalink":"/posts/devoops/git/git-principle/","section":"博客","summary":"Git 和其它版本控制系统（包括 Subversion 和近似工具）的主要差别在于 Git 对待数据的方式。 从概念上来说，其它大部分系统以文件变更列表的方式存储信息，而 Git 是把数据看作是对小型文件系统的一系列快照。","title":"Git Principle"},{"content":"Git 帮助手册 # 国外网友制作了一张 Git Cheat Sheet，总结很精炼，各位不妨收藏一下。\n本节选择性介绍 git 中比较常用的命令行场景。\n安装 # （1）Debian/Ubuntu 环境安装\n如果你使用的系统是 Debian/Ubuntu ， 安装命令为：\n$ apt-get install libcurl4-gnutls-dev libexpat1-dev gettext \\ \u0026gt; libz-dev libssl-dev $ apt-get install git-core $ git --version git version 1.8.1.2 （2）Centos/RedHat 环境安装\n如果你使用的系统是 Centos/RedHat ，安装命令为：\n$ yum install curl-devel expat-devel gettext-devel \\ \u0026gt; openssl-devel zlib-devel $ yum -y install git-core $ git --version git version 1.7.1 （3）Windows 环境安装\n在 Git 官方下载地址下载 exe 安装包。按照安装向导安装即可。\n建议安装 Git Bash 这个 git 的命令行工具。\n（4）Mac 环境安装\n在 Git 官方下载地址下载 mac 安装包。按照安装向导安装即可。\n配置 # Git 自带一个 git config 的工具来帮助设置控制 Git 外观和行为的配置变量。 这些变量存储在三个不同的位置：\n/etc/gitconfig 文件: 包含系统上每一个用户及他们仓库的通用配置。 如果使用带有 --system 选项的 git config 时，它会从此文件读写配置变量。 ~/.gitconfig 或 ~/.config/git/config 文件：只针对当前用户。 可以传递 --global 选项让 Git 读写此文件。 当前使用仓库的 Git 目录中的 config 文件（就是 .git/config）：针对该仓库。 每一个级别覆盖上一级别的配置，所以 .git/config 的配置变量会覆盖 /etc/gitconfig 中的配置变量。\n在 Windows 系统中，Git 会查找 $HOME 目录下（一般情况下是 C:\\Users\\$USER）的 .gitconfig 文件。 Git 同样也会寻找 /etc/gitconfig 文件，但只限于 MSys 的根目录下，即安装 Git 时所选的目标位置。\n配置用户信息 # 当安装完 Git 应该做的第一件事就是设置你的用户名称与邮件地址。 这样做很重要，因为每一个 Git 的提交都会使用这些信息，并且它会写入到你的每一次提交中，不可更改：\ngit config --global user.name \u0026#34;John Doe\u0026#34; git config --global user.email johndoe@example.com 再次强调，如果使用了 --global 选项，那么该命令只需要运行一次，因为之后无论你在该系统上做任何事情， Git 都会使用那些信息。 当你想针对特定项目使用不同的用户名称与邮件地址时，可以在那个项目目录下运行没有 --global 选项的命令来配置。\n很多 GUI 工具都会在第一次运行时帮助你配置这些信息。\n给 Git 命令添加别名 # 在 OS X 和 Linux 下, 你的 Git 的配置文件储存在 ~/.gitconfig。我在[alias] 部分添加了一些快捷别名(和一些我容易拼写错误的)，如下:\n[alias] a = add amend = commit --amend c = commit ca = commit --amend ci = commit -a co = checkout d = diff dc = diff --changed ds = diff --staged f = fetch loll = log --graph --decorate --pretty=oneline --abbrev-commit m = merge one = log --pretty=oneline outstanding = rebase -i @{u} s = status unpushed = log @{u} wc = whatchanged wip = rebase -i @{u} zap = fetch -p 缓存一个仓库的用户名和密码 # 你可能有一个仓库需要授权，这时你可以缓存用户名和密码，而不用每次推/拉(push/pull)的时候都输入，Credential helper 能帮你。\ngit config --global credential.helper cache ## Set git to use the credential memory cache git config --global credential.helper \u0026#39;cache --timeout=3600\u0026#39; ## Set the cache to timeout after 1 hour (setting is in seconds) 仓库 # 初始化仓库 # $ git init 克隆仓库 # # 通过 SSH $ git clone ssh://user@domain.com/repo.git # 通过 HTTP $ git clone http://domain.com/user/repo.git 储藏 # 有时，我们需要在同一个项目的不同分支上工作。当需要切换分支时，偏偏本地的工作还没有完成，此时，提交修改显得不严谨，但是不提交代码又无法切换分支。这时，你可以使用 git stash 将本地的修改内容作为草稿储藏起来。\n官方称之为储藏，但我个人更喜欢称之为存草稿。\n# 1. 将修改作为当前分支的草稿保存 $ git stash # 2. 查看草稿列表 $ git stash list stash@{0}: WIP on master: 6fae349 📝 Writing docs. # 3.1 删除草稿 $ git stash drop stash@{0} # 3.2 读取草稿 $ git stash apply stash@{0} 暂存 # git add 命令用于将修改添加到暂存区。\n暂存指定文件 # git add xxx 暂存当前目录下所有修改 # git add . 暂存所有修改 # git add -A 暂存文件部分内容 # 暂存文件部分内容\ngit add --patch filename.x -p 简写。这会打开交互模式， 你将能够用 s 选项来分隔提交(commit)；\n然而, 如果这个文件是新的, 会没有这个选择， 添加一个新文件时，这样做:\ngit add -N filename.x 然后, 你需要用 e 选项来手动选择需要添加的行，执行 git diff --cached 将会显示哪些行暂存了哪些行只是保存在本地了。\n把暂存的内容变成未暂存，把未暂存的内容暂存起来 # 这个有点困难， 我能想到的最好的方法是先 stash 未暂存的内容， 然后重置(reset)，再 pop 第一步 stashed 的内容, 最后再 add 它们。\ngit stash -k git reset --hard git stash pop git add -A 提交 # git commit 命令用于将修改保存到到本地仓库。\n查看最近一次提交 # git show 或者\ngit log -n1 -p 提交本地的所有修改 # git commit -a 提交暂存的修改 # git commit 把暂存的内容添加到上一次的提交 # git commit --amend 附加消息提交 # git commit -m \u0026#39;commit message\u0026#39; 修改提交信息 # 如果你的提交信息写错了且这次提交（commit）还没有推送（push），可以使用以下命令修改：\ngit commit --amend 或者\ngit commit --amend -m \u0026#39;xxxxxxx\u0026#39; 修改提交信息中的用户名和邮箱 # git commit --amend --author \u0026#34;New Authorname \u0026lt;authoremail@mydomain.com\u0026gt;\u0026#34; 从提交中移除一个文件 # git checkout HEAD^ myfile git add -A git commit --amend 删除最后一次提交 # 如果你需要删除推了的提交(pushed commits)，你可以使用下面的方法。可是，这会不可逆的改变你的历史，也会搞乱那些已经从该仓库拉取(pulled)了的人的历史。简而言之，如果你不是很确定，千万不要这么做。\ngit reset HEAD^ --hard git push -f [remote] [branch] 如果你还没有推到远程, 把 Git 重置(reset)到你最后一次提交前的状态就可以了(同时保存暂存的变化):\n(my-branch*)$ git reset --soft HEAD@{1} 这只能在没有推送之前有用. 如果你已经推了, 唯一安全能做的是 git revert SHAofBadCommit， 那会创建一个新的提交(commit)用于撤消前一个提交的所有变化(changes)； 或者, 如果你推的这个分支是 rebase-safe 的 (例如： 其它开发者不会从这个分支拉), 只需要使用 git push -f； 更多, 请参考 the above section。\n删除任意提交 # 同样的警告：不到万不得已的时候不要这么做.\ngit rebase --onto SHA1_OF_BAD_COMMIT^ SHA1_OF_BAD_COMMIT git push -f [remote] [branch] 或者做一个 交互式 rebase 删除那些你想要删除的提交(commit)里所对应的行。\n我尝试推一个修正后的提交(amended commit)到远程，但是报错 # To https://github.com/yourusername/repo.git ! [rejected] mybranch -\u0026gt; mybranch (non-fast-forward) error: failed to push some refs to \u0026#39;https://github.com/tanay1337/webmaker.org.git\u0026#39; hint: Updates were rejected because the tip of your current branch is behind hint: its remote counterpart. Integrate the remote changes (e.g. hint: \u0026#39;git pull ...\u0026#39;) before pushing again. hint: See the \u0026#39;Note about fast-forwards\u0026#39; in \u0026#39;git push --help\u0026#39; for details. 注意, rebasing(见下面)和修正(amending)会用一个新的提交(commit)代替旧的, 所以如果之前你已经往远程仓库上推过一次修正前的提交(commit)，那你现在就必须强推(force push) (-f)。 注意 – 总是 确保你指明一个分支!\n(my-branch)$ git push origin mybranch -f 一般来说, 要避免强推. 最好是创建和推(push)一个新的提交(commit)，而不是强推一个修正后的提交。后者会使那些与该分支或该分支的子分支工作的开发者，在源历史中产生冲突。\n不小心强制重置，想找回内容 # 如果你意外的做了 git reset --hard, 你通常能找回你的提交(commit), 因为 Git 对每件事都会有日志，且都会保存几天。\n(master)$ git reflog 你将会看到一个你过去提交(commit)的列表, 和一个重置的提交。 选择你想要回到的提交(commit)的 SHA，再重置一次:\n(master)$ git reset --hard SHA1234 这样就完成了。\n重置 # 撤销本地修改：\n# 移除缓存区的所有文件（i.e. 撤销上次git add） $ git reset HEAD # 将HEAD重置到上一次提交的版本，并将之后的修改标记为未添加到缓存区的修改 $ git reset \u0026lt;commit\u0026gt; # 将HEAD重置到上一次提交的版本，并保留未提交的本地修改 $ git reset --keep \u0026lt;commit\u0026gt; # 放弃工作目录下的所有修改 $ git reset --hard HEAD # 将HEAD重置到指定的版本，并抛弃该版本之后的所有修改 $ git reset --hard \u0026lt;commit-hash\u0026gt; # 用远端分支强制覆盖本地分支 $ git reset --hard \u0026lt;remote/branch\u0026gt; e.g., upstream/master, origin/my-feature # 放弃某个文件的所有本地修改 $ git checkout HEAD \u0026lt;file\u0026gt; 删除添加.gitignore文件前错误提交的文件：\n# 提交一条 git 记录，提交信息为 remove xyz file $ git rm -r --cached . $ git add . $ git commit -m \u0026#34;remove xyz file\u0026#34; 撤销远程修改（创建一个新的提交，并回滚到指定版本）：\n# revert 哈希号为 commit-hash 的记录 $ git revert \u0026lt;commit-hash\u0026gt; 彻底删除指定版本：\n# 执行下面命令后，commit-hash 提交后的记录都会被彻底删除，使用需谨慎 $ git reset --hard \u0026lt;commit-hash\u0026gt; $ git push -f 更新 # # 下载远程端版本，但不合并到HEAD中 $ git fetch \u0026lt;remote\u0026gt; # 将远程端版本合并到本地版本中 $ git pull origin master # 以rebase方式将远端分支与本地合并 $ git pull --rebase \u0026lt;remote\u0026gt; \u0026lt;branch\u0026gt; 推送 # 推送提交到远程仓库 # git push remote \u0026lt;remote\u0026gt; \u0026lt;branch\u0026gt; 发布标签 # git push --tags 未暂存 # 未暂存(Unstaged)的内容\n把未暂存的内容移动到一个新分支 # git checkout -b my-branch 我想把未暂存的内容移动到另一个已存在的分支 # git stash git checkout my-branch git stash pop 丢弃本地未提交的变化 # 如果你只是想重置源(origin)和你本地(local)之间的一些提交(commit)，你可以：\n## one commit $ git reset --hard HEAD^ ## two commits $ git reset --hard HEAD^^ ## four commits $ git reset --hard HEAD~4 ## or $ git checkout -f 重置某个特殊的文件, 你可以用文件名做为参数:\ngit reset filename 我想丢弃某些未暂存的内容 # 如果你想丢弃工作拷贝中的一部分内容，而不是全部。\n签出(checkout)不需要的内容，保留需要的。\n$ git checkout -p ## Answer y to all of the snippets you want to drop 另外一个方法是使用 stash， Stash 所有要保留下的内容, 重置工作拷贝, 重新应用保留的部分。\n$ git stash -p ## Select all of the snippets you want to save $ git reset --hard $ git stash pop 或者, stash 你不需要的部分, 然后 stash drop。\n$ git stash -p ## Select all of the snippets you don\u0026#39;t want to save $ git stash drop 分支 # 分支(Branches)\n列出所有的分支 # git branch 列出所有的远端分支 # git branch -r 基于当前分支创建新分支 # git branch \u0026lt;new-branch\u0026gt; 基于远程分支创建新分支 # git branch --track \u0026lt;new-branch\u0026gt; \u0026lt;remote-branch\u0026gt; 删除本地分支 # git branch -d \u0026lt;branch\u0026gt; 强制删除本地分支 # 注意：强制删除本地分支，将会丢失未合并的修改\ngit branch -D \u0026lt;branch\u0026gt; 删除远程分支 # git push \u0026lt;remote\u0026gt; :\u0026lt;branch\u0026gt; (since Git v1.5.0) git push \u0026lt;remote\u0026gt; --delete \u0026lt;branch\u0026gt; (since Git v1.7.0) 切换分支 # git checkout \u0026lt;branch\u0026gt; 创建并切换到新分支 # git checkout -b \u0026lt;branch\u0026gt; 我从错误的分支拉取了内容，或把内容拉取到了错误的分支 # 这是另外一种使用 git reflog 情况，找到在这次错误拉(pull) 之前 HEAD 的指向。\n(master)$ git reflog ab7555f HEAD@{0}: pull origin wrong-branch: Fast-forward c5bc55a HEAD@{1}: checkout: checkout message goes here 重置分支到你所需的提交(desired commit):\ngit reset --hard c5bc55a 完成。\n我想扔掉本地的提交(commit)，以便我的分支与远程的保持一致 # 先确认你没有推(push)你的内容到远程。\ngit status 会显示你领先(ahead)源(origin)多少个提交:\n(my-branch)$ git status ## On branch my-branch ## Your branch is ahead of \u0026#39;origin/my-branch\u0026#39; by 2 commits. ## (use \u0026#34;git push\u0026#34; to publish your local commits) # 一种方法是:\n(master)$ git reset --hard origin/my-branch 我需要提交到一个新分支，但错误的提交到了 master # 在 master 下创建一个新分支，不切换到新分支,仍在 master 下:\n(master)$ git branch my-branch 把 master 分支重置到前一个提交:\n(master)$ git reset --hard HEAD^ HEAD^ 是 HEAD^1 的简写，你可以通过指定要设置的HEAD来进一步重置。\n或者, 如果你不想使用 HEAD^, 找到你想重置到的提交(commit)的 hash(git log 能够完成)， 然后重置到这个 hash。 使用git push 同步内容到远程。\n例如, master 分支想重置到的提交的 hash 为a13b85e:\n(master)$ git reset --hard a13b85e HEAD is now at a13b85e 签出(checkout)刚才新建的分支继续工作:\n(master)$ git checkout my-branch 我想保留来自另外一个 ref-ish 的整个文件 # 假设你正在做一个原型方案(原文为 working spike (see note)), 有成百的内容，每个都工作得很好。现在, 你提交到了一个分支，保存工作内容:\n(solution)$ git add -A \u0026amp;\u0026amp; git commit -m \u0026#34;Adding all changes from this spike into one big commit.\u0026#34; 当你想要把它放到一个分支里 (可能是feature, 或者 develop), 你关心是保持整个文件的完整，你想要一个大的提交分隔成比较小。\n假设你有:\n分支 solution, 拥有原型方案， 领先 develop 分支。 分支 develop, 在这里你应用原型方案的一些内容。 我去可以通过把内容拿到你的分支里，来解决这个问题:\n(develop)$ git checkout solution -- file1.txt 这会把这个文件内容从分支 solution 拿到分支 develop 里来:\n## On branch develop ## Your branch is up-to-date with \u0026#39;origin/develop\u0026#39;. ## Changes to be committed: ## (use \u0026#34;git reset HEAD \u0026lt;file\u0026gt;...\u0026#34; to unstage) # ## modified: file1.txt 然后, 正常提交。\nNote: Spike solutions are made to analyze or solve the problem. These solutions are used for estimation and discarded once everyone gets clear visualization of the problem. ~ Wikipedia.\n我把几个提交(commit)提交到了同一个分支，而这些提交应该分布在不同的分支里 # 假设你有一个master分支， 执行git log, 你看到你做过两次提交:\n(master)$ git log commit e3851e817c451cc36f2e6f3049db528415e3c114 Author: Alex Lee \u0026lt;alexlee@example.com\u0026gt; Date: Tue Jul 22 15:39:27 2014 -0400 Bug #21 - Added CSRF protection commit 5ea51731d150f7ddc4a365437931cd8be3bf3131 Author: Alex Lee \u0026lt;alexlee@example.com\u0026gt; Date: Tue Jul 22 15:39:12 2014 -0400 Bug #14 - Fixed spacing on title commit a13b85e984171c6e2a1729bb061994525f626d14 Author: Aki Rose \u0026lt;akirose@example.com\u0026gt; Date: Tue Jul 21 01:12:48 2014 -0400 First commit 让我们用提交 hash(commit hash)标记 bug (e3851e8 for #21, 5ea5173 for #14).\n首先, 我们把master分支重置到正确的提交(a13b85e):\n(master)$ git reset --hard a13b85e HEAD is now at a13b85e 现在, 我们对 bug #21 创建一个新的分支:\n(master)$ git checkout -b 21 (21)$ 接着, 我们用 cherry-pick 把对 bug #21 的提交放入当前分支。 这意味着我们将应用(apply)这个提交(commit)，仅仅这一个提交(commit)，直接在 HEAD 上面。\n(21)$ git cherry-pick e3851e8 这时候, 这里可能会产生冲突， 参见 交互式 rebasing 章 冲突节 解决冲突.\n再者， 我们为 bug #14 创建一个新的分支, 也基于master分支\n(21)$ git checkout master (master)$ git checkout -b 14 (14)$ 最后, 为 bug #14 执行 cherry-pick:\n(14)$ git cherry-pick 5ea5173 我想删除上游(upstream)分支被删除了的本地分支 # 一旦你在 github 上面合并(merge)了一个 pull request, 你就可以删除你 fork 里被合并的分支。 如果你不准备继续在这个分支里工作, 删除这个分支的本地拷贝会更干净，使你不会陷入工作分支和一堆陈旧分支的混乱之中。\ngit fetch -p 我不小心删除了我的分支 # 如果你定期推送到远程, 多数情况下应该是安全的，但有些时候还是可能删除了还没有推到远程的分支。 让我们先创建一个分支和一个新的文件:\n(master)$ git checkout -b my-branch (my-branch)$ git branch (my-branch)$ touch foo.txt (my-branch)$ ls README.md foo.txt 添加文件并做一次提交\n(my-branch)$ git add . (my-branch)$ git commit -m \u0026#39;foo.txt added\u0026#39; (my-branch)$ foo.txt added 1 files changed, 1 insertions(+) create mode 100644 foo.txt (my-branch)$ git log commit 4e3cd85a670ced7cc17a2b5d8d3d809ac88d5012 Author: siemiatj \u0026lt;siemiatj@example.com\u0026gt; Date: Wed Jul 30 00:34:10 2014 +0200 foo.txt added commit 69204cdf0acbab201619d95ad8295928e7f411d5 Author: Kate Hudson \u0026lt;katehudson@example.com\u0026gt; Date: Tue Jul 29 13:14:46 2014 -0400 Fixes #6: Force pushing after amending commits 现在我们切回到主(master)分支，‘不小心的’删除my-branch分支\n(my-branch)$ git checkout master Switched to branch \u0026#39;master\u0026#39; Your branch is up-to-date with \u0026#39;origin/master\u0026#39;. (master)$ git branch -D my-branch Deleted branch my-branch (was 4e3cd85). (master)$ echo oh noes, deleted my branch! oh noes, deleted my branch! 在这时候你应该想起了reflog, 一个升级版的日志，它存储了仓库(repo)里面所有动作的历史。\n(master)$ git reflog 69204cd HEAD@{0}: checkout: moving from my-branch to master 4e3cd85 HEAD@{1}: commit: foo.txt added 69204cd HEAD@{2}: checkout: moving from master to my-branch 正如你所见，我们有一个来自删除分支的提交 hash(commit hash)，接下来看看是否能恢复删除了的分支。\n(master)$ git checkout -b my-branch-help Switched to a new branch \u0026#39;my-branch-help\u0026#39; (my-branch-help)$ git reset --hard 4e3cd85 HEAD is now at 4e3cd85 foo.txt added (my-branch-help)$ ls README.md foo.txt 看! 我们把删除的文件找回来了。 Git 的 reflog 在 rebasing 出错的时候也是同样有用的。\n我想删除一个分支 # 删除一个远程分支:\n(master)$ git push origin --delete my-branch 你也可以:\n(master)$ git push origin :my-branch 删除一个本地分支:\n(master)$ git branch -D my-branch 我想从别人正在工作的远程分支签出(checkout)一个分支 # 首先, 从远程拉取(fetch) 所有分支:\n(master)$ git fetch --all 假设你想要从远程的daves分支签出到本地的daves\n(master)$ git checkout --track origin/daves Branch daves set up to track remote branch daves from origin. Switched to a new branch \u0026#39;daves\u0026#39; (--track 是 git checkout -b [branch] [remotename]/[branch] 的简写)\n这样就得到了一个daves分支的本地拷贝, 任何推过(pushed)的更新，远程都能看到.\n标签 # 添加标签 # $ git tag \u0026lt;tag-name\u0026gt; 添加标签并附加消息 # $ git tag -a \u0026lt;tag-name\u0026gt; 删除标签 # git tag -d \u0026lt;tag_name\u0026gt; git push \u0026lt;remote\u0026gt; :refs/tags/\u0026lt;tag_name\u0026gt; 恢复已删除标签 # 如果你想恢复一个已删除标签(tag), 可以按照下面的步骤: 首先, 需要找到无法访问的标签(unreachable tag):\ngit fsck --unreachable | grep tag 记下这个标签(tag)的 hash，然后用 Git 的 update-ref:\ngit update-ref refs/tags/\u0026lt;tag_name\u0026gt; \u0026lt;hash\u0026gt; 这时你的标签(tag)应该已经恢复了。\nRebase 和 Merge # merge 与 rebase 虽然是 git 常用功能，但是强烈建议不要使用 git 命令来完成这项工作。\n因为如果出现代码冲突，在没有代码比对工具的情况下，实在太艰难了。\n你可以考虑使用各种 Git GUI 工具。\n将分支合并到当前 HEAD 中 # git merge \u0026lt;branch\u0026gt; 将当前 HEAD 版本重置到分支中 # git rebase \u0026lt;branch\u0026gt; 撤销 rebase/merge # 你可以合并(merge)或 rebase 了一个错误的分支, 或者完成不了一个进行中的 rebase/merge。 Git 在进行危险操作的时候会把原始的 HEAD 保存在一个叫 ORIG_HEAD 的变量里, 所以要把分支恢复到 rebase/merge 前的状态是很容易的。\n(my-branch)$ git reset --hard ORIG_HEAD 我已经 rebase 过, 但是我不想强推(force push) # 不幸的是，如果你想把这些变化(changes)反应到远程分支上，你就必须得强推(force push)。 是因你快进(Fast forward)了提交，改变了 Git 历史, 远程分支不会接受变化(changes)，除非强推(force push)。这就是许多人使用 merge 工作流, 而不是 rebasing 工作流的主要原因之一， 开发者的强推(force push)会使大的团队陷入麻烦。使用时需要注意，一种安全使用 rebase 的方法是，不要把你的变化(changes)反映到远程分支上, 而是按下面的做:\n(master)$ git checkout my-branch (my-branch)$ git rebase -i master (my-branch)$ git checkout master (master)$ git merge --ff-only my-branch 更多, 参见 this SO thread.\n我需要组合(combine)几个提交(commit) # 假设你的工作分支将会做对于 master 的 pull-request。 一般情况下你不关心提交(commit)的时间戳，只想组合 所有 提交(commit) 到一个单独的里面, 然后重置(reset)重提交(recommit)。 确保主(master)分支是最新的和你的变化都已经提交了, 然后:\n(my-branch)$ git reset --soft master (my-branch)$ git commit -am \u0026#34;New awesome feature\u0026#34; 如果你想要更多的控制, 想要保留时间戳, 你需要做交互式 rebase (interactive rebase):\n(my-branch)$ git rebase -i master 如果没有相对的其它分支， 你将不得不相对自己的HEAD 进行 rebase。 例如：你想组合最近的两次提交(commit), 你将相对于HEAD\\~2 进行 rebase， 组合最近 3 次提交(commit), 相对于HEAD\\~3, 等等。\n(master)$ git rebase -i HEAD~2 在你执行了交互式 rebase 的命令(interactive rebase command)后, 你将在你的编辑器里看到类似下面的内容:\npick a9c8a1d Some refactoring pick 01b2fd8 New awesome feature pick b729ad5 fixup pick e3851e8 another fix ## Rebase 8074d12..b729ad5 onto 8074d12 # ## Commands: ## p, pick = use commit ## r, reword = use commit, but edit the commit message ## e, edit = use commit, but stop for amending ## s, squash = use commit, but meld into previous commit ## f, fixup = like \u0026#34;squash\u0026#34;, but discard this commit\u0026#39;s log message ## x, exec = run command (the rest of the line) using shell # ## These lines can be re-ordered; they are executed from top to bottom. # ## If you remove a line here THAT COMMIT WILL BE LOST. # ## However, if you remove everything, the rebase will be aborted. # ## Note that empty commits are commented out 所有以 # 开头的行都是注释, 不会影响 rebase.\n然后，你可以用任何上面命令列表的命令替换 pick, 你也可以通过删除对应的行来删除一个提交(commit)。\n例如, 如果你想 单独保留最旧(first)的提交(commit),组合所有剩下的到第二个里面, 你就应该编辑第二个提交(commit)后面的每个提交(commit) 前的单词为 f:\npick a9c8a1d Some refactoring pick 01b2fd8 New awesome feature f b729ad5 fixup f e3851e8 another fix 如果你想组合这些提交(commit) 并重命名这个提交(commit), 你应该在第二个提交(commit)旁边添加一个r，或者更简单的用s 替代 f:\npick a9c8a1d Some refactoring pick 01b2fd8 New awesome feature s b729ad5 fixup s e3851e8 another fix 你可以在接下来弹出的文本提示框里重命名提交(commit)。\nNewer, awesomer features ## Please enter the commit message for your changes. Lines starting ## with \u0026#39;#\u0026#39; will be ignored, and an empty message aborts the commit. ## rebase in progress; onto 8074d12 ## You are currently editing a commit while rebasing branch \u0026#39;master\u0026#39; on \u0026#39;8074d12\u0026#39;. # ## Changes to be committed: # modified: README.md # 如果成功了, 你应该看到类似下面的内容:\n(master)$ Successfully rebased and updated refs/heads/master. 安全合并(merging)策略 # --no-commit 执行合并(merge)但不自动提交, 给用户在做提交前检查和修改的机会。 no-ff 会为特性分支(feature branch)的存在过留下证据, 保持项目历史一致。\n(master)$ git merge --no-ff --no-commit my-branch 我需要将一个分支合并成一个提交(commit) # (master)$ git merge --squash my-branch 我只想组合(combine)未推的提交(unpushed commit) # 有时候，在将数据推向上游之前，你有几个正在进行的工作提交(commit)。这时候不希望把已经推(push)过的组合进来，因为其他人可能已经有提交(commit)引用它们了。\n(master)$ git rebase -i @{u} 这会产生一次交互式的 rebase(interactive rebase), 只会列出没有推(push)的提交(commit)， 在这个列表时进行 reorder/fix/squash 都是安全的。\n检查是否分支上的所有提交(commit)都合并(merge)过了 # 检查一个分支上的所有提交(commit)是否都已经合并(merge)到了其它分支, 你应该在这些分支的 head(或任何 commits)之间做一次 diff:\n(master)$ git log --graph --left-right --cherry-pick --oneline HEAD...feature/120-on-scroll 这会告诉你在一个分支里有而另一个分支没有的所有提交(commit), 和分支之间不共享的提交(commit)的列表。 另一个做法可以是:\n(master)$ git log master ^feature/120-on-scroll --no-merges 交互式 rebase(interactive rebase)可能出现的问题 # 这个 rebase 编辑屏幕出现\u0026rsquo;noop' # 如果你看到的是这样:\nnoop 这意味着你 rebase 的分支和当前分支在同一个提交(commit)上, 或者 领先(ahead) 当前分支。 你可以尝试:\n检查确保主(master)分支没有问题 rebase HEAD\\~2 或者更早 有冲突的情况 # 如果你不能成功的完成 rebase, 你可能必须要解决冲突。\n首先执行 git status 找出哪些文件有冲突:\n(my-branch)$ git status On branch my-branch Changes not staged for commit: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) (use \u0026#34;git checkout -- \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) modified: README.md 在这个例子里面, README.md 有冲突。 打开这个文件找到类似下面的内容:\n\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD some code ========= some code \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; new-commit 你需要解决新提交的代码(示例里, 从中间==线到new-commit的地方)与HEAD 之间不一样的地方.\n有时候这些合并非常复杂，你应该使用可视化的差异编辑器(visual diff editor):\n(master*)$ git mergetool -t opendiff 在你解决完所有冲突和测试过后, git add 变化了的(changed)文件, 然后用git rebase --continue 继续 rebase。\n(my-branch)$ git add README.md (my-branch)$ git rebase --continue 如果在解决完所有的冲突过后，得到了与提交前一样的结果, 可以执行git rebase --skip。\n任何时候你想结束整个 rebase 过程，回来 rebase 前的分支状态, 你可以做:\n(my-branch)$ git rebase --abort 查看信息 # 显示工作路径下已修改的文件：git status\n显示与上次提交版本文件的不同：git diff\n显示提交历史：\n# 从最新提交开始，显示所有的提交记录（显示hash， 作者信息，提交的标题和时间） $ git log # 显示某个用户的所有提交 $ git log --author=\u0026#34;username\u0026#34; # 显示某个文件的所有修改 $ git log -p \u0026lt;file\u0026gt; 显示搜索内容：\n# 从当前目录的所有文件中查找文本内容 $ git grep \u0026#34;Hello\u0026#34; # 在某一版本中搜索文本 $ git grep \u0026#34;Hello\u0026#34; v2.5 其他 # 克隆所有子模块 # git clone --recursive git://github.com/foo/bar.git 如果已经克隆了:\ngit submodule update --init --recursive 已删除补丁(patch) # 如果某人在 GitHub 上给你发了一个 pull request, 但是然后他删除了他自己的原始 fork, 你将没法克隆他们的提交(commit)或使用 git am。在这种情况下, 最好手动的查看他们的提交(commit)，并把它们拷贝到一个本地新分支，然后做提交。\n做完提交后, 再修改作者，参见 变更作者。 然后, 应用变化, 再发起一个新的 pull request。\n跟踪文件(Tracking Files) # 我只想改变一个文件名字的大小写，而不修改内容 # (master)$ git mv --force myfile MyFile 我想从 Git 删除一个文件，但保留该文件 # (master)$ git rm --cached log.txt Fork 项目 # GitHub 中 Fork 是 服务端的代码仓库克隆（即 新克隆出来的代码仓库在远程服务端），包含了原来的仓库（即 upstream repository，上游仓库）所有内容，如分支、Tag、提交。代码托管服务（如 Github、BitBucket）提供了方便的完成 Fork 操作的功能（在仓库页面点一下 Fork 按钮）。这样有了一个你自己的可以自由提交的远程仓库，然后可以通过的 Pull Request 把你的提交贡献回 原仓库。而对于原仓库 Owner 来说，鼓励别人 Fork 自己的仓库，通过 Pull Request 给自己的仓库做贡献，也能提高了自己仓库的知名度。\n参考： Fork a repo\n（1）执行 git remote -v，您将看到当前为 fork 配置的远程存储库。\n（2）添加上游项目的仓库地址\ngit remote add upstream \u0026lt;github仓库地址\u0026gt; （3）确认是否添加成功，再次键入 git remote -v。\n（4）获取上游项目更新，可以执行 git fetch upstream\n（5）同步上游项目的代码到新仓库\n# merge git merge upstream/master # rebase git rebase upstream/master origin/master 我不知道我做错了些什么 # 你把事情搞砸了：你 重置(reset) 了一些东西, 或者你合并了错误的分支, 亦或你强推了后找不到你自己的提交(commit)了。有些时候, 你一直都做得很好, 但你想回到以前的某个状态。\n这就是 git reflog 的目的， reflog 记录对分支顶端(the tip of a branch)的任何改变, 即使那个顶端没有被任何分支或标签引用。基本上, 每次 HEAD 的改变, 一条新的记录就会增加到reflog。遗憾的是，这只对本地分支起作用，且它只跟踪动作 (例如，不会跟踪一个没有被记录的文件的任何改变)。\n(master)$ git reflog 0a2e358 HEAD@{0}: reset: moving to HEAD\\~2 0254ea7 HEAD@{1}: checkout: moving from 2.2 to master c10f740 HEAD@{2}: checkout: moving from master to 2.2 上面的 reflog 展示了从 master 分支签出(checkout)到 2.2 分支，然后再签回。 那里，还有一个硬重置(hard reset)到一个较旧的提交。最新的动作出现在最上面以 HEAD@{0}标识.\n如果事实证明你不小心回移(move back)了提交(commit), reflog 会包含你不小心回移前 master 上指向的提交(0254ea7)。\ngit reset --hard 0254ea7 然后使用 git reset 就可以把 master 改回到之前的 commit，这提供了一个在历史被意外更改情况下的安全网。\n📚 资料 # 官方资源 Git 官网 Git Github Github 官方教程 模板 gitignore 模板 - .gitignore 文件模板 gitattributes 模板 - .gitattributes 文件模板 github-cheat-sheet - git 命令简略图表 Git 教程 Learn Git branching - 交互式教程 Git 官方推荐教程 - Scott Chacon 的 Git 书。 git-flight-rules git-tips Git 中文教程 廖雪峰的 Git 教程 有关 git 的学习资源 文章 Git Cookbook Git 奇技淫巧 Git 风格指南 Git 在团队中的最佳实践\u0026ndash;如何正确使用 Git Flow Commit message 和 Change log 编写指南 Git 工具 guis - Git 官网展示的客户端工具列表。 gogs - 极易搭建的自助 Git 服务。 gitflow - 应用 fit-flow 模型的工具。 firstaidgit.io 一个可搜索的最常被问到的 Git 的问题 git-extra-commands - 一堆有用的额外的 Git 脚本 git-extras - GIT 工具集 \u0026ndash; repo summary, repl, changelog population, author commit percentages and more git-fire - git-fire 是一个 Git 插件，用于帮助在紧急情况下添加所有当前文件, 做提交(committing), 和推(push)到一个新分支(阻止合并冲突)。 git-tips - Git 小提示 git-town - 通用，高级 Git 工作流支持！ GUI 客户端 GitKraken - 豪华的 Git 客户端 Windows, Mac \u0026amp; Linux git-cola - 另外一个 Git 客户端 Windows \u0026amp; OS X GitUp - 一个新的 Git 客户端，在处理 Git 的复杂性上有自己的特点 gitx-dev - 图形化的 Git 客户端 OS X Source Tree - 免费的图形化 Git 客户端 Windows \u0026amp; OS X Tower - 图形化 Git 客户端 OS X(付费) git cheat sheet github-git-cheat-sheet ","date":"19 October 2023","permalink":"/posts/devoops/git/git-tutorial/","section":"博客","summary":"Git 帮助手册","title":"Git Tutorial"},{"content":"","date":"19 October 2023","permalink":"/tags/shell/","section":"Tags","summary":"","title":"Shell"},{"content":" 由于 bash 是 Linux 标准默认的 shell 解释器，可以说 bash 是 shell 编程的基础。\n本文主要介绍 bash 的语法，对于 linux 指令不做任何介绍。\n███████╗██╗ ██╗███████╗██╗ ██╗ ██╔════╝██║ ██║██╔════╝██║ ██║ ███████╗███████║█████╗ ██║ ██║ ╚════██║██╔══██║██╔══╝ ██║ ██║ ███████║██║ ██║███████╗███████╗███████╗ 简介 # 什么是 shell # Shell 是一个用 C 语言编写的程序，它是用户使用 Linux 的桥梁。 Shell 既是一种命令语言，又是一种程序设计语言。 Shell 是指一种应用程序，这个应用程序提供了一个界面，用户通过这个界面访问 Linux 内核的服务。 Ken Thompson 的 sh 是第一种 Unix Shell，Windows Explorer 是一个典型的图形界面 Shell。\n什么是 shell 脚本 # Shell 脚本（shell script），是一种为 shell 编写的脚本程序，一般文件后缀为 .sh。\n业界所说的 shell 通常都是指 shell 脚本，但 shell 和 shell script 是两个不同的概念。\nShell 环境 # Shell 编程跟 java、php 编程一样，只要有一个能编写代码的文本编辑器和一个能解释执行的脚本解释器就可以了。\nShell 的解释器种类众多，常见的有：\nsh - 即 Bourne Shell。sh 是 Unix 标准默认的 shell。 bash - 即 Bourne Again Shell。bash 是 Linux 标准默认的 shell。 fish - 智能和用户友好的命令行 shell。 xiki - 使 shell 控制台更友好，更强大。 zsh - 功能强大的 shell 与脚本语言。 指定脚本解释器 # 在 shell 脚本，#! 告诉系统其后路径所指定的程序即是解释此脚本文件的 Shell 解释器。#! 被称作 shebang（也称为 Hashbang ）。\n所以，你应该会在 shell 中，见到诸如以下的注释：\n指定 sh 解释器 #!/bin/sh 指定 bash 解释器 #!/bin/bash 注意\n上面的指定解释器的方式是比较常见的，但有时候，你可能也会看到下面的方式：\n#!/usr/bin/env bash 这样做的好处是，系统会自动在 PATH 环境变量中查找你指定的程序（本例中的bash）。相比第一种写法，你应该尽量用这种写法，因为程序的路径是不确定的。这样写还有一个好处，操作系统的PATH变量有可能被配置为指向程序的另一个版本。比如，安装完新版本的bash，我们可能将其路径添加到PATH中，来\u0026quot;隐藏\u0026quot;老版本。如果直接用#!/bin/bash，那么系统会选择老版本的bash来执行脚本，如果用#!/usr/bin/env bash，则会使用新版本。\n模式 # shell 有交互和非交互两种模式。\n交互模式 # 简单来说，你可以将 shell 的交互模式理解为执行命令行。\n看到形如下面的东西，说明 shell 处于交互模式下：\nuser@host:~$ 接着，便可以输入一系列 Linux 命令，比如 ls，grep，cd，mkdir，rm 等等。\n非交互模式 # 简单来说，你可以将 shell 的非交互模式理解为执行 shell 脚本。\n在非交互模式下，shell 从文件或者管道中读取命令并执行。\n当 shell 解释器执行完文件中的最后一个命令，shell 进程终止，并回到父进程。\n可以使用下面的命令让 shell 以非交互模式运行：\nsh /path/to/script.sh bash /path/to/script.sh source /path/to/script.sh ./path/to/script.sh 上面的例子中，script.sh是一个包含 shell 解释器可以识别并执行的命令的普通文本文件，sh和bash是 shell 解释器程序。你可以使用任何喜欢的编辑器创建script.sh（vim，nano，Sublime Text, Atom 等等）。\n其中，source /path/to/script.sh 和 ./path/to/script.sh 是等价的。\n除此之外，你还可以通过chmod命令给文件添加可执行的权限，来直接执行脚本文件：\nchmod +x /path/to/script.sh #使脚本具有执行权限 /path/to/test.sh 这种方式要求脚本文件的第一行必须指明运行该脚本的程序，比如：\n💻 『示例源码』\n#!/usr/bin/env bash echo \u0026#34;Hello, world!\u0026#34; 上面的例子中，我们使用了一个很有用的命令echo来输出字符串到屏幕上。\n基本语法 # 解释器 # 前面虽然两次提到了#! ，但是本着重要的事情说三遍的精神，这里再强调一遍：\n在 shell 脚本，#! 告诉系统其后路径所指定的程序即是解释此脚本文件的 Shell 解释器。#! 被称作 shebang（也称为 Hashbang ）。\n#! 决定了脚本可以像一个独立的可执行文件一样执行，而不用在终端之前输入sh, bash, python, php等。\n# 以下两种方式都可以指定 shell 解释器为 bash，第二种方式更好 #!/bin/bash #!/usr/bin/env bash 注释 # 注释可以说明你的代码是什么作用，以及为什么这样写。\nshell 语法中，注释是特殊的语句，会被 shell 解释器忽略。\n单行注释 - 以 # 开头，到行尾结束。 多行注释 - 以 :\u0026lt;\u0026lt;EOF 开头，到 EOF 结束。 💻 『示例源码』\n#-------------------------------------------- # shell 注释示例 # author：zp #-------------------------------------------- # echo \u0026#39;这是单行注释\u0026#39; ########## 这是分割线 ########## :\u0026lt;\u0026lt;EOF echo \u0026#39;这是多行注释\u0026#39; echo \u0026#39;这是多行注释\u0026#39; echo \u0026#39;这是多行注释\u0026#39; EOF echo # echo 用于字符串的输出。\n输出普通字符串：\necho \u0026#34;hello, world\u0026#34; # Output: hello, world 输出含变量的字符串：\necho \u0026#34;hello, \\\u0026#34;zp\\\u0026#34;\u0026#34; # Output: hello, \u0026#34;zp\u0026#34; 输出含变量的字符串：\nname=zp echo \u0026#34;hello, \\\u0026#34;${name}\\\u0026#34;\u0026#34; # Output: hello, \u0026#34;zp\u0026#34; 输出含换行符的字符串：\n# 输出含换行符的字符串 echo \u0026#34;YES\\nNO\u0026#34; # Output: YES\\nNO echo -e \u0026#34;YES\\nNO\u0026#34; # -e 开启转义 # Output: # YES # NO 输出含不换行符的字符串：\necho \u0026#34;YES\u0026#34; echo \u0026#34;NO\u0026#34; # Output: # YES # NO echo -e \u0026#34;YES\\c\u0026#34; # -e 开启转义 \\c 不换行 echo \u0026#34;NO\u0026#34; # Output: # YESNO 输出重定向至文件\necho \u0026#34;test\u0026#34; \u0026gt; test.txt 输出执行结果\necho `pwd` # Output:(当前目录路径) 💻 『示例源码』\n#!/usr/bin/env bash # 输出普通字符串 echo \u0026#34;hello, world\u0026#34; # Output: hello, world # 输出含变量的字符串 echo \u0026#34;hello, \\\u0026#34;zp\\\u0026#34;\u0026#34; # Output: hello, \u0026#34;zp\u0026#34; # 输出含变量的字符串 name=zp echo \u0026#34;hello, \\\u0026#34;${name}\\\u0026#34;\u0026#34; # Output: hello, \u0026#34;zp\u0026#34; # 输出含换行符的字符串 echo \u0026#34;YES\\nNO\u0026#34; # Output: YES\\nNO echo -e \u0026#34;YES\\nNO\u0026#34; # -e 开启转义 # Output: # YES # NO # 输出含不换行符的字符串 echo \u0026#34;YES\u0026#34; echo \u0026#34;NO\u0026#34; # Output: # YES # NO echo -e \u0026#34;YES\\c\u0026#34; # -e 开启转义 \\c 不换行 echo \u0026#34;NO\u0026#34; # Output: # YESNO # 输出内容定向至文件 echo \u0026#34;test\u0026#34; \u0026gt; test.txt # 输出执行结果 echo `pwd` # Output:(当前目录路径) printf # printf 用于格式化输出字符串。\n默认，printf 不会像 echo 一样自动添加换行符，如果需要换行可以手动添加 \\n。\n💻 『示例源码』\n# 单引号 printf \u0026#39;%d %s\\n\u0026#39; 1 \u0026#34;abc\u0026#34; # Output:1 abc # 双引号 printf \u0026#34;%d %s\\n\u0026#34; 1 \u0026#34;abc\u0026#34; # Output:1 abc # 无引号 printf %s abcdef # Output: abcdef(并不会换行) # 格式只指定了一个参数，但多出的参数仍然会按照该格式输出 printf \u0026#34;%s\\n\u0026#34; abc def # Output: # abc # def printf \u0026#34;%s %s %s\\n\u0026#34; a b c d e f g h i j # Output: # a b c # d e f # g h i # j # 如果没有参数，那么 %s 用 NULL 代替，%d 用 0 代替 printf \u0026#34;%s and %d \\n\u0026#34; # Output: # and 0 # 格式化输出 printf \u0026#34;%-10s %-8s %-4s\\n\u0026#34; 姓名 性别 体重kg printf \u0026#34;%-10s %-8s %-4.2f\\n\u0026#34; 郭靖 男 66.1234 printf \u0026#34;%-10s %-8s %-4.2f\\n\u0026#34; 杨过 男 48.6543 printf \u0026#34;%-10s %-8s %-4.2f\\n\u0026#34; 郭芙 女 47.9876 # Output: # 姓名 性别 体重kg # 郭靖 男 66.12 # 杨过 男 48.65 # 郭芙 女 47.99 printf 的转义符 # 序列 说明 \\a 警告字符，通常为 ASCII 的 BEL 字符 \\b 后退 \\c 抑制（不显示）输出结果中任何结尾的换行字符（只在%b 格式指示符控制下的参数字符串中有效），而且，任何留在参数里的字符、任何接下来的参数以及任何留在格式字符串中的字符，都被忽略 \\f 换页（formfeed） \\n 换行 \\r 回车（Carriage return） \\t 水平制表符 \\v 垂直制表符 \\\\ 一个字面上的反斜杠字符 \\ddd 表示 1 到 3 位数八进制值的字符。仅在格式字符串中有效 \\0ddd 表示 1 到 3 位的八进制值字符 变量 # 跟许多程序设计语言一样，你可以在 bash 中创建变量。\nBash 中没有数据类型，bash 中的变量可以保存一个数字、一个字符、一个字符串等等。同时无需提前声明变量，给变量赋值会直接创建变量。\n变量命名原则 # 命名只能使用英文字母，数字和下划线，首个字符不能以数字开头。 中间不能有空格，可以使用下划线（_）。 不能使用标点符号。 不能使用 bash 里的关键字（可用 help 命令查看保留关键字）。 声明变量 # 访问变量的语法形式为：${var} 和 $var 。\n变量名外面的花括号是可选的，加不加都行，加花括号是为了帮助解释器识别变量的边界，所以推荐加花括号。\nword=\u0026#34;hello\u0026#34; echo ${word} # Output: hello 只读变量 # 使用 readonly 命令可以将变量定义为只读变量，只读变量的值不能被改变。\nrword=\u0026#34;hello\u0026#34; echo ${rword} readonly rword # rword=\u0026#34;bye\u0026#34; # 如果放开注释，执行时会报错 删除变量 # 使用 unset 命令可以删除变量。变量被删除后不能再次使用。unset 命令不能删除只读变量。\ndword=\u0026#34;hello\u0026#34; # 声明变量 echo ${dword} # 输出变量值 # Output: hello unset dword # 删除变量 echo ${dword} # Output: （空） 变量类型 # 局部变量 - 局部变量是仅在某个脚本内部有效的变量。它们不能被其他的程序和脚本访问。 环境变量 - 环境变量是对当前 shell 会话内所有的程序或脚本都可见的变量。创建它们跟创建局部变量类似，但使用的是 export 关键字，shell 脚本也可以定义环境变量。 常见的环境变量：\n变量 描述 $HOME 当前用户的用户目录 $PATH 用分号分隔的目录列表，shell 会到这些目录中查找命令 $PWD 当前工作目录 $RANDOM 0 到 32767 之间的整数 $UID 数值类型，当前用户的用户 ID $PS1 主要系统输入提示符 $PS2 次要系统输入提示符 这里 有一张更全面的 Bash 环境变量列表。\n💻 『示例源码』\n#!/usr/bin/env bash ################### 声明变量 ################### name=\u0026#34;world\u0026#34; echo \u0026#34;hello ${name}\u0026#34; # Output: hello world ################### 输出变量 ################### folder=$(pwd) echo \u0026#34;current path: ${folder}\u0026#34; ################### 只读变量 ################### rword=\u0026#34;hello\u0026#34; echo ${rword} # Output: hello readonly rword # rword=\u0026#34;bye\u0026#34; # 如果放开注释，执行时会报错 ################### 删除变量 ################### dword=\u0026#34;hello\u0026#34; # 声明变量 echo ${dword} # 输出变量值 # Output: hello unset dword # 删除变量 echo ${dword} # Output: （空） ################### 系统变量 ################### echo \u0026#34;UID:$UID\u0026#34; echo LOGNAME:$LOGNAME echo User:$USER echo HOME:$HOME echo PATH:$PATH echo HOSTNAME:$HOSTNAME echo SHELL:$SHELL echo LANG:$LANG ################### 自定义变量 ################### days=10 user=\u0026#34;admin\u0026#34; echo \u0026#34;$user logged in $days days age\u0026#34; days=5 user=\u0026#34;root\u0026#34; echo \u0026#34;$user logged in $days days age\u0026#34; # Output: # admin logged in 10 days age # root logged in 5 days age ################### 从变量读取列表 ################### colors=\u0026#34;Red Yellow Blue\u0026#34; colors=$colors\u0026#34; White Black\u0026#34; for color in $colors do echo \u0026#34; $color\u0026#34; done 字符串 # 单引号和双引号 # shell 字符串可以用单引号 ''，也可以用双引号 “”，也可以不用引号。\n单引号的特点 单引号里不识别变量 单引号里不能出现单独的单引号（使用转义符也不行），但可成对出现，作为字符串拼接使用。 双引号的特点 双引号里识别变量 双引号里可以出现转义字符 综上，推荐使用双引号。\n拼接字符串 # # 使用单引号拼接 name1=\u0026#39;white\u0026#39; str1=\u0026#39;hello, \u0026#39;${name1}\u0026#39;\u0026#39; str2=\u0026#39;hello, ${name1}\u0026#39; echo ${str1}_${str2} # Output: # hello, white_hello, ${name1} # 使用双引号拼接 name2=\u0026#34;black\u0026#34; str3=\u0026#34;hello, \u0026#34;${name2}\u0026#34;\u0026#34; str4=\u0026#34;hello, ${name2}\u0026#34; echo ${str3}_${str4} # Output: # hello, black_hello, black 获取字符串长度 # text=\u0026#34;12345\u0026#34; echo ${#text} # Output: # 5 截取子字符串 # text=\u0026#34;12345\u0026#34; echo ${text:2:2} # Output: # 34 从第 3 个字符开始，截取 2 个字符\n查找子字符串 # #!/usr/bin/env bash text=\u0026#34;hello\u0026#34; echo `expr index \u0026#34;${text}\u0026#34; ll` # Execute: ./str-demo5.sh # Output: # 3 查找 ll 子字符在 hello 字符串中的起始位置。\n💻 『示例源码』\n#!/usr/bin/env bash ################### 使用单引号拼接字符串 ################### name1=\u0026#39;white\u0026#39; str1=\u0026#39;hello, \u0026#39;${name1}\u0026#39;\u0026#39; str2=\u0026#39;hello, ${name1}\u0026#39; echo ${str1}_${str2} # Output: # hello, white_hello, ${name1} ################### 使用双引号拼接字符串 ################### name2=\u0026#34;black\u0026#34; str3=\u0026#34;hello, \u0026#34;${name2}\u0026#34;\u0026#34; str4=\u0026#34;hello, ${name2}\u0026#34; echo ${str3}_${str4} # Output: # hello, black_hello, black ################### 获取字符串长度 ################### text=\u0026#34;12345\u0026#34; echo \u0026#34;${text} length is: ${#text}\u0026#34; # Output: # 12345 length is: 5 # 获取子字符串 text=\u0026#34;12345\u0026#34; echo ${text:2:2} # Output: # 34 ################### 查找子字符串 ################### text=\u0026#34;hello\u0026#34; echo `expr index \u0026#34;${text}\u0026#34; ll` # Output: # 3 ################### 判断字符串中是否包含子字符串 ################### result=$(echo \u0026#34;${str}\u0026#34; | grep \u0026#34;feature/\u0026#34;) if [[ \u0026#34;$result\u0026#34; != \u0026#34;\u0026#34; ]]; then echo \u0026#34;feature/ 是 ${str} 的子字符串\u0026#34; else echo \u0026#34;feature/ 不是 ${str} 的子字符串\u0026#34; fi ################### 截取关键字左边内容 ################### full_branch=\u0026#34;feature/1.0.0\u0026#34; branch=`echo ${full_branch#feature/}` echo \u0026#34;branch is ${branch}\u0026#34; ################### 截取关键字右边内容 ################### full_version=\u0026#34;0.0.1-SNAPSHOT\u0026#34; version=`echo ${full_version%-SNAPSHOT}` echo \u0026#34;version is ${version}\u0026#34; ################### 字符串分割成数组 ################### str=\u0026#34;0.0.0.1\u0026#34; OLD_IFS=\u0026#34;$IFS\u0026#34; IFS=\u0026#34;.\u0026#34; array=( ${str} ) IFS=\u0026#34;$OLD_IFS\u0026#34; size=${#array[*]} lastIndex=`expr ${size} - 1` echo \u0026#34;数组长度：${size}\u0026#34; echo \u0026#34;最后一个数组元素：${array[${lastIndex}]}\u0026#34; for item in ${array[@]} do echo \u0026#34;$item\u0026#34; done ################### 判断字符串是否为空 ################### #-n 判断长度是否非零 #-z 判断长度是否为零 str=testing str2=\u0026#39;\u0026#39; if [[ -n \u0026#34;$str\u0026#34; ]] then echo \u0026#34;The string $str is not empty\u0026#34; else echo \u0026#34;The string $str is empty\u0026#34; fi if [[ -n \u0026#34;$str2\u0026#34; ]] then echo \u0026#34;The string $str2 is not empty\u0026#34; else echo \u0026#34;The string $str2 is empty\u0026#34; fi #\tOutput: #\tThe string testing is not empty #\tThe string is empty ################### 字符串比较 ################### str=hello str2=world if [[ $str = \u0026#34;hello\u0026#34; ]]; then echo \u0026#34;str equals hello\u0026#34; else echo \u0026#34;str not equals hello\u0026#34; fi if [[ $str2 = \u0026#34;hello\u0026#34; ]]; then echo \u0026#34;str2 equals hello\u0026#34; else echo \u0026#34;str2 not equals hello\u0026#34; fi 数组 # bash 只支持一维数组。\n数组下标从 0 开始，下标可以是整数或算术表达式，其值应大于或等于 0。\n创建数组 # # 创建数组的不同方式 nums=([2]=2 [0]=0 [1]=1) colors=(red yellow \u0026#34;dark blue\u0026#34;) 访问数组元素 # 访问数组的单个元素： echo ${nums[1]} # Output: 1 访问数组的所有元素： echo ${colors[*]} # Output: red yellow dark blue echo ${colors[@]} # Output: red yellow dark blue 上面两行有很重要（也很微妙）的区别：\n为了将数组中每个元素单独一行输出，我们用 printf 命令：\nprintf \u0026#34;+ %s\\n\u0026#34; ${colors[*]} # Output: # + red # + yellow # + dark # + blue 为什么dark和blue各占了一行？尝试用引号包起来：\nprintf \u0026#34;+ %s\\n\u0026#34; \u0026#34;${colors[*]}\u0026#34; # Output: # + red yellow dark blue 现在所有的元素都在一行输出 —— 这不是我们想要的！让我们试试${colors[@]}\nprintf \u0026#34;+ %s\\n\u0026#34; \u0026#34;${colors[@]}\u0026#34; # Output: # + red # + yellow # + dark blue 在引号内，${colors[@]}将数组中的每个元素扩展为一个单独的参数；数组元素中的空格得以保留。\n访问数组的部分元素： echo ${nums[@]:0:2} # Output: # 0 1 在上面的例子中，${array[@]} 扩展为整个数组，:0:2取出了数组中从 0 开始，长度为 2 的元素。\n访问数组长度 # echo ${#nums[*]} # Output: # 3 向数组中添加元素 # 向数组中添加元素也非常简单：\ncolors=(white \u0026#34;${colors[@]}\u0026#34; green black) echo ${colors[@]} # Output: # white red yellow dark blue green black 上面的例子中，${colors[@]} 扩展为整个数组，并被置换到复合赋值语句中，接着，对数组colors的赋值覆盖了它原来的值。\n从数组中删除元素 # 用unset命令来从数组中删除一个元素：\nunset nums[0] echo ${nums[@]} # Output: # 1 2 💻 『示例源码』\n#!/usr/bin/env bash ################### 创建数组 ################### nums=( [ 2 ] = 2 [ 0 ] = 0 [ 1 ] = 1 ) colors=( red yellow \u0026#34;dark blue\u0026#34; ) ################### 访问数组的单个元素 ################### echo ${nums[1]} # Output: 1 ################### 访问数组的所有元素 ################### echo ${colors[*]} # Output: red yellow dark blue echo ${colors[@]} # Output: red yellow dark blue printf \u0026#34;+ %s\\n\u0026#34; ${colors[*]} # Output: # + red # + yellow # + dark # + blue printf \u0026#34;+ %s\\n\u0026#34; \u0026#34;${colors[*]}\u0026#34; # Output: # + red yellow dark blue printf \u0026#34;+ %s\\n\u0026#34; \u0026#34;${colors[@]}\u0026#34; # Output: # + red # + yellow # + dark blue ################### 访问数组的部分元素 ################### echo ${nums[@]:0:2} # Output: # 0 1 ################### 获取数组长度 ################### echo ${#nums[*]} # Output: # 3 ################### 向数组中添加元素 ################### colors=( white \u0026#34;${colors[@]}\u0026#34; green black ) echo ${colors[@]} # Output: # white red yellow dark blue green black ################### 从数组中删除元素 ################### unset nums[ 0 ] echo ${nums[@]} # Output: # 1 2 运算符 # 算术运算符 # 下表列出了常用的算术运算符，假定变量 x 为 10，变量 y 为 20：\n运算符 说明 举例 + 加法 expr $x + $y 结果为 30。 - 减法 expr $x - $y 结果为 -10。 * 乘法 expr $x * $y 结果为 200。 / 除法 expr $y / $x 结果为 2。 % 取余 expr $y % $x 结果为 0。 = 赋值 x=$y 将把变量 y 的值赋给 x。 == 相等。用于比较两个数字，相同则返回 true。 [ $x == $y ] 返回 false。 != 不相等。用于比较两个数字，不相同则返回 true。 [ $x != $y ] 返回 true。 **注意：**条件表达式要放在方括号之间，并且要有空格，例如: [$x==$y] 是错误的，必须写成 [ $x == $y ]。\n💻 『示例源码』\nx=10 y=20 echo \u0026#34;x=${x}, y=${y}\u0026#34; val=`expr ${x} + ${y}` echo \u0026#34;${x} + ${y} = $val\u0026#34; val=`expr ${x} - ${y}` echo \u0026#34;${x} - ${y} = $val\u0026#34; val=`expr ${x} \\* ${y}` echo \u0026#34;${x} * ${y} = $val\u0026#34; val=`expr ${y} / ${x}` echo \u0026#34;${y} / ${x} = $val\u0026#34; val=`expr ${y} % ${x}` echo \u0026#34;${y} % ${x} = $val\u0026#34; if [[ ${x} == ${y} ]] then echo \u0026#34;${x} = ${y}\u0026#34; fi if [[ ${x} != ${y} ]] then echo \u0026#34;${x} != ${y}\u0026#34; fi # Output: # x=10, y=20 # 10 + 20 = 30 # 10 - 20 = -10 # 10 * 20 = 200 # 20 / 10 = 2 # 20 % 10 = 0 # 10 != 20 关系运算符 # 关系运算符只支持数字，不支持字符串，除非字符串的值是数字。\n下表列出了常用的关系运算符，假定变量 x 为 10，变量 y 为 20：\n运算符 说明 举例 -eq 检测两个数是否相等，相等返回 true。 [ $a -eq $b ]返回 false。 -ne 检测两个数是否相等，不相等返回 true。 [ $a -ne $b ] 返回 true。 -gt 检测左边的数是否大于右边的，如果是，则返回 true。 [ $a -gt $b ] 返回 false。 -lt 检测左边的数是否小于右边的，如果是，则返回 true。 [ $a -lt $b ] 返回 true。 -ge 检测左边的数是否大于等于右边的，如果是，则返回 true。 [ $a -ge $b ] 返回 false。 -le 检测左边的数是否小于等于右边的，如果是，则返回 true。 [ $a -le $b ]返回 true。 💻 『示例源码』\nx=10 y=20 echo \u0026#34;x=${x}, y=${y}\u0026#34; if [[ ${x} -eq ${y} ]]; then echo \u0026#34;${x} -eq ${y} : x 等于 y\u0026#34; else echo \u0026#34;${x} -eq ${y}: x 不等于 y\u0026#34; fi if [[ ${x} -ne ${y} ]]; then echo \u0026#34;${x} -ne ${y}: x 不等于 y\u0026#34; else echo \u0026#34;${x} -ne ${y}: x 等于 y\u0026#34; fi if [[ ${x} -gt ${y} ]]; then echo \u0026#34;${x} -gt ${y}: x 大于 y\u0026#34; else echo \u0026#34;${x} -gt ${y}: x 不大于 y\u0026#34; fi if [[ ${x} -lt ${y} ]]; then echo \u0026#34;${x} -lt ${y}: x 小于 y\u0026#34; else echo \u0026#34;${x} -lt ${y}: x 不小于 y\u0026#34; fi if [[ ${x} -ge ${y} ]]; then echo \u0026#34;${x} -ge ${y}: x 大于或等于 y\u0026#34; else echo \u0026#34;${x} -ge ${y}: x 小于 y\u0026#34; fi if [[ ${x} -le ${y} ]]; then echo \u0026#34;${x} -le ${y}: x 小于或等于 y\u0026#34; else echo \u0026#34;${x} -le ${y}: x 大于 y\u0026#34; fi # Output: # x=10, y=20 # 10 -eq 20: x 不等于 y # 10 -ne 20: x 不等于 y # 10 -gt 20: x 不大于 y # 10 -lt 20: x 小于 y # 10 -ge 20: x 小于 y # 10 -le 20: x 小于或等于 y 布尔运算符 # 下表列出了常用的布尔运算符，假定变量 x 为 10，变量 y 为 20：\n运算符 说明 举例 ! 非运算，表达式为 true 则返回 false，否则返回 true。 [ ! false ] 返回 true。 -o 或运算，有一个表达式为 true 则返回 true。 [ $a -lt 20 -o $b -gt 100 ] 返回 true。 -a 与运算，两个表达式都为 true 才返回 true。 [ $a -lt 20 -a $b -gt 100 ] 返回 false。 💻 『示例源码』\nx=10 y=20 echo \u0026#34;x=${x}, y=${y}\u0026#34; if [[ ${x} != ${y} ]]; then echo \u0026#34;${x} != ${y} : x 不等于 y\u0026#34; else echo \u0026#34;${x} != ${y}: x 等于 y\u0026#34; fi if [[ ${x} -lt 100 \u0026amp;\u0026amp; ${y} -gt 15 ]]; then echo \u0026#34;${x} 小于 100 且 ${y} 大于 15 : 返回 true\u0026#34; else echo \u0026#34;${x} 小于 100 且 ${y} 大于 15 : 返回 false\u0026#34; fi if [[ ${x} -lt 100 || ${y} -gt 100 ]]; then echo \u0026#34;${x} 小于 100 或 ${y} 大于 100 : 返回 true\u0026#34; else echo \u0026#34;${x} 小于 100 或 ${y} 大于 100 : 返回 false\u0026#34; fi if [[ ${x} -lt 5 || ${y} -gt 100 ]]; then echo \u0026#34;${x} 小于 5 或 ${y} 大于 100 : 返回 true\u0026#34; else echo \u0026#34;${x} 小于 5 或 ${y} 大于 100 : 返回 false\u0026#34; fi # Output: # x=10, y=20 # 10 != 20 : x 不等于 y # 10 小于 100 且 20 大于 15 : 返回 true # 10 小于 100 或 20 大于 100 : 返回 true # 10 小于 5 或 20 大于 100 : 返回 false 逻辑运算符 # 以下介绍 Shell 的逻辑运算符，假定变量 x 为 10，变量 y 为 20:\n运算符 说明 举例 \u0026amp;\u0026amp; 逻辑的 AND [[ ${x} -lt 100 \u0026amp;\u0026amp; ${y} -gt 100 ]] 返回 false ` ` 💻 『示例源码』\nx=10 y=20 echo \u0026#34;x=${x}, y=${y}\u0026#34; if [[ ${x} -lt 100 \u0026amp;\u0026amp; ${y} -gt 100 ]] then echo \u0026#34;${x} -lt 100 \u0026amp;\u0026amp; ${y} -gt 100 返回 true\u0026#34; else echo \u0026#34;${x} -lt 100 \u0026amp;\u0026amp; ${y} -gt 100 返回 false\u0026#34; fi if [[ ${x} -lt 100 || ${y} -gt 100 ]] then echo \u0026#34;${x} -lt 100 || ${y} -gt 100 返回 true\u0026#34; else echo \u0026#34;${x} -lt 100 || ${y} -gt 100 返回 false\u0026#34; fi # Output: # x=10, y=20 # 10 -lt 100 \u0026amp;\u0026amp; 20 -gt 100 返回 false # 10 -lt 100 || 20 -gt 100 返回 true 字符串运算符 # 下表列出了常用的字符串运算符，假定变量 a 为 \u0026ldquo;abc\u0026rdquo;，变量 b 为 \u0026ldquo;efg\u0026rdquo;：\n运算符 说明 举例 = 检测两个字符串是否相等，相等返回 true。 [ $a = $b ] 返回 false。 != 检测两个字符串是否相等，不相等返回 true。 [ $a != $b ] 返回 true。 -z 检测字符串长度是否为 0，为 0 返回 true。 [ -z $a ] 返回 false。 -n 检测字符串长度是否为 0，不为 0 返回 true。 [ -n $a ] 返回 true。 str 检测字符串是否为空，不为空返回 true。 [ $a ] 返回 true。 💻 『示例源码』\nx=\u0026#34;abc\u0026#34; y=\u0026#34;xyz\u0026#34; echo \u0026#34;x=${x}, y=${y}\u0026#34; if [[ ${x} = ${y} ]]; then echo \u0026#34;${x} = ${y} : x 等于 y\u0026#34; else echo \u0026#34;${x} = ${y}: x 不等于 y\u0026#34; fi if [[ ${x} != ${y} ]]; then echo \u0026#34;${x} != ${y} : x 不等于 y\u0026#34; else echo \u0026#34;${x} != ${y}: x 等于 y\u0026#34; fi if [[ -z ${x} ]]; then echo \u0026#34;-z ${x} : 字符串长度为 0\u0026#34; else echo \u0026#34;-z ${x} : 字符串长度不为 0\u0026#34; fi if [[ -n \u0026#34;${x}\u0026#34; ]]; then echo \u0026#34;-n ${x} : 字符串长度不为 0\u0026#34; else echo \u0026#34;-n ${x} : 字符串长度为 0\u0026#34; fi if [[ ${x} ]]; then echo \u0026#34;${x} : 字符串不为空\u0026#34; else echo \u0026#34;${x} : 字符串为空\u0026#34; fi # Output: # x=abc, y=xyz # abc = xyz: x 不等于 y # abc != xyz : x 不等于 y # -z abc : 字符串长度不为 0 # -n abc : 字符串长度不为 0 # abc : 字符串不为空 文件测试运算符 # 文件测试运算符用于检测 Unix 文件的各种属性。\n属性检测描述如下：\n操作符 说明 举例 -b file 检测文件是否是块设备文件，如果是，则返回 true。 [ -b $file ] 返回 false。 -c file 检测文件是否是字符设备文件，如果是，则返回 true。 [ -c $file ] 返回 false。 -d file 检测文件是否是目录，如果是，则返回 true。 [ -d $file ] 返回 false。 -f file 检测文件是否是普通文件（既不是目录，也不是设备文件），如果是，则返回 true。 [ -f $file ] 返回 true。 -g file 检测文件是否设置了 SGID 位，如果是，则返回 true。 [ -g $file ] 返回 false。 -k file 检测文件是否设置了粘着位(Sticky Bit)，如果是，则返回 true。 [ -k $file ]返回 false。 -p file 检测文件是否是有名管道，如果是，则返回 true。 [ -p $file ] 返回 false。 -u file 检测文件是否设置了 SUID 位，如果是，则返回 true。 [ -u $file ] 返回 false。 -r file 检测文件是否可读，如果是，则返回 true。 [ -r $file ] 返回 true。 -w file 检测文件是否可写，如果是，则返回 true。 [ -w $file ] 返回 true。 -x file 检测文件是否可执行，如果是，则返回 true。 [ -x $file ] 返回 true。 -s file 检测文件是否为空（文件大小是否大于 0），不为空返回 true。 [ -s $file ] 返回 true。 -e file 检测文件（包括目录）是否存在，如果是，则返回 true。 [ -e $file ] 返回 true。 💻 『示例源码』\nfile=\u0026#34;/etc/hosts\u0026#34; if [[ -r ${file} ]]; then echo \u0026#34;${file} 文件可读\u0026#34; else echo \u0026#34;${file} 文件不可读\u0026#34; fi if [[ -w ${file} ]]; then echo \u0026#34;${file} 文件可写\u0026#34; else echo \u0026#34;${file} 文件不可写\u0026#34; fi if [[ -x ${file} ]]; then echo \u0026#34;${file} 文件可执行\u0026#34; else echo \u0026#34;${file} 文件不可执行\u0026#34; fi if [[ -f ${file} ]]; then echo \u0026#34;${file} 文件为普通文件\u0026#34; else echo \u0026#34;${file} 文件为特殊文件\u0026#34; fi if [[ -d ${file} ]]; then echo \u0026#34;${file} 文件是个目录\u0026#34; else echo \u0026#34;${file} 文件不是个目录\u0026#34; fi if [[ -s ${file} ]]; then echo \u0026#34;${file} 文件不为空\u0026#34; else echo \u0026#34;${file} 文件为空\u0026#34; fi if [[ -e ${file} ]]; then echo \u0026#34;${file} 文件存在\u0026#34; else echo \u0026#34;${file} 文件不存在\u0026#34; fi # Output:(根据文件的实际情况，输出结果可能不同) # /etc/hosts 文件可读 # /etc/hosts 文件可写 # /etc/hosts 文件不可执行 # /etc/hosts 文件为普通文件 # /etc/hosts 文件不是个目录 # /etc/hosts 文件不为空 # /etc/hosts 文件存在 控制语句 # 条件语句 # 跟其它程序设计语言一样，Bash 中的条件语句让我们可以决定一个操作是否被执行。结果取决于一个包在 $[[ ]]$ 里的表达式。\n由 $[[ ]]$ （sh中是[ ]）包起来的表达式被称作 检测命令 或 基元。这些表达式帮助我们检测一个条件的结果。这里可以找到有关 bash 中单双中括号区别的答案。\n共有两个不同的条件表达式：if和case。\nif # （1）if 语句\nif在使用上跟其它语言相同。如果中括号里的表达式为真，那么then和fi之间的代码会被执行。fi标志着条件代码块的结束。\n# 写成一行 if [[ 1 -eq 1 ]]; then echo \u0026#34;1 -eq 1 result is: true\u0026#34;; fi # Output: 1 -eq 1 result is: true # 写成多行 if [[ \u0026#34;abc\u0026#34; -eq \u0026#34;abc\u0026#34; ]] then echo \u0026#34;\u0026#34;abc\u0026#34; -eq \u0026#34;abc\u0026#34; result is: true\u0026#34; fi # Output: abc -eq abc result is: true （2）if else 语句\n同样，我们可以使用if..else语句，例如：\nif [[ 2 -ne 1 ]]; then echo \u0026#34;true\u0026#34; else echo \u0026#34;false\u0026#34; fi # Output: true （3）if elif else 语句\n有些时候，if..else不能满足我们的要求。别忘了if..elif..else，使用起来也很方便。\n💻 『示例源码』\nx=10 y=20 if [[ ${x} \u0026gt; ${y} ]]; then echo \u0026#34;${x} \u0026gt; ${y}\u0026#34; elif [[ ${x} \u0026lt; ${y} ]]; then echo \u0026#34;${x} \u0026lt; ${y}\u0026#34; else echo \u0026#34;${x} = ${y}\u0026#34; fi # Output: 10 \u0026lt; 20 case # 如果你需要面对很多情况，分别要采取不同的措施，那么使用case会比嵌套的if更有用。使用case来解决复杂的条件判断，看起来像下面这样：\n💻 『示例源码』\nexec case ${oper} in \u0026#34;+\u0026#34;) val=`expr ${x} + ${y}` echo \u0026#34;${x} + ${y} = ${val}\u0026#34; ;; \u0026#34;-\u0026#34;) val=`expr ${x} - ${y}` echo \u0026#34;${x} - ${y} = ${val}\u0026#34; ;; \u0026#34;*\u0026#34;) val=`expr ${x} \\* ${y}` echo \u0026#34;${x} * ${y} = ${val}\u0026#34; ;; \u0026#34;/\u0026#34;) val=`expr ${x} / ${y}` echo \u0026#34;${x} / ${y} = ${val}\u0026#34; ;; *) echo \u0026#34;Unknown oper!\u0026#34; ;; esac 每种情况都是匹配了某个模式的表达式。|用来分割多个模式，)用来结束一个模式序列。第一个匹配上的模式对应的命令将会被执行。*代表任何不匹配以上给定模式的模式。命令块儿之间要用;;分隔。\n循环语句 # 循环其实不足为奇。跟其它程序设计语言一样，bash 中的循环也是只要控制条件为真就一直迭代执行的代码块。\nBash 中有四种循环：for，while，until和select。\nfor循环 # for与它在 C 语言中的姊妹非常像。看起来是这样：\nfor arg in elem1 elem2 ... elemN do ### 语句 done 在每次循环的过程中，arg依次被赋值为从elem1到elemN。这些值还可以是通配符或者 大括号扩展。\n当然，我们还可以把for循环写在一行，但这要求do之前要有一个分号，就像下面这样：\nfor i in {1..5}; do echo $i; done 还有，如果你觉得for..in..do对你来说有点奇怪，那么你也可以像 C 语言那样使用for，比如：\nfor (( i = 0; i \u0026lt; 10; i++ )); do echo $i done 当我们想对一个目录下的所有文件做同样的操作时，for就很方便了。举个例子，如果我们想把所有的.bash文件移动到script文件夹中，并给它们可执行权限，我们的脚本可以这样写：\n💻 『示例源码』\nDIR=/home/zp for FILE in ${DIR}/*.sh; do mv \u0026#34;$FILE\u0026#34; \u0026#34;${DIR}/scripts\u0026#34; done # 将 /home/zp 目录下所有 sh 文件拷贝到 /home/zp/scripts while循环 # while循环检测一个条件，只要这个条件为 真，就执行一段命令。被检测的条件跟if..then中使用的 基元并无二异。因此一个while循环看起来会是这样：\nwhile [[ condition ]] do ### 语句 done 跟for循环一样，如果我们把do和被检测的条件写到一行，那么必须要在do之前加一个分号。\n💻 『示例源码』\n### 0到9之间每个数的平方 x=0 while [[ ${x} -lt 10 ]]; do echo $((x * x)) x=$((x + 1)) done # Output: # 0 # 1 # 4 # 9 # 16 # 25 # 36 # 49 # 64 # 81 until循环 # until循环跟while循环正好相反。它跟while一样也需要检测一个测试条件，但不同的是，只要该条件为 假 就一直执行循环：\n💻 『示例源码』\nx=0 until [[ ${x} -ge 5 ]]; do echo ${x} x=`expr ${x} + 1` done # Output: # 0 # 1 # 2 # 3 # 4 select循环 # select循环帮助我们组织一个用户菜单。它的语法几乎跟for循环一致：\nselect answer in elem1 elem2 ... elemN do ### 语句 done select会打印elem1..elemN以及它们的序列号到屏幕上，之后会提示用户输入。通常看到的是$?（PS3变量）。用户的选择结果会被保存到answer中。如果answer是一个在1..N之间的数字，那么语句会被执行，紧接着会进行下一次迭代 —— 如果不想这样的话我们可以使用break语句。\n💻 『示例源码』\n#!/usr/bin/env bash PS3=\u0026#34;Choose the package manager: \u0026#34; select ITEM in bower npm gem pip do echo -n \u0026#34;Enter the package name: \u0026#34; \u0026amp;\u0026amp; read PACKAGE case ${ITEM} in bower) bower install ${PACKAGE} ;; npm) npm install ${PACKAGE} ;; gem) gem install ${PACKAGE} ;; pip) pip install ${PACKAGE} ;; esac break # 避免无限循环 done 这个例子，先询问用户他想使用什么包管理器。接着，又询问了想安装什么包，最后执行安装操作。\n运行这个脚本，会得到如下输出：\n$ ./my_script 1) bower 2) npm 3) gem 4) pip Choose the package manager: 2 Enter the package name: gitbook-cli break 和 continue # 如果想提前结束一个循环或跳过某次循环执行，可以使用 shell 的break和continue语句来实现。它们可以在任何循环中使用。\nbreak语句用来提前结束当前循环。\ncontinue语句用来跳过某次迭代。\n💻 『示例源码』\n# 查找 10 以内第一个能整除 2 和 3 的正整数 i=1 while [[ ${i} -lt 10 ]]; do if [[ $((i % 3)) -eq 0 ]] \u0026amp;\u0026amp; [[ $((i % 2)) -eq 0 ]]; then echo ${i} break; fi i=`expr ${i} + 1` done # Output: 6 💻 『示例源码』\n# 打印10以内的奇数 for (( i = 0; i \u0026lt; 10; i ++ )); do if [[ $((i % 2)) -eq 0 ]]; then continue; fi echo ${i} done # Output: # 1 # 3 # 5 # 7 # 9 函数 # bash 函数定义语法如下：\n[ function ] funname [()] { action; [return int;] } 💡 说明：\n函数定义时，function 关键字可有可无。 函数返回值 - return 返回函数返回值，返回值类型只能为整数（0-255）。如果不加 return 语句，shell 默认将以最后一条命令的运行结果，作为函数返回值。 函数返回值在调用该函数后通过 $? 来获得。 所有函数在使用前必须定义。这意味着必须将函数放在脚本开始部分，直至 shell 解释器首次发现它时，才可以使用。调用函数仅使用其函数名即可。 💻 『示例源码』\n#!/usr/bin/env bash calc(){ PS3=\u0026#34;choose the oper: \u0026#34; select oper in + - \\* / # 生成操作符选择菜单 do echo -n \u0026#34;enter first num: \u0026#34; \u0026amp;\u0026amp; read x # 读取输入参数 echo -n \u0026#34;enter second num: \u0026#34; \u0026amp;\u0026amp; read y # 读取输入参数 exec case ${oper} in \u0026#34;+\u0026#34;) return $((${x} + ${y})) ;; \u0026#34;-\u0026#34;) return $((${x} - ${y})) ;; \u0026#34;*\u0026#34;) return $((${x} * ${y})) ;; \u0026#34;/\u0026#34;) return $((${x} / ${y})) ;; *) echo \u0026#34;${oper} is not support!\u0026#34; return 0 ;; esac break done } calc echo \u0026#34;the result is: $?\u0026#34; # $? 获取 calc 函数返回值 执行结果：\n$ ./function-demo.sh 1) + 2) - 3) * 4) / choose the oper: 3 enter first num: 10 enter second num: 10 the result is: 100 位置参数 # 位置参数是在调用一个函数并传给它参数时创建的变量。\n位置参数变量表：\n变量 描述 $0 脚本名称 $1 … $9 第 1 个到第 9 个参数列表 ${10} … ${N} 第 10 个到 N 个参数列表 $* or $@ 除了$0外的所有位置参数 $# 不包括$0在内的位置参数的个数 $FUNCNAME 函数名称（仅在函数内部有值） 💻 『示例源码』\n#!/usr/bin/env bash x=0 if [[ -n $1 ]]; then echo \u0026#34;第一个参数为：$1\u0026#34; x=$1 else echo \u0026#34;第一个参数为空\u0026#34; fi y=0 if [[ -n $2 ]]; then echo \u0026#34;第二个参数为：$2\u0026#34; y=$2 else echo \u0026#34;第二个参数为空\u0026#34; fi paramsFunction(){ echo \u0026#34;函数第一个入参：$1\u0026#34; echo \u0026#34;函数第二个入参：$2\u0026#34; } paramsFunction ${x} ${y} 执行结果：\n$ ./function-demo2.sh 第一个参数为空 第二个参数为空 函数第一个入参：0 函数第二个入参：0 $ ./function-demo2.sh 10 20 第一个参数为：10 第二个参数为：20 函数第一个入参：10 函数第二个入参：20 执行 ./variable-demo4.sh hello world ，然后在脚本中通过 $1、$2 \u0026hellip; 读取第 1 个参数、第 2 个参数。。。\n函数处理参数 # 另外，还有几个特殊字符用来处理参数：\n参数处理 说明 $# 返回参数个数 $* 返回所有参数 $$ 脚本运行的当前进程 ID 号 $! 后台运行的最后一个进程的 ID 号 $@ 返回所有参数 $- 返回 Shell 使用的当前选项，与 set 命令功能相同。 $? 函数返回值 💻 『示例源码』\nrunner() { return 0 } name=zp paramsFunction(){ echo \u0026#34;函数第一个入参：$1\u0026#34; echo \u0026#34;函数第二个入参：$2\u0026#34; echo \u0026#34;传递到脚本的参数个数：$#\u0026#34; echo \u0026#34;所有参数：\u0026#34; printf \u0026#34;+ %s\\n\u0026#34; \u0026#34;$*\u0026#34; echo \u0026#34;脚本运行的当前进程 ID 号：$$\u0026#34; echo \u0026#34;后台运行的最后一个进程的 ID 号：$!\u0026#34; echo \u0026#34;所有参数：\u0026#34; printf \u0026#34;+ %s\\n\u0026#34; \u0026#34;$@\u0026#34; echo \u0026#34;Shell 使用的当前选项：$-\u0026#34; runner echo \u0026#34;runner 函数的返回值：$?\u0026#34; } paramsFunction 1 \u0026#34;abc\u0026#34; \u0026#34;hello, \\\u0026#34;zp\\\u0026#34;\u0026#34; # Output: # 函数第一个入参：1 # 函数第二个入参：abc # 传递到脚本的参数个数：3 # 所有参数： # + 1 abc hello, \u0026#34;zp\u0026#34; # 脚本运行的当前进程 ID 号：26400 # 后台运行的最后一个进程的 ID 号： # 所有参数： # + 1 # + abc # + hello, \u0026#34;zp\u0026#34; # Shell 使用的当前选项：hB # runner 函数的返回值：0 Shell 扩展 # 扩展 发生在一行命令被分成一个个的 记号（tokens） 之后。换言之，扩展是一种执行数学运算的机制，还可以用来保存命令的执行结果，等等。\n感兴趣的话可以阅读 关于 shell 扩展的更多细节。\n大括号扩展 # 大括号扩展让生成任意的字符串成为可能。它跟 文件名扩展 很类似，举个例子：\necho beg{i,a,u}n ### begin began begun 大括号扩展还可以用来创建一个可被循环迭代的区间。\necho {0..5} ### 0 1 2 3 4 5 echo {00..8..2} ### 00 02 04 06 08 命令置换 # 命令置换允许我们对一个命令求值，并将其值置换到另一个命令或者变量赋值表达式中。当一个命令被``或$()包围时，命令置换将会执行。举个例子：\nnow=`date +%T` ### or now=$(date +%T) echo $now ### 19:08:26 算数扩展 # 在 bash 中，执行算数运算是非常方便的。算数表达式必须包在$(( ))中。算数扩展的格式为：\nresult=$(( ((10 + 5*3) - 7) / 2 )) echo $result ### 9 在算数表达式中，使用变量无需带上$前缀：\nx=4 y=7 echo $(( x + y )) ### 11 echo $(( ++x + y++ )) ### 12 echo $(( x + y )) ### 13 单引号和双引号 # 单引号和双引号之间有很重要的区别。在双引号中，变量引用或者命令置换是会被展开的。在单引号中是不会的。举个例子：\necho \u0026#34;Your home: $HOME\u0026#34; ### Your home: /Users/\u0026lt;username\u0026gt; echo \u0026#39;Your home: $HOME\u0026#39; ### Your home: $HOME 当局部变量和环境变量包含空格时，它们在引号中的扩展要格外注意。随便举个例子，假如我们用echo来输出用户的输入：\nINPUT=\u0026#34;A string with strange whitespace.\u0026#34; echo $INPUT ### A string with strange whitespace. echo \u0026#34;$INPUT\u0026#34; ### A string with strange whitespace. 调用第一个echo时给了它 5 个单独的参数 —— $INPUT 被分成了单独的词，echo在每个词之间打印了一个空格。第二种情况，调用echo时只给了它一个参数（整个$INPUT 的值，包括其中的空格）。\n来看一个更严肃的例子：\nFILE=\u0026#34;Favorite Things.txt\u0026#34; cat $FILE ### 尝试输出两个文件: `Favorite` 和 `Things.txt` cat \u0026#34;$FILE\u0026#34; ### 输出一个文件: `Favorite Things.txt` 尽管这个问题可以通过把 FILE 重命名成Favorite-Things.txt来解决，但是，假如这个值来自某个环境变量，来自一个位置参数，或者来自其它命令（find, cat, 等等）呢。因此，如果输入 可能 包含空格，务必要用引号把表达式包起来。\n流和重定向 # Bash 有很强大的工具来处理程序之间的协同工作。使用流，我们能将一个程序的输出发送到另一个程序或文件，因此，我们能方便地记录日志或做一些其它我们想做的事。\n管道给了我们创建传送带的机会，控制程序的执行成为可能。\n学习如何使用这些强大的、高级的工具是非常非常重要的。\n输入、输出流 # Bash 接收输入，并以字符序列或 字符流 的形式产生输出。这些流能被重定向到文件或另一个流中。\n有三个文件描述符：\n代码 描述符 描述 0 stdin 标准输入 1 stdout 标准输出 2 stderr 标准错误输出 重定向 # 重定向让我们可以控制一个命令的输入来自哪里，输出结果到什么地方。这些运算符在控制流的重定向时会被用到：\nOperator Description \u0026gt; 重定向输出 \u0026amp;\u0026gt; 重定向输出和错误输出 \u0026amp;\u0026gt;\u0026gt; 以附加的形式重定向输出和错误输出 \u0026lt; 重定向输入 \u0026lt;\u0026lt; Here 文档 语法 \u0026lt;\u0026lt;\u0026lt; Here 字符串 以下是一些使用重定向的例子：\n### ls的结果将会被写到list.txt中 ls -l \u0026gt; list.txt ### 将输出附加到list.txt中 ls -a \u0026gt;\u0026gt; list.txt ### 所有的错误信息会被写到errors.txt中 grep da * 2\u0026gt; errors.txt ### 从errors.txt中读取输入 less \u0026lt; errors.txt /dev/null 文件 # 如果希望执行某个命令，但又不希望在屏幕上显示输出结果，那么可以将输出重定向到 /dev/null：\n$ command \u0026gt; /dev/null /dev/null 是一个特殊的文件，写入到它的内容都会被丢弃；如果尝试从该文件读取内容，那么什么也读不到。但是 /dev/null 文件非常有用，将命令的输出重定向到它，会起到\u0026quot;禁止输出\u0026quot;的效果。\n如果希望屏蔽 stdout 和 stderr，可以这样写：\n$ command \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 Debug # shell 提供了用于 debug 脚本的工具。\n如果想采用 debug 模式运行某脚本，可以在其 shebang 中使用一个特殊的选项：\n#!/bin/bash options options 是一些可以改变 shell 行为的选项。下表是一些可能对你有用的选项：\nShort Name Description -f noglob 禁止文件名展开（globbing） -i interactive 让脚本以 交互 模式运行 -n noexec 读取命令，但不执行（语法检查） -t — 执行完第一条命令后退出 -v verbose 在执行每条命令前，向stderr输出该命令 -x xtrace 在执行每条命令前，向stderr输出该命令以及该命令的扩展参数 举个例子，如果我们在脚本中指定了-x例如：\n#!/bin/bash -x for (( i = 0; i \u0026lt; 3; i++ )); do echo $i done 这会向stdout打印出变量的值和一些其它有用的信息：\n$ ./my_script + (( i = 0 )) + (( i \u0026lt; 3 )) + echo 0 0 + (( i++ )) + (( i \u0026lt; 3 )) + echo 1 1 + (( i++ )) + (( i \u0026lt; 3 )) + echo 2 2 + (( i++ )) + (( i \u0026lt; 3 )) 有时我们值需要 debug 脚本的一部分。这种情况下，使用set命令会很方便。这个命令可以启用或禁用选项。使用-启用选项，+禁用选项：\n💻 『示例源码』\n# 开启 debug set -x for (( i = 0; i \u0026lt; 3; i++ )); do printf ${i} done # 关闭 debug set +x # Output: # + (( i = 0 )) # + (( i \u0026lt; 3 )) # + printf 0 # 0+ (( i++ )) # + (( i \u0026lt; 3 )) # + printf 1 # 1+ (( i++ )) # + (( i \u0026lt; 3 )) # + printf 2 # 2+ (( i++ )) # + (( i \u0026lt; 3 )) # + set +x for i in {1..5}; do printf ${i}; done printf \u0026#34;\\n\u0026#34; # Output: 12345 参考资料 # awesome-shell - shell 资源列表 awesome-bash - bash 资源列表 bash-handbook bash-guide - bash 基本用法指南 bash-it - 为你日常使用、开发以及维护 shell 脚本和自定义命令提供了一个可靠的框架 dotfiles.github.io - 上面有 bash 和其它 shell 的各种 dotfiles 集合以及 shell 框架的链接 Runoob Shell 教程 shellcheck - 一个静态 shell 脚本分析工具，本质上是 bash／sh／zsh 的 lint。 最后，Stack Overflow 上 bash 标签下有很多你可以学习的问题，当你遇到问题时，也是一个提问的好地方。\n","date":"19 October 2023","permalink":"/posts/skills/shell/","section":"博客","summary":"Shell 是一个用 C 语言编写的程序，是用户使用 Linux 的桥梁，它既是一种命令语言，又是一种程序设计语言。","title":"Shell 简介"},{"content":"","date":"18 October 2023","permalink":"/tags/functional-programming/","section":"Tags","summary":"","title":"Functional Programming"},{"content":"我们将注意力转向过程抽象，这是一种将复杂程序分解成 functions (也称为 procedures 或 subroutines 。这些术语在不同语境中的用法有细微差别，但就我们的目的而言，我们将把它们视为同义词) 形式的较小代码片段的策略。函数将某些计算封装在一个接口之后，与任何抽象概念一样，函数的用户只需知道函数做了什么，而不需要知道函数是如何完成计算的。函数还通过接收影响其计算的参数来概括计算。计算的结果就是函数的返回值。\n在本单元中，我们首先介绍 Lisp 家族中的函数式语言 Scheme。然后，我们将讨论与所有 procedural languages 相关的函数方面的问题，然后再仔细研究 functional programming，这是一种以数学函数为计算模型的编程范式。\nIntroduction to Scheme # R5RS Scheme 语言采用了与 Python 非常相似的计算模型，但只使用 expressions (不使用statements)，擅长 symbolic computation。\nScheme 是 Lisp 的一种方言，Lisp 是当今仍在广泛使用的第二古老的编程语言（仅次于 Fortran）。几十年来，Lisp 程序员社区一直在蓬勃发展，而新的 Lisp 方言（如 Clojure）也是所有现代编程语言中开发者社区发展最快的。要跟上本文的示例，可以下载 Scheme 解释器或使用在线解释器。\nExpressions # Scheme 程序由 expressions 组成，expressions 可以是简单表达式，也可以是列表形式的组合。简单表达式由一个文字或符号组成。组合表达式是一种 compound expression，由一个运算符表达式和零个或多个操作数子表达式组成。运算符和操作数都包含在括号中：\n\u0026gt; (quotient 10 2) 5 Scheme 只使用前缀符号。操作符通常是符号，如 + 和 *。复合表达式可以嵌套，也可以跨一行以上：\n\u0026gt; (+ (* 3 5) (- 10 6)) 19 \u0026gt; (+ (* 3 (+ (* 2 4) (+ 3 5) ) ) (+ (- 10 7) 6 ) ) 57 对组合进行求值时，首先需要检查运算符是否代表 special form，因为 special form 有自己的求值程序。如果运算符不是 special form，那么运算符和操作数表达式将按照任意顺序进行求值。然后，作为运算符值的函数将应用于作为操作数值的参数。\n在 Scheme 中，if 表达式是特殊形式的一个例子。虽然它在语法上看起来与调用表达式相似，但它的评估过程却与调用表达式不同。if 表达式的一般形式是\n(if \u0026lt;predicate\u0026gt; \u0026lt;consequent\u0026gt; \u0026lt;alternative\u0026gt;) 要对 if 表达式进行求值，解释器首先会对表达式的 \u0026lt;predicate\u0026gt; 部分进行求值。如果 \u0026lt;predicate\u0026gt; 求值为 true，解释器将求值 \u0026lt;consequent\u0026gt; 并返回其值。否则，解释器将求值 \u0026lt;alternative\u0026gt;，并返回其值，\u0026lt;alternative\u0026gt;可省略。\nNumerical values 可以使用熟悉的比较运算符进行比较，但在这种情况下也使用 prefix notation：\n\u0026gt; (\u0026gt;= 2 1) #t 在 Scheme 中，真值 (包括布尔值 #t 表示真， #f 表示假) 可以与布尔特殊形式相结合，它们的求值过程如下：\n(and \u0026lt;e1\u0026gt; ... \u0026lt;en\u0026gt;) 解释器按从左到右的顺序逐次求值表达式 \u0026lt;e\u0026gt;。如果任何 \u0026lt;e\u0026gt; 的值为 false，则 and 表达式的值就是该 false，其余 \u0026lt;e\u0026gt; 的值不予求值。如果所有 \u0026lt;e\u0026gt; 的值都为 true，那么 and 表达式的值就是最后一个 \u0026lt;e\u0026gt; 的值。 (or \u0026lt;e1\u0026gt; ... \u0026lt;en\u0026gt;) 解释器按从左到右的顺序，一次评估一个 \u0026lt;e\u0026gt; 表达式。如果任何 \u0026lt;e\u0026gt; 的值为 true，该值将作为 or 表达式的值返回，其余的 \u0026lt;e\u0026gt; 将不被求值。如果所有 \u0026lt;e\u0026gt; 的值都为 false，则 or 表达式的值就是最后一个 \u0026lt;e\u0026gt; 的值。 true 也可以用 not 程序来处理：\n(not \u0026lt;e\u0026gt;) 当表达式 \u0026lt;e\u0026gt; 的值为假值时，not 表达式的值为 #t，否则为 #f。 Definitions # 可以使用 define 特殊形式对值进行命名：\n\u0026gt; (define pi 3.14) \u0026gt; (* pi 2) 6.28 新函数（在 Scheme 中通常称为 procedures）可以使用 define 特殊形式的第二个版本来定义。例如，要定义平方，我们可以写下\n(define (square x) (* x x)) 程序定义的一般形式是\n(define (\u0026lt;name\u0026gt; \u0026lt;formal parameters\u0026gt;) \u0026lt;body\u0026gt;) \u0026lt;name\u0026gt; 是与环境中存储过程定义相关联的符号。 \u0026lt;formal parameters\u0026gt; 是存储过程正文中使用的名称，用于指代存储过程的相应参数。 \u0026lt;body\u0026gt; 是一个表达式，当形式参数被存储过程的实际参数替换时，它将产生存储过程应用的值。 \u0026lt;name\u0026gt; 和 \u0026lt;formal parameters\u0026gt; 放在括号中，就像在实际调用存储过程时一样。 定义了 square 之后，我们就可以在调用表达式中使用它了:\n\u0026gt; (square 21) 441 \u0026gt; (square (+ 2 5)) 49 \u0026gt; (square (square 3)) 81 用户自定义函数可以接受多个参数，并在函数体中包含特殊形式：\n\u0026gt; (define (average x y) (/ (+ x y) 2)) \u0026gt; (average 1 3) 2 \u0026gt; (define (abs x) (if (\u0026lt; x 0) (- x) x ) ) \u0026gt; (abs -3) 3 Scheme 支持具有 static scope 的局部函数定义。我们将推迟到讨论 高阶函数时再讨论这个问题。\n匿名函数也称为 lambda 函数，是使用 lambda 特殊形式创建的。使用 lambda 创建存储过程的方法与定义相同，只是不指定存储过程的名称：\n(lambda (\u0026lt;formal-parameters\u0026gt;) \u0026lt;body\u0026gt;) 由此产生的存储过程与使用 define 创建的存储过程一样。唯一不同的是，它没有与环境中的任何名称相关联。事实上，下面的表达式是等价的：\n\u0026gt; (define (plus4 x) (+ x 4)) \u0026gt; (define plus4 (lambda (x) (+ x 4))) 与任何以 procedure 为值的表达式一样，lambda 表达式也可以用作 call expression 中的操作符：\n\u0026gt; ((lambda (x y z) (+ x y (square z))) 1 2 3) 12 Compound Values # Pairs 是内置于 Scheme 语言中的。由于历史原因，Pairs 使用 cons 内置函数创建，因此，Pairs 也被称为 cons 单元，Pairs 中的元素使用 car 和 cdr 访问：\n\u0026gt; (define x (cons 1 2)) \u0026gt; x (1 . 2) \u0026gt; (car x) 1 \u0026gt; (cdr x) 2 该语言还使用成对的方式建立 Recursive lists 。用 '() 表示的特殊值代表 empty list。递归列表值的呈现方式是将其元素放在括号内，中间用空格隔开：\n\u0026gt; (cons 1 (cons 2 (cons 3 (cons 4 \u0026#39;()) ) ) ) (1 2 3 4) \u0026gt; (list 1 2 3 4) (1 2 3 4) \u0026gt; (define one-through-four (list 1 2 3 4)) \u0026gt; (car one-through-four) 1 \u0026gt; (cdr one-through-four) (2 3 4) \u0026gt; (car (cdr one-through-four)) 2 \u0026gt; (cons 10 one-through-four) (10 1 2 3 4) \u0026gt; (cons 5 one-through-four) (5 1 2 3 4) 下图为文本表示为 (1 2 3 4) 的列表对应的结构由一连串的对组成，以空列表在图中表示为包含符号 $\\emptyset$：\n以空列表以外的其他元素结束的数对序列称为 improper list。如上面的 (cons 1 2) 的结果就是一个例子，它只包含序列中的 pair。下面演示的是一个更复杂的序列：\n\u0026gt; (cons 1 (cons 2 (cons 3 4) ) ) (1 2 3 . 4) 证明了 pairs 和其他 compound objects 具有引用语义 \u0026ndash; 配对的 cdr 部分存储了对序列中下一对配对的引用。下面的代码通过变量进一步演示了这些引用语义：\n\u0026gt; (define x (cons 1 2)) \u0026gt; (define y x) \u0026gt; (eqv? x y) #t \u0026gt; (set-car! y 7) \u0026gt; x (7 . 2) 在这里，定义 (define y x) 的结果是 x 和 y 指向同一个数据 pair object。只有当两个参数指向同一个对象对时，存储过程 eqv? 才会返回 true（而 equal? 则从结构上对对象对进行比较）。此外，当我们使用 set-car! 变量修改 y 所引用的配对的第一个项目时，我们可以看到 x 引用了同一个配对，因为它也显示了修改。\n一个对象是否为空列表，可以使用原始的 null? 。利用它，我们可以定义用于计算适当列表长度和选择元素的标准序列操作：\n\u0026gt; (define (list-length items) (if (null? items) 0 (+ 1 (list-length (cdr items))) ) ) \u0026gt; (define (getitem items n) (if (= n 0) (car items) (getitem (cdr items) (- n 1)) ) ) \u0026gt; (define squares (list 1 4 9 16 25)) \u0026gt; (length squares) 5 \u0026gt; (getitem squares 3) 16 内置的 length 和 list-ref 程序提供了与这里的 list-length 和 getitem 相同的功能。\nSymbolic Data # 我们迄今为止使用过的所有复合数据对象最终都是由数字构建的。使用任意符号作为数据是 Scheme 的优势之一。\n为了操作符号，我们需要在语言中加入一个新元素：引用数据对象的能力。假设我们要构造列表 (a b)。我们不能用 (list a b) 来实现这个目的，因为这个表达式构造的是一个包含 a 和 b 值的列表，而不是符号本身。在 Scheme 中，我们会在符号 a 和 b 之前加上单引号，来指代它们，而不是它们的值：\n\u0026gt; (define a 1) \u0026gt; (define b 2) \u0026gt; (list a b) (1 2) \u0026gt; (list \u0026#39;a \u0026#39;b) (a b) \u0026gt; (list \u0026#39;a b) (a 2) 在 Scheme 中，任何未被求值的表达式都被称为 quoted。引号的概念源于一个经典的哲学，即一个事物，如到处乱跑并吠叫的狗与 \u0026ldquo;狗\u0026rdquo; 这个词之间的区别，\u0026ldquo;狗\u0026rdquo; 这个词是用于指定此类事物的语言结构。当我们使用带引号的 \u0026ldquo;狗\u0026rdquo; 时，我们指的并不是某只狗，而是一个词。在语言中，引号允许我们谈论语言本身，在 Scheme 中也是如此：\n\u0026gt; (list \u0026#39;define \u0026#39;list) (define list) 引号还允许我们使用传统的列表打印表示法键入复合对象。我们已经看到，'() 表示空列表。下面是其他例子：\n\u0026gt; (car \u0026#39;(a b c)) a \u0026gt; (cdr \u0026#39;(a b c)) (b c) Scheme 中的引号与字符串不同，后者表示字符格式的原始、非结构化数据，而前者表示结构化数据。\n\u0026gt; \u0026#34;(- 3)\u0026#34; ; a string containing the characters #\\( #\\- #\\space #\\3 #\\) \u0026#34;(- 3)\u0026#34; \u0026gt; \u0026#39;(- 3) ; produces a list containing the symbol - and number 3 (- 3) \u0026gt; (car \u0026#39;(- 3)) - \u0026gt; (cdr \u0026#39;(- 3)) (3) \u0026gt; (- 3) ; calls the - procedure on the number 3 -3 在上面的示例中，字符串字面形式 \u0026quot;(- 3)\u0026quot; 的值为其本身，带引号的表达式 '(- 3) 求值为一个列表，列表的第一个元素是符号 -，第二个元素是数字 3。最后一个示例对符号 - 进行求值以获得相应的存储过程，将数字 3 求值为自身，然后在数字 3 上调用存储过程 -，得到 -3。换句话说，字符串字面量中的数据仍然是字符数据，既不会被求值，也不会被解析。带引号表达式会被解析，但不会被求值，而是产生数据的结构化表示。未加引号的表达式会被解释器解析和求值。\n完整的 Scheme 语言还包含其他功能，如 mutation operations、vectors 和 maps。不过，我们迄今为止介绍的子集提供了一种丰富的函数式编程语言，能够实现我们迄今为止讨论过的许多想法。\nFunctions # 我们首先要考虑的是以参数形式向函数传递数据的各种方案。我们将出现在函数定义中的参数，也称为 formal parameters，与调用函数时传递给函数的实际值区分开来，后者通常被称为 actual parameter。\n本文将使用 argument 一词来指代 actual parameter， 用 parameter 一词指代 formal parameters。 Keyword Arguments # 有些语言允许甚至要求在调用函数时提供参数名，这种策略称为 named parameters 或 keyword arguments。\n关键字参数通常允许以不同于函数参数列表的顺序提供参数。例如，在 Python 中，keyword argument 可以用于任何参数。请看下面的代码：\ndef foo(x, y): print(x, y) 调用不带关键字参数的 foo() 时，第一个参数会作为第一个参数传递，第二个参数会作为第二个参数传递：\n\u0026gt;\u0026gt;\u0026gt; foo(1, 2) 1 2 不过，参数可以使用参数名重新排序：\n\u0026gt;\u0026gt;\u0026gt; foo(y = 1, x = 2) 2 1 Python 还提供了将参数定义为 positional-only 或 keyword-only 的机制，但我们不会在这里讨论这些机制。\n有少数语言要求默认为所有或大部分参数提供名称，并要求以与参数相同的顺序提供参数。下面是 Swift 3 中的一个示例：\nfunc greet(name: String, withGreeting: String) { print(withGreeting + \u0026#34; \u0026#34; + name) } greet(name: \u0026#34;world\u0026#34;, withGreeting: \u0026#34;hello\u0026#34;) 以相反的参数顺序调用 greet() 是错误的。\nSwift 允许为一个参数指定不同的参数名和参数名，这一点也很罕见。这意味着调用函数时为参数提供的名称可能与函数主体中使用的参数内部名称不同。\nDefault Arguments # 在某些语言中，函数声明或定义可能会提供一个 default argument，允许在没有该参数的情况下调用函数。这可以替代重载，即编写单独的函数定义来处理存在或缺少参数的情况。\n下面是一个 Python 示例：\ndef power(base, exponent=2): return base ** exponent power() 函数可以调用一个参数，在这种情况下，默认参数 2 用于计算数字的平方。也可以使用两个参数来计算任意幂：\n\u0026gt;\u0026gt;\u0026gt; power(3) 9 \u0026gt;\u0026gt;\u0026gt; power(3, 4) 81 有 default arguments 一般必须出现在参数列表的末尾。对于何时以及在哪种环境下评估默认参数，不同语言的做法各不相同。最常见的策略是每次调用函数时都评估缺省参数，但在定义 environment (static scope) 中进行。Python 的罕见之处在于，它只在函数定义语句执行时评估一次缺省参数。这意味着，如果在函数中修改了参数值，那么对同一函数的后续调用可能会对同一参数使用不同的缺省值。例如\ndef test(x=[]): x.append(1) print(x) test() test() // output [1] [1, 1] C 和 C++ 有许多关于缺省参数的规则，这是因为一个实体可以声明多次。默认参数既可以在独立声明中提供，也可以在定义中提供。但是，同一实体的多个可见声明为同一参数提供默认参数是非法的，即使提供的值是相同的。缺省参数集是同一作用域内所有可见声明的集合，只有在前面和当前声明已为所有后续参数提供缺省参数的情况下，声明才能为参数引入缺省参数。缺省参数中使用的名称在声明时进行解析，但参数表达式在调用函数时进行求值。\n下面是 C++ 中多重声明的一个合法示例：\nint foo(int x, int y = 4); int foo(int x = 3, int y) { return x + y; } 除函数参数外，C++ 还允许模板参数使用默认参数，其有效性规则与此类似。\nVariadic Functions # 一种语言可能会提供一种机制，让函数在调用时可以使用数量可变的参数。这种特性通常被称为 varargs，使用这种特性的函数被称为变量函数 (variadic)。这种机制可能提供类型安全，也可能允许不安全的使用，从而导致错误或未定义的行为。可变参数一般必须出现在参数列表的末尾，它匹配的是非可变参数匹配后剩余的参数。通常只允许使用一个变量参数。\n在提供安全变量函数的语言中，一种常见的机制是自动将变量参数打包到一个 container 中，例如 array 或 tuple。例如，下面的 Python 函数计算其参数的乘积：\ndef product(*args): result = 1 for i in args: result *= i return result 参数名前面的 * 表示变量参数，变量参数以绑定到参数名的元组形式传递。上述函数遍历元组中的元素，更新总乘积。要调用 product()，必须提供 0 个或更多参数：\n\u0026gt;\u0026gt;\u0026gt; product() 1 \u0026gt;\u0026gt;\u0026gt; product(1, 2, 3) 6 Python 还提供了可变关键字参数，这些参数被打包成一个字典。在参数前面加上 ** 表示它是一个可变关键字参数，并且这样的参数必须是最后一个。例如，下面的函数同时包含一个非关键字可变参数和一个可变关键字参数，打印出前者对应的元组和后者对应的字典：\ndef print_args(*args, **kwargs): print(args) print(kwargs) \u0026gt;\u0026gt;\u0026gt; print_args(3, 4, x = 5, y = 6) (3, 4) {\u0026#39;x\u0026#39;: 5, \u0026#39;y\u0026#39;: 6} 最后，Python 允许使用 * 或 ** 操作符对序列或字典进行 \u0026ldquo;解包\u0026rdquo;，从而将解包后的值用于需要值列表的地方。例如，下面的代码将一个列表解包，以调用 product()：\nproduct(*[1, 2, 3]) 6 此外，Scheme 还支持可变参数。一个存储过程可以使用一个不恰当的列表作为参数列表，并以一个符号而不是空列表结束，这样就可以定义一个存储过程来接受可变参数。可变参数与任意数量的参数绑定，并打包成一个列表：\n\u0026gt; (define (func . args) args ) \u0026gt; (func) () \u0026gt; (func 1 2 3) (1 2 3) 存储过程 func 可以接收任意数量的参数，并返回包含这些参数的 list。因此，它的行为与内置的 list 存储过程相同。我们还可以定义一个存储过程，同时接收必参数和可变参数，例如下面的 average 定义：\n\u0026gt; (define (average x . nums) (/ (apply + x nums) (+ 1 (length nums)) ) ) \u0026gt; (average 1) 1 \u0026gt; (average 1 3) 2 \u0026gt; (average 1 3 5 7) 4 procedure 接收一个或多个参数，其中第一个参数与参数 x 绑定，其余参数封装在一个与变量 nums 参数绑定的列表中。我们可以使用 apply 来转发变量参数，它接收一个存储过程、任意数量的常规参数，最后是一个包含其余参数的列表。例如，(apply + 1 2 '(3 4)) 相当于调用 (+ 1 2 3 4)。在上面第一个使用 average 的示例中，nums 在调用 (average 1) 时绑定为一个空列表，而 (apply + x nums) 相当于 (apply + 1 '()) ，后者本身相当于 (+ 1)。在第三个例子中，nums 绑定到一个列表 (3 5 7)，因此 (apply + x nums) 等价于 (apply + 1 '(3 5 7))，而 (apply + 1 '(3 5 7)) 又等价于 (+ 1 3 5 7)。\n在 Python 和 Scheme 中，可变参数可以匹配任何类型的参数，因为这两种语言都是动态类型的。然而，在静态类型语言中，可变参数通常被限制为单一类型，尽管该类型可能是多态的。例如，下面是 Java 中的一个变量方法：\npublic static void print_all(String... args) { for (String s : args) { System.out.println(s); } } print_all() 的参数必须是字符串，并将它们打包成一个字符串数组。Java 也允许将单个字符串数组作为参数传递：\nprint_all(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;); print_all(new String[] { \u0026#34;good\u0026#34;, \u0026#34;bye\u0026#34; }); C 和 C++ 也有一种可变参数机制，但它存在严重的安全问题。尤其是，它无法向被调用的函数提供关于参数个数及其类型的信息。下面是一个返回参数之和的函数示例：\n#include \u0026lt;stdarg.h\u0026gt; int sum(int count, ...) { va_list args; int total = 0; int i; va_start(args, count); for (i = 0; i \u0026lt; count; i++) { total += va_arg(args, int); } va_end(args); return total; } 在该函数中，第一个参数被假定为其余参数的个数，而后一个参数被假定为 int 类型。如果违反其中任何一个条件，都会产生未定义的行为。另一种策略是使用格式字符串来确定参数的数量和类型，如 printf() 和类似函数中使用的方法。可变参数缺乏安全性，会导致 格式字符串攻击等漏洞。\nC++11 提供了类型安全的 variadic templates。\nParameter Passing # 语言的另一个不同之处在于函数与其调用者之间传递参数的 semantics 和 mechanism。函数参数可以是单向的，仅用于向函数传递输入或仅用于从函数向调用者传递输出，也可以是双向的。这些情况被称为 input、output 和 input/output 参数。一种语言不必支持所有三种参数类别。\n各种语言使用不同的参数传递技术或调用模式。这些技术会影响参数和参数的语义以及支持的参数类别。以下是不同语言使用的具体调用模式：\nCall by value，参数代表函数调用框架中的一个新变量。参数值被复制到与新变量相关的存储空间中。按值调用参数只为函数提供输入，如下面的 C++ 示例：\nvoid foo(int x) { x++; cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; endl; } int main() { int y = 3; foo(y); // prints 4 cout \u0026lt;\u0026lt; y \u0026lt;\u0026lt; endl; // prints 3 } 即使 foo() 修改了输入值，修改后的值也不会传回 caller。\nCall by reference，参数必须传递一个 l-value，因为参数 aliases 了传递进来的对象。对参数的任何修改都会反映在参数对象中。因此，引用调用参数同时提供输入和输出。在 C++ 中，引用参数提供了引用调用，并且可以通过声明 const 将其限制为仅提供输入。下面的 C++ 示例使用引用调用交换了两个对象的值：\nvoid swap(int \u0026amp;x, int \u0026amp;y) { int tmp = x; x = y; y = tmp; } int main() { int x = 3, y = 4; swap(x, y); cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; y \u0026lt;\u0026lt; endl; // prints 4 3 } 引用调用有时用来指使用指针间接传递对象。下面的 C/C++ 函数使用指针交换对象值：\nvoid swap(int *x, int *y) { int tmp = *x; *x = *y; *y = tmp; } int main() { int x = 3, y = 4; swap(\u0026amp;x, \u0026amp;y); printf(\u0026#34;%d %d\\n\u0026#34;, x, y); // prints 4 3 } 但从技术上讲，参数和参量是独立的指针对象，通过值传递。尽管如此，这种效果模拟了引用调用，使输入和输出都能通过一个参数来实现。\nCall by result，在这种模式下，参数代表一个新变量，调用者不对其进行初始化。相反，调用者会为参数指定一个 l-value，当函数调用结束时，参数的最终值会被复制到 l-value 中。因此，按结果调用只提供输出参数。下面是一个使用类似 C 的语法、按结果调用的示例：\nvoid foo(result int x) { x = 3; x++; // x is now 4 } int y = 5; foo(y); // y is now 4 print(y); // prints 4 Call by value-result，这是 Call by value 和 Call by result 的组合。参数值被复制到与参数相对应的新变量中，然后从函数返回时，参数值又被复制到调用者提供的 l-value 值中。这与引用调用的不同之处在于，在进入和退出函数时都会进行复制。可以通过将相同的 l-value 传递给多个参数来说明这一点，例如在下面的示例中使用了类似于 C 语言的语法，即按 Call by value-result：\nint foo(value-result int x, value-result int y) { x++; return x - y; } int z = 3; print(foo(z, z)); // prints 1 print(z); // prints 3 or 4, depending on the semantics 在这段代码中，x 和 y 是新变量，它们被初始化为 z 的值，即 3。x 的增量不会影响 y，因为它们是独立的变量，所以调用 foo() 返回 1。因此，1 会被打印出来。z 的最终值取决于从 x 还是从 y 复制的语言语义。如果使用引用调用，那么 x 和 y 将 alias 为同一个对象，调用 foo() 将返回 0。\nCall by name，在这种模式下，可以提供一个完整的表达式作为参数，但在调用函数时不会对其进行求值。相反，在函数中出现参数名的地方，参数名会被表达式替换，而表达式会在主体中遇到时进行求值。这是一种 lazy evaluation，即在需要时才计算值。下面是一个使用 C-like syntax、按名称调用的示例：\nvoid foo(name int a, name int b) { print(b); // becomes print(++y) print(b); // becomes print(++y) } int x = -1, y = 3; foo(++x, ++y); // prints 4, then 4 or 5 depending on the exact // language semantics; y is now 4 or 5 print(x); // prints -1 -- x is unchanged 在本例中，参数表达式 ++x 从未被求值，因为相应的逐名调用参数 a 从未被使用。另一方面，表达式 ++y 被计算，因为相应的参数 b 确实被使用了。根据语言语义的不同，表达式可能只被求值一次，其值被缓存以备后续使用，也可能在每次使用参数时都被求值。\n按名称调用会产生一个微妙的问题。请看下面这段代码，它使用了 C-like syntax 和按名称调用：\nvoid bar(name int x) { int y = 3; print(x + y); } int y = 1; bar(y + 1); 如果我们用参数表达式替换 bar() 中出现的参数 x，就会得到 y + 1 + y 作为 print() 的参数。如果在 bar() 的环境中求值，结果将是 7。这是不可取的，因为这意味着局部声明 y 的实现细节改变了函数的行为。\n相反，参数表达式应在调用者的环境中进行评估。这就要求在函数调用时同时传递参数及其环境。使用名称调用的语言通常使用编译器生成的局部函数，称为 thunk ，来封装参数表达式及其环境。然后将 thunk 传递给被调用的函数，当遇到参数时，就会调用 thunk。\n在某些语言中，与 call-by-name parameter 相对应的表达式仅在首次引用该参数时进行评估，并缓存评估结果。缓存的结果将用于随后每次出现的参数。\ncall by value 是大多数现代语言使用的调用模式，包括 C、C++（用于非引用参数）、Java、Scheme 和 Python。程序员经常误以为后三种语言使用 call by reference，但实际上，它们将 call by value 与 call by reference 语义结合在一起。这种组合有时被称为 object reference 。下面的示例说明 Python 使用的是 call by value：\ndef swap(x, y): tmp = x x = y y = tmp \u0026gt;\u0026gt;\u0026gt; x, y = 1, 2 \u0026gt;\u0026gt;\u0026gt; swap(x, y) \u0026gt;\u0026gt;\u0026gt; x, y (1, 2) 错误的 swap() 函数只是改变了局部变量的值，从而改变了它们所引用的对象，而没有影响作为参数的变量。这表明全局 x 和 y 的存储空间与参数的存储空间是不同的，因此 Python 没有使用引用调用。事实上，Python 甚至不能像 C 和 C++ 指针那样模拟引用调用。\nl-value and r-value # l-value 和 r-value 是 C++ 表达式的基础。简单地说，l-value 是对象引用，r-value 是值。l-value 和 r-value 之间的区别在表达式的编写和理解中起着重要作用。\nl-value 是产生对象引用的表达式，例如变量名、数组下标引用、取消引用指针或返回引用的函数调用。l-value 总是有一个定义的存储区域，因此可以获取其地址。 r-value 是指不是 l-value 的表达式。r-value 的例子包括字面量、大多数运算符的结果以及返回非引用的函数调用。r-value 不一定与任何存储空间相关联。 严格来说，函数是一个 l-value，但它的唯一用途是用于调用函数或确定函数的地址。大多数情况下，l-value 指的是对象 l-value。\nEvaluation of Function Calls # 下面我们总结一下函数调用的实现过程：\n第一步是确定嵌套函数调用的非本地环境。在使用嵌套函数和静态作用域的语言中，当执行嵌套函数定义本身时，非本地环境的引用会存储在关联的函数对象中。在具有深绑定的动态作用域下，非本地环境是在函数名称被引用时确定的。最后，在浅绑定的动态作用域中，非本地环境是函数被调用时处于活动状态的环境。\n下一步是使用新创建的函数调用激活记录将参数传递给函数。参数在现有环境中进行评估，并按如下方式传递给被调用者：\nCall by value and call by value-result : 对参数进行评估以获得其 r-value。r-value 将被复制到新激活记录中相应参数的存储空间。 Call by reference : 参数的 l-value。相应的参数会绑定到与 l-value 相关的对象上。 Call by result : 参数进行评估，以获得其 l-value 。在新的激活记录中，存储空间会被分配，但不会被初始化。 Call by name : 参数表达式会被打包到一个包含当前环境的 thunk 中。参数绑定到 thunk 的引用上。 一旦参数被传递，调用者的执行就会暂停，而被调用者的主体将在一个由新创建的激活记录和被调用者的非本地环境组成的环境中执行。对于 call by name，根据语言的语义，call by name 参数的出现会在参数第一次被指名或每次被指名时调用相应的 thunk。\n当被调用的函数返回时，其返回值（如果有的话）会被放置在指定的存储位置，通常是在调用者的激活记录中。对于 call-by-result 或 call-by-value-result 参数，参数的当前 r-value 会被复制到与相应函数调用参数的 l-value 相关联的对象中。然后，被调用者的激活记录将被销毁，调用者将在函数调用后恢复执行。函数调用本身的评估结果就是函数的返回值。\nRecursion # Recursion 是一种利用函数和函数应用进行重复的机制。它涉及函数直接或间接地调用自身，通常使用在某种意义上比前一个参数 \u0026ldquo;小 \u0026ldquo;的参数。递归计算在达到基数时终止，基数是指无需进行任何递归调用即可直接计算出结果的输入。\n一种语言要想达到图灵完备性，只需提供 recursion 和 conditionals 即可。\nActivation Records # 在机器上，递归之所以起作用，是因为函数的每次调用都有自己的激活记录，将局部变量映射为值。请看下面的阶乘递归定义：\ndef factorial(n): if n == 0: return 1 return n * factorial(n - 1) 调用 factorial(4) 会导致五次调用 factorial()，参数从 4 到 0，每次都有自己的激活记录和参数 n 的绑定：\nfactorial(4): n --\u0026gt; 4 factorial(3): n --\u0026gt; 3 factorial(2): n --\u0026gt; 2 factorial(1): n --\u0026gt; 1 factorial(0): n --\u0026gt; 0 在执行 factorial() 主体时查找 n，每次调用都会获得自己的 n 值，而不会受到其他激活记录的影响。\n要使函数调用生效，激活记录需要的不仅仅是参数和局部变量的存储空间。临时值也需要存储在某个地方，由于每个调用都需要自己的临时值存储空间，因此这些临时值通常也要放在激活记录中。调用还需要知道在哪里存储其返回值，通常是在调用者框架中的临时存储区。最后，函数需要知道如何将执行返回给调用者。具体细节不在本文讨论范围之内，但这些信息包括调用者函数调用后的指令地址和调用者激活记录的地址。\n临时对象集可以通过静态方式保守地确定，因此激活记录的大小以及对象在其中的位置都可以在编译时确定。对于上面的 factor()，需要临时存储 n - 1 以及递归调用 factorial() 的结果。递归调用会使用后者在调用程序中的位置来存储其返回值。根据不同的实现，调用 factorial(0) 的激活记录中可能仍然有这些临时对象的空间，即使它们不会被使用。\nTail Recursion # 递归计算对函数的每次调用都使用单独的激活记录。存储这些记录所需的空间与激活函数调用的次数成正比。在上面的 factorial(n) 中，当计算达到 factorial(0) 时，所有 n + 1 次调用都同时激活，需要的空间为 O(n)。与之相比，下面的迭代实现使用的空间是恒定的：\ndef factorial_iter(n): result = 1 while n \u0026gt; 0: result *= n n -= 1 return result 然而，递归版本的 factorial() 所需的空间并不是使用递归的内在原因，而是函数编写方式的结果。事实上，由于在递归调用之后还需要完成调用的工作，因此在递归调用期间必须保留其激活记录，这就导致了线性空间需求。\n考虑另一种阶乘递推计算方法：\ndef factorial_tail(n, partial_result = 1): if n == 0: return partial_result return factorial_tail(n - 1, n * partial_result) 请注意，在完成递归调用后，factorial_tail() 函数不做任何工作。这意味着在进行递归调用时，它不再需要存储参数、局部变量或临时对象。此外，由于 factorial(n, k) 直接返回递归调用 factorial(n - 1, n * k) 的结果，因此后者可以将其返回值存储在 factorial(n, k) 的调用者中用于存放 factorial(n, k) 返回值的位置，并直接将执行返回给该调用者。因此，优化后的实现可以为 factorial_tail(n - 1, n * k) 重用 factorial_tail(n, k) 的激活记录空间，因为前者不再需要激活记录。\n这个过程可以推广到任何函数调用，而不仅仅是递归调用。如果函数的调用者直接返回调用值，而不执行任何额外的计算，那么该函数调用就是尾调用。如果一个函数的所有递归调用都是尾调用，那么这个函数就是尾递归函数。因此，factorial_tail() 是尾部递归函数。\n尾递归计算只使用固定数量的激活记录，因此其空间使用量与等效的迭代计算相当。事实上，许多函数式语言并不提供迭代的构造，因为它们可以等效地使用尾递归来表达。这些语言通常要求实现执行尾调用优化，尽可能重复使用激活记录的空间。\n由于尾调用要求在返回后不执行任何计算，因此在语法上看似尾调用的调用可能不是尾调用，因为隐式计算可能发生在函数的末尾。这方面的一个具体例子是基于作用域的资源管理，如下例所示：\nint sum(vector\u0026lt;int\u0026gt; values, int index, int partial_result = 0) { if (values.size() == index) { return 0; } return sum(values, index + 1, partial_result + values[index]) } 虽然这段代码在递归调用后似乎没有进行计算，但本地 vector\u0026lt;int\u0026gt; 对象有一个析构函数，必须在递归调用完成后运行。因此，对 sum() 的递归调用不是尾部调用，该计算也不是尾部递归计算。\n另一种阻碍尾调用优化的情况是，在使用静态作用域并支持高阶函数全部功能的语言中，函数内部包含一个函数定义。嵌套函数需要访问其定义环境，因此如果嵌套函数可以在其外层函数调用完成后或在尾调用中使用，则必须保留该环境。\nHigher-Order Functions # first-class entity 是一种支持对语言中的其他 entities 进行操作的 entity，包括作为参数传递、从函数返回和动态创建。\n在 functions 是 first-class entity 的语言中，可以编写 higher-order functions，将另一个 function 作为参数传递或返回一个 function。 其他语言也可能支持 higher-order functions，但是在这些语言中 function 不是可以在运行时动态创建的 entity 。 Function Objects # 在某些语言中，可以定义本身不是函数但提供与函数相同接口的对象。这些对象被称为函数对象或函数器。一般来说，语言通过允许重载函数调用操作符来编写函数器。请看下面的 C++ 示例：\nclass Counter { public: Counter : count(0) {} int operator()() { return count++; } private: int count; }; Counter 类实现了一个函数，可以返回它被调用的次数。可以同时存在多个 Counter 对象，每个对象都有自己的计数：\nCounter counter1, counter2; cout \u0026lt;\u0026lt; counter1() \u0026lt;\u0026lt; endl; // prints 0 cout \u0026lt;\u0026lt; counter1() \u0026lt;\u0026lt; endl; // prints 1 cout \u0026lt;\u0026lt; counter1() \u0026lt;\u0026lt; endl; // prints 2 cout \u0026lt;\u0026lt; counter2() \u0026lt;\u0026lt; endl; // prints 0 cout \u0026lt;\u0026lt; counter2() \u0026lt;\u0026lt; endl; // prints 1 cout \u0026lt;\u0026lt; counter1() \u0026lt;\u0026lt; endl; // prints 3 函数允许 function-like object 存在多个实例，每个实例都有自己的状态，并在函数的生命周期内持续存在。这与 function 截然不同，function 中的自动对象不会在单次调用后持续存在，而静态对象则会在整个程序执行过程中持续存在。\nPython 还允许通过定义特殊的 __call__ 方法来编写函数：\nclass Counter: def __init__(self): self.count = 0 def __call__(self): self.count += 1 return self.count - 1 一般来说，在重载函数调用操作符时，可以指定额外的参数，以模拟可以接收这些参数的函数。\n有些语言不允许重载函数调用操作符本身，但规定了允许定义和使用类函数对象的约定。例如，下面是 Counter 在 Java 中使用 Supplier\u0026lt;T\u0026gt; 接口的实现，该接口指定了一个产生 T 的零参数方法：\nclass Counter implements Supplier\u0026lt;Integer\u0026gt; { public Integer get() { return count++; } private int count = 0; } 然后通过明确调用 get() 方法来调用这个类函数对象：\nSupplier\u0026lt;Integer\u0026gt; counter1 = new Counter(); Supplier\u0026lt;Integer\u0026gt; counter2 = new Counter(); System.out.println(counter1.get()); // prints 0 System.out.println(counter1.get()); // prints 1 System.out.println(counter1.get()); // prints 2 System.out.println(counter2.get()); // prints 0 System.out.println(counter2.get()); // prints 1 System.out.println(counter1.get()); // prints 3 再比如，Java 中的 Predicate 接口是通过 functor-like objects 实现的，这些对象接收一个参数并返回一个布尔值：\ninterface Predicate\u0026lt;T\u0026gt; { boolean test(T t); ... } class GreaterThan implements Predicate\u0026lt;Integer\u0026gt; { public GreaterThan(int threshold) { this.threshold = threshold; } public boolean test(Integer i) { return i \u0026gt; threshold; } private int threshold; } 使用这些 functor-like objects 的代码会调用 test() 方法，而不是直接调用对象：\nGreaterThan gt3 = new GreaterThan(3); System.out.println(gt3.test(2)); // prints out false System.out.println(gt3.test(20)); // prints out true java.util.function 函数库包中为常见模式提供了单独的接口。\nFunctions as Parameters # higher-order function 可以将另一个函数作为参数。我们首先研究那些只有 top-level functions 并允许将函数指针或引用作为参数传递的语言。然后，我们将研究将函数作为参数传递会如何影响函数代码的执行环境。\nFunction Pointers # 在某些语言中，函数可以作为参数或返回值传递，但不能在另一个函数的上下文中创建。在这些语言中，所有函数都是在顶层定义的，只有指向函数的指针或引用才能作为值使用。下面是 C 语言中的一个例子，C 语言提供了函数指针：\nvoid apply(int *array, size_t size, int (*func)(int)) { for (; size \u0026gt; 0; --size, ++array) { *array = func(*array); } } int add_one(int x) { return x + 1; } int main() { int A[5] = { 1, 2, 3, 4, 5 }; apply(A, 5, add_one); printf(\u0026#34;%d, %d, %d, %d, %d\\n\u0026#34;, A[0], A[1], A[2], A[3], A[4]); return 0; } apply() 函数接收数组、数组大小和一个指向接收 int 并返回 int 的 function pointer。它将函数应用于数组中的每个元素，并用结果替换原值。add_one() 函数作为参数传递给 apply()，C 语言会自动将函数转换为函数指针，其结果是 A 中的每个元素都被递增。\nBinding Policy # 在上面的代码中，有三个环境与 add_one() 函数相关联：定义环境、在 main() 中引用环境和在 apply() 中调用环境。根据语言的语义，这三个环境中的任何一个都可能是 add_one() 主体执行环境的组成部分。\n在静态作用域中，函数中的代码可以访问其定义环境中的名称，而在动态作用域中，它可以访问其使用环境中的名称。考虑到动态作用域，函数的非本地环境是函数被引用的环境还是函数被调用的环境？下面是一个与这种区别有关的示例：\nint foo(int (*bar)()) { int x = 3; return bar(); } int baz() { return x; } int main() { int x = 4; print(foo(baz)); } 在动态作用域中，函数可以访问其使用环境。然而，在上面的示例中，根据 baz() 的使用环境是函数被引用的地方还是被调用的地方，结果是不同的。\n函数被引用的地方的情况下，baz() 的非本地环境是 main() 的环境，baz() 主体中的 x 将引用 main() 中定义的 x。这就是所谓的深度绑定。 函数被调用的地方的情况下，baz() 的非本地环境是 foo() 的环境，baz() 中的 x 将引用 foo() 中定义的 x。这就是所谓的浅绑定。这两种方法都是有效的，语言的绑定策略决定了使用哪种方法。 使用静态作用域时，绑定策略也会对递归函数内部本地定义的函数产生影响。然而，在使用静态作用域的语言中，深度绑定被普遍使用，因此函数定义时所建立的环境就是函数所能访问的环境。\nNested Functions # 函数式编程的一个主要特点是可以在另一个函数中定义一个函数，从而动态创建一个函数。在具有静态作用域的语言中，这种嵌套函数可以访问其定义环境，函数与其定义环境的组合称为 closure。嵌套函数中使用但在外层环境中定义的变量被称为 closure 所捕获。如果嵌套函数从外层函数中返回或泄漏，外层函数的环境通常必须在函数返回后继续存在，因为嵌套函数可能会访问其中的绑定。\n举例来说，请看下面这个返回嵌套函数的 Python higher-order function：\ndef make_greater_than(threshold): def greater_than(x): return x \u0026gt; threshold return greater_than make_greater_than() 函数接收一个阈值，并构造一个嵌套函数来判断其输入是否大于阈值。threshold 变量位于 make_greater_than() 的激活记录中，但被 greater_than() 捕获。由于后者会返回阈值，因此激活记录必须持续存在，这样 greater_than() 的调用才能访问 threshold 的绑定。\n请注意，每次调用 make_greater_than()，都会创建一个不同的 greater_than() 实例，并拥有自己的外层环境。因此，不同的 make_greater_than() 调用会产生不同的函数：\n\u0026gt;\u0026gt;\u0026gt; gt3 = make_greater_than(3) \u0026gt;\u0026gt;\u0026gt; gt30 = make_greater_than(30) \u0026gt;\u0026gt;\u0026gt; gt3(2) False \u0026gt;\u0026gt;\u0026gt; gt3(20) True \u0026gt;\u0026gt;\u0026gt; gt30(20) False \u0026gt;\u0026gt;\u0026gt; gt30(200) True 调用的父框架是 threshold 绑定为 3 的框架，因此 x \u0026gt; threshold 的值为 false。\n非纯函数式语言可能允许修改捕获的变量。例如，下面使用嵌套函数定义了一个银行账户的数据抽象：\ndef make_account(balance): def deposit(amount): nonlocal balance balance += amount return balance def withdraw(amount): nonlocal balance if 0 \u0026lt;= amount \u0026lt;= balance: balance -= amount return amount else: return 0 return deposit, withdraw Python 需要 nonlocal ，因为它默认赋值给本地变量。然后，我们可以如下使用创建的函数：\n\u0026gt;\u0026gt;\u0026gt; deposit, withdraw = make_account(100) \u0026gt;\u0026gt;\u0026gt; withdraw(10) 10 \u0026gt;\u0026gt;\u0026gt; deposit(0) 90 \u0026gt;\u0026gt;\u0026gt; withdraw(20) 20 \u0026gt;\u0026gt;\u0026gt; deposit(0) 70 \u0026gt;\u0026gt;\u0026gt; deposit(10) 80 \u0026gt;\u0026gt;\u0026gt; withdraw(100) 0 \u0026gt;\u0026gt;\u0026gt; deposit(0) 80 Decorators # Python 中的一种常见模式是通过应用高阶函数来转换函数或类。这样的高阶函数被称为 decorator，Python 有专门的语法来装饰函数：\n@\u0026lt;decorator\u0026gt; def \u0026lt;name\u0026gt;(\u0026lt;parameters\u0026gt;): \u0026lt;body\u0026gt; 这在很大程度上相当于\ndef \u0026lt;name\u0026gt;(\u0026lt;parameters\u0026gt;): \u0026lt;body\u0026gt; \u0026lt;name\u0026gt; = \u0026lt;decorator\u0026gt;(\u0026lt;name\u0026gt;) 被装饰函数的定义被正常执行，然后在函数上调用装饰器。调用的结果与函数名称绑定。\n举个例子，假设我们想通过打印函数名称及其参数来跟踪函数被调用的时间。我们可以定义一个高阶函数，接收一个函数并返回一个新的嵌套函数，该函数首先打印出原始函数的名称及其参数，然后调用该函数：\ndef trace(fn): def tracer(*args): args_string = \u0026#39;, \u0026#39;.join(repr(arg) for arg in args) print(f\u0026#39;{fn.__name__}({args_string})\u0026#39;) return fn(*args) return tracer 在这里，我们使用变量参数为原始函数传递任意数量的参数。为了简单起见，我们忽略了关键字参数。然后，我们可以使用装饰器语法将其应用到函数中：\n@trace def factorial(n): return 1 if n == 0 else n * factorial(n - 1) 现在，只要调用 factorial()，我们就能得到参数的打印输出：\n\u0026gt;\u0026gt;\u0026gt; factorial(5) factorial(5) factorial(4) factorial(3) factorial(2) factorial(1) factorial(0) 120 请注意，递归调用也会调用转换后的函数。这是因为在 factorial() 的外层环境中，factorial 这个名称现在与嵌套的跟踪函数绑定在一起，因此查找这个名称时，会调用跟踪函数，而不是原来的函数。这样做的一个副作用是产生了相互递归，即一组函数通过彼此间接地进行递归调用。\nLambda Functions # 嵌套函数定义允许在运行时构造函数，从而满足了函数成为 first-class entity 的要求之一。不过，到目前为止，我们只看到了嵌套函数定义的命名，即在定义环境中引入了绑定。这与其他一流实体，如数据值，形成了鲜明对比，后者可以在不绑定名称的情况下创建。就像构造不带名称的值很有用一样，比如将其作为参数传递或返回时，构造不带名称的函数也很有用。这些函数被称为匿名函数或 lambda 函数。\nlambda 函数在函数式语言中无处不在，但许多常用的命令式语言也提供某种形式的 lambda 函数。不同语言的语法和功能各不相同，我们将研究几个具有代表性的示例。\nScheme # 在以函数式为主的 Lisp 系列语言中，lambda 是一种常见的构造，Scheme 也不例外。lambda 特殊形式构造了一个匿名函数：\n(lambda (\u0026lt;parameters\u0026gt;) \u0026lt;body\u0026gt;) 使用 define 形式的函数定义可视为变量定义和 lambda 的简写：\n(define (\u0026lt;name\u0026gt; \u0026lt;parameters\u0026gt;) \u0026lt;body\u0026gt;) --\u0026gt; (define \u0026lt;name\u0026gt; (lambda (\u0026lt;parameters\u0026gt;) \u0026lt;body\u0026gt;)) 例如，下面的函数创建并返回一个匿名函数，该函数将给定的数字添加到参数中：\n(define (make-adder n) (lambda (x) (+ x n) ) ) 这比只使用 define 的等价定义更简单、更恰当：\n(define (make-adder n) (define (adder x) (+ x n) ) adder ) 然后，我们就可以在各个参数上调用 make-adder 的结果：\n\u0026gt; (define add3 (make-adder 3)) \u0026gt; (add3 4) 7 \u0026gt; (add3 5) 8 \u0026gt; ((make-adder 4) 5) 9 Scheme 中的嵌套函数使用静态作用域，因此匿名函数可以访问其定义环境中的变量 n。然后，它将自己的参数 x 与 n 相加，返回总和。\nScheme 并非纯函数式，它允许变量和复合数据的变异。嵌套函数，无论是否匿名，都可以修改其非本地环境中的变量。下面的函数创建了一个计数器函数，返回它被调用的次数：\n(define (make-counter) (let ((count 0)) (lambda () (set! count (+ count 1)) (- count 1) ) ) ) set! 将变量变为给定值。这样，我们就可以使用 make-counter 函数了：\n\u0026gt; (define counter (make-counter)) \u0026gt; (counter) 0 \u0026gt; (counter) 1 \u0026gt; (counter) 2 Python # Python 支持使用 lambda 表达式的匿名函数。其形式如下：\nlambda \u0026lt;parameters\u0026gt;: \u0026lt;body expression\u0026gt; Python 中 lambda 表达式的语法对匿名函数产生了命名嵌套函数所没有的限制：主体必须是一个表达式，该表达式的值自动成为函数的返回值。在实践中，这个限制通常不是问题，因为 lambda 通常用于语句和副作用可能不合适的函数式上下文中。\nResources # https://eecs390.github.io/notes/functional.html ","date":"18 October 2023","permalink":"/posts/language/functional-programming/","section":"博客","summary":"我们将注意力转向过程抽象，这是一种将复杂程序分解成 functions (也称为 procedures 或 subroutines 。这些术语在不同语境中的用法有细微差别，但就我们的目的而言，我们将把它们视为同义词) 形式的较小代码片段的策略。函数将某些计算封装在一个接口之后，与任何抽象概念一样，函数的用户只需知道函数做了什么，而不需要知道函数是如何完成计算的。函数还通过接收影响其计算的参数来概括计算。计算的结果就是函数的返回值。","title":"Functional Programming"},{"content":"","date":"18 October 2023","permalink":"/tags/language/","section":"Tags","summary":"","title":"Language"},{"content":"一、书名和作者 # 《人件》 作者： Tom DeMarco Timothy Lister 二、书籍概览 # 主要论点和结构：软件开发不仅仅是技术问题，更是管理问题。它强调了管理人力资源的关键性，特别是在软件领域，由于脑力劳动的特性，软件开发者与传统的体力劳动者有很大的不同。这本书提供了许多实际的管理方法，以促进团队协作，提高生产效率，同时提供了实例和案例来支持这些观点。 目标读者和应用场景：该书的主要目标读者是软件组织的管理者，项目经理，团队领导者以及任何对软件开发和团队管理感兴趣的人。它适用于各种软件开发项目，从小型创业公司到大型企业。这本书提供的原则和实践可以适用于各种团队管理场景，不仅限于软件开发。 三、核心观点与主题 # 1. 主题一: 管理人力资源 # 子观点1\n软件开发是一项知识密集型工作，与传统体力劳动有着明显的不同。因此，管理者应该鼓励开发人员参与关键决策，包括允许他们犯错。这种方式可以增强员工的责任感和参与度。\n子观点2\n尊重员工的个性和特征是至关重要的。每个人都有独特的特点，而好的管理应该能够适应并利用这些特点，从而提高团队的整体效能。\n实例或案例\n书中提到了在软件开发中采用\u0026quot;风险谋而后动\u0026quot;的方法，鼓励大规模头脑风暴以应对压力和风险的增加。这种方法反映了管理者如何应对团队中不同个性和需求的实际案例。\n在过去，曾经采取过一种措施，以防止某产品线或团队在非常规发布日期（不在周二或周四的发布日）进行紧急发布。有的公司前台放置了一个看板，上面贴着一张猪头图片，这个看板每个人都能看到。尽管这个措施看似是一种嬉笑怒骂的方式来阻止团队进行紧急发布，但它实际上起到了作用，紧急发布次数从每周数十次骤减至十几次。然而，这个措施也带来了一些负面影响：\n开发人员不再愿意修复一些小问题以优化用户体验，即使这些问题很容易修复，他们也会等到下一次正常发布日才会发布。例如，有一个SQL查询较慢，但不会严重影响性能，开发人员也会推迟修复，等待下一次发布日再处理。\n开发人员更加谨慎地对待新技术的引入，更倾向于使用已存在的代码，而不愿意自己编写新的代码，即使明知这些已存在的代码在质量和性能方面不够理想。例如，他们可能选择继续使用老员工编写的工具类，而不愿意自己编写新的反应式编程框架，尽管明白这些工具类的性能不如自行实现的框架。\n2. 主题二: 工作压力和质量 # 子观点1\n项目经理通常试图通过紧张的项目时间表来提高生产效率，但书中强调，压力并不一定导致更高质量或更快速的工作。\n子观点2\n过多的压力可能降低员工的工作满意度，导致低质量的工作和潜在的缺陷。\n实例或案例\n书中提到，项目经理常常抱怨开发人员的代码质量低下，但事实上，这种低质量可能是由于施加的过多压力造成的。\n举例来说，假设一个软件开发项目的截止日期紧迫，项目经理对开发团队施加了巨大的时间压力，要求在非常有限的时间内完成大量的工作。开发人员可能发现他们不得不加班工作，几乎每天都在高压下工作，以满足项目经理设定的紧急截止日期。\n在这种情况下，开发人员可能感到极大的焦虑和紧张。他们可能被迫忽略了代码的质量，因为他们没有足够的时间来执行详尽的测试和质量保证。他们可能会采用快速而不经思考的解决方案，以满足紧迫的交付需求，而不是投入时间来设计和实施更为健壮、可扩展的代码。\n结果，虽然项目可能会按时交付，但代码的质量可能会因为过度的时间压力而下降。这种低质量的代码可能导致后续的问题和缺陷，需要更多的时间和资源来修复，从而反而增加了整体的开发和维护成本。\n3. 主题三: 雇佣合适的人 # 子观点1\n雇佣合适的人比试图改变不合适的人更为有效。如果一个人一开始就不适合特定职位，那么即使经过培训和努力，他们可能永远也不会胜任。\n子观点2\n强调个性特征比知识因素更为重要。在招聘中，不应只寻找与现有员工相似的候选人，而应更加注重个体特点。\n实例或案例\n书中提到，强制性规则和流程会降低员工的活力和创新性。相反，应鼓励员工尝试新事物，以激发团队的创造力。\n一个具体的例子是，某个开发人员可能决定尝试一个全新的编程语言，因为他认为它可以更好地满足项目的需求。在严格的团队中，这种尝试可能会受到限制，但在开放的团队中，他们鼓励这种尝试，因为他们相信员工的创新可能会带来更好的结果。\n4. 主题四: 提高团队生产力 # 子观点1\n团队通常效率低于个体，但具有团结性的团队有更高的成就感。共同的目标和低流动率是团结团队的标志。\n子观点2\n优秀的项目经理在人际关系处理方面表现出色，鼓励团队成员与产品相关联。\n实例或案例\n书中提到自由协作活动，如晚餐或聚会，可以提高员工参与度，让他们感到自己未受管理。自由协作活动使员工感到不受管理，激发了他们的自发创新和合作精神。这种自由的环境可以改善员工的参与度，让他们更有归属感，同时也促进了创造力和团队协作。通过这样的自由交流，公司最终受益于更积极的员工和更多的创新点子。\n四、亮点与启发 # 最有影响的观点或实例:\n本书中最有影响的观点之一是管理人力资源的关键性。管理人员在软件开发过程中的作用常常被低估，但这本书强调了管理在成功的项目完成中所起的关键作用。以下是关于这个观点的详细阐述：这本书深刻地强调了软件开发是一项高度知识密集的工作，与传统的体力劳动有着显著的不同。在这个背景下，管理人员需要理解软件开发人员的需求和心理特点，以有效地领导他们。本书提出了一种管理方法，即尊重员工的个性和特征，这不仅提高了员工的工作满意度，还促进了更高的生产力。更进一步，书中认为，管理者应该鼓励开发人员参与关键决策，包括允许他们犯错误。这种方法增强了员工的责任感和参与感，使他们更有动力投入工作。管理者应理解，软件开发者大多是对自己的工作充满热情的人，因此需要以一种充满尊重和理解的方式来管理他们。\n另一个强调的观点是关于强制加班无意义。尽管许多项目经理试图通过延长员工的工作时间来提高生产力，但本书提醒我们，加班不一定会导致更好的结果。强制加班可能会导致员工疲劳、降低工作质量，甚至加速员工的离职。这一观点在当前强调工作与生活平衡的时代尤为重要。\n对个人或专业发展的启示:\n软件开发领域不仅仅关乎技术，更关乎管理。管理者应该以员工的需求和个性为重，鼓励创新和自主决策，以提高团队的效能。这本书提醒我们，软件开发是一项协作的工作，管理者的作用是创造一个积极的工作环境，促进团队的成功。这一观点不仅适用于软件开发，还可以应用于各种其他领域的团队管理。\n五、批评与局限性 # 任何有争议、模糊或过时的信息:\n尽管《人件》中提到了强制加班对于软件开发团队的负面影响，仍然有一些管理者可能会认为强制加班是提高生产力的有效方法。这涉及到一个争议点，即是否迫使员工加班在某些情况下可以提高项目的速度和完成时间。一些管理者可能会坚持认为，紧迫的项目时程要求在特殊情况下需要额外的工作时间，因此强制加班可能是必要的。\n然而，这一观点通常忽略了员工的工作生活平衡和健康问题。长期强制加班可能会导致员工疲惫、焦虑和生活质量下降，这不仅会降低他们的工作效率，还可能导致员工流失。因此，强制加班通常被视为不可持续的管理方法，特别是在现代强调员工福祉和工作与生活平衡的环境中。\n可能的不足或缺陷:\n《人件》中提出的方法在所有情况下是否都适用可能是一个问题。一些团队可能会发现某些原则在其特定情境下不适用。例如，某些项目可能因其特殊性质而需要更紧张的时间表，这可能会涉及到一定程度的加班。因此，管理者需要灵活运用《人件》中的原则，以适应其项目的特定需求。\n另一个潜在的不足是，一些管理者可能发现难以完全接受本书中提出的管理理念。改变传统的管理方法需要时间和努力，而一些组织可能对采用这些新方法感到抵触。管理者需要克服这种抵触情感，才能真正实施《人件》中的管理原则。\n此外，书中的一些建议和案例也可能在不同文化和国家之间产生不同的效果。因为文化、法规和员工期望因地区而异，所以需要谨慎考虑如何将这些原则应用到不同的全球背景中。\n六、实际应用和拓展 # 在实际工作/学习中如何应用这些概念:\n读者可以将本书中的观点应用于他们的团队管理实践中，尤其是关于管理人力资源和团队凝聚力的原则。此外，他们可以实验不同的方法来提高员工的工作满意度和创造性。\n对未来研究或实践的建议:\n未来研究可以进一步探讨不同行业和团队类型中的管理实践，并分析其成功案例。此外，可以研究更多关于工作满意度、员工流动率和团队效能之间关系的领域。\n七、总结与评价 # 对书籍的整体评价:\n人件》是一本具有深远影响的重要著作，特别是在软件开发领域。它强调了管理人力资源的重要性，以及如何在软件开发团队中培养卓越的团队凝聚力。这本书不仅为管理者提供了有力的管理理念，还为软件行业的从业者提供了宝贵的见解。作者的观点和建议旨在帮助团队更好地理解软件开发的复杂性，同时也为提高团队绩效和员工满意度提供了实际的方法。\n书籍的长处和短处:\n长处\n深刻的观点: 《人件》强调软件开发中人力资源管理的重要性，突出了员工的个性和需求对于团队成功的关键性。这种人本主义的管理方法在现代职场中变得越来越重要，因为员工的福祉和满意度与生产力密切相关。书中的深刻观点引导管理者更好地理解并尊重员工的需求。 实际的管理建议: 该书提供了大量实际的管理建议，包括如何提高团队绩效、培养创新文化、减少项目压力等。这些建议为管理者提供了操作指南，可帮助他们改善团队的工作效率和员工的满意度。 短处\n不适用于所有情况: 尽管《人件》中的原则在大多数情况下都是有益的，但并非所有情况下都适用。某些项目可能因其特殊性质而需要更紧张的时间表，这可能会涉及到一定程度的加班。因此，管理者需要在灵活运用这些原则时考虑其项目的特定需求。 可能受到文化差异的影响: 一些书中的建议和案例可能在不同文化和国家之间产生不同的效果。文化、法规和员工期望因地区而异，所以需要谨慎考虑如何将这些原则应用到不同的全球背景中。 八、参考文献 # 博客： https://www.cnblogs.com/sunjichen/p/12074505.html https://github.com/clsaa/thinking-in-se/blob/master/2.%E4%BA%BA%E4%BB%B6%E8%AF%BB%E5%90%8E%E6%84%9F.md https://www.cnblogs.com/xing901022/p/4230961.html ","date":"18 October 2023","permalink":"/read/%E4%BA%BA%E4%BB%B6/","section":"阅读","summary":"软件开发不仅仅是技术问题，更是管理问题。它强调了管理人力资源的关键性，特别是在软件领域，由于脑力劳动的特性，软件开发者与传统的体力劳动者有很大的不同。这本书提供了许多实际的管理方法，以促进团队协作，提高生产效率，同时提供了实例和案例来支持这些观点。该书的主要目标读者是软件组织的管理者，项目经理，团队领导者以及任何对软件开发和团队管理感兴趣的人。它适用于各种软件开发项目，从小型创业公司到大型企业。这本书提供的原则和实践可以适用于各种团队管理场景，不仅限于软件开发。","title":"《人件》读书笔记"},{"content":"Serverless Devs 是一个开源开放的 Serverless 开发者平台，致力于为开发者提供强大的工具链体系。通过该平台，开发者不仅可以一键体验多云 Serverless 产品，极速部署 Serverless 项目，还可以在 Serverless 应用全生命周期进行项目的管理，并且非常简单快速的将 Serverless Devs 与其他工具/平台进行结合，进一步提升研发、运维效能。\n平台/产品支持 # 目前 Serverless Devs 项目已经支持的 FaaS 平台/产品：\nHosted 阿里云函数计算（FC）: 项目仓库 AWS Lambda: 项目仓库 百度智能云函数计算（CFC）: 项目仓库 华为云函数工作流（FG）: 项目仓库 腾讯云云函数（SCF）: 项目仓库 Installable OpenFunction（ofn）: 项目仓库 Laf: 开发中\u0026hellip; 项目期望 # Serverless Devs 希望可以为 Serverless 开发者们提供一款可以无厂商锁定的，可以在 Serverless 应用全生命周期发挥作用的 Serverless 开发者工具； Serverless Registry 希望可以为 Serverless 生态提供一套完整的包管理规范，与 Python 中的 pypi， Nodejs 中的 npm 等类似，将以此来开放和分享 Serverless Package，建设 Serverless 生态； Serverless Developer Meetup 希望可以打造最符合 Serverless 开发者的社区活动，通过这个活动，希望更多人可以一起交流、学习 Serverless 相关的产品； 快速上手 # 本快速上手案例以 阿里云函数计算 为例的快速上手 Serverless Devs\n工具安装 # 第一步：安装 Node.js(\u0026gt;=12.0.0) 与 NPM 包管理工具； 第二步：安装 Serverless Devs 开发者工具； $ npm install @serverless-devs/s -g 第三步：可以通过s -v判断工具是否安装成功，如果安装成功可以看到相对应的版本信息，例如： @serverless-devs/s: 2.1.2, core: 0.1.41, s-home: /Users/xxx/.s, darwin-x64, node-v17.7.1 密钥配置 # 以阿里云密钥配置为例：\n获取密钥页面：https://usercenter.console.aliyun.com/#/manage/ak 打开 获取密钥页面 获取密钥信息 ：\n执行s config add，并选择Alibaba Cloud (alibaba)：\n$ s config add ? Please select a provider: Alibaba Cloud (alibaba) 🧭 Refer to the document for alibaba key: http://config.devsapp.net/account/alibaba ? AccessKeyID: 此时，可以按照引导，进行密钥的配置：\n? Please select a template: Alibaba Cloud (alibaba) 🧭 Refer to the document for alibaba key: http://config.devsapp.net/account/alibaba ? AccessKeyID 此处填写AccessKeyID ? AccessKeySecret 此处填写AccessKeySecret ? Please create alias for key pair. If not, please enter to skip alibaba-access Alias: alibaba-access AccountID: 自动获取AccountID AccessKeyID: 此处填写AccessKeyID AccessKeySecret: 此处填写AccessKeySecret ✔ Configuration successful 为了验证密钥是否正确配置，可以通过s config get -a alibaba-access进行指定密钥的查看：\n$ s config get -a alibaba-access alibaba-access: AccountID: 此处填*******tID AccessKeyID: 此处填*********yID AccessKeySecret: 此处填*************ret 上手体验 # Serverless：Hello World # 执行s命令：\n$ s ? No Serverless-Devs project is currently detected. Do you want to create a new project? (Y/n) 填写y，并按回车，可以进入到创建引导部分：\n🚀 More applications: https://registry.serverless-devs.com ? Hello Serverless for Cloud Vendors (Use arrow keys or type to search) ❯ Alibaba Cloud Serverless AWS Cloud Serverless Baidu Cloud Serverless Huawei Cloud Serverless Tencent Cloud Serverless Dev Template for Serverless Devs 此时只需要选择对应的选项，按照引导进行操作，即可。例如选择Alibaba Cloud Serverless，就可以看到阿里云Serverless产品下的应用模板分类:\n? Hello, serverlesser. Which template do you like? (Use arrow keys or type to search) ❯ Quick start [Deploy a Hello World function to FaaS] Container example [Deploy function to FaaS with custom-container] Web Framework [Deploy a web framework to FaaS] Static website [Deploy a static website] Best practice [Experience serverless project] 此时可以继续选择某分类下的具体应用进行初始化，例如选择Quick start之后，可以看到该分类下的具体模板应用：\n? Which template do you like? (Use arrow keys or type to search) ❯ [HTTP] Node.js 14 - 快速部署一个 nodejs14 http函数 [HTTP] Python3 - 快速部署一个 python3 http函数 [HTTP] Java8 - 快速部署一个 java8 http函数 [HTTP] PHP7 - 快速部署一个 php http函数 [HTTP] C++ (custom)- 快速部署一个 C++ http函数 [Event] Node.js 14 - 快速部署一个 nodejs14 event函数 [Event] Python3 - 快速部署一个 python3 event函数 ... ... 选择[HTTP] Node.js 14即可完成创建，在引导的过程中，可能会出现填写项目名称以及选择密钥的过程：\n项目名称可以是：start-fc-http-nodejs14 地域可以是：cn-hangzhou 服务名可以是： hello-world-service 函数名可以是： start-fc-http-nodejs14 密钥可以选择我们上文中创建过的：alibaba-access 例如：\n🚀 More applications: https://registry.serverless-devs.com ? Hello Serverless for Cloud Vendors Alibaba Cloud Serverless ? Hello, serverlesser. Which template do you like? Quick start [Deploy a Hello World function to FaaS] ? Which template do you like? [HTTP] Node.js 14 😋 Create application command: [s init devsapp/start-fc-http-nodejs14] ? Please input your project name (init dir) start-fc-http-nodejs14 ✔ file decompression completed Serverless Devs Application Case Cloud services required： - FC : https://fc.console.aliyun.com/ Tips： - FC Component: https://www.serverless-devs.com/fc/readme 创建应用所在的地区 ? 地域 cn-hangzhou 服务名称，只能包含字母、数字、下划线和中划线。不能以数字、中划线开头。长度在 1-128 之间 ? 服务名 hello-world-service 函数名称，只能包含字母、数字、下划线和中划线。不能以数字、中划线开头。长度在 1-64 之间 ? 函数名 start-fc-http-nodejs14 ? please select credential alias alibaba-access * Before using, please check whether the actions command in Yaml file is available * Carefully reading the notes in s.yaml is helpful for the use of the tool * If need help in the use process, please apply to join the Dingtalk Group: 33947367 🏄‍ Thanks for using Serverless-Devs 👉 You could [cd /Users/nanxuanli/work/demo/devs/start-fc-http-nodejs14] and enjoy your serverless journey! 🧭️ If you need help for this example, you can use [s -h] after you enter folder. 💞 Document ❤ Star: https://github.com/Serverless-Devs/Serverless-Devs 🚀 More applications: https://registry.serverless-devs.com ? Do you want to deploy the project immediately? (Y/n) 可以看到，系统在最后有一个提醒，是否要部署该项目，此时可以输入y，直接进行项目的部署，稍等片刻，可以看到部署结果：\nhelloworld: region: cn-hangzhou service: name: hello-world-service function: name: start-fc-http-nodejs14 runtime: nodejs14 handler: index.handler memorySize: 128 timeout: 60 url: system_url: https://start-fp-nodejs-hello-w-service-uxcvfbhdii.cn-hangzhou.fcapp.run custom_domain: - domain: http://start-fc-http-nodejs14.hello-world-service.1816647648916833.cn-hangzhou.fc.devsapp.net triggers: - type: http name: httpTrigger 此时可以打开domain返回给我们的域名，进行测试。\n人工智能：目标检测 # 初始化一个已有的人工智能目标检测项目：s init devsapp/image-prediction-app，初始化过程中可能会出现填写项目名称以及选择密钥的过程：\n项目名称可以是：image-prediction-app 密钥可以选择我们上文中创建过的：alibaba-access 例如：\n$ s init devsapp/image-prediction-app 🚀 Serverless Awesome: https://github.com/Serverless-Devs/package-awesome ? Please input your project name (init dir) image-prediction-app ✔ file decompression completed ? please select credential alias alibaba-access ___ __ __ _______ _______ _______ | | | |_| || _ || || | | | | || |_| || ___|| ___| | | | || || | __ | |___ | | | || || || || ___| | | | ||_|| || _ || |_| || |___ |___| |_| |_||__| |__||_______||_______| Welcome to the image-prediction-app application This application requires to open these services: FC : https://fc.console.aliyun.com/ This application can help you quickly deploy the image-prediction-app project. The application uses FC component：https://github.com/devsapp/fc The application homepage: https://github.com/devsapp/image-prediction-app 🏄‍ Thanks for using Serverless-Devs 👉 You could [cd /Users/jiangyu/start-application/image-prediction-app] and enjoy your serverless journey! 🧭️ If you need help for this example, you can use [s -h] after you enter folder. 💞 Document ❤ Star：https://github.com/Serverless-Devs/Serverless-Devs 进入项目目录：cd image-prediction-app\n通过deploy命令进行项目的部署：\nTips for next step ====================== * Display information of the deployed resource: s info * Display metrics: s metrics * Display logs: s logs * Invoke remote function: s invoke * Remove Service: s remove service * Remove Function: s remove function * Remove Trigger: s remove trigger * Remove CustomDomain: s remove domain imageAi: region: cn-hangzhou url: custom_domain: - domain: http://server.ai-cv-image-prediction.1583208943291465.cn-hangzhou.fc.devsapp.net 此时可以打开系统分配的测试域名，并上传一张图片进行测试。\n传统框架：基于Django的博客项目 # 初始化一个已有的基于Django的博客项目：s init django-blog，初始化过程中可能会出现填写项目名称以及选择密钥的过程：\n项目名称可以是：django-blog 密钥可以选择我们上文中创建过的：alibaba-access 例如：\n$ s init django-blog 🚀 Serverless Awesome: https://github.com/Serverless-Devs/package-awesome ? Please input your project name (init dir) django-blog ✔ file decompression completed ? please select credential alias alibaba-access ______ ___ _______ __ _ _______ _______ _______ ___ _______ _______ | | | || _ || | | || || || _ || | | || | | _ | | || |_| || |_| || ___|| _ || |_| || | | _ || ___| | | | | | || || || | __ | | | || || | | | | || | __ | |_| | ___| || || _ || || || |_| || _ | | |___ | |_| || || | | || || _ || | | || |_| || || |_| || || || |_| | |______| |_______||__| |__||_| |__||_______||_______||_______||_______||_______||_______| Welcome to the django-blog application This application requires to open these services: FC : https://fc.console.aliyun.com/ This application can help you quickly deploy the django-blog project. The application uses Django component：https://github.com/devsapp/django The application homepage: https://github.com/devsapp/django-blog * Python 3.7 is recommended; * If the version is greater than Python 3.7: * Operation error: ImportError: cannot import name \u0026#39;metadata\u0026#39; from \u0026#39;importlib\u0026#39;, you can refer to: https://stackoverflow.com/questions/59216175/importerror-cannot-import-name-metadata-from-importlib * Default information: * Admin：/admin * Default Admin Username: blog * Default Admin Password: myblog12345! 🏄‍ Thanks for using Serverless-Devs 👉 You could [cd /Users/jiangyu/django-blog] and enjoy your serverless journey! 🧭️ If you need help for this example, you can use [s -h] after you enter folder. 💞 Document ❤ Star：https://github.com/Serverless-Devs/Serverless-Devs 进入项目目录：cd django-blog\n通过deploy命令进行项目的部署：\nTips for next step ====================== * Invoke remote function: s invoke ✔ Try container acceleration djangoBlog: region: cn-shenzhen serviceName: serverless-devs-django functionName: django customDomains: - http://django.serverless-devs-django.1583208943291465.cn-shenzhen.fc.devsapp.net Resources # Github: https://github.com/Serverless-Devs/Serverless-Devs ","date":"17 October 2023","permalink":"/posts/architecture/serverless/serverless-dev/","section":"博客","summary":"Serverless Devs 是一个开源开放的 Serverless 开发者平台，致力于为开发者提供强大的工具链体系。通过该平台，开发者不仅可以一键体验多云 Serverless 产品，极速部署 Serverless 项目，还可以在 Serverless 应用全生命周期进行项目的管理，并且非常简单快速的将 Serverless Devs 与其他工具/平台进行结合，进一步提升研发、运维效能。","title":"Serverless Dev"},{"content":"Serverless 是一种计算模型，它使得开发者能够在无需管理服务器和基础架构的情况下运行代码（或称函数）。使用无服务器计算，开发者可以将代码上传到云平台，平台会在需要时根据流量自动进行资源分配和处理。\nServerless 的特点\n按需分配 无服务器计算基于事件驱动和按需调用，只在需要时才会进行计算资源的分配和管理 弹性伸缩 无服务器计算平台会自动根据负载量的变化进行资源的动态分配和优化，无需手动干预 简化开发与部署 开发者专注于编写核心业务逻辑代码，简化应用开发以及部署流程 Concept Models # Serverless 核心资源 # Service\n阿里云提供服务这一抽象 服务是函数计算资源管理的单位，同一服务下的所有函数共享一些设置，如服务授权和日志配置 一个应用可拆分为多个服务，一个服务可由多个函数组成 Function\n云函数，云函数由代码和运行环境描述组成 云函数可能依赖于其他云函数，或者外部服务，如对象存储，API 网关，消息队列 Trigger\n触发器，用于在满足某些条件时，触发 Function 的执行 基于事件驱动 常见的触发器类型 定时触发器 Cron Trigger API 网关触发器 HTTP Trigger 消息队列触发器 MQ Trigger External Service\n云函数在运行过程中，可能调用外部的服务完成任务，如调用 Redis 或 RDBMS 存储状态 Function # 表示 Serverless 函数\nFunction 基本信息\n包括函数的代码 URI，运行时，处理函数名称等\nFunction 资源需求\n指定函数运行时所需的计算资源，如内存，CPU 等\nFunction 触发器\nexport interface Function { runtime: string; codeDir: string; // source code directory to bundle resource: { memory: string; cpu: string; }; triggers: {}[]; } Trigger # 函数触发器，基于事件驱动机制触发函数执行\nexport interface Trigger { name: string; type: string; props: unknown; } Application # Application 表示当前部署的 Serverless 应用\n一个 Application 可能包含多个 Function\nApplication 需要指定默认的部署参数，比如使用的 Provider\nexport interface Application { provider: { name: string; props: {}; }; functions: Function[]; } 执行器会解析入口模块的 Application，并执行实际的部署\nFunction 部署过程涉及到的阶段 # 函数部署会涉及以下阶段\n状态查询：向集群查询部署状态，判断是否已有部署 构建：打包用户代码和依赖，执行构建，生成 Artifact 上传：将构建好的 Artifact 上传到指定的存储服务 执行部署：调用 Provider 提供的 API，创建函数资源 对于基于 Kubernetes 的 Serverless 平台，如 OpenFaas，OpenWhisk 和 Knative。函数的运行基本单位为 POD，产物为 Docker 镜像。\n对于腾讯云，阿里云等公有云厂商，产物为构建好的二进制文件，打包为 zip，并需要上传至 OSS\n","date":"17 October 2023","permalink":"/posts/architecture/serverless/serverless-concept-models/","section":"博客","summary":"Serverless 是一种计算模型，它使得开发者能够在无需管理服务器和基础架构的情况下运行代码（或称函数）。使用无服务器计算，开发者可以将代码上传到云平台，平台会在需要时根据流量自动进行资源分配和处理。","title":"Serverless Concept Models"},{"content":"元编程是编写可在其他程序上运行的计算机程序的技术。诸如编译器和程序分析器之类的系统可以被视为元程序，因为它们将其他程序作为输入。我们将在这里讨论的元编程形式特别关注生成要作为程序一部分包含的代码。从某种意义上说，它们可以被认为是初级编译器。\nMacros and Code Generation # macro 是将输入序列转换为某种替换输出序列的规则。这个翻译过程称为 macro expansion，一些语言提供宏作为其规范的一部分。宏设施可以被实现为 preprocessing step，其中宏扩展发生在 lexical and syntactic analysis 之前，或者它可以被合并为 syntax analysis 或 a later translation step。\n使用最广泛的 macro systems 之一是 C 预处理器（CPP），它作为处理程序的第一步被包含在 C 和 C++ 中。预处理器指令以散列符号开头，包括 #include、#define、#if 等。例如，下面定义了一个类似函数的 macro 来交换两个项目：\n#define SWAP(a, b) { auto tmp = b; b = a; a = tmp; } 然后，我们可以如下使用宏：\nint main() { int x = 3; int y = 4; SWAP(x, y); cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; y \u0026lt;\u0026lt; endl; } // output 4 3 通过向 g++ 传递 -E 标志，可以获得宏扩展的结果：\n$ g++ -E \u0026lt;source\u0026gt; 不过，如果使用 #includes 指令，结果可能会非常混乱，因为该指令会从给定文件中调入代码。\nCPP macros perform text 替换，因此上述代码等同于：\nint main() { int x = 3; int y = 4; { auto tmp = y; y = x; x = tmp; }; cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; y \u0026lt;\u0026lt; endl; } 使用 SWAP 宏后的分号仍然保留，表示空语句。不过，在需要使用单一语句的情况下，例如一个没有被 block 括住的条件分支，这就会造成问题：\nif (x \u0026lt; y) SWAP(x, y); else cout \u0026lt;\u0026lt; \u0026#34;no swap\u0026#34; \u0026lt;\u0026lt; endl; 避免这一问题的常用方法是将 macro 的扩展代码放在 do/while 中：\n#define SWAP(a, b) do { \\ auto tmp = b; \\ b = a; \\ a = tmp; \\ } while (false) 在这里，我们在一行的末尾添加了一个 \\，表示下一行应被视为上一行的继续。do/while 循环在语法上以分号结束，因此 SWAP(x, y); 中的分号在语法上是 do/while 循环的一部分。因此，扩展代码的语法是正确的：\nif (x \u0026lt; y) do { auto tmp = b; b = a; a = tmp; } while (false); else cout \u0026lt;\u0026lt; \u0026#34;no swap\u0026#34; \u0026lt;\u0026lt; endl; 虽然 textual replacement 很有用，但它也有缺点，因为虽然宏在语法上类似函数，但它们的行为却不像函数。具体来说，它们不把参数作为自己的实体，也不引入独立的作用域。请看下面的例子：\nint main() { int x = 3; int y = 4; int z = 5; SWAP(x \u0026lt; y ? x : y, z); cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; y \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; z \u0026lt;\u0026lt; endl; } // output 3 4 3 使用 g++ -E，我们可以看到预处理后的代码。只看 main() 的输出，我们会发现\nint main() { int x = 3; int y = 4; int z = 5; do { auto tmp = z; z = x \u0026lt; y ? x : y; x \u0026lt; y ? x : y = tmp; } while (false); cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; y \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; z \u0026lt;\u0026lt; endl; } 在这里，我们手动添加了换行符和空格，以使输出更易读；而预处理器本身会将宏输出放在一行中。罪魁祸首是最后生成的语句：\nx \u0026lt; y ? x : y = tmp; 在 C++ 中，条件运算符 ? : 和赋值运算符 = 具有相同的优先级，并且从右向左关联，因此这等同于\nx \u0026lt; y ? x : (y = tmp); 由于 x \u0026lt; y，这里没有赋值。因此，x 的值保持不变。\n我们可以在每次使用 macro argument 时加上括号来解决这个问题：\n#define SWAP(a, b) do { \\ auto tmp = (b); \\ (b) = (a); \\ (a) = tmp; \\ } while (false) 现在会产生预期的结果，因为括号明确地将这些操作联系起来：\nint main() { int x = 3; int y = 4; int z = 5; do { auto tmp = (z); (z) = (x \u0026lt; y ? x : y); (x \u0026lt; y ? x : y) = tmp; } while (false); cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; y \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; z \u0026lt;\u0026lt; endl; } 然而，第二个问题并不那么容易解决。考虑一下当我们将 SWAP 宏应用于名为 tmp 的变量时会发生什么：\nint main() { int x = 3; int tmp = 4; SWAP(tmp, x); cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; tmp \u0026lt;\u0026lt; endl; } // output 3 4 没有发生交换！再次使用 g++ -E 检查输出，我们可以看到（模数间隔）\nint main() { int x = 3; int tmp = 4; do { auto tmp = (x); (x) = (tmp); (tmp) = tmp; } while (false); cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; tmp \u0026lt;\u0026lt; endl; } 由于 SWAP 使用的临时变量与参数同名，因此临时变量会捕获生成代码中出现的参数。这是因为宏只是执行文本替换，并不能确保名称被解析到适当的作用域。因此，宏实际上并不使用按名称调用，而按名称调用可确保参数中的名称解析到适当的作用域。对文本替换的依赖使 CPP 成为一个不卫生的宏系统。其他系统（如 Scheme 的系统）是卫生的，它们为宏引入的名称创建了单独的作用域，并确保参数不会被捕获。\nScheme Macros # 作为 R5RS Scheme 规范的一部分，宏系统是卫生的。宏由 define-syntax、let-syntax 或 letrec-syntax 中的一种形式引入，并将给定的名称与宏绑定。例如，下面是 let 作为宏的定义：\n(define-syntax let (syntax-rules () ((let ((name val) ...) body1 body2 ... ) ((lambda (name ...) body1 body2 ... ) val ... ) ) ) ) syntax-rules 指定了宏转换的规则。第一个参数是规则模式与输入之间必须匹配的字面形式列表。例如，cond 形式中的 else 标识符。但在这种情况下，没有字面意义。syntax-rules 的其余参数指定了转换。转换的第一项是输入模式，第二项是输出模式。...的作用类似于克莱因星，将前一项与输入中出现的零次或多次相匹配。输入模式中出现但不在字面量列表中的名称，除了作为宏名称的第一项，都是与输入元素相匹配的卫生变量。这些变量可以在输出模式中引用，以指定如何构造输出。\n在全局环境中评估上述表达式时，会将 let 名称与一个转换为 lambda 的宏绑定。\n宏主体引入的标识符保证避免与其他标识符冲突，解释器通常会重命名标识符以避免冲突。下面是 swap 宏的定义：\n(define-syntax swap (syntax-rules () ((swap a b) (let ((tmp b)) (set! b a) (set! a tmp) ) ) ) ) 这就将 swap 的使用转化为一个表达式，通过临时变量 tmp 交换两个参数。因此\n\u0026gt; (define x 3) \u0026gt; (define y 4) \u0026gt; (swap x y) \u0026gt; x 4 \u0026gt; y 3 不过，与 CPP 宏不同的是，swap 宏引入的 tmp 与其他任何 tmp 都是不同的：\n\u0026gt; (define tmp 5) \u0026gt; (swap x tmp) \u0026gt; x 5 \u0026gt; tmp 4 因为宏在 Scheme 中是卫生的，所以我们会得到预期的行为。\n为了支持宏，Scheme 解释器的评估过程会像往常一样评估列表中的第一个项目。如果评估结果是一个宏，那么解释器将对列表的其余部分执行宏扩展，而不会首先评估参数。扩展引入的任何名称都与其他名称置于不同的作用域。扩展后，解释器会对扩展结果重复求值过程，因此，如果最终结果是一个 let 表达式 (如上文 swap 中的表达式)，就会对该表达式进行求值。\n一个宏定义可以指定多个模式规则。再加上扩展的结果会被求值，这使得宏定义可以递归，如下面的 let* 定义：\n(define-syntax let* (syntax-rules () ((let* () body1 body2 ... ) (let () body1 body2 ... ) ) ((let* ((name1 val1) (name2 val2) ...) body1 body2 ... ) (let ((name1 val1)) (let* ((name2 val2) ...) body1 body2 ... ) ) ) ) ) 当 let* 没有绑定时，有一种基本模式，在这种情况下，它直接转化为一个 let。当至少有一个绑定时，也有一个递归模式，在这种情况下，let* 会转化为一个嵌套在 let 中的更简单的 let*。宏定义中的省略号 (...) 类似于正则表达式中的 Kleene 星号 (*)，表示前一项可以匹配零次或多次。因此，具有单一绑定的 let* 与上述第二条模式规则相匹配，其中 (name2 val2) 匹配次数为零。\nCPP Macros # 我们再来看看 CPP 宏。尽管宏不卫生，但在涉及元编程的任务中却非常有用。\nCPP 允许我们使用 #define 来定义两种类型的 macro，即 object-like macro 和 function-like macro。object-like macro 是一种简单的文本替换，用一个文本序列替换另一个文本序列。在历史上，定义常量是一种常用的方法：\n#define PI 3.1415926535 int main() { cout \u0026lt;\u0026lt; \u0026#34;pi = \u0026#34; \u0026lt;\u0026lt; PI \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; \u0026#34;tau = \u0026#34; \u0026lt;\u0026lt; PI * 2 \u0026lt;\u0026lt; endl; } 在 C++ 中，更好的做法是使用 const 或 constexpr 定义常量。\nfunction-like macro 接受参数 (如上文的 SWAP)，并能将参数文本替换到替换文本中的特定位置。\n使用 function-like macro 的一个更复杂的例子是对遵循相同模式的多段代码进行抽象定义。请看表示复数的类型定义：\nstruct Complex { double real; double imag; }; ostream \u0026amp;operator\u0026lt;\u0026lt;(ostream \u0026amp;os, Complex c) { return os \u0026lt;\u0026lt; \u0026#34;(\u0026#34; \u0026lt;\u0026lt; c.real \u0026lt;\u0026lt; \u0026#34;+\u0026#34; \u0026lt;\u0026lt; c.imag \u0026lt;\u0026lt; \u0026#34;i)\u0026#34;; } 假设除了上述重载的流插入操作符之外，我们还希望支持算术运算 +、- 和 *。这些运算的基本形式相同：\nComplex operator \u0026lt;op\u0026gt;(Complex a, Complex b) { return Complex{ \u0026lt;expression for real\u0026gt;, \u0026lt;expression for imag\u0026gt; }; } 在这里，我们使用了 uniform initialization syntax 来初始化一个含有其成员值的 Complex。然后，我们可以编写一个 function-like macro 来抽象这个结构：\n#define COMPLEX_OP(op, real_part, imag_part) \\ Complex operator op(Complex a, Complex b) { \\ return Complex{ real_part, imag_part }; \\ } 该 macro 的参数包括运算符、计算实部的表达式和计算虚部的表达式。我们可以使用下面的宏来定义运算：\nCOMPLEX_OP(+, a.real+b.real, a.imag+b.imag); COMPLEX_OP(-, a.real-b.real, a.imag-b.imag); COMPLEX_OP(*, a.real*b.real - a.imag*b.imag, a.imag*b.real + a.real*b.imag); 与我们最初的 SWAP 实现一样，尾部的分号是多余的，但却提高了可读性以及与语法高亮程序的交互性。使用 g++ -E 在预处理器中运行代码，我们可以得到（修改间距）：\nComplex operator +(Complex a, Complex b) { return Complex{ a.real+b.real, a.imag+b.imag }; }; Complex operator -(Complex a, Complex b) { return Complex{ a.real-b.real, a.imag-b.imag }; }; Complex operator *(Complex a, Complex b) { return Complex{ a.real*b.real - a.imag*b.imag, a.imag*b.real + a.real*b.imag }; }; 接下来，我们可以定义 Complex 和 double 之间的运算。我们再次发现，这种操作具有特定的模式：\nComplex operator \u0026lt;op\u0026gt;(\u0026lt;type1\u0026gt; a, \u0026lt;type2\u0026gt; b) { return \u0026lt;expr1\u0026gt; \u0026lt;op\u0026gt; \u0026lt;expr2\u0026gt;; } 这里，\u0026lt;exprN\u0026gt; 是转换为 Complex 表示的相应参数。我们可以使用宏对其进行抽象：\n#define REAL_OP(op, typeA, typeB, argA, argB) \\ Complex operator op(typeA a, typeB b) { \\ return argA op argB; \\ } 我们还可以定义一个宏，将 double 转换为 Complex：\n#define CONVERT(a) \\ (Complex{ a, 0 }) 这样，我们就可以对操作进行如下定义：\nREAL_OP(+, Complex, double, a, CONVERT(b)); REAL_OP(+, double, Complex, CONVERT(a), b); REAL_OP(-, Complex, double, a, CONVERT(b)); REAL_OP(-, double, Complex, CONVERT(a), b); REAL_OP(*, Complex, double, a, CONVERT(b)); REAL_OP(*, double, Complex, CONVERT(a), b); 通过预处理器运行，我们可以得到\nComplex operator +(Complex a, double b) { return a + (Complex{ b, 0 }); }; Complex operator +(double a, Complex b) { return (Complex{ a, 0 }) + b; }; Complex operator -(Complex a, double b) { return a - (Complex{ b, 0 }); }; Complex operator -(double a, Complex b) { return (Complex{ a, 0 }) - b; }; Complex operator *(Complex a, double b) { return a * (Complex{ b, 0 }); }; Complex operator *(double a, Complex b) { return (Complex{ a, 0 }) * b; }; 现在我们可以如下使用复数：\nint main() { Complex c1{ 3, 4 }; Complex c2{ -1, 2 }; double d = 0.5; cout \u0026lt;\u0026lt; c1 + c2 \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; c1 - c2 \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; c1 * c2 \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; c1 + d \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; c1 - d \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; c1 * d \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; d + c1 \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; d - c1 \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; d * c1 \u0026lt;\u0026lt; endl; } (2+6i) (4+2i) (-11+2i) (3.5+4i) (2.5+4i) (1.5+2i) (3.5+4i) (-2.5+-4i) (1.5+2i) Stringification and Concatenation # 在使用宏时，将宏参数转换为字符串或将其与其他标记连接起来可能很有用。例如，假设我们要编写一个交互式应用程序，读取用户的输入并执行相应的操作。对于复数，目标函数可能如下：\nComplex Complex_conjugate(Complex c) { return Complex{ c.real, -c.imag }; } string Complex_polar(Complex c) { return \u0026#34;(\u0026#34; + to_string(sqrt(pow(c.real, 2) + pow(c.imag, 2))) + \u0026#34;,\u0026#34; + to_string(atan(c.imag / c.real)) + \u0026#34;)\u0026#34;; } 应用程序会将用户输入与代表操作的字符串进行比较，调用相应的函数，并打印出结果。这就是常见的模式：\nif (\u0026lt;input\u0026gt; == \u0026#34;\u0026lt;action\u0026gt;\u0026#34;) cout \u0026lt;\u0026lt; Complex_\u0026lt;action\u0026gt;(\u0026lt;value\u0026gt;) \u0026lt;\u0026lt; endl; 在这里，我们既需要动作的字符串表示法，也需要将 Complex_ 标记与动作标记本身连接起来的能力。我们可以为这种模式定义如下宏：\n#define ACTION(str, name, arg) \\ if (str == #name) \\ cout \u0026lt;\u0026lt; Complex_ ## name(arg) \u0026lt;\u0026lt; endl 标记前的 ## 是字符串化运算符，将标记转换为字符串。Complex_ 和 name 之间的 ## 是标记粘贴操作符，用于连接两边的标记。\n这样，我们就可以编写如下应用代码：\nComplex c1 { 3, 4 }; string s; while (cin \u0026gt;\u0026gt; s) { ACTION(s, conjugate, c1); ACTION(s, polar, c1); } 通过预处理器运行这个程序，我们就能得到想要的结果：\nComplex c1 { 3, 4 }; string s; while (cin \u0026gt;\u0026gt; s) { if (s == \u0026#34;conjugate\u0026#34;) cout \u0026lt;\u0026lt; Complex_conjugate(c1) \u0026lt;\u0026lt; endl; if (s == \u0026#34;polar\u0026#34;) cout \u0026lt;\u0026lt; Complex_polar(c1) \u0026lt;\u0026lt; endl; } The Macro Namespace # 使用 CPP 宏的一个缺陷是，它们不包含在任何特定的命名空间中。事实上，只要定义了宏，它就能替换任何符合条件的标识符，无论该标识符位于何处。因此，定义一个宏就相当于让一个特定的标识符充当保留关键字，程序员无法使用。这也是为什么常量通常最好定义为变量，限定为 const 或 constexpr，而不是类似对象的宏的原因之一。\n为避免污染全局命名空间，使用了几种约定。\n第一种是在所有宏的前缀加上定义宏的库所特有的字符，以避免与其他库发生冲突。例如，我们的复数宏可以用 COMPLEX_ 作为前缀，以避免与其他宏或标识符冲突。 第二种策略是在不再需要宏时，使用 #undef 预处理器指令取消对宏的定义。例如，在库代码的末尾，我们可能有如下代码： #undef COMPLEX_OP #undef REAL_OP #undef CONVERT #undef ACTION 这样，标识符就可以在以后的代码中用于其他目的。\nCode Generation # 虽然 macros 允许我们使用一种语言提供的宏设施生成代码，但在某些情况下，这种设施无法使用或不足以满足我们的目的。在这种情况下，在外部程序中用同一种语言或另一种语言编写代码生成器可能会比较方便。这种技术也称为 automatic programming。\n例如，R5RS Scheme 规范要求实现提供 car 和 cdr 的组合，最深可达四层。例如，(caar x) 应等同于 (car (car x))，(caddar x) 应等同于 (car (cdr (cdr (car x))))。除了 car 和 cdr 本身外，还需要提供 28 种组合，手工编写既繁琐又容易出错。相反，我们可以定义下面的 Python 脚本来生成一个 Scheme 库文件：\nimport itertools def cadrify(seq): if len(seq): return \u0026#39;(c{0}r {1})\u0026#39;.format(seq[0], cadrify(seq[1:])) return \u0026#39;x\u0026#39; def defun(seq): return \u0026#39;(define (c{0}r x) {1})\u0026#39;.format(\u0026#39;\u0026#39;.join(seq), cadrify(seq)) for i in range(2, 5): for seq in itertools.product((\u0026#39;a\u0026#39;, \u0026#39;d\u0026#39;), repeat=i): print(defun(seq)) cadrify() 函数是一个递归函数，它接收一个序列，如 ('a','d','a')，并使用第一个项目和序列其余部分的递归结果构造一个调用。在本例中，后者是 (cdr (car x))，因此结果是 (car (cdr (car x)))。基本情况是序列为空，只产生 x。\ndefun() 函数接收一个序列，并用它为相应的组合构建定义。它调用 cadrify() 构建正文。对于序列 ('a','d','a')，结果是\n(define (cadar x) (car (cdr (car x)))) 最后，循环结束时会产生每个长度的 a 和 d 的所有组合。它使用函数库中的 itertools.product() 函数来获得一个序列，该序列是元组 ('a','d') 的第 i 次幂。对于每个组合，它都会调用 defun() 生成该组合的函数。\n(define (caar x) (car (car x))) (define (cadr x) (car (cdr x))) (define (cdar x) (cdr (car x))) (define (cddr x) (cdr (cdr x))) (define (caaar x) (car (car (car x)))) (define (caadr x) (car (car (cdr x)))) ... (define (cdddar x) (cdr (cdr (cdr (car x))))) (define (cddddr x) (cdr (cdr (cdr (cdr x))))) 我们可以将生成的代码放入标准库中，由 Scheme 解释器加载。\nTemplate Metaprogramming # Template metaprogramming 是一种在编译时使用模板生成源代码的技术，然后将源代码与程序的其他代码一起编译。它通常是指利用语言的模板实例化规则进行编译时执行的一种形式。Template metaprogramming 在 C++ 中最为常见，但也有少数其他语言可以使用。\nC++ 中模板元编程的关键是 template specialization，它允许编写专门的定义来实例化带有特定参数的模板。例如，考虑一个包含静态值域的类模板，如果模板参数为 int，该值域为 true，否则为 false。我们可以如下编写通用模板：\ntemplate \u0026lt;class T\u0026gt; struct is_int { static const bool value = false; }; 现在我们可以定义当参数为 int 时该模板的特殊化：\ntemplate \u0026lt;\u0026gt; struct is_int\u0026lt;int\u0026gt; { static const bool value = true; }; 特化中的模板参数列表包含非特化参数。在上面的例子中，没有任何参数，所以是空的。然后，在模板名称之后，我们提供了实例化的全部参数集，在本例中只有 int。然后，我们提供实例化的其余定义。\n现在，当我们使用模板时，如果模板参数与特化兼容，编译器就会使用特化，否则就会使用通用模板：\ncout \u0026lt;\u0026lt; is_int\u0026lt;double\u0026gt;::value \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; is_int\u0026lt;int\u0026gt;::value \u0026lt;\u0026lt; endl; // output 0 1 模板特化使我们能够编写以模板参数为条件的代码。与递归实例化相结合，这使得模板实例化具有图灵完备性。模板不对可变变量进行编码，因此模板元编程实际上是函数式编程的一种形式。\nPairs # 举个更复杂的例子，让我们定义可在编译时操作的 pairs 和 lists。这些结构中存储的元素将是任意类型。\n在开始定义 pairs 之前，我们先构建一个报告机制，以便在编译时检查结果。我们将在编译器生成的错误信息中包含相关信息：\ntemplate \u0026lt;class A, int I\u0026gt; struct report { static_assert(I \u0026lt; 0, \u0026#34;report\u0026#34;); }; 为了简单起见，我们使用了一个 integer 模板参数，当然也可以使用类型对数字进行编码。在实例化 report 模板时，如果模板参数 I 为非负，static_assert 会引发错误。请看下面的内容：\nreport\u0026lt;int, 5\u0026gt; foo; 编译器会报错，指出是哪个实例导致 static_assert 失败。在 Clang 中，我们会得到类似下面的错误：\npair.cpp:64:3: error: static_assert failed \u0026#34;report\u0026#34; static_assert(I \u0026lt; 0, \u0026#34;report\u0026#34;); ^ ~~~~~ pair.cpp:67:16: note: in instantiation of template class \u0026#39;report\u0026lt;int, 5\u0026gt;\u0026#39; requested here report\u0026lt;int, 5\u0026gt; foo; 使用 GCC，错误如下：\npair.cpp: In instantiation of \u0026#39;struct report\u0026lt;int, 5\u0026gt;\u0026#39;: pair.cpp:67:16: required from here main.cpp:64:3: error: static assertion failed: report static_assert(I \u0026lt; 0, \u0026#34;report\u0026#34;); ^ 两个编译器都报告了相关信息，即报告模板的参数是 int 和 5。\n这样，我们就可以定义 pair 模板如下：\ntemplate \u0026lt;class First, class Second\u0026gt; struct pair { using car = First; using cdr = Second; }; 在 template 中，我们定义了类型别名 car 和 cdr，用于指代配对的第一项和第二项。因此，pair\u0026lt;int, double\u0026gt;::car 是 int 的别名，而 pair\u0026lt;int, double\u0026gt;::cdr 是 double 的别名。\n我们还可以定义类型别名，以便从数据对中提取第一项和第二项：\ntemplate \u0026lt;class Pair\u0026gt; using car_t = typename Pair::car; template \u0026lt;class Pair\u0026gt; using cdr_t = typename Pair::cdr; 在 Pair::car 和 Pair::cdr 之前需要使用 typename 关键字，因为我们使用的是嵌套类型，其外层类型依赖于模板参数。在这种情况下，C++ 无法确定我们命名的是一个类型而不是一个值，因此 typename 关键字明确表示这是一个类型。使用上述别名，car_t\u0026lt;pair\u0026lt;int, double\u0026gt;\u0026gt; 是 int 的别名，而 cdr_t\u0026lt;pair\u0026lt;int, double\u0026gt;\u0026gt; 是 double 的别名。\n为了表示递归列表，我们需要一个空列表的表示方法：\nstruct nil { }; 现在我们可以定义一个模板来判断一个列表是否为空，这个列表可以用空列表 nil 或以 nil 结尾的对序列来表示。我们定义了一个通用模板，然后针对以 nil 作为参数的情况定义了一个特殊化模板：\ntemplate \u0026lt;class List\u0026gt; struct is_empty { static const bool value = false; }; template \u0026lt;\u0026gt; struct is_empty\u0026lt;nil\u0026gt; { static const bool value = true; }; 为了在编译时使用 value，它必须是一个 compile-time constant，我们可以将其设置为 static 和 const，并使用编译时常量对其进行初始化。在 C++14 中，我们还可以定义全局变量模板（global variable templates）来编码 list 的长度：\ntemplate \u0026lt;class List\u0026gt; const bool is_empty_v = is_empty\u0026lt;List\u0026gt;::value; is_empty_v\u0026lt;nil\u0026gt; 的值为 true，而 is_empty\u0026lt;pair\u0026lt;int, nil\u0026gt;\u0026gt; 的值为 false。这样，我们就可以在编译时确定列表是否为空：\nusing x = pair\u0026lt;char, pair\u0026lt;int, pair\u0026lt;double, nil\u0026gt;\u0026gt;\u0026gt;; using y = pair\u0026lt;float, pair\u0026lt;bool, nil\u0026gt;\u0026gt;; using z = nil; report\u0026lt;x, is_empty_v\u0026lt;x\u0026gt;\u0026gt; a; report\u0026lt;y, is_empty_v\u0026lt;y\u0026gt;\u0026gt; b; report\u0026lt;z, is_empty_v\u0026lt;z\u0026gt;\u0026gt; c; 在这里，我们为列表引入了类型别名，作为编译时不可变的变量。然后，我们用类型和是否为空实例化报告。这样，GCC 会给出以下错误信息：\npair.cpp: In instantiation of \u0026#39;struct report\u0026lt;pair\u0026lt;char, pair\u0026lt;int, pair\u0026lt;double, nil\u0026gt; \u0026gt; \u0026gt;, 0\u0026gt;\u0026#39;: pair.cpp:82:28: required from here pair.cpp:73:3: error: static assertion failed: report static_assert(I \u0026lt; 0, \u0026#34;report\u0026#34;); ^~~~~~~~~~~~~ pair.cpp: In instantiation of \u0026#39;struct report\u0026lt;pair\u0026lt;float, pair\u0026lt;bool, nil\u0026gt; \u0026gt;, 0\u0026gt;\u0026#39;: pair.cpp:83:28: required from here pair.cpp:73:3: error: static assertion failed: report pair.cpp: In instantiation of \u0026#39;struct report\u0026lt;nil, 1\u0026gt;\u0026#39;: pair.cpp:84:28: required from here pair.cpp:73:3: error: static assertion failed: report 检查报告的整数参数，我们会发现列表 pair\u0026lt;char、pair\u0026lt;int、pair\u0026lt;double、nil\u0026gt;\u0026gt; 和 pair\u0026lt;float、pair\u0026lt;bool、nil\u0026gt;\u0026gt; 不是空的，但列表 nil 是空的。\n我们可以使用递归计算列表的长度：\ntemplate \u0026lt;class List\u0026gt; struct length { static const int value = length\u0026lt;cdr_t\u0026lt;List\u0026gt;\u0026gt;::value + 1; }; template \u0026lt;\u0026gt; struct length\u0026lt;nil\u0026gt; { static const int value = 0; }; template \u0026lt;class List\u0026gt; const int length_v = length\u0026lt;List\u0026gt;::value; 在这里，我们使用的是 length 结构递归实例化的值。由于 value 是通过由编译时常量之间的运算组成的表达式初始化的，因此它也是一个编译时常量。与 is_empty_v 一样，我们定义了一个变量模板 length_v 来对结果进行编码。我们可以计算并报告 x 类型别名的长度：\nreport\u0026lt;x, length_v\u0026lt;x\u0026gt;\u0026gt; d; 报告的第一个参数是任意的，因为我们只关心第二个参数，所以我们只传递 x 本身。我们得到：\npair.cpp: In instantiation of \u0026#39;struct report\u0026lt;pair\u0026lt;char, pair\u0026lt;int, pair\u0026lt;double, nil\u0026gt; \u0026gt; \u0026gt;, 3\u0026gt;\u0026#39;: pair.cpp:85:26: required from here pair.cpp:73:3: error: static assertion failed: report 相关信息是长度为 3。\n我们还可以定义更复杂的列表操作。例如，我们可以将列表倒转如下：\ntemplate \u0026lt;class List, class SoFar\u0026gt; struct reverse_helper { using type = typename reverse_helper\u0026lt;cdr_t\u0026lt;List\u0026gt;, pair\u0026lt;car_t\u0026lt;List\u0026gt;, SoFar\u0026gt;\u0026gt;::type; }; template \u0026lt;class SoFar\u0026gt; struct reverse_helper\u0026lt;nil, SoFar\u0026gt; { using type = SoFar; }; template \u0026lt;class List\u0026gt; using reverse_t = typename reverse_helper\u0026lt;List, nil\u0026gt;::type; 在这里，我们使用一个辅助模板来执行反转，其中第一个模板参数是剩余列表，第二个模板参数是到目前为止的反转列表。在每一步中，我们以 pair\u0026lt;car_t\u0026lt;List\u0026gt;, SoFar\u0026gt; 的形式计算新的部分结果，将剩余列表中的第一个项目添加到前一个部分结果的前面。然后 cdr_t\u0026lt;List\u0026gt; 是除去第一个项目的剩余列表。\n递归的基本情况是剩余列表为 nil，在这种情况下，最终结果与部分结果相同。我们使用部分类模板特化来实现这一点，它允许我们只特化类模板的部分参数。在 reverse_helper 中，我们对第一个参数进行了特殊化，这样，任何第一个参数为 nil 的 reverse_helper 实例都将使用特殊化。特化保留了一个模板参数，该参数包含在参数列表中。完整的参数列表会出现在模板名称之后，包括特化和未特化的参数。\nRESOURCES # https://eecs390.github.io/notes/metaprogramming.html http://philo.top/2021/03/14/metaprogramming/ ","date":"16 October 2023","permalink":"/posts/language/metaprogramming/","section":"博客","summary":"元编程是编写可在其他程序上运行的计算机程序的技术。诸如编译器和程序分析器之类的系统可以被视为元程序，因为它们将其他程序作为输入。我们将在这里讨论的元编程形式特别关注生成要作为程序一部分包含的代码。从某种意义上说，它们可以被认为是初级编译器。","title":"meta-programming"},{"content":"","date":"16 October 2023","permalink":"/tags/metaprogramming/","section":"Tags","summary":"","title":"Metaprogramming"},{"content":"Introduction # 有限状态机 (finite-state machine, FSM) 是一种抽象机器，在任何给定时间内都可以处于有限个状态中的一个状态。FSM 可以根据某些外部输入从一种状态转换到另一种状态，从一种状态转换到另一种状态称为转换。FSM 是由其状态列表、初始状态和每个转换的条件定义的。\n状态是对等待执行转换的系统状态的描述。\nExample # 下面是一个简单状态机的直观描述。 这是一个简单的开关模型。\n它由 \u0026ldquo;开 \u0026ldquo;和 \u0026ldquo;关 \u0026ldquo;两种状态组成。因此，这台机器在任何时刻都可以处于这两种状态中的一种。换句话说，状态之间的转换是瞬时的。flick 事件会导致机器在不同状态间转换。 当机器进入 on 状态时，会产生一个副作用。一盏灯被打开。 当机器退出 on 状态时，会产生另一个副作用。一盏灯被关闭。 这个简单的状态机就相当于一个布尔变量，来控制某件事情的 on 与 off。 What is state anyway? # 程序状态是程序中所有变量及其在任意时间点上的值的集合 Wikipedia 。\n一个程序或软件组件有五个独立变量，每个变量都可能为真或假，那么理论上它可以处于 $2^5=32$ 种状态中的任何一种。然而，程序经常会出现 invalid 状态，而在传统软件中，变量都会经过仔细检查和处理，以避免出现这些 invalid 状态。\nRelationship with statecharts # 理解状态机几乎等同于理解 statecharts。 在许多方面，statecharts是状态机的 \u0026ldquo;大哥\u0026rdquo;，旨在克服状态机的一些局限性。statecharts 本质上是一种状态机，它允许任何状态以分层的方式包含更多的状态机。这是为了克服状态机固有的一些局限性。\nAbstract machine vs run-time # 抽象机器本身（如状态机的绘制或代码）与特定抽象机器更具体的运行时执行之间必须作出重要区分。这种区别类似于类（抽象定义）和对象（具体实例化）之间的区别。同样，对于单个抽象机器来说，可能有许多执行，就像任何特定类往往有许多实例一样。\n术语 \u0026ldquo;statechart\u0026rdquo;、\u0026ldquo;state machine \u0026ldquo;和 \u0026ldquo;FSM \u0026ldquo;通常既指抽象形式，也指运行时形式，尽管运行时形式有时带有限定词 \u0026ldquo;run\u0026rdquo; 或 \u0026ldquo;execution\u0026rdquo;，如 \u0026ldquo;state machine execution\u0026rdquo; 或 \u0026ldquo;statetchart run\u0026rdquo;。\nabstract state machine 是一种软件组件，它定义了一组有限的状态：\n有一种状态被定义为 initial state。当机器开始执行时，它会自动进入这种状态。 每个状态都可以定义机器进入或退出该状态时发生的操作。操作通常会产生 side effects。 每个状态都能定义触发转换的 transition。 transition 定义了机器如何对事件做出反应，即退出一种状态并进入另一种状态。 transition 可以定义过渡发生时的 actions。动作通常会产生 side effects。 在运行状态机时，会执行这个抽象状态机。首先，状态机进入initial state。 然后，一旦有事件发生，就会立即传递给状态机。 当事件发生时\n将根据当前状态的转换对事件进行检查 如果某个转场与事件相匹配，该转场就会发生 由于 transition 发生，状态 exited 或 entered，并执行相关操作 机器立即进入新状态，准备处理下一个事件。 Resources # Wikipedia defines a finite-state machine statecharts ","date":"16 October 2023","permalink":"/posts/architecture/iot/state-machine/","section":"博客","summary":"有限状态机 (finite-state machine, FSM) 是一种抽象机器，在任何给定时间内都可以处于有限个状态中的一个状态。FSM 可以根据某些外部输入从一种状态转换到另一种状态，从一种状态转换到另一种状态称为转换。","title":"State Machine"},{"content":"","date":"15 October 2023","permalink":"/tags/antlr/","section":"Tags","summary":"","title":"Antlr"},{"content":"简介 # ANTLR 是 ANother Tool for Language Recognition 的缩写，是一个功能强大的解析器生成器框架，用于从语法文件中构建语言识别器、编译器和翻译器，语法文件中包含从源语言到目标语言的每个语句所要执行的操作。\n使用编译器设计的概念来定义每种现代编程语言的写作风格。这是一套典型的步骤，首先是 Lexical, Syntactical 和 Semantic Analysis，确定语言的基本编写方式，以便识别。接下来是一系列非常有趣的步骤：中间代码生成、优化和目标代码生成。\n目前的版本为 4.7，它提供了一种方便的、对开发人员友好的方式来定义自己的规则集（又称语法），它由一系列标记和操作组成，这些标记和操作定义了语句在源语言中的书写方式，从而可以正确识别和解析语句。更有趣的是，它还能让用户对代码进行操作，并将其生成目标代码，所有这一切都可以用您选择的语言来实现。\n那么，谁在使用 ANTLR 呢？\n编程语言：Boo、Groovy、Mantra、Nemerle、XRuby 等。 其他工具、框架：Apache Spark、Hibernate、Intellij IDEA、Jazillian、JBoss Rules、Keynote(Apple)、WebLogic（Oracle）等。 The Basics # def sum(a, b): return a + b 考虑到上面 Python 的例子，这些编译器设计步骤从识别 Python 中编写的每条语句（源代码）的基本单元开始，并将其分解为 a stream of tokens，每个标记都被识别或映射为特定类型，也就是 Lexical Analysis。\n然后，根据这些标记出现的顺序来确定书面语句的上下文，并通过语义分析构建一棵树（或 Abstract Syntax Tree）来检查其正确性，同时提供使用现有树遍历方法之一进行遍历的能力。\nTLDR 概念 # Lexer : converts a stream of characters to a stream of tokens. Parser : processes of tokens, possibly creating AST. Abstract Syntax Tree(AST): an intermediate tree representation of the parsed input that is simpler to process than the stream of tokens. Tree Parser: It processes an AST. String Template: a library that supports using templates with placeholders for outputting text (something very specific to ANTLR). ANTLR 是一种 LL parser（Left-to-right, Leftmost derivation），是一种自顶向下的剖析器，适用于无上下文语言的子集。它从左到右解析输入，对句子进行 Leftmost derivation。它简化了许多步骤，使创建语言识别器和解析器变得更容易、更方便。\nExample1 # 下面是我为解析 python 函数而编写的解析器的一个快速示例。\ngrammar PythonParserExample; tokens { INDENT, DEDENT, LINE_BREAK } statement : (single_input)? EOF ; single_input : LINE_BREAK | simple_stmt | complex_stmt (LINE_BREAK)? EOF ; complex_stmt: funcdef; simple_stmt : small_stmt (SEMI_COLON small_stmt)* SEMI_COLON? (LINE_BREAK)? EOF ; stmt : simple_stmt | complex_stmt ; funcdef : ASYNC? DEF name OPEN_PAREN typedargslist? CLOSE_PAREN (ARROW name)? COLON \u0026#39;\\n\u0026#39;? funcbody ; typedargslist : (def_parameters COMMA)? (args (COMMA def_parameters)? ) COMMA? | def_parameters COMMA? ; funcbody : simple_stmt | LINE_BREAK INDENT stmt+ DEDENT ; args : STAR name ; def_parameters : def_parameter (COMMA def_parameter)* ; small_stmt : RETURN expression #return_stmt ; def_parameter : name | STAR ; expression: name op=OPERATOR name ; name: NAME; DEF : D E F; SEMI_COLON : \u0026#39;;\u0026#39;; STAR : \u0026#39;*\u0026#39;; OPERATOR : STAR|\u0026#39;+\u0026#39;|\u0026#39;/\u0026#39;|\u0026#39;**\u0026#39;|\u0026#39;-\u0026#39;; RETURN : R E T U R N; ASYNC : A S Y N C; COMMA : \u0026#39;,\u0026#39;; OPEN_PAREN : \u0026#39;(\u0026#39;; CLOSE_PAREN : \u0026#39;)\u0026#39;; ARROW : \u0026#39;-\u0026gt;\u0026#39;; COLON : \u0026#39;:\u0026#39;; NAME : ID_START ID_CONTINUE*; WS : [ \\t]+ {HandleSpaces();} -\u0026gt; channel(HIDDEN); 它的一个主要优点是，用户可以使用相同的 syntax 进行 lexing 和 parsing。然而，在语法层面上，这里的区别在于命名约定:\n以大写字母开头的规则是 lexer rules 其他的都是 parse rules 一旦定义完毕， complete ANLTR jar 文件就会提供一个选项，将其生成一组文件，并使用您喜欢的编程语言代码，也就是一个 parser。\njava -Xmx500M -cp \u0026lt;path to ANTLR complete JAR\u0026gt; org.antlr.v4.Tool -Dlanguage=\u0026lt;target_language\u0026gt; PythonParserExample.g4 由于我使用 Python3 作为生成解析器的目标，ANTLR 的配置会生成 3 个 python 文件，这些文件可以作为代码翻译过程的一部分，用于将一种语言的源代码转换为另一种语言。\nSetting up an ANTLR Project # ANTLR plugin for VSCode # 这里使用的设置将是在 VSCode 上创建的 Java-Maven 项目。\nANTLR plugin for VSCode 提供了各种选项（甚至比 Intellij 还多）来调试语法文件，并创建了美观的 parse trees，以便轻松调试用户输入语句的特定配置。\n要生成这些可视化效果，需要使用 vscode 的 ANTLR 启动配置在调试模式下运行语法文件，并为语法指定输入文件。下面是 VS Code 上 ANTLR 的 launch.json 配置文件：\n{ \u0026#34;name\u0026#34;: \u0026#34;Debug ANTLR4 grammar\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;antlr-debug\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;input\u0026#34;: \u0026#34;input.txt\u0026#34;, \u0026#34;grammar\u0026#34;: \u0026#34;BooleanExprParser.g4\u0026#34;, \u0026#34;startRule\u0026#34;: \u0026#34;parse\u0026#34;, \u0026#34;printParseTree\u0026#34;: true, \u0026#34;visualParseTree\u0026#34;: true } Grammar # 让我们先为解析器创建一个基本语法或 BooleanExpr.g4 文件。\ngrammar BooleanExpr; @header { package antlrsource; } 请注意 parser file 是如何以 grammar BooleanExpr; 开始的。这可以通过将 Lexer tokens (大写字母表示) 和 parser tokens (所有其他标记) 保存在两个不同的文件中来分解：\nlexer grammar BooleanExprLexer; @header { package antlrsource; } parser grammar BooleanExprParser; @header { package antlrsource; } 一个用于 parser，另一个用于 lexer，这样更便于维护。接下来，我们先定义一个头文件和软件包名称，放在生成的解析器类的开头。这将允许我们指定一个包，以便在 Java 代码中导入。\n从 Lexer 开始，我们将 IDENTIFIER 定义为 lexer rule，并提供与之匹配的描述：\nIDENTIFIER : [a-zA-Z_] [a-zA-Z_0-9]* ; Lexer rules 总是以大写字母开头。这些规则是 parser 的基本构件，重点是构建 parser rules 的基础。对正则表达式稍有接触的人来说，这应该有点熟悉。\n这里，A-Z 表示 A 和 Z 之间的字母，而 a-z 表示 a 和 z 之间的字母。同样，0-9 表示数字 0 和 9 之间的数字。由于规则可能包含也可能不包含这些字母的多次出现，因此可以用 (*/+) 运算符作为后缀，表示这些字母出现的频率。这里，* 表示可能完全不出现（0 次或更多次）。这意味着，我们的 IDENTIFIER 规则将匹配大写字母、小写字母（总是以大写/小写字母开头）和整数字符的任意组合，但不匹配空字符。\n一般来说，所有空白都会被词法识别器标记化。因此，您必须在解析器规则中定义空格以及所有可能使用空格的地方。不过，由于我们的源布尔表达式在某些地方不需要对空格敏感，因此我们可以编写一条词法规则来处理这个问题。\nWS: [ \\r\\t\\u000C\\n]+ -\u0026gt; skip; 请注意留白标记的定义是如何编写的，以识别一个或多个空格、制表符和换行符，并让 ANTLR 跳过它们。箭头（-\u0026gt;）运算符定义了遇到标记时要执行的操作（本例中为跳过操作）。接下来是为布尔表达式定义标记，其中包括多个运算符和操作数。这包括以下标记：\nAND: A N D; OR: O R; NOT: N O T; TRUE: \u0026#39;True\u0026#39;; FALSE: \u0026#39;False\u0026#39;; GT: G T {setText(\u0026#34; \u0026gt; \u0026#34;);}; GE: G E {setText(\u0026#34; \u0026gt;= \u0026#34;);}; LT: L T {setText(\u0026#34; \u0026lt; \u0026#34;);}; LE: L E {setText(\u0026#34; \u0026lt;= \u0026#34;);}; EQ: E Q {setText(\u0026#34; == \u0026#34;);}; LPARENTHESIS: \u0026#39;(\u0026#39;; RPARENTHESIS: \u0026#39;)\u0026#39;; DECIMAL_NUMBER: \u0026#39;-\u0026#39;? [0-9]+ ( \u0026#39;.\u0026#39; [0-9]+)?; Embedding Actions # 规则 GT、GE、LT、LE 和 EQ 包含代码块，允许它们在遇到各自的标记时执行某些动作。这样就可以在语法文件中定义某些动作，但需要注意的是，只能定义简单的小动作，而不能定义复杂的代码块。\n如果我们不希望产生构建解析树的开销，我们可以在解析过程中即时计算值或打印内容。另一方面，这意味着要在表达式语法中嵌入任意代码，这就比较困难；我们必须了解这些操作对解析器的影响，以及这些操作的位置。 The Definitive ANTLR 4 Reference\n请注意，每条规则都由用空格隔开的字母组成。这些被称为 fragments。它们的主要目的是减少每个标记的杂乱定义，这基本上需要处理对大小写敏感的用例。这样，用户就不必为识别同一个 token 而写下所有可能的文本组合。其定义如下\nfragment A : [aA]; // match either an \u0026#39;a\u0026#39; or \u0026#39;A\u0026#39; fragment B : [bB]; ... fragment Z : [zZ]; 虽然大多数字母数字令牌都可以通过使用片段来创建，但其他令牌则可以通过自定义正则表达式定义或使用引号括起来的纯字符串（如 LPARENTHESIS 和 DECIMAL_NUMBER）来创建。\n而 parser rules（所有其他规则）则以小写字母开头。这些规则的主要目的是在 DSL 中定义布尔表达式的上下文，并帮助从生成的词法标记中构建解析树或抽象语法树。\nBasic Building Blocks # 让我们开始定义规则。首先，我们定义根节点（或通常所说的解析节点），它本身只能指向一条规则（此处为 basicBooleanExpression）。首先是一个返回语句，其中包含它应该返回的变量（可选，但在我们的例子中是必需的）及其返回类型。\n这条规则指向另一条名为 basicBooleanExpression 的规则，该规则后跟有 EOF（或文件结束）字符。不包含该字符实质上意味着您正试图解析整个输入内容，而只解析部分输入内容是可以接受的，这将避免任何语法错误。\nparse returns[String str] @init {$str=\u0026#34;\u0026#34;;}: basicBooleanExpression {$str=$basicBooleanExpression.str;} EOF; 使用 EOF character 的原因是，如果在解析 basicbooleanExpression 规则时出现语法错误，那么解析规则将尝试恢复语法错误并报告收集到的语法错误，然后继续解析，因为 EOF 是完成规则所必需的，而且解析器尚未到达 EOF。\n由于我们已经定义了语法并将其分成了两个独立的文件，因此我们可以选择在解析器文件中将词法作为定义规则的词汇:\noptions { tokenVocab = BooleanExprLexer; } 回到我们的解析器，\n第一条规则或 basicBooleanExpresion 规则定义了三个选项，在我们的 python 目标代码评估中应始终返回一个布尔值。第一种是后两种规则的组合，即两个布尔表达式与一个逻辑和/或运算符的组合； 第二种是另一种三元表达式，即使用比较器（如、小于或 LT）比较两个基本表达式彼此返回的某个值； 最后，第三种是单元表达式（只有一个布尔值，如 True 或 False）。 这些规则由运算符 | 分隔。这意味着，basicBooleanExpression 在识别输入字符串时，可以根据从左到右的文本识别，递归地引用其子规则中的任一规则。\nbasicBooleanExpression returns[String str]: left = basicBooleanExpression op = logicalOperator right = basicBooleanExpression {$str=$left.str +\u0026#34; \u0026#34;+$op.text+\u0026#34; \u0026#34;+$right.str;} # logicalExpression | left = expression op = comparator right = expression {$str=\u0026#34;(\u0026#34;+$left.text +\u0026#34; \u0026#34;+$op.text+\u0026#34; \u0026#34;+$right.text+\u0026#34;)\u0026#34;;} # comparisonExpression | bool {$str=$bool.str;} # booleanExpression; basicBooleanExpression 中的每条规则都分配给一个变量名，如 left、right（表达式中的左右操作数）和 op（操作数的缩写），或者是一条单标记规则。$str 变量用于分配当前表达式解析的结果，并使用规则开头的 [String str] 返回值返回。\n# 用于标记每条规则，使其在目标语言解析器（在我们的例子中是 Java 解析器类）中有专门的监听器方法。\nlexer grammar BooleanExprLexer; @header { package antlrsource; } AND: A N D; OR: O R; NOT: N O T; TRUE: \u0026#39;True\u0026#39;; FALSE: \u0026#39;False\u0026#39;; GT: G T {setText(\u0026#34; \u0026gt; \u0026#34;);}; GE: G E {setText(\u0026#34; \u0026gt;= \u0026#34;);}; LT: L T {setText(\u0026#34; \u0026lt; \u0026#34;);}; LE: L E {setText(\u0026#34; \u0026lt;= \u0026#34;);}; EQ: E Q {setText(\u0026#34; == \u0026#34;);}; LPARENTHESIS: \u0026#39;(\u0026#39;; RPARENTHESIS: \u0026#39;)\u0026#39;; DECIMAL_NUMBER: \u0026#39;-\u0026#39;? [0-9]+ ( \u0026#39;.\u0026#39; [0-9]+)?; IDENTIFIER: [a-zA-Z_] [a-zA-Z_0-9]*; WS: [ \\r\\t\\u000C\\n]+ -\u0026gt; skip; fragment A : [aA]; // match either an \u0026#39;a\u0026#39; or \u0026#39;A\u0026#39; fragment B : [bB]; fragment C : [cC]; fragment D : [dD]; fragment E : [eE]; fragment F : [fF]; fragment G : [gG]; fragment H : [hH]; fragment I : [iI]; fragment J : [jJ]; fragment K : [kK]; fragment L : [lL]; fragment M : [mM]; fragment N : [nN]; fragment O : [oO]; fragment P : [pP]; fragment Q : [qQ]; fragment R : [rR]; fragment S : [sS]; fragment T : [tT]; fragment U : [uU]; fragment V : [vV]; fragment W : [wW]; fragment X : [xX]; fragment Y : [yY]; fragment Z : [zZ]; BooleanExprLexer.g4\nparser grammar BooleanExprParser; @header { package antlrsource; } options { tokenVocab = BooleanExprLexer; } parse returns[String str] @init {$str=\u0026#34;\u0026#34;;}: basicBooleanExpression {$str=$basicBooleanExpression.str;} EOF; basicBooleanExpression returns[String str]: left = basicBooleanExpression op = logicalOperator right = basicBooleanExpression\t{$str=$left.str +\u0026#34; \u0026#34;+$op.text+\u0026#34; \u0026#34;+$right.str;} # logicalExpression | left = expression op = comparator right = expression {$str=\u0026#34;(\u0026#34;+$left.text +\u0026#34; \u0026#34;+$op.text+\u0026#34; \u0026#34;+$right.text+\u0026#34;)\u0026#34;;}\t# comparisonExpression | bool\t{$str=$bool.str;}\t# booleanExpression; expression returns[String str]: LPARENTHESIS expression RPARENTHESIS\t# parenthesisExpression | NOT expression\t# notExpression | bool\t# unaryboolExpression | IDENTIFIER\t# identifierExpression | DECIMAL_NUMBER\t# decimalExpression ; comparator returns[String str]: GT | GE | LT | LE | EQ; logicalOperator returns[String str]: AND | OR; bool returns[String str]: TRUE | FALSE; BooleanExprParser.g4\na gt b and c gt d a eq b input demo\nMaven Configuration # 现在，让我们继续生成解析器文件。这次，我将使用 maven 配置和 VS Code 的 ANTLR 插件来生成这些文件：\n\u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.antlr\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;antlr4-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.8-1\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;antlr\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;libDirectory\u0026gt;${basedir}/src/main/antlr\u0026lt;/libDirectory\u0026gt; \u0026lt;sourceDirectory\u0026gt;${basedir}/src/main/antlr\u0026lt;/sourceDirectory\u0026gt;\u0026lt;outputDirectory\u0026gt;${basedir}/src/main/java/antlrsource\u0026lt;/outputDirectory\u0026gt; \u0026lt;visitor\u0026gt;false\u0026lt;/visitor\u0026gt; \u0026lt;listener\u0026gt;true\u0026lt;/listener\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;antlr4\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; 每个标签都定义了 ANTLR 在生成解析器类时应使用的目录或需要或不需要生成的文件。例如，监听器和访问者标签的定义都是为了根据它们的布尔值生成相应的 java classes/interfaces。\nTraversal Patterns # 让我们深入了解 Listener vs Visitor traversal patterns ，并探索 BooleanExprParserBaseListener class 的功能。\nANTLR4 提供了两种遍历语法树的方法：\nListener(default)：listener pattern 是一种事件驱动方法，用于遍历每个解析器规则类型的语法树。为每个解析器规则提供一个包含进入和退出事件方法的接口。 Visitor：这使得用户也可以控制解析树的遍历。解析树中的节点（解析器规则）将使用提供的访问方法明确遍历或访问。 根据使用环境的不同，Listener 和 Visitor 模式各有利弊。\n相同点\n两种实现方式的规则语法规则完全相同。 两种实现方式的解析器输出也完全相同。 不同点\n由于 Listener pattern 依赖于用户来定义其遍历序列，因此它使用调用堆栈来管理这些遍历，这意味着大量输入可能会导致溢出，而在已分配堆上使用堆栈的监听器则不会出现这种问题。 Listener pattern 和 Visitor pattern 的最大区别在于，监听器方法是由 ANTLR 提供的行走器对象独立调用的，而访问者方法必须通过显式访问调用来行走其子节点。如果忘记在节点的子节点上调用访问者方法，就意味着这些子树不会被访问。\n跳回到我们生成的 Listener\n接口类的实现只针对某些语言，一般来说，implementing class/module 是在目标语言中定义的。请注意，interface 和 implementation 中都定义了根（或解析）节点监听器方法。所有方法都有相应的上下文对象，该对象由生成的解析器类提供。这样就可以在遍历解析树时对该规则的上下文进行操作。\n让我们创建与解析器的第一次 interaction ，并生成一个简单表达式的输出：\n// Creates a lexer for the input string to generate the tokens BooleanExprLexer lexer = new BooleanExprLexer(CharStreams.fromString(\u0026#34;a eq b\u0026#34;)); // Stores the tokens generated by the lexer for the input string CommonTokenStream tokens = new CommonTokenStream(lexer); /** Creates a parser for generation of an Abstract Syntax tree from the stream of tokens to identify context */ BooleanExprParser parser = new BooleanExprParser(tokens); /** Creates a parse tree for generating the output string and manipulation of the parser and lexer tokens in the parse tree. The tree is create considering the parse rule in the grammar as the root node. This tree will be used later for listener. */ ParseTree tree = parser.parse(); /** Convert the root node\u0026#39;s output to it\u0026#39;s rule context and use its attribute for printing the parser\u0026#39;s output string. */ ParseContext context = (ParseContext) tree; System.out.println(context.str); 由于我们已经在语法文件中添加了将基本运算符转换为 python 对应运算符的操作，因此解析器的输出表达式已经是解析形式。例如，一个简单的比较表达式，如 a eq b，将转换为 python 表达式 a == b 。\n现在，我们已经转换了表达式，可以使用监听器文件中的某些更改将其转换为函数，并使用 template 将该表达式替换为 placeholder function text，用于多个类似表达式：\ndef \u0026lt;function_name\u0026gt;(\u0026lt;list_of_parameters\u0026gt;): return a == b parsed expression 可以很容易地替换成上面的表达式。不用担心替换部分，我们稍后会讲到。现在，为了从每个表达式中提取参数列表，我们将添加一个列表，以便在每次触发标识符类型（解析器规则）的输入事件监听器方法时捕获每个标识符名称。\n让我们定义一个字符串列表作为实例变量，在无参数构造函数中对其进行初始化，定义其 getter 方法，并添加一个方法来清除该列表，以便每次表达式解析和替换完成后都能清除该列表。\npublic class BooleanExprParserBaseListener implements BooleanExprParserListener { private List\u0026lt;String\u0026gt; identifiersList; public BooleanExprParserBaseListener(){ identifiersList= new ArrayList\u0026lt;\u0026gt;(); } ... 让我们把上下文 getText 方法中的标识符名称添加到我们为 identifierExpression 每次触发输入事件时创建的列表中：\npublic void clearIdentifiers() { identifiersList.clear(); } public List\u0026lt;String\u0026gt; getIdentifiersList() { return identifiersList; } @Override public void enterIdentifierExpression(BooleanExprParser.IdentifierExpressionContext ctx) { identifiersList.add(ctx.getText()); } 最后，我们需要在树上遍历，以触发这些事件，从而收集表达式的所有参数。ANTLR 为此提供了 ParseTreeWalker 类。顾名思义，该类允许在走过解析树时同时使用监听器和访问者实现类。让我们使用监听器来遍历上面定义的解析树：\nBooleanExprParserBaseListener booleanExprBaseListener = new BooleanExprParserBaseListener(); ParseTreeWalker walker = new ParseTreeWalker(); walker.walk(booleanExprBaseListener, tree); List\u0026lt;String\u0026gt; identifiers = booleanExprBaseListener.getIdentifiersList(); booleanExprBaseListener.clearIdentifiers(); 请注意，这里的 walk 方法使用 listener 来监听在解析树上行走时触发的事件。接下来，我们使用 getter 方法获取解析表达式时获取的所有标识符名称列表。\nGenerate Code # 将使用上一部分 generated expression 或 the output of intermediate code generation，并将其替换为模板组文件(templating engine 使用的东西)，这样我们就可以将渲染的函数串写入 python 文件。\ntemplating engine 将以我们的目标语言（即 Python）生成实际可用的代码，从而实现代码生成的目标。\n提到 templating engine ，你首先想到的就是 web frameworks。几乎所有的现代 web frameworks 都有一个共同的目标，那就是使用模板引擎生成动态的、业务就绪的网页。每个模板引擎的最终目标都是将获取的输出结果替换为模板文件，以便即时显示给最终用户。\ntemplating engines compared side-by-side\nStringTemplate # 我将使用一个名为 StringTemplate 的类似模板引擎。它被广泛用于网页模板化，但也支持用于创建目标语言代码文件的基本模板操作。\n\u0026lt;attribute\u0026gt; 如果存在，则求值为属性的字符串值，否则为空字符串。 例如，在使用 Java 中的 StringTemplate 对象时，\u0026lt;expression\u0026gt; 将以 key expression 表示。因此，如果用户在 expression 键上输入任何值，它就会在模板中被称为 expression 属性。 对于模板内的自定义或用户定义对象，请使用 \u0026lt;attribute.property\u0026gt; ，将属性作为属性查找，然后使用 getProperty() 或 isProperty() 或 hasProperty() 等访问器方法。 \u0026lt;attribute:t1(argument-list)：... :tN(argument-list)\u0026gt; 迭代同一个模板替换的对象列表。从左到右依次应用多个模板。在多值属性上应用模板的结果是另一个多值属性。整个表达式的求值结果是所有模板元素的连接结果 \u0026lt;! comment !\u0026gt; StringTemplate 会忽略已定义的注释。 模板定义看起来就像带有未键入参数的函数定义： templateName(arg1, arg2, ..., argN) ::= \u0026quot;single-line template\u0026quot; templateName(arg1, arg2, ..., argN) ::= \u0026lt;\u0026lt;multi-line template\u0026gt;\u0026gt; templateName(arg1, arg2, ..., argN) ::= \u0026lt;%multi-line template that ignores indentation and newlines%\u0026gt; 下面我们来看看 Python StringTemplateExample.stg 文件的示例：\n\u0026lt;! StringTemplateExample.stg !\u0026gt; templateExample(functions) ::= \u0026lt;\u0026lt; \u0026lt;functions :{function | def \u0026lt;function.function_name\u0026gt;(\u0026lt;function.params_list\u0026gt;) return \u0026lt;function.expression\u0026gt; }\u0026gt; \u0026gt;\u0026gt; 请注意，为了保持缩进和两行间隙，我使用了上面基础示例中的第二种模板类型，因为这需要遵循 Python PEP8 的规则，即在 Python 方法之间有两行间隙。下面我们来看看该模板在 Java 中的用法：\nList\u0026lt;String\u0026gt; lines = reader.lines().collect(Collectors.toList()); String functionName = \u0026#34;generated_function_%1$s\u0026#34;; List\u0026lt;Map\u0026gt; functions = new ArrayList\u0026lt;\u0026gt;(); Map \u0026lt;String,String\u0026gt; function; for (int i = 0; i \u0026lt; lines.size(); i++) { BooleanExprLexer lexer = new BooleanExprLexer(CharStreams.fromString(lines.get(i))); CommonTokenStream tokens = new CommonTokenStream(lexer); BooleanExprParser parser = new BooleanExprParser(tokens); ParseTree tree = parser.parse(); ParseContext context = (ParseContext) tree; System.out.println(context.str); BooleanExprParserBaseListener booleanExprBaseListener = new BooleanExprParserBaseListener(); ParseTreeWalker walker = new ParseTreeWalker(); walker.walk(booleanExprBaseListener, tree); List\u0026lt;String\u0026gt; identifiers = booleanExprBaseListener.getIdentifiersList(); function = new HashMap\u0026lt;\u0026gt;(); function.put(\u0026#34;function_name\u0026#34;, String.format(functionName, i)); function.put(\u0026#34;expression\u0026#34;, context.str); function.put(\u0026#34;params_list\u0026#34;, identifiers.stream().collect(Collectors.joining(\u0026#34;, \u0026#34;))); functions.add(function); booleanExprBaseListener.clearIdentifiers(); } stringTemplateExample.add(\u0026#34;functions\u0026#34;, functions); System.out.println(stringTemplateExample.render()); INPUT\na gt b and c gt d a eq b Output\ndef generated_function_0(a, b, c, d) return (a \u0026gt; b) and (c \u0026gt; d) def generated_function_1(a, b) return (a == b) Resources # repo : https://github.com/WFUing/BooleanParser https://medium.com/analytics-vidhya/antlr-and-code-generation-a71ead442005 https://medium.com/analytics-vidhya/python-from-expressions-the-antlr-series-part-1-3d7696c3a01c https://medium.com/analytics-vidhya/python-from-expressions-the-antlr-series-part-2-5436ef00bcf https://medium.com/analytics-vidhya/python-from-expressions-the-antlr-series-part-3-7ac541a1d08c ","date":"15 October 2023","permalink":"/posts/language/code-generation/antlr-code-generation/","section":"博客","summary":"ANTLR 是 \u003cstrong\u003eAN\u003c/strong\u003eother \u003cstrong\u003eT\u003c/strong\u003eool for \u003cstrong\u003eL\u003c/strong\u003eanguage \u003cstrong\u003eR\u003c/strong\u003eecognition 的缩写，是一个功能强大的解析器生成器框架，用于从语法文件中构建语言识别器、编译器和翻译器，语法文件中包含从源语言到目标语言的每个语句所要执行的操作。","title":"Antlr Code Generation"},{"content":"","date":"15 October 2023","permalink":"/tags/code-generation/","section":"Tags","summary":"","title":"Code Generation"},{"content":"","date":"15 October 2023","permalink":"/posts/language/code-generation/","section":"博客","summary":"代码生成是指生成程序代码的软件技术。IDE就是一个典型例子：只需点击一个按钮，就能创建一个骨架类来实现接口或类似功能。","title":"Code Generation"},{"content":"下面列出了所有类型的编程语言的完整分类列表。编程语言没有严格的分类方案。因此，我们可以将一种语言视为不止一种编程语言的示例。\n让我们一一理解这些编程语言。由于列表很大，因此不可能详细讨论所有这些内容。在这里，我正在用所有这些各种编程语言的示例编写简短的介绍。\n编程语言流行度排名 编译语言 # 编译语言是一种编程语言，其中我们使用编译器来编译和执行代码。编译器通常是从我们的书面源代码生成机器级代码的翻译器。\nC\nC ++\nC＃\nALGOL\nCobol\nFortran\nJava\nVisual Basic\nSmalltalk\n解释语言 # 解释语言是一种编程语言，在其中，无需将程序编译为机器语言的指令，我们就可以直接自由地执行指令。解释器逐行执行程序。语言解释为编译后的实现（如平台独立性，动态范围，动态类型等）提供了更多的灵活性。\nPython\nRuby\nPerl\nPascal\nLisp\nBASIC\nAPL\n脚本语言 # 脚本语言是控制应用程序的编程语言。可以在任何其他应用程序上独立执行的脚本。它们被广泛应用于它们所控制的应用中，并被用于自动化领域。\nPHP\nVBScript\nWindows PowerShell\nF-Script\nBeanShell\nAutoIt\nR\nGame Maker Language\n标记语言 # 标记语言是一种人工语言，用于对文档进行注释，以便在语法上与文本（可定义文本显示方式的文本）区分开。\nHTML\nXML\nXHTML\nSGML\nCurl\n程序语言 # 程序（命令式）编程意味着指定程序达到预期状态应采取的步骤。过程不过是一组可以通过过程调用引用的指令。这有助于代码的重用。这种类型的编程使程序结构化并易于跟踪程序流。\nHyperTalk\nGo\nPL/C\nPL/I\nMATLAB\nCurl\nMathematica\nMATLAB\n函数式语言 # 函数式编程语言将每次计算都定义为数学评估。他们专注于函数的应用。一些函数式编程语言是纯函数式语言，但是许多所谓的函数式语言是不纯净的，包含命令式功能，它们不是纯函数式语言。\nPure Functional\nAgda\nSAC\nSASL\nCuneiform\nCurry\nFuthark\nHaskell\n不纯功能语言 # APL\nC++ (since C++11)\nC#\nVB.NET\nCeylon\nKotlin\nLisp\nClojure\nJScript\nPHP\nPython\n基于逻辑的编程语言 # 逻辑编程是一种编程范例，主要基于形式逻辑。基于逻辑的编程是一组逻辑形式的语句，这些语句表达有关问题域的事实和规则。\nProlog\nROOP\nALF\nAlma-0\nCurry\nFril\nJanus\n面向对象的语言 # 面向对象的编程（OOP）是基于对象概念的高级编程范例，该对象可能包含字段形式的数据，通常称为属性。在OOP中，计算机程序将相关数据和功能绑定到对象中，并实现对象及其相关过程以创建软件程序。\nScala\nC++\nJava\nPython\nC#\nRuby\nScala\n数据流语言 # 数据流编程语言依赖于表示数据流。在数据流语言中，数据流从一条指令传递到另一条指令以执行。条件执行会跳转数据，并在过程调用中将数据路由到其他位置。\nAnalytica\nBMDFM\nHartmann pipelines\nLucid\nMax\nOz\nPrograph\nPure Data\n嵌入式语言 # 主要是动态脚本和编程语言。它也可以用作独立于平台的通用编程语言。嵌入式语言有两种类型：\n服务端 : 服务器端嵌入式语言更加灵活。动态生成附加标记是拥有服务器端代码片段的主要目的。服务该页面时，嵌入在网页中的服务器端是自动丢弃的代码，并由输出替换。 客户端 : 客户端嵌入式语言旨在为网页提供动态特性，从而减少重新连接服务器的开销。 服务器端\nPHP\nVBScript\nSMX\nTcl\nWebDNA\n客户端\nActionScript\nJavaScript\nVBScript\n机器语言 # 这些语言可由计算中央处理器直接执行。机器语言通常以八进制或十六进制形式的位模式编码。\nARM\nDEC\nx86\nIBM System/360\nMIPS\nSun, Oracle SPARC\n系统语言 # 这些语言用于内存管理或任务管理中使用的低级语言。与应用软件相比，通常用于系统编程的系统编程语言（例如，用于编写系统软件的语言）通常需要不同的开发方法。\nAda\nNim\nRust\nSwift\nESPOL\n并发语言 # 这些语言是为了在消息传递语言中并发而构造的。例如，Java显示共享内存并发。\nGo\nJava\nJulia\nclojure\nScala\n范式语言 # 这些类型的语言支持多种编程语言或编程范式。多范式语言允许使用多种编程风格。没有一种特定的语言能够以最简单或有效的方式解决所有问题，这就是我们使用Multiparadigm语言的原因。\nAda\nAPL\nBETA\nC++\nC#\nCobra\n扩展语言 # 这些语言用作其他语言的扩展。扩展编程语言嵌入到另一个程序中，并用于在扩展脚本中利用其功能。\nAutoLISP\nBeanShell\nPerl\nPike\nRuby\n迭代语言 # 这些语言围绕生成器提供或提供生成器。\nAldor\nAlphard\nPHP\nCLU\nCobra\n硬件描述语言 # 这些编程语言用于电子产品，硬件描述语言或HDL用于描述电子电路或数字逻辑电路的结构，设计和操作。Verilog和VHDL在工业中使用的各种最流行和得到良好支持的HDL品种中。\n模拟电路的HDL：\nVerilog-AMS\nVHDL-AMS\n数字电路的HDL：\nAdvanced Boolean Expression Language(ABEL)\nAltera Hardware Description Language(AHDL)\nBluespec\nLava\nELLA\n视觉语言 # 在Viual Languages中，用户可以以二维或多种方式指定程序，而不能使用视觉语言中的一维（文本字符串）来指定程序，我们使用图形元素和图形来开发程序。\nAnalytica\nBlockly\nDRAKON\nFabrik\nScratch\nSimulink\nSpreadsheets\n基于列表的语言 # 列表的语言基于列表数据结构。\n例：\nLisp\nArc\nClojure\nR\nDylan\nJoy\n同步语言 # 这些编程语言用于对反应系统进行编程。编程反应系统是被中断并立即响应的系统。这些系统中的一些也称为实时系统，并且被广泛使用。\nArgus\nAverest\nEsterel\nLustre\nSignal\n宏语言 # 这些语言用于将一个源代码文件转换为另一个。宏是一小段文本，可以扩展为更大的文本。宏语言通常用于预处理源代码。预处理程序提供文件包含等功能。\ncpp (the C preprocessor)\nm4\nML/I (general purpose macro processor)\n查询语言 # 数据库和信息系统中使用这些语言进行查询。\nSQL\nXPath\nAQL\nPQL\nXQuery\n元编程语言 # 元编程语言是编写程序，该程序编写或操纵其他程序（包括其自身）作为数据，或者完成在编译时在运行时执行的部分工作。\nC++\nCWIC\nCurl\nD\neC\nEmacs Lisp\nElixir\nF#\n基于规则的语言 # 当被一组数据中的条件激活时，基于规则的语言实例化规则。将选择某些集合，并执行属于那些规则的语句。\nawk\nCLIPS\nConstraint Handling Rules\nDrools\nJess\nOPS5\nProlog\n数值分析语言 # 在数值分析中，我们分析和实现用于数值解的算法，以解决涉及连续变量的现实数学模型的巨大问题。我们在数值分析中使用以下编程语言。\nMathematica\nMATLAB\nPROSE\nR\n语法处理语言 # 这些语言可帮助生成词法分析器和解析器以实现上下文无关的语法。\nANTLR\nCoco/R (EBNF with semantics)\nGNU bison (FSF\u0026rsquo;s version of Yacc)\nGNU Flex (FSF version of Lex)\nlex (Lexical Analysis, from Bell Labs)\nParsing expression grammar (PEG)\n非基于英语的语言 # 有几种编程语言，它们是用英语以外的其他语言开发的。在这种情况下，语言不是障碍。\nChinese BASIC - Chinese\nFjölnir - Icelandic\nLanguage Symbolique d\u0026rsquo;Enseignement - French\nLexico - Spanish\nRapira - Russian\nChaScript-Bengali\nezhil-Tamil\n基于XML的语言 # 这些语言用于将XML文档转换为人类可读的格式。\nAnt\nC?\nXPath\nXQuery\nXProc\n","date":"14 October 2023","permalink":"/posts/language/programming-language-pool/","section":"博客","summary":"下面列出了所有类型的编程语言的完整分类列表。编程语言没有严格的分类方案。因此，我们可以将一种语言视为不止一种编程语言的示例。","title":"Programming Language List"},{"content":"","date":"14 October 2023","permalink":"/posts/architecture/distributed/zookeeper/","section":"博客","summary":"ZooKeeper 是 Apache 的顶级项目。ZooKeeper 为分布式应用提供了高效且可靠的分布式协调服务，提供了诸如统一命名服务、配置管理和分布式锁等分布式的基础服务。在解决分布式数据一致性方面，ZooKeeper 并没有直接采用 Paxos 算法，而是采用了名为 ZAB 的一致性协议。","title":"ZooKeeper"},{"content":"Resources # 官方 ZooKeeper 官网 ZooKeeper 官方文档 ZooKeeper Github 博客 ZooKeeper源码阅读心得分享+源码基本结构+源码环境搭建 手摸手教你阅读和调试大型开源项目 ZooKeeper ","date":"14 October 2023","permalink":"/posts/architecture/distributed/zookeeper/zookeeper-code/","section":"博客","summary":"Resources # 官方 ZooKeeper 官网 ZooKeeper 官方文档 ZooKeeper Github 博客 ZooKeeper源码阅读心得分享+源码基本结构+源码环境搭建 手摸手教你阅读和调试大型开源项目 ZooKeeper ","title":"Zookeeper Code"},{"content":"ZooKeeper 简介 # ZooKeeper 是什么 # ZooKeeper 是 Apache 的顶级项目。ZooKeeper 为分布式应用提供了高效且可靠的分布式协调服务，提供了诸如统一命名服务、配置管理和分布式锁等分布式的基础服务。在解决分布式数据一致性方面，ZooKeeper 并没有直接采用 Paxos 算法，而是采用了名为 ZAB 的一致性协议。\nZooKeeper 主要用来解决分布式集群中应用系统的一致性问题，它能提供基于类似于文件系统的目录节点树方式的数据存储。但是 ZooKeeper 并不是用来专门存储数据的，它的作用主要是用来维护和监控存储数据的状态变化。通过监控这些数据状态的变化，从而可以达到基于数据的集群管理。\n很多大名鼎鼎的框架都基于 ZooKeeper 来实现分布式高可用，如：Dubbo、Kafka 等。\nZooKeeper 官方支持 Java 和 C 的 Client API。ZooKeeper 社区为大多数语言（.NET，python 等）提供非官方 API。\nZooKeeper 的应用场景 # 配置管理 集群节点可以通过中心源获取启动配置 更简单的部署 分布式集群管理 节点加入/离开 节点的实时状态 命名服务，如：DNS 分布式同步：如锁、栅栏、队列 分布式系统的选主 中心化和高可靠的数据注册 ZooKeeper 的特性 # ZooKeeper 具有以下特性：\n顺序一致性 - 所有客户端看到的服务端数据模型都是一致的。从一个客户端发起的事务请求，最终都会严格按照其发起顺序被应用到 ZooKeeper 中。具体的实现可见： 原子广播 原子性 - 所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，即整个集群要么都成功应用了某个事务，要么都没有应用。 实现方式可见： 事务 单一视图 - 无论客户端连接的是哪个 Zookeeper 服务器，其看到的服务端数据模型都是一致的。 高性能 - ZooKeeper 将数据全量存储在内存中，所以其性能很高。需要注意的是：由于 ZooKeeper 的所有更新和删除都是基于事务的，因此 ZooKeeper 在读多写少的应用场景中有性能表现较好，如果写操作频繁，性能会大大下滑。 高可用 - ZooKeeper 的高可用是基于副本机制实现的，此外 ZooKeeper 支持故障恢复，可见： 选举 Leader ZooKeeper 的设计目标 # 简单的数据模型：ZooKeeper 的数据模型是一个树形结构的文件系统，树中的节点被称为 znode。 可以构建集群：ZooKeeper 支持集群模式，可以通过伸缩性，来控制集群的吞吐量。需要注意的是：由于 ZooKeeper 采用一主多从架构，所以其写性能是有上限的，比较适合于读多写少的场景。 顺序访问：对于来自客户端的每个更新请求，Zookeeper 都会分配一个全局唯一的递增 ID，这个 ID 反映了所有事务请求的先后顺序。 高性能、高可用：ZooKeeper 将数据存全量储在内存中以保持高性能，并通过服务集群来实现高可用，由于 Zookeeper 的所有更新和删除都是基于事务的，所以其在读多写少的应用场景中有着很高的性能表现。 ZooKeeper 核心概念 # 服务 # Zookeeper 服务是一个基于主从复制的高可用集群，集群中每个节点都存储了一份数据副本（内存中）。\n客户端只会连接一个 ZooKeeper 服务器节点，并维持 TCP 连接。\n数据模型 # ZooKeeper 的数据模型是一个树形结构的文件系统。\n树中的节点被称为 znode，其中根节点为 /，每个节点上都会保存自己的数据和节点信息。znode 可以用于存储数据，并且有一个与之相关联的 ACL（详情可见 ACL）。ZooKeeper 的设计目标是实现协调服务，而不是真的作为一个文件存储，因此 znode 存储数据的大小被限制在 1MB 以内。\nZooKeeper 的数据访问具有原子性。其读写操作都是要么全部成功，要么全部失败。\nznode 通过路径被引用。znode 节点路径必须是绝对路径。\nznode 有两种类型：\n临时的（ EPHEMERAL ） - 户端会话结束时，ZooKeeper 就会删除临时的 znode。不允许有子节点。 持久的（PERSISTENT ） - 除非客户端主动执行删除操作，否则 ZooKeeper 不会删除持久的 znode。 节点信息 # znode 上有一个顺序标志（ SEQUENTIAL ）。如果在创建 znode 时，设置了顺序标志（ SEQUENTIAL ），那么 ZooKeeper 会使用计数器为 znode 添加一个单调递增的数值，即 zxid。ZooKeeper 正是利用 zxid 实现了严格的顺序访问控制能力。\n每个 znode 节点在存储数据的同时，都会维护一个叫做 Stat 的数据结构，里面存储了关于该节点的全部状态信息。如下：\n状态属性 说明 czxid 数据节点创建时的事务 ID ctime 数据节点创建时的时间 mzxid 数据节点最后一次更新时的事务 ID mtime 数据节点最后一次更新时的时间 pzxid 数据节点的子节点最后一次被修改时的事务 ID cversion 子节点的更改次数 version 节点数据的更改次数 aversion 节点的 ACL 的更改次数 ephemeralOwner 如果节点是临时节点，则表示创建该节点的会话的 SessionID；如果节点是持久节点，则该属性值为 0 dataLength 数据内容的长度 numChildren 数据节点当前的子节点个数 集群角色 # Zookeeper 集群是一个基于主从复制的高可用集群，集群中每个节点都存储了一份数据副本（内存中）。此外，每个服务器节点承担如下三种角色中的一种：\nLeader - 它负责 发起并维护与各 Follwer 及 Observer 间的心跳。所有的写操作必须要通过 Leader 完成再由 Leader 将写操作广播给其它服务器。一个 Zookeeper 集群同一时间只会有一个实际工作的 Leader。 Follower - 它会响应 Leader 的心跳。Follower 可直接处理并返回客户端的读请求，同时会将写请求转发给 Leader 处理，并且负责在 Leader 处理写请求时对请求进行投票。一个 Zookeeper 集群可能同时存在多个 Follower。 Observer - 角色与 Follower 类似，但是无投票权。 客户端可以从任意 ZooKeeper 服务器节点读取数据，但只能通过 Leader 服务写数据并需要半数以上 Follower 的 ACK，才算写入成功。记住这个重要的知识点，下文会详细讲述。\nACL # ZooKeeper 采用 ACL（Access Control Lists）策略来进行权限控制。\n每个 znode 创建时都会带有一个 ACL 列表，用于决定谁可以对它执行何种操作。\nACL 依赖于 ZooKeeper 的客户端认证机制。ZooKeeper 提供了以下几种认证方式：\ndigest - 用户名和密码 来识别客户端 sasl - 通过 kerberos 来识别客户端 ip - 通过 IP 来识别客户端 ZooKeeper 定义了如下五种权限：\nCREATE - 允许创建子节点； READ - 允许从节点获取数据并列出其子节点； WRITE - 允许为节点设置数据； DELETE - 允许删除子节点； ADMIN - 允许为节点设置权限。 ZooKeeper 工作原理 # 读操作 # Leader/Follower/Observer 都可直接处理读请求，从本地内存中读取数据并返回给客户端即可。\n由于处理读请求不需要服务器之间的交互，Follower/Observer 越多，整体系统的读请求吞吐量越大，也即读性能越好。\n写操作 # 所有的写请求实际上都要交给 Leader 处理。Leader 将写请求以事务形式发给所有 Follower 并等待 ACK，一旦收到半数以上 Follower 的 ACK，即认为写操作成功。\n写 Leader # 由上图可见，通过 Leader 进行写操作，主要分为五步：\n客户端向 Leader 发起写请求 Leader 将写请求以事务 Proposal 的形式发给所有 Follower 并等待 ACK Follower 收到 Leader 的事务 Proposal 后返回 ACK Leader 得到过半数的 ACK（Leader 对自己默认有一个 ACK）后向所有的 Follower 和 Observer 发送 Commmit Leader 将处理结果返回给客户端 注意\nLeader 不需要得到 Observer 的 ACK，即 Observer 无投票权 Leader 不需要得到所有 Follower 的 ACK，只要收到过半的 ACK 即可，同时 Leader 本身对自己有一个 ACK。上图中有 4 个 Follower，只需其中两个返回 ACK 即可，因为 $$\\frac{2+1}{4+1} \u0026gt; \\frac{1}{2}$$ Observer 虽然无投票权，但仍须同步 Leader 的数据从而在处理读请求时可以返回尽可能新的数据 写 Follower/Observer # Follower/Observer 均可接受写请求，但不能直接处理，而需要将写请求转发给 Leader 处理。 除了多了一步请求转发，其它流程与直接写 Leader 无任何区别。 事务 # 对于来自客户端的每个更新请求，ZooKeeper 具备严格的顺序访问控制能力。\n为了保证事务的顺序一致性，ZooKeeper 采用了递增的事务 id 号（zxid）来标识事务。\nLeader 服务会为每一个 Follower 服务器分配一个单独的队列，然后将事务 Proposal 依次放入队列中，并根据 FIFO(先进先出) 的策略进行消息发送。Follower 服务在接收到 Proposal 后，会将其以事务日志的形式写入本地磁盘中，并在写入成功后反馈给 Leader 一个 Ack 响应。当 Leader 接收到超过半数 Follower 的 Ack 响应后，就会广播一个 Commit 消息给所有的 Follower 以通知其进行事务提交，之后 Leader 自身也会完成对事务的提交。而每一个 Follower 则在接收到 Commit 消息后，完成事务的提交。\n所有的提议（proposal）都在被提出的时候加上了 zxid。zxid 是一个 64 位的数字，它的高 32 位是 epoch 用来标识 Leader 关系是否改变，每次一个 Leader 被选出来，它都会有一个新的 epoch，标识当前属于那个 leader 的统治时期。低 32 位用于递增计数。\n详细过程如下：\nLeader 等待 Server 连接； Follower 连接 Leader，将最大的 zxid 发送给 Leader； Leader 根据 Follower 的 zxid 确定同步点； 完成同步后通知 follower 已经成为 uptodate 状态； Follower 收到 uptodate 消息后，又可以重新接受 client 的请求进行服务了。 观察 # ZooKeeper 允许客户端监听它关心的 znode，当 znode 状态发生变化（数据变化、子节点增减变化）时，ZooKeeper 服务会通知客户端。\n客户端和服务端保持连接一般有两种形式：\n客户端向服务端不断轮询 服务端向客户端推送状态 Zookeeper 的选择是服务端主动推送状态，也就是观察机制（ Watch ）。\nZooKeeper 的观察机制允许用户在指定节点上针对感兴趣的事件注册监听，当事件发生时，监听器会被触发，并将事件信息推送到客户端。\n监听器实时触发 监听器总是有序的 创建新的 znode 数据前，客户端就能收到监听事件。 客户端使用 getData 等接口获取 znode 状态时传入了一个用于处理节点变更的回调，那么服务端就会主动向客户端推送节点的变更：\npublic byte[] getData(final String path, Watcher watcher, Stat stat) 从这个方法中传入的 Watcher 对象实现了相应的 process 方法，每次对应节点出现了状态的改变，WatchManager 都会通过以下的方式调用传入 Watcher 的方法：\nSet\u0026lt;Watcher\u0026gt; triggerWatch(String path, EventType type, Set\u0026lt;Watcher\u0026gt; supress) { WatchedEvent e = new WatchedEvent(type, KeeperState.SyncConnected, path); Set\u0026lt;Watcher\u0026gt; watchers; synchronized (this) { watchers = watchTable.remove(path); } for (Watcher w : watchers) { w.process(e); } return watchers; } Zookeeper 中的所有数据其实都是由一个名为 DataTree 的数据结构管理的，所有的读写数据的请求最终都会改变这颗树的内容，在发出读请求时可能会传入 Watcher 注册一个回调函数，而写请求就可能会触发相应的回调，由 WatchManager 通知客户端数据的变化。\n通知机制的实现其实还是比较简单的，通过读请求设置 Watcher 监听事件，写请求在触发事件时就能将通知发送给指定的客户端。\n会话 # ZooKeeper 客户端通过 TCP 长连接连接到 ZooKeeper 服务集群。会话 (Session) 从第一次连接开始就已经建立，之后通过心跳检测机制来保持有效的会话状态。通过这个连接，客户端可以发送请求并接收响应，同时也可以接收到 Watch 事件的通知。\n每个 ZooKeeper 客户端配置中都配置了 ZooKeeper 服务器集群列表。启动时，客户端会遍历列表去尝试建立连接。如果失败，它会尝试连接下一个服务器，依次类推。\n一旦一台客户端与一台服务器建立连接，这台服务器会为这个客户端创建一个新的会话。每个会话都会有一个超时时间，若服务器在超时时间内没有收到任何请求，则相应会话被视为过期。一旦会话过期，就无法再重新打开，且任何与该会话相关的临时 znode 都会被删除。\n通常来说，会话应该长期存在，而这需要由客户端来保证。客户端可以通过心跳方式（ping）来保持会话不过期。\nZooKeeper 的会话具有四个属性：\nsessionID - 会话 ID，唯一标识一个会话，每次客户端创建新的会话时，Zookeeper 都会为其分配一个全局唯一的 sessionID。 TimeOut - 会话超时时间，客户端在构造 Zookeeper 实例时，会配置 sessionTimeout 参数用于指定会话的超时时间，Zookeeper 客户端向服务端发送这个超时时间后，服务端会根据自己的超时时间限制最终确定会话的超时时间。 TickTime - 下次会话超时时间点，为了便于 Zookeeper 对会话实行分桶策略管理，同时为了高效低耗地实现会话的超时检查与清理，Zookeeper 会为每个会话标记一个下次会话超时时间点，其值大致等于当前时间加上 TimeOut。 isClosing - 标记一个会话是否已经被关闭，当服务端检测到会话已经超时失效时，会将该会话的 isClosing 标记为已关闭，这样就能确保不再处理来自该会话的心情求了。 Zookeeper 的会话管理主要是通过 SessionTracker 来负责，其采用了分桶策略（将类似的会话放在同一区块中进行管理）进行管理，以便 Zookeeper 对会话进行不同区块的隔离处理以及同一区块的统一处理。\nZAB 协议 # ZooKeeper 并没有直接采用 Paxos 算法，而是采用了名为 ZAB 的一致性协议。ZAB 协议不是 Paxos 算法，只是比较类似，二者在操作上并不相同。Multi-Paxos 实现的是一系列值的共识，不关心最终达成共识的值是什么，不关心各值的顺序。而 ZooKeeper 需要确保操作的顺序性。\nZAB 协议是 Zookeeper 专门设计的一种支持崩溃恢复的原子广播协议。\nZAB 协议是 ZooKeeper 的数据一致性和高可用解决方案。\nZAB 协议定义了两个可以无限循环的流程：\n选举 Leader - 用于故障恢复，从而保证高可用。 原子广播 - 用于主从同步，从而保证数据一致性。 选举 Leader # ZooKeeper 的故障恢复\nZooKeeper 集群采用一主（称为 Leader）多从（称为 Follower）模式，主从节点通过副本机制保证数据一致。\n如果 Follower 节点挂了 - ZooKeeper 集群中的每个节点都会单独在内存中维护自身的状态，并且各节点之间都保持着通讯，只要集群中有半数机器能够正常工作，那么整个集群就可以正常提供服务。 如果 Leader 节点挂了 - 如果 Leader 节点挂了，系统就不能正常工作了。此时，需要通过 ZAB 协议的选举 Leader 机制来进行故障恢复。 ZAB 协议的选举 Leader 机制简单来说，就是：基于过半选举机制产生新的 Leader，之后其他机器将从新的 Leader 上同步状态，当有过半机器完成状态同步后，就退出选举 Leader 模式，进入原子广播模式。\n术语 # myid - 每个 Zookeeper 服务器，都需要在数据文件夹下创建一个名为 myid 的文件，该文件包含整个 Zookeeper 集群唯一的 ID（整数）。 zxid - 类似于 RDBMS 中的事务 ID，用于标识一次更新操作的 Proposal ID。为了保证顺序性，该 zxid 必须单调递增。因此 Zookeeper 使用一个 64 位的数来表示，高 32 位是 Leader 的 epoch，从 1 开始，每次选出新的 Leader，epoch 加一。低 32 位为该 epoch 内的序号，每次 epoch 变化，都将低 32 位的序号重置。这样保证了 zxid 的全局递增性。 服务器状态 # LOOKING - 不确定 Leader 状态。该状态下的服务器认为当前集群中没有 Leader，会发起 Leader 选举 FOLLOWING - 跟随者状态。表明当前服务器角色是 Follower，并且它知道 Leader 是谁 LEADING - 领导者状态。表明当前服务器角色是 Leader，它会维护与 Follower 间的心跳 OBSERVING - 观察者状态。表明当前服务器角色是 Observer，与 Folower 唯一的不同在于不参与选举，也不参与集群写操作时的投票 选票数据结构 # 每个服务器在进行领导选举时，会发送如下关键信息\nlogicClock - 每个服务器会维护一个自增的整数，名为 logicClock，它表示这是该服务器发起的第多少轮投票 state - 当前服务器的状态 self_id - 当前服务器的 myid self_zxid - 当前服务器上所保存的数据的最大 zxid vote_id - 被推举的服务器的 myid vote_zxid - 被推举的服务器上所保存的数据的最大 zxid 投票流程 # 自增选举轮次 - Zookeeper 规定所有有效的投票都必须在同一轮次中。每个服务器在开始新一轮投票时，会先对自己维护的 logicClock 进行自增操作。 初始化选票 - 每个服务器在广播自己的选票前，会将自己的投票箱清空。该投票箱记录了所收到的选票。例：服务器 2 投票给服务器 3，服务器 3 投票给服务器 1，则服务器 1 的投票箱为(2, 3), (3, 1), (1, 1)。票箱中只会记录每一投票者的最后一票，如投票者更新自己的选票，则其它服务器收到该新选票后会在自己票箱中更新该服务器的选票。 发送初始化选票 - 每个服务器最开始都是通过广播把票投给自己。 接收外部投票 - 服务器会尝试从其它服务器获取投票，并记入自己的投票箱内。如果无法获取任何外部投票，则会确认自己是否与集群中其它服务器保持着有效连接。如果是，则再次发送自己的投票；如果否，则马上与之建立连接。 判断选举轮次 - 收到外部投票后，首先会根据投票信息中所包含的 logicClock 来进行不同处理 外部投票的 logicClock 大于自己的 logicClock。说明该服务器的选举轮次落后于其它服务器的选举轮次，立即清空自己的投票箱并将自己的 logicClock 更新为收到的 logicClock，然后再对比自己之前的投票与收到的投票以确定是否需要变更自己的投票，最终再次将自己的投票广播出去。 外部投票的 logicClock 小于自己的 logicClock。当前服务器直接忽略该投票，继续处理下一个投票。 外部投票的 logickClock 与自己的相等。当时进行选票 PK。 选票 PK - 选票 PK 是基于(self_id, self_zxid) 与 (vote_id, vote_zxid) 的对比 外部投票的 logicClock 大于自己的 logicClock，则将自己的 logicClock 及自己的选票的 logicClock 变更为收到的 logicClock 若 logicClock 一致，则对比二者的 vote_zxid，若外部投票的 vote_zxid 比较大，则将自己的票中的 vote_zxid 与 vote_myid 更新为收到的票中的 vote_zxid 与 vote_myid 并广播出去，另外将收到的票及自己更新后的票放入自己的票箱。如果票箱内已存在(self_myid, self_zxid)相同的选票，则直接覆盖 若二者 vote_zxid 一致，则比较二者的 vote_myid，若外部投票的 vote_myid 比较大，则将自己的票中的 vote_myid 更新为收到的票中的 vote_myid 并广播出去，另外将收到的票及自己更新后的票放入自己的票箱 统计选票 - 如果已经确定有过半服务器认可了自己的投票（可能是更新后的投票），则终止投票。否则继续接收其它服务器的投票。 更新服务器状态 - 投票终止后，服务器开始更新自身状态。若过半的票投给了自己，则将自己的服务器状态更新为 LEADING，否则将自己的状态更新为 FOLLOWING 通过以上流程分析，我们不难看出：要使 Leader 获得多数 Server 的支持，则 ZooKeeper 集群节点数必须是奇数。且存活的节点数目不得少于 N + 1 。\n每个 Server 启动后都会重复以上流程。在恢复模式下，如果是刚从崩溃状态恢复的或者刚启动的 server 还会从磁盘快照中恢复数据和会话信息，zk 会记录事务日志并定期进行快照，方便在恢复时进行状态恢复。\n原子广播（Atomic Broadcast） # ZooKeeper 通过副本机制来实现高可用。\n那么，ZooKeeper 是如何实现副本机制的呢？答案是：ZAB 协议的原子广播。\nZAB 协议的原子广播要求：\n所有的写请求都会被转发给 Leader，Leader 会以原子广播的方式通知 Follow。当半数以上的 Follow 已经更新状态持久化后，Leader 才会提交这个更新，然后客户端才会收到一个更新成功的响应。这有些类似数据库中的两阶段提交协议。\n在整个消息的广播过程中，Leader 服务器会每个事务请求生成对应的 Proposal，并为其分配一个全局唯一的递增的事务 ID(ZXID)，之后再对其进行广播。\nZAB 是通过一切以领导者为准的强领导者模型和严格按照顺序提交日志，来实现操作的顺序性的，这一点和 Raft 是一样的。\nZooKeeper 应用 # ZooKeeper 可以用于发布/订阅、负载均衡、命令服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能 。\n命名服务 # 在分布式系统中，通常需要一个全局唯一的名字，如生成全局唯一的订单号等，ZooKeeper 可以通过顺序节点的特性来生成全局唯一 ID，从而可以对分布式系统提供命名服务。\n配置管理 # 利用 ZooKeeper 的观察机制，可以将其作为一个高可用的配置存储器，允许分布式应用的参与者检索和更新配置文件。\n分布式锁 # 可以通过 ZooKeeper 的临时节点和 Watcher 机制来实现分布式排它锁。\n举例来说，有一个分布式系统，有三个节点 A、B、C，试图通过 ZooKeeper 获取分布式锁。\n（1）访问 /lock （这个目录路径由程序自己决定），创建 带序列号的临时节点（EPHEMERAL） 。\n（2）每个节点尝试获取锁时，拿到 /locks节点下的所有子节点（id_0000,id_0001,id_0002），判断自己创建的节点是不是序列号最小的\n如果序列号是最小的，则成功获取到锁。 释放锁：执行完操作后，把创建的节点给删掉。 如果不是，则监听比自己要小 1 的节点变化。 （3）释放锁，即删除自己创建的节点。\n图中，NodeA 删除自己创建的节点 id_0000，NodeB 监听到变化，发现自己的节点已经是最小节点，即可获取到锁。\n集群管理 # ZooKeeper 还能解决大多数分布式系统中的问题：\n如可以通过创建临时节点来建立心跳检测机制。如果分布式系统的某个服务节点宕机了，则其持有的会话会超时，此时该临时节点会被删除，相应的监听事件就会被触发。 分布式系统的每个服务节点还可以将自己的节点状态写入临时节点，从而完成状态报告或节点工作进度汇报。 通过数据的订阅和发布功能，ZooKeeper 还能对分布式系统进行模块的解耦和任务的调度。 通过监听机制，还能对分布式系统的服务节点进行动态上下线，从而实现服务的动态扩容。 选举 Leader 节点 # 分布式系统一个重要的模式就是主从模式 (Master/Salves)，ZooKeeper 可以用于该模式下的 Matser 选举。可以让所有服务节点去竞争性地创建同一个 ZNode，由于 ZooKeeper 不能有路径相同的 ZNode，必然只有一个服务节点能够创建成功，这样该服务节点就可以成为 Master 节点。\n队列管理 # ZooKeeper 可以处理两种类型的队列：\n当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达，这种是同步队列。 队列按照 FIFO 方式进行入队和出队操作，例如实现生产者和消费者模型。 同步队列用 ZooKeeper 实现的实现思路如下：\n创建一个父目录 /synchronizing，每个成员都监控标志（Set Watch）位目录 /synchronizing/start 是否存在，然后每个成员都加入这个队列，加入队列的方式就是创建 /synchronizing/member_i 的临时目录节点，然后每个成员获取 /synchronizing 目录的所有目录节点，也就是 member_i。判断 i 的值是否已经是成员的个数，如果小于成员个数等待 /synchronizing/start 的出现，如果已经相等就创建 /synchronizing/start。\nZooKeeper 的缺点 # ZooKeeper 的监听是一次性的。\nZooKeeper 不是为高可用性设计的 # 生产环境中常常需要通过多机房部署来容灾。出于成本考虑，一般多机房都是同时提供服务的，即一个机房撑不住所有流量。ZooKeeper 集群只能有一个 Leader，一旦机房之间连接出现故障，那么只有 Leader 所在的机房可以正常工作，其他机房只能停摆。于是所有流量集中到 Leader 所在的机房，由于处理不过来而导致崩溃。\n即使是在同一个机房里面，由于网段的不同，在调整机房交换机的时候偶尔也会发生网段隔离的情况。实际上机房每个月基本上都会发生短暂的网络隔离之类的子网段调整。在那个时刻 ZooKeeper 将处于不可用状态。如果业务系统重度依赖 ZooKeeper（比如用 Dubbo 作为 RPC，且使用 ZooKeeper 作为注册中心），则系统的可用性将非常脆弱。\n由于 ZooKeeper 对于网络隔离的极度敏感，导致 ZooKeeper 对于网络的任何风吹草动都会做出激烈反应。这使得 ZooKeeper 的不可用时间比较多。我们不能让 ZooKeeper 的不可用，变成系统的不可用。\nZooKeeper 的选举过程速度很慢 # 互联网环境中，网络不稳定几乎是必然的，而 ZooKeeper 网络隔离非常敏感。一旦出现网络隔离，zookeeper 就要发起选举流程。\nZooKeeper 的选举流程通常耗时 30 到 120 秒，期间 ZooKeeper 由于没有 Leader，都是不可用的。\n对于网络里面偶尔出现的，比如半秒一秒的网络隔离，ZooKeeper 会由于选举过程，而把不可用时间放大几十倍。\nZooKeeper 的性能是有限的 # 典型的 ZooKeeper 的 TPS 大概是一万多，无法支撑每天动辄几十亿次的调用。因此，每次请求都去 ZooKeeper 获取业务系统信息是不可能的。\n为此，ZooKeeper 的 client 必须自己缓存业务系统的信息。这就导致 ZooKeeper 提供的强一致性实际上是做不到的。如果我们需要强一致性，还需要其他机制来进行保障：比如用自动化脚本把业务系统的 old master 给 kill 掉，但是这可能会引发很多其他问题。\nZooKeeper 无法进行有效的权限控制 # ZooKeeper 的权限控制非常弱。在大型的复杂系统里面，使用 ZooKeeper 必须自己再额外的开发一套权限控制系统，通过那套权限控制系统再访问 ZooKeeper。\n额外的权限控制系统不但增加了系统复杂性和维护成本，而且降低了系统的总体性能。\n即使有了 ZooKeeper 也很难避免业务系统的数据不一致 # 由于 ZooKeeper 的性能限制，我们无法让每次系统内部调用都走 ZooKeeper，因此总有某些时刻，业务系统会存在两份数据（业务系统 client 那边缓存的业务系统信息是定时从 ZooKeeper 更新的，因此会有更新不同步的问题）。\n如果要保持数据的强一致性，唯一的方法是先 kill 掉当前 Leader，再在 ZooKeeper 上更新 Leader 信息。是否要 kill 掉当前 Leader 这个问题上，程序是无法完全自动决定的（因为网络隔离的时候 ZooKeeper 已经不可用了，自动脚本没有全局信息，不管怎么做都可能是错的，什么都不做也可能是错的。当网络故障的时候，只有运维人员才有全局信息，程序是无法得知其他机房的情况的）。因此系统无法自动的保障数据一致性，必须要人工介入。而人工介入的典型时间是半个小时以上，我们不能让系统这么长时间不可用。因此我们必须在某个方向上进行妥协，最常见的妥协方式是放弃强一致性，而接受最终一致性。\n如果我们需要人工介入才能保证可靠的强一致性，那么 ZooKeeper 的价值就大打折扣。\nResources # 官方 ZooKeeper 官网 ZooKeeper 官方文档 ZooKeeper Github 书籍 《Hadoop 权威指南（第四版）》 《从 Paxos 到 Zookeeper 分布式一致性原理与实践》 文章 分布式服务框架 ZooKeeper \u0026ndash; 管理分布式环境中的数据 ZooKeeper 的功能以及工作原理 ZooKeeper 简介及核心概念 详解分布式协调服务 ZooKeeper 深入浅出 Zookeeper（一） Zookeeper 架构及 FastLeaderElection 机制 Introduction to Apache ZooKeeper Zookeeper 的优缺点 ","date":"13 October 2023","permalink":"/posts/architecture/distributed/zookeeper/zookeeper-theory/","section":"博客","summary":"ZooKeeper 是 Apache 的顶级项目。ZooKeeper 为分布式应用提供了高效且可靠的分布式协调服务，提供了诸如统一命名服务、配置管理和分布式锁等分布式的基础服务。在解决分布式数据一致性方面，ZooKeeper 并没有直接采用 Paxos 算法，而是采用了名为 ZAB 的一致性协议。","title":"Zookeeper 原理"},{"content":"Background # IIoT（工业物联网）架构通常是分布式和异步的，通信由事件驱动，如消息的发布（和相应的订阅）。这些异步架构提高了可扩展性和对变化的耐受性，但也引发了互操作性问题，因为架构各元素之间对消息内部结构及其分类（主题）的明确知识被稀释了。\n事实上，这也是 REST 应用程序接口面临的一个问题，直到业界联合起来，提出了一种定义同步应用程序接口结构和模式的标准方法： OpenAPI（源自 Swagger）。\nIntroduction # 对于异步架构，受 OpenAPI 的启发，AsyncAPI 的出现解决了这一问题：\nAsyncAPI 提供了一种规范，允许您以机器可读的格式定义消息驱动的 API。它与协议无关，因此可以用于通过 Kafka、MQTT、AMQP、WebSockets、STOMP 等工作的 API。该规范与 OpenAPI/Swagger 非常相似，所以如果你熟悉它，AsyncAPI 对你来说应该很容易。\n在 AsyncAPI 中，API 的规格可以用 YAML 或 JSON 定义，例如可以指定消息代理、感兴趣的主题或与每个主题相关的不同消息格式等。不过，AsyncAPI 还处于开发的早期阶段，AsyncAPI 工具市场还不发达，主要局限于生成供人类使用的文档。\nAsyncAPI 最初的贡献就是上图中展示的方法。\nAsyncAPI Toolkit # 如上图所示，AsyncAPI 团队扩展了这一初始框架。基于 AsyncAPI 规范在 Xtext 中开发 AsyncAPI JSON 语法的，该语法可验证符合 AsyncAPI 规范的消息驱动 API 定义。同样，根据该语法，Xtext 会自动生成相应的 AsyncAPI 元模型和所有工具（带内容辅助功能的编辑器、解析器等），以便轻松创建 AsyncAPI JSON 定义并将其转换为符合 AsyncAPI 元模型的 AsyncAPI 模型。\n有了 AsyncAPI 元模型和作为符合模型的应用程序接口规范，就可以通过执行 M2T 转换（生成内部 DSL）来继续工作流程。目前， AsyncAPI Toolkit 支持 Java 语言，并生成一个库，通过提供流畅的 API 来协助开发人员创建、发布和接收格式良好的消息。\n值得注意的是，由于这些架构都是基于 message 的，因此数据建模起着至关重要的作用。因此，我们在上述工作流程中使用了另一种（图形化）具体语法，重点是对要交换的消息进行建模。这可用于引导 AsyncAPI JSON 定义，随后可对其进行手动完善。\nImporting / Modeling an AsyncAPI 规范 # 首先，基于 AsyncAPI 规范，我们创建了一个 Xtext 语法。根据该语法，我们自动生成了一个 Ecore metamodel，以及一套编辑器和基于 Eclipse 的工具。这些编辑器允许使用 AsyncAPI 创建基于 JSON 的消息驱动 API 规范。使用这些编辑器创建的规范会被自动解析并重新整合为 AsyncAPI 元模型的实例。\n生成代码，轻松处理 AsyncAPI 规范中的信息 # 此外，原型还能生成 Java 代码，支持根据建模的 AsyncAPI（包括嵌套 JSON 对象）创建和序列化基于 JSON 的消息有效载荷。但目前还不支持数组。下面的节选显示了原型支持的 AsyncAPI 规范示例：\n{ \u0026#34;asyncapi\u0026#34;: \u0026#34;1.2.0\u0026#34;, \u0026#34;info\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Sample AsyncAPI specification\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.1.0\u0026#34;, }, \u0026#34;servers\u0026#34;: [ { \u0026#34;url\u0026#34;: \u0026#34;broker.url:{port}\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;mqtt\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;This is an example description\u0026#34;, \u0026#34;variables\u0026#34;: { \u0026#34;port\u0026#34;: { \u0026#34;default\u0026#34;: \u0026#34;1883\u0026#34;, \u0026#34;enum\u0026#34;: [ \u0026#34;1883\u0026#34;, \u0026#34;8883\u0026#34; ] } } } ], \u0026#34;topics\u0026#34;: { \u0026#34;messages/device2controller\u0026#34;: { \u0026#34;publish\u0026#34;: { \u0026#34;$ref\u0026#34; : \u0026#34;#/components/messages/request\u0026#34; } } } }, \u0026#34;components\u0026#34;: { \u0026#34;schemas\u0026#34;: { \u0026#34;protocol_version\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Protocol version\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;default\u0026#34;: 2, \u0026#34;x-friendly-name\u0026#34;: \u0026#34;ProtocolVersion\u0026#34; }, \u0026#34;id\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;ID\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;XXXXXX YY ZZZZZZ W\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Status\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;OK\u0026#34;, \u0026#34;ERROR\u0026#34;], \u0026#34;x-friendly-name\u0026#34; : \u0026#34;Status\u0026#34; }, \u0026#34;environment\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Environment\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;DEV\u0026#34;, \u0026#34;STAG\u0026#34;,\u0026#34;PROD\u0026#34; ], \u0026#34;x-friendly-name\u0026#34; : \u0026#34;Environment\u0026#34; } }, \u0026#34;messages\u0026#34; : { \u0026#34;request\u0026#34; : { \u0026#34;summary\u0026#34; : \u0026#34;Request connectivity.\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Request connectivity when status changes\u0026#34;, \u0026#34;payload\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;P\u0026#34;: { \u0026#34;$ref\u0026#34;: \u0026#34;#/components/schemas/protocol_version\u0026#34; }, \u0026#34;ID\u0026#34;: { \u0026#34;$ref\u0026#34;: \u0026#34;#/components/schemas/id\u0026#34; }, \u0026#34;E\u0026#34;: { \u0026#34;$ref\u0026#34;: \u0026#34;#/components/schemas/environment\u0026#34; }, \u0026#34;M\u0026#34;: { \u0026#34;x-friendly-name\u0026#34; : \u0026#34;Message\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;S\u0026#34;: { \u0026#34;$ref\u0026#34;: \u0026#34;#/components/schemas/status\u0026#34; }, \u0026#34;C\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Content\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;x-friendly-name\u0026#34;: \u0026#34;Content\u0026#34; } } } } } } } } 根据上述规范，可以生成如下信息：\npackage tests; import messages.device2controller.Request; import messages.device2controller.Request.Payload.Environment; import messages.device2controller.Request.Payload.Message; import messages.device2controller.Request.Payload.PayloadBuilder; import messages.device2controller.Request.Payload.Message.Status; public class Test { public static void main(String[] args) { PayloadBuilder builder = Request.payloadBuilder(); Request.Payload payload = builder .withProtocolVersion(2) .withEnvironment(Environment.DEV) .withID(\u0026#34;id\u0026#34;) .withMessage( Message.newBuilder() .withStatus(Status.OK) .withContent(\u0026#34;Content\u0026#34;) .build() ).build(); System.out.println(payload.toJson(true)); System.out.println(Request.Payload.fromJson(payload.toJson()).toJson(true)); } } 从 Ecore 模型生成新的 AsyncAPI # 在此之前，我们假设您要么已经有一个 AsyncAPI 文件要导入，要么您将使用我们的 AsyncAPI 编辑器创建一个文件。事实上，还有第三种选择：使用现有的 Ecore 模型，并从中生成一个骨架 AsyncAPI 规范。\n生成器将为每个领域类创建一个可重复使用的 JSON 模式。通道将由注释过的 EClasses 创建。此外，还可通过 EAnnotations 指定主机信息。\n除了其局限性外，获得基于 JSON 的 Ecore 模型表示法还有几个优点：\n允许开发人员和架构师创建一个可用的 AsyncAPI 定义，而无需深入了解规范， 同时保持建模环境的简单性和可管理性； 以及让不熟悉建模的人也能遵守 AsyncAPI 规范还能让有经验的开发人员和架构师完善和完成无法用 Ecore 轻松捕获的架构细节 为了在建议的开发工作流程中集成数据模型，定义了 Ecore 到 AsyncAPI 的模型到模型（M2M）和 AsyncAPI 到 JSON 的 M2T 转换。\n例子\nResources # tutorial: https://modeling-languages.com/asyncapi-modeling-editor-code-generator/ A model-based approach for developing event-driven architectures with AsyncAPI Model-driven development of asynchronous message-driven architectures with AsyncAPI Grammar # grammar io.github.abelgomez.asyncapi.AsyncApi hidden(WS) generate asyncApi \u0026#34;http://io.github.abelgomez/asyncapi/AsyncApi\u0026#34; import \u0026#34;http://www.eclipse.org/emf/2002/Ecore\u0026#34; as ecore AsyncAPI: {AsyncAPI} \u0026#39;{\u0026#39;\t( ( \u0026#39;\u0026#34;asyncapi\u0026#34;\u0026#39; \u0026#39;:\u0026#39; version=VersionNumber \u0026#39;,\u0026#39;? ) \u0026amp; ( \u0026#39;\u0026#34;info\u0026#34;\u0026#39; \u0026#39;:\u0026#39; info=Info \u0026#39;,\u0026#39;? ) \u0026amp; ( \u0026#39;\u0026#34;servers\u0026#34;\u0026#39; \u0026#39;:\u0026#39; \u0026#39;{\u0026#39; servers+=Server (\u0026#39;,\u0026#39; servers+=Server)* \u0026#39;}\u0026#39; \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;channels\u0026#34;\u0026#39; \u0026#39;:\u0026#39; \u0026#39;{\u0026#39; channels+=Channel (\u0026#39;,\u0026#39; channels+=Channel)* \u0026#39;}\u0026#39; \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;components\u0026#34;\u0026#39; \u0026#39;:\u0026#39; components=Components \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;x-sla\u0026#34;\u0026#39; \u0026#39;:\u0026#39; sla=Sla \u0026#39;,\u0026#39;? )? //\t\u0026amp; ( GenericJsonTuple \u0026#39;,\u0026#39;? )* ) \u0026#39;}\u0026#39;; Info: {Info} \u0026#39;{\u0026#39; ( ( \u0026#39;\u0026#34;title\u0026#34;\u0026#39; \u0026#39;:\u0026#39; title=AnyString \u0026#39;,\u0026#39;? ) \u0026amp; ( \u0026#39;\u0026#34;version\u0026#34;\u0026#39; \u0026#39;:\u0026#39; version=AnyString \u0026#39;,\u0026#39;? ) \u0026amp; ( \u0026#39;\u0026#34;description\u0026#34;\u0026#39; \u0026#39;:\u0026#39; description=AnyString \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;termsOfService\u0026#34;\u0026#39; \u0026#39;:\u0026#39; termsOfService=AnyString \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;contact\u0026#34;\u0026#39; \u0026#39;:\u0026#39; contact=Contact \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;license\u0026#34;\u0026#39; \u0026#39;:\u0026#39; license=License \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;x-basePackage\u0026#34;\u0026#39; \u0026#39;:\u0026#39; basePackage=AnyString \u0026#39;,\u0026#39;? )? //\t\u0026amp; ( GenericJsonTuple \u0026#39;,\u0026#39;? )* ) \u0026#39;}\u0026#39;; Contact: {Contact} \u0026#39;{\u0026#39; ( ( \u0026#39;\u0026#34;name\u0026#34;\u0026#39; \u0026#39;:\u0026#39; name=AnyString \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;url\u0026#34;\u0026#39; \u0026#39;:\u0026#39; url=AnyString \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;email\u0026#34;\u0026#39; \u0026#39;:\u0026#39; email=AnyString \u0026#39;,\u0026#39;? )? //\t\u0026amp; ( GenericJsonTuple \u0026#39;,\u0026#39;? )* ) \u0026#39;}\u0026#39;; License: {License} \u0026#39;{\u0026#39; ( ( \u0026#39;\u0026#34;name\u0026#34;\u0026#39; \u0026#39;:\u0026#39; name=AnyString \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;url\u0026#34;\u0026#39; \u0026#39;:\u0026#39; url=AnyString \u0026#39;,\u0026#39;? )? //\t\u0026amp; ( GenericJsonTuple \u0026#39;,\u0026#39;? )* ) \u0026#39;}\u0026#39;; Server: {Server} name=AnyString \u0026#39;:\u0026#39; \u0026#39;{\u0026#39; ( ( \u0026#39;\u0026#34;url\u0026#34;\u0026#39; \u0026#39;:\u0026#39; url=AnyString \u0026#39;,\u0026#39;? ) \u0026amp; ( \u0026#39;\u0026#34;protocol\u0026#34;\u0026#39; \u0026#39;:\u0026#39; protocol=Protocol \u0026#39;,\u0026#39;? ) \u0026amp; ( \u0026#39;\u0026#34;description\u0026#34;\u0026#39; \u0026#39;:\u0026#39; description=AnyString \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;variables\u0026#34;\u0026#39; \u0026#39;:\u0026#39; \u0026#39;{\u0026#39; variables+=Variable (\u0026#39;,\u0026#39; variables+=Variable)* \u0026#39;}\u0026#39; \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;x-isMonitored\u0026#34;\u0026#39; \u0026#39;:\u0026#39; isMonitored=Boolean \u0026#39;,\u0026#39;? )? //\t\u0026amp; ( GenericJsonTuple \u0026#39;,\u0026#39;? )* ) \u0026#39;}\u0026#39;; Variable: {Variable} name=AnyString \u0026#39;:\u0026#39; \u0026#39;{\u0026#39; ( ( \u0026#39;\u0026#34;description\u0026#34;\u0026#39; \u0026#39;:\u0026#39; description=AnyString \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;default\u0026#34;\u0026#39; \u0026#39;:\u0026#39; default=AnyString \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;enum\u0026#34;\u0026#39; \u0026#39;:\u0026#39; \u0026#39;[\u0026#39; ^enum+=AnyString (\u0026#39;,\u0026#39; ^enum+=AnyString)* \u0026#39;]\u0026#39; \u0026#39;,\u0026#39;? )? //\t\u0026amp; ( GenericJsonTuple \u0026#39;,\u0026#39;? )* ) \u0026#39;}\u0026#39;; Channel: {Channel} name=AnyString \u0026#39;:\u0026#39; \u0026#39;{\u0026#39; ( ( \u0026#39;\u0026#34;description\u0026#34;\u0026#39; \u0026#39;:\u0026#39; description=AnyString \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;publish\u0026#34;\u0026#39; \u0026#39;:\u0026#39; publish=Operation \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;subscribe\u0026#34;\u0026#39; \u0026#39;:\u0026#39; subscribe=Operation \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;parameters\u0026#34;\u0026#39; \u0026#39;:\u0026#39; \u0026#39;{\u0026#39; parameters+=NamedParameter (\u0026#39;,\u0026#39; parameters+=NamedParameter)* \u0026#39;}\u0026#39; \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;x-title\u0026#34;\u0026#39; \u0026#39;:\u0026#39; title=AnyString \u0026#39;,\u0026#39;? )? //\t\u0026amp; ( GenericJsonTuple \u0026#39;,\u0026#39;? )* ) \u0026#39;}\u0026#39;; Operation: {Operation} \u0026#39;{\u0026#39; ( ( \u0026#39;\u0026#34;operationId\u0026#34;\u0026#39; \u0026#39;:\u0026#39; operationId=AnyString \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;summary\u0026#34;\u0026#39; \u0026#39;:\u0026#39; summary=AnyString \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;description\u0026#34;\u0026#39; \u0026#39;:\u0026#39; description=AnyString \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;message\u0026#34;\u0026#39; \u0026#39;:\u0026#39; message=AbstractMessage \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;traits\u0026#34;\u0026#39; \u0026#39;:\u0026#39; \u0026#39;[\u0026#39; traits+=AbstractOperationTrait ( \u0026#39;,\u0026#39; traits+=AbstractOperationTrait )* \u0026#39;]\u0026#39; \u0026#39;,\u0026#39;? )? //\t\u0026amp; ( GenericJsonTuple \u0026#39;,\u0026#39;? )* ) \u0026#39;}\u0026#39;; AbstractMessage: Reference | Message; Message: {Message} \u0026#39;{\u0026#39; ( ( \u0026#39;\u0026#34;name\u0026#34;\u0026#39; \u0026#39;:\u0026#39; name=AnyString \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;title\u0026#34;\u0026#39; \u0026#39;:\u0026#39; title=AnyString \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;summary\u0026#34;\u0026#39; \u0026#39;:\u0026#39; summary=AnyString \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;description\u0026#34;\u0026#39; \u0026#39;:\u0026#39; description=AnyString \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;deprecated\u0026#34;\u0026#39; \u0026#39;:\u0026#39; deprecated=Boolean \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;headers\u0026#34;\u0026#39; \u0026#39;:\u0026#39; headers=AbstractSchema \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;tags\u0026#34;\u0026#39; \u0026#39;:\u0026#39; \u0026#39;[\u0026#39; tags+=Tag ( \u0026#39;,\u0026#39; tags+=Tag )* \u0026#39;]\u0026#39; \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;payload\u0026#34;\u0026#39; \u0026#39;:\u0026#39; payload=AbstractSchema \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;traits\u0026#34;\u0026#39; \u0026#39;:\u0026#39; \u0026#39;[\u0026#39; traits+=AbstractMessageTrait ( \u0026#39;,\u0026#39; traits+=AbstractMessageTrait )* \u0026#39;]\u0026#39; \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;x-identifier\u0026#34;\u0026#39; \u0026#39;:\u0026#39; identifier=MessageIdentifier )? //\t\u0026amp; ( GenericJsonTupleButRef \u0026#39;,\u0026#39;? )* ) \u0026#39;}\u0026#39;; NamedMessage: {NamedMessage} name=AnyString \u0026#39;:\u0026#39; message=AbstractMessage; Tag: {Tag} \u0026#39;{\u0026#39; ( (\u0026#39;\u0026#34;name\u0026#34;\u0026#39; \u0026#39;:\u0026#39; name=AnyString \u0026#39;,\u0026#39;?)? \u0026amp; (\u0026#39;\u0026#34;description\u0026#34;\u0026#39; \u0026#39;:\u0026#39; description=AnyString \u0026#39;,\u0026#39;?)? //\t\u0026amp; ( GenericJsonTuple \u0026#39;,\u0026#39;? )* ) \u0026#39;}\u0026#39;; AbstractSchema: Reference | Schema; Schema: {Schema} \u0026#39;{\u0026#39; ( ( \u0026#39;\u0026#34;title\u0026#34;\u0026#39; \u0026#39;:\u0026#39; title=AnyString \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;type\u0026#34;\u0026#39; \u0026#39;:\u0026#39; type=JsonType \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;description\u0026#34;\u0026#39; \u0026#39;:\u0026#39; description=AnyString \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;format\u0026#34;\u0026#39; \u0026#39;:\u0026#39; format=AnyString \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;minimum\u0026#34;\u0026#39; \u0026#39;:\u0026#39; minimum=INT \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;maximum\u0026#34;\u0026#39; \u0026#39;:\u0026#39; maximum=INT \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;minItems\u0026#34;\u0026#39; \u0026#39;:\u0026#39; minItems=INT \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;maxItems\u0026#34;\u0026#39; \u0026#39;:\u0026#39; maxItems=INT \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;default\u0026#34;\u0026#39; \u0026#39;:\u0026#39; default=PrimitiveValue\u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;properties\u0026#34;\u0026#39; \u0026#39;:\u0026#39; \u0026#39;{\u0026#39; properties+=NamedSchema (\u0026#39;,\u0026#39; properties+=NamedSchema)* \u0026#39;}\u0026#39; \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;enum\u0026#34;\u0026#39; \u0026#39;:\u0026#39; \u0026#39;[\u0026#39; ^enum+=PrimitiveValue (\u0026#39;,\u0026#39; ^enum+=PrimitiveValue)* \u0026#39;]\u0026#39; \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;items\u0026#34;\u0026#39; \u0026#39;:\u0026#39; items=AbstractSchema \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;required\u0026#34;\u0026#39; \u0026#39;:\u0026#39; \u0026#39;[\u0026#39; required+=AnyString (\u0026#39;,\u0026#39; required+=AnyString)* \u0026#39;]\u0026#39; \u0026#39;,\u0026#39;? )? //\t\u0026amp; ( GenericJsonTupleButRef \u0026#39;,\u0026#39;? )* ) \u0026#39;}\u0026#39;; NamedSchema: {NamedSchema} name=AnyString \u0026#39;:\u0026#39; schema=AbstractSchema; AbstractParameter: Reference | Parameter; Parameter: {Parameter} \u0026#39;{\u0026#39; ( ( \u0026#39;\u0026#34;description\u0026#34;\u0026#39; \u0026#39;:\u0026#39; description=AnyString \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;schema\u0026#34;\u0026#39; \u0026#39;:\u0026#39; schema=AbstractSchema \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;location\u0026#34;\u0026#39; \u0026#39;:\u0026#39; location=AnyString \u0026#39;,\u0026#39;? )? //\t\u0026amp; ( GenericJsonTupleButRef \u0026#39;,\u0026#39;? )* ) \u0026#39;}\u0026#39;; NamedParameter: {NamedParameter} name=AnyString \u0026#39;:\u0026#39; parameter=AbstractParameter; AbstractOperationTrait: Reference | OperationTrait; OperationTrait: {OperationTrait} \u0026#39;{\u0026#39; ( ( \u0026#39;\u0026#34;operationId\u0026#34;\u0026#39; \u0026#39;:\u0026#39; operationId=AnyString \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;summary\u0026#34;\u0026#39; \u0026#39;:\u0026#39; summary=AnyString \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;description\u0026#34;\u0026#39; \u0026#39;:\u0026#39; description=AnyString \u0026#39;,\u0026#39;? )? //\t\u0026amp; ( GenericJsonTupleButRef \u0026#39;,\u0026#39;? )* ) \u0026#39;}\u0026#39;; NamedOperationTrait: {NamedOperationTrait} name=AnyString \u0026#39;:\u0026#39; operationTrait=AbstractOperationTrait; AbstractMessageTrait: Reference | MessageTrait; MessageTrait: {MessageTrait} \u0026#39;{\u0026#39; ( ( \u0026#39;\u0026#34;summary\u0026#34;\u0026#39; \u0026#39;:\u0026#39; summary=AnyString \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;description\u0026#34;\u0026#39; \u0026#39;:\u0026#39; description=AnyString \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;deprecated\u0026#34;\u0026#39; \u0026#39;:\u0026#39; deprecated=Boolean \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;headers\u0026#34;\u0026#39; \u0026#39;:\u0026#39; headers=AbstractSchema \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;tags\u0026#34;\u0026#39; \u0026#39;:\u0026#39; \u0026#39;[\u0026#39; tags+=Tag ( \u0026#39;,\u0026#39; tags+=Tag )* \u0026#39;]\u0026#39; \u0026#39;,\u0026#39;? )? //\t\u0026amp; ( GenericJsonTupleButRef \u0026#39;,\u0026#39;? )* ) \u0026#39;}\u0026#39;; NamedMessageTrait: {NamedMessageTrait} name=AnyString \u0026#39;:\u0026#39; messageTrait=AbstractMessageTrait; Components: {Components} \u0026#39;{\u0026#39; ( ( \u0026#39;\u0026#34;schemas\u0026#34;\u0026#39; \u0026#39;:\u0026#39; \u0026#39;{\u0026#39; schemas+=NamedSchema (\u0026#39;,\u0026#39; schemas+=NamedSchema)* \u0026#39;}\u0026#39; \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;messages\u0026#34;\u0026#39; \u0026#39;:\u0026#39; \u0026#39;{\u0026#39; messages+=NamedMessage (\u0026#39;,\u0026#39; messages+=NamedMessage)* \u0026#39;}\u0026#39; \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;parameters\u0026#34;\u0026#39; \u0026#39;:\u0026#39; \u0026#39;{\u0026#39; parameters+=NamedParameter (\u0026#39;,\u0026#39; parameters+=NamedParameter)* \u0026#39;}\u0026#39; \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;operationTraits\u0026#34;\u0026#39; \u0026#39;:\u0026#39; \u0026#39;{\u0026#39; operationTraits+=NamedOperationTrait (\u0026#39;,\u0026#39; operationTraits+=NamedOperationTrait)* \u0026#39;}\u0026#39; \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;messageTraits\u0026#34;\u0026#39; \u0026#39;:\u0026#39; \u0026#39;{\u0026#39; messageTraits+=NamedMessageTrait (\u0026#39;,\u0026#39; messageTraits+=NamedMessageTrait)* \u0026#39;}\u0026#39; \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;x-qosMetrics\u0026#34;\u0026#39; \u0026#39;:\u0026#39; \u0026#39;[\u0026#39; qosMetrics+=QoSMetric (\u0026#39;,\u0026#39; qosMetrics+=QoSMetric)* \u0026#39;]\u0026#39; \u0026#39;,\u0026#39;? )? //\t\u0026amp; ( GenericJsonTupleButRef \u0026#39;,\u0026#39;? )* ) \u0026#39;}\u0026#39;; Sla: {Sla} \u0026#39;{\u0026#39; ( ( \u0026#39;\u0026#34;guaranteeTerm\u0026#34;\u0026#39; \u0026#39;:\u0026#39; guaranteeTerm+=GuaranteeTerm (\u0026#39;,\u0026#39; guaranteeTerm+=GuaranteeTerm)* ) ) \u0026#39;}\u0026#39;; GuaranteeTerm: {GuaranteeTerm} \u0026#39;{\u0026#39; ( ( \u0026#39;\u0026#34;scopes\u0026#34;\u0026#39; \u0026#39;:\u0026#39; \u0026#39;{\u0026#39; scopes+=Scope (\u0026#39;,\u0026#39; scopes+=Scope)* \u0026#39;}\u0026#39; \u0026#39;,\u0026#39; ) ( \u0026#39;\u0026#34;qualifyingConditions\u0026#34;\u0026#39; \u0026#39;:\u0026#39; \u0026#39;{\u0026#39; qualifyingConditions+=QualifyingCondition (\u0026#39;,\u0026#39; qualifyingConditions+=QualifyingCondition)* \u0026#39;}\u0026#39;\u0026#39;,\u0026#39;)? ( \u0026#39;\u0026#34;slos\u0026#34;\u0026#39; \u0026#39;:\u0026#39; \u0026#39;{\u0026#39; slos+=Slo (\u0026#39;,\u0026#39; slos+=Slo)* \u0026#39;}\u0026#39;)\t) \u0026#39;}\u0026#39;; Scope: {Scope}( name=AnyString \u0026#39;:\u0026#39; reference= [Channel|AnyString] ); QualifyingCondition: {QualifyingCondition} name=AnyString \u0026#39;:\u0026#39; condition=BooleanExpression ; Slo: {Slo} name=AnyString \u0026#39;:\u0026#39; condition=BooleanExpression ;\tAbstractQoSMetric: QoSMetricReference | QoSMetric; QoSMetricReference: metric= [QoSMetric|AnyString]\t; QoSMetric: (\u0026#39;{\u0026#39; ( ( \u0026#39;\u0026#34;name\u0026#34;\u0026#39; \u0026#39;:\u0026#39; name=AnyString \u0026#39;,\u0026#39;? )\t\u0026amp; ( \u0026#39;\u0026#34;metricType\u0026#34;\u0026#39; \u0026#39;:\u0026#39; metricType=QoSMetricType \u0026#39;,\u0026#39;? )\t\u0026amp; ( \u0026#39;\u0026#34;description\u0026#34;\u0026#39; \u0026#39;:\u0026#39; description=AnyString \u0026#39;,\u0026#39;? )? \u0026amp; ( \u0026#39;\u0026#34;unit\u0026#34;\u0026#39; \u0026#39;:\u0026#39; unit=QoSMetricUnit \u0026#39;,\u0026#39;? ) \u0026amp; ( \u0026#39;\u0026#34;groupedByEvent\u0026#34;\u0026#39; \u0026#39;:\u0026#39; groupedByEvent=Boolean \u0026#39;,\u0026#39;? )\t) (DerivedQoSMetric)?\t// Això està al final de tot, pq Xtext es queixa que no pot haver-hi una unasssigned rule dins d\u0026#39;una unordered list.\t\u0026#39;}\u0026#39;); DerivedQoSMetric: {DerivedQoSMetric}( \u0026#39;\u0026#34;derivedQoSMetricDefinition\u0026#34;\u0026#39; \u0026#39;:\u0026#39; \u0026#39;{\u0026#39; ( (\u0026#39;\u0026#34;window\u0026#34;\u0026#39; \u0026#39;:\u0026#39; window = AnyString \u0026#39;,\u0026#39;? ) \u0026amp; (\u0026#39;\u0026#34;windowUnit\u0026#34;\u0026#39; \u0026#39;:\u0026#39; windowUnit = WindowUnit \u0026#39;,\u0026#39;? ) \u0026amp; (\u0026#39;\u0026#34;aggregationFunction\u0026#34;\u0026#39; \u0026#39;:\u0026#39; aggregationFunction = AggregationFunction \u0026#39;,\u0026#39;? ) ) \u0026#39;}\u0026#39; ) ; BooleanExpression: AndExpression | OrExpression | ComparisonExpression; AndExpression: {AndExpression} \u0026#39;{\u0026#39; \u0026#39;\u0026#34;AND\u0026#34;\u0026#39; \u0026#39;:\u0026#39; \u0026#39;[\u0026#39; conditions+=BooleanExpression (\u0026#39;,\u0026#39; conditions+=BooleanExpression)* \u0026#39;]\u0026#39; \u0026#39;}\u0026#39; ; OrExpression: {OrExpression} \u0026#39;{\u0026#39; \u0026#39;\u0026#34;OR\u0026#34;\u0026#39; \u0026#39;:\u0026#39; \u0026#39;[\u0026#39; conditions+=BooleanExpression (\u0026#39;,\u0026#39; conditions+=BooleanExpression)* \u0026#39;]\u0026#39; \u0026#39;}\u0026#39;; ComparisonExpression: {ComparisonExpression} \u0026#39;{\u0026#39; \u0026#39;\u0026#34;qosMetric\u0026#34;\u0026#39; \u0026#39;:\u0026#39; qosMetric = AbstractQoSMetric \u0026#39;,\u0026#39; \u0026#39;\u0026#34;operator\u0026#34;\u0026#39; \u0026#39;:\u0026#39; operator = Operator \u0026#39;,\u0026#39; \u0026#39;\u0026#34;value\u0026#34;\u0026#39; \u0026#39;:\u0026#39; value = AnyString \u0026#39;}\u0026#39; ; Reference: {Reference} \u0026#39;{\u0026#39; \u0026#39;\u0026#34;$ref\u0026#34;\u0026#39; \u0026#39;:\u0026#39; uri=AnyString \u0026#39;}\u0026#39;; //GenericJsonExpression: //\tPrimitiveValue //\t| GenericJsonObject //\t| GenericJsonArray; // //GenericJsonObject: //\t\u0026#39;{\u0026#39; \u0026#39;}\u0026#39; | \u0026#39;{\u0026#39; GenericJsonTuple (\u0026#39;,\u0026#39; GenericJsonTuple)* \u0026#39;}\u0026#39;; // //GenericJsonArray: //\t\u0026#39;[\u0026#39; \u0026#39;]\u0026#39; | \u0026#39;[\u0026#39; GenericJsonExpression (\u0026#39;,\u0026#39; GenericJsonExpression)* \u0026#39;]\u0026#39;; // //GenericJsonTuple: AnyString \u0026#39;:\u0026#39; GenericJsonExpression; // //GenericJsonTupleButRef: AnyStringButRef \u0026#39;:\u0026#39; GenericJsonExpression; enum WindowUnit: seconds = \u0026#39;\u0026#34;seconds\u0026#34;\u0026#39; | minutes = \u0026#39;\u0026#34;minutes\u0026#34;\u0026#39; | hours = \u0026#39;\u0026#34;hours\u0026#34;\u0026#39; | days = \u0026#39;\u0026#34;days\u0026#34;\u0026#39; | messages = \u0026#39;\u0026#34;messages\u0026#34;\u0026#39; ; enum AggregationFunction: AVG = \u0026#39;\u0026#34;AVG\u0026#34;\u0026#39; | MEDIAN = \u0026#39;\u0026#34;MEDIAN\u0026#34;\u0026#39; | MAX = \u0026#39;\u0026#34;MAX\u0026#34;\u0026#39; | MIN = \u0026#39;\u0026#34;MIN\u0026#34;\u0026#39; ; enum QoSMetricType: availability = \u0026#39;\u0026#34;availability\u0026#34;\u0026#39; | bandwith = \u0026#39;\u0026#34;bandwith\u0026#34;\u0026#39; | cpu = \u0026#39;\u0026#34;cpu\u0026#34;\u0026#39; | capacity = \u0026#39;\u0026#34;capacity\u0026#34;\u0026#39; | disaster = \u0026#39;\u0026#34;disaster\u0026#34;\u0026#39; | resiliance = \u0026#39;\u0026#34;resiliance\u0026#34;\u0026#39; | discoverability = \u0026#39;\u0026#34;discoverability\u0026#34;\u0026#39; | documentation = \u0026#39;\u0026#34;documentation\u0026#34;\u0026#39; | exception_handling = \u0026#39;\u0026#34;exception_handling\u0026#34;\u0026#39; | expected_failures = \u0026#39;\u0026#34;expected_failures\u0026#34;\u0026#39; | failover = \u0026#39;\u0026#34;failover\u0026#34;\u0026#39; | jitter = \u0026#39;\u0026#34;jitter\u0026#34;\u0026#39; | latency = \u0026#39;\u0026#34;latency\u0026#34;\u0026#39; | load_balancing = \u0026#39;\u0026#34;load_balancing\u0026#34;\u0026#39; | maximum_throughput = \u0026#39;\u0026#34;maximum_throughput\u0026#34;\u0026#39; | memory_aapacity = \u0026#39;\u0026#34;memory_aapacity\u0026#34;\u0026#39; | packet_loss = \u0026#39;\u0026#34;packet_loss\u0026#34;\u0026#39; | precision = \u0026#39;\u0026#34;precision\u0026#34;\u0026#39; | probability_of_correctness = \u0026#39;\u0026#34;probability_of_correctness\u0026#34;\u0026#39; | round_trip_time = \u0026#39;\u0026#34;round_trip_time\u0026#34;\u0026#39; | throughput = \u0026#39;\u0026#34;throughput\u0026#34;\u0026#39; | time_to_tail = \u0026#39;\u0026#34;time_to_tail\u0026#34;\u0026#39; | time_to_tepair = \u0026#39;\u0026#34;time_to_tepair\u0026#34;\u0026#39; | type_consistency = \u0026#39;\u0026#34;type_consistency\u0026#34;\u0026#39; | uptime = \u0026#39;\u0026#34;uptime\u0026#34;\u0026#39; | up_to_dateness = \u0026#39;\u0026#34;up-to-dateness\u0026#34;\u0026#39; ; enum QoSMetricUnit: milliseconds = \u0026#39;\u0026#34;milliseconds\u0026#34;\u0026#39; | seconds = \u0026#39;\u0026#34;seconds\u0026#34;\u0026#39; | minutes = \u0026#39;\u0026#34;minutes\u0026#34;\u0026#39; | hours = \u0026#39;\u0026#34;hours\u0026#34;\u0026#39; | null = \u0026#39;\u0026#34;null\u0026#34;\u0026#39; ; enum Operator: greater = \u0026#39;\u0026#34;\u0026gt;\u0026#34;\u0026#39; | greater_equal = \u0026#39;\u0026#34;\u0026gt;=\u0026#34;\u0026#39; | equal = \u0026#39;\u0026#34;=\u0026#34;\u0026#39; | less_equal = \u0026#39;\u0026#34;\u0026lt;=\u0026#34;\u0026#39; | less = \u0026#39;\u0026#34;\u0026lt;\u0026#34;\u0026#39;\t; enum JsonType: string = \u0026#39;\u0026#34;string\u0026#34;\u0026#39; | number = \u0026#39;\u0026#34;number\u0026#34;\u0026#39; | integer = \u0026#39;\u0026#34;integer\u0026#34;\u0026#39; | boolean = \u0026#39;\u0026#34;boolean\u0026#34;\u0026#39; | object = \u0026#39;\u0026#34;object\u0026#34;\u0026#39; | array = \u0026#39;\u0026#34;array\u0026#34;\u0026#39; | any = \u0026#39;\u0026#34;any\u0026#34;\u0026#39; | null = \u0026#39;\u0026#34;null\u0026#34;\u0026#39;; enum Boolean: _false = \u0026#34;false\u0026#34; | _true = \u0026#34;true\u0026#34;; enum VersionNumber: _200 = \u0026#39;\u0026#34;2.0.0\u0026#34;\u0026#39;; enum MessageIdentifier: none =\u0026#39;\u0026#34;none\u0026#34;\u0026#39; | generated = \u0026#39;\u0026#34;generated\u0026#34;\u0026#39; | md5 = \u0026#39;\u0026#34;md5\u0026#34;\u0026#39; | sha256 = \u0026#39;\u0026#34;sha-256\u0026#34;\u0026#39;; enum Protocol: amqp = \u0026#39;\u0026#34;amqp\u0026#34;\u0026#39; | amqps = \u0026#39;\u0026#34;amqps\u0026#34;\u0026#39; | http = \u0026#39;\u0026#34;http\u0026#34;\u0026#39; | https = \u0026#39;\u0026#34;https\u0026#34;\u0026#39; | jms = \u0026#39;\u0026#34;jms\u0026#34;\u0026#39; | kafka = \u0026#39;\u0026#34;kafka\u0026#34;\u0026#39; | kafka_secure = \u0026#39;\u0026#34;kafka-secure\u0026#34;\u0026#39; | mqtt = \u0026#39;\u0026#34;mqtt\u0026#34;\u0026#39; | secure_mqtt = \u0026#39;\u0026#34;secure-mqtt\u0026#34;\u0026#39; | ws = \u0026#39;\u0026#34;ws\u0026#34;\u0026#39; | wss = \u0026#39;\u0026#34;wss\u0026#34;\u0026#39; | stomp = \u0026#39;\u0026#34;stomp\u0026#34;\u0026#39; | stomps = \u0026#39;\u0026#34;stomps\u0026#34;\u0026#39;; PrimitiveValue: AnyString | \u0026#34;true\u0026#34; | \u0026#34;false\u0026#34; | INT; AnyStringButRef: STRING | Keyword; AnyString: STRING | \u0026#39;\u0026#34;$ref\u0026#34;\u0026#39; | Keyword; terminal ID: \u0026#39;^\u0026#39;?(\u0026#39;a\u0026#39;..\u0026#39;z\u0026#39;|\u0026#39;A\u0026#39;..\u0026#39;Z\u0026#39;|\u0026#39;_\u0026#39;) (\u0026#39;a\u0026#39;..\u0026#39;z\u0026#39;|\u0026#39;A\u0026#39;..\u0026#39;Z\u0026#39;|\u0026#39;_\u0026#39;|\u0026#39;0\u0026#39;..\u0026#39;9\u0026#39;)*; terminal INT returns ecore::EInt: (\u0026#39;0\u0026#39;..\u0026#39;9\u0026#39;)+; terminal STRING: \u0026#39;\u0026#34;\u0026#39; ( \u0026#39;\\\\\u0026#39; . | !(\u0026#39;\\\\\u0026#39;|\u0026#39;\u0026#34;\u0026#39;) )* \u0026#39;\u0026#34;\u0026#39; | \u0026#34;\u0026#39;\u0026#34; ( \u0026#39;\\\\\u0026#39; . | !(\u0026#39;\\\\\u0026#39;|\u0026#34;\u0026#39;\u0026#34;) )* \u0026#34;\u0026#39;\u0026#34;; terminal WS: (\u0026#39; \u0026#39;|\u0026#39;\\t\u0026#39;|\u0026#39;\\r\u0026#39;|\u0026#39;\\n\u0026#39;)+; Keyword: \u0026#39;\u0026#34;2.0.0\u0026#34;\u0026#39; | \u0026#39;\u0026#34;\u0026lt;\u0026#34;\u0026#39; | \u0026#39;\u0026#34;\u0026lt;=\u0026#34;\u0026#39; | \u0026#39;\u0026#34;=\u0026#34;\u0026#39; | \u0026#39;\u0026#34;\u0026gt;\u0026#34;\u0026#39; | \u0026#39;\u0026#34;\u0026gt;=\u0026#34;\u0026#39; | \u0026#39;\u0026#34;AND\u0026#34;\u0026#39; | \u0026#39;\u0026#34;AVG\u0026#34;\u0026#39; | \u0026#39;\u0026#34;MAX\u0026#34;\u0026#39; | \u0026#39;\u0026#34;MEDIAN\u0026#34;\u0026#39; | \u0026#39;\u0026#34;MIN\u0026#34;\u0026#39; | \u0026#39;\u0026#34;OR\u0026#34;\u0026#39; | \u0026#39;\u0026#34;aggregationFunction\u0026#34;\u0026#39; | \u0026#39;\u0026#34;amqp\u0026#34;\u0026#39; | \u0026#39;\u0026#34;amqps\u0026#34;\u0026#39; | \u0026#39;\u0026#34;any\u0026#34;\u0026#39; | \u0026#39;\u0026#34;array\u0026#34;\u0026#39; | \u0026#39;\u0026#34;asyncapi\u0026#34;\u0026#39; | \u0026#39;\u0026#34;availability\u0026#34;\u0026#39; | \u0026#39;\u0026#34;bandwith\u0026#34;\u0026#39; | \u0026#39;\u0026#34;boolean\u0026#34;\u0026#39; | \u0026#39;\u0026#34;capacity\u0026#34;\u0026#39; | \u0026#39;\u0026#34;channels\u0026#34;\u0026#39; | \u0026#39;\u0026#34;components\u0026#34;\u0026#39; | \u0026#39;\u0026#34;contact\u0026#34;\u0026#39; | \u0026#39;\u0026#34;cpu\u0026#34;\u0026#39; | \u0026#39;\u0026#34;dataType\u0026#34;\u0026#39; | \u0026#39;\u0026#34;days\u0026#34;\u0026#39; | \u0026#39;\u0026#34;default\u0026#34;\u0026#39; | \u0026#39;\u0026#34;deprecated\u0026#34;\u0026#39; | \u0026#39;\u0026#34;derivedQoSMetricDefinition\u0026#34;\u0026#39; | \u0026#39;\u0026#34;description\u0026#34;\u0026#39; | \u0026#39;\u0026#34;disaster\u0026#34;\u0026#39; | \u0026#39;\u0026#34;discoverability\u0026#34;\u0026#39; | \u0026#39;\u0026#34;documentation\u0026#34;\u0026#39; | \u0026#39;\u0026#34;email\u0026#34;\u0026#39; | \u0026#39;\u0026#34;enum\u0026#34;\u0026#39; | \u0026#39;\u0026#34;exception_handling\u0026#34;\u0026#39; | \u0026#39;\u0026#34;expected_failures\u0026#34;\u0026#39; | \u0026#39;\u0026#34;failover\u0026#34;\u0026#39; | \u0026#39;\u0026#34;format\u0026#34;\u0026#39; | \u0026#39;\u0026#34;groupedByEvent\u0026#34;\u0026#39; | \u0026#39;\u0026#34;guaranteeTerm\u0026#34;\u0026#39; | \u0026#39;\u0026#34;headers\u0026#34;\u0026#39; | \u0026#39;\u0026#34;hours\u0026#34;\u0026#39; | \u0026#39;\u0026#34;http\u0026#34;\u0026#39; | \u0026#39;\u0026#34;https\u0026#34;\u0026#39; | \u0026#39;\u0026#34;info\u0026#34;\u0026#39; | \u0026#39;\u0026#34;integer\u0026#34;\u0026#39; | \u0026#39;\u0026#34;items\u0026#34;\u0026#39; | \u0026#39;\u0026#34;jitter\u0026#34;\u0026#39; | \u0026#39;\u0026#34;jms\u0026#34;\u0026#39; | \u0026#39;\u0026#34;kafka\u0026#34;\u0026#39; | \u0026#39;\u0026#34;kafka-secure\u0026#34;\u0026#39; | \u0026#39;\u0026#34;latency\u0026#34;\u0026#39; | \u0026#39;\u0026#34;license\u0026#34;\u0026#39; | \u0026#39;\u0026#34;load_balancing\u0026#34;\u0026#39; | \u0026#39;\u0026#34;location\u0026#34;\u0026#39; | \u0026#39;\u0026#34;maxItems\u0026#34;\u0026#39; | \u0026#39;\u0026#34;maximum\u0026#34;\u0026#39; | \u0026#39;\u0026#34;maximum_throughput\u0026#34;\u0026#39; | \u0026#39;\u0026#34;memory_aapacity\u0026#34;\u0026#39; | \u0026#39;\u0026#34;message\u0026#34;\u0026#39; | \u0026#39;\u0026#34;messageTraits\u0026#34;\u0026#39; | \u0026#39;\u0026#34;messages\u0026#34;\u0026#39; | \u0026#39;\u0026#34;metricType\u0026#34;\u0026#39; | \u0026#39;\u0026#34;milliseconds\u0026#34;\u0026#39; | \u0026#39;\u0026#34;minItems\u0026#34;\u0026#39; | \u0026#39;\u0026#34;minimum\u0026#34;\u0026#39; | \u0026#39;\u0026#34;minutes\u0026#34;\u0026#39; | \u0026#39;\u0026#34;mqtt\u0026#34;\u0026#39; | \u0026#39;\u0026#34;mqtts\u0026#34;\u0026#39; | \u0026#39;\u0026#34;name\u0026#34;\u0026#39; | \u0026#39;\u0026#34;null\u0026#34;\u0026#39; | \u0026#39;\u0026#34;number\u0026#34;\u0026#39; | \u0026#39;\u0026#34;object\u0026#34;\u0026#39; | \u0026#39;\u0026#34;operationId\u0026#34;\u0026#39; | \u0026#39;\u0026#34;operationTraits\u0026#34;\u0026#39; | \u0026#39;\u0026#34;operator\u0026#34;\u0026#39; | \u0026#39;\u0026#34;packet_loss\u0026#34;\u0026#39; | \u0026#39;\u0026#34;parameters\u0026#34;\u0026#39; | \u0026#39;\u0026#34;payload\u0026#34;\u0026#39; | \u0026#39;\u0026#34;precision\u0026#34;\u0026#39; | \u0026#39;\u0026#34;probability_of_correctness\u0026#34;\u0026#39; | \u0026#39;\u0026#34;properties\u0026#34;\u0026#39; | \u0026#39;\u0026#34;protocol\u0026#34;\u0026#39; | \u0026#39;\u0026#34;publish\u0026#34;\u0026#39; | \u0026#39;\u0026#34;qosMetric\u0026#34;\u0026#39; | \u0026#39;\u0026#34;qualifyingConditions\u0026#34;\u0026#39; | \u0026#39;\u0026#34;required\u0026#34;\u0026#39; | \u0026#39;\u0026#34;resiliance\u0026#34;\u0026#39; | \u0026#39;\u0026#34;round_trip_time\u0026#34;\u0026#39; | \u0026#39;\u0026#34;schema\u0026#34;\u0026#39; | \u0026#39;\u0026#34;schemas\u0026#34;\u0026#39; | \u0026#39;\u0026#34;scopes\u0026#34;\u0026#39; | \u0026#39;\u0026#34;seconds\u0026#34;\u0026#39; | \u0026#39;\u0026#34;secure-mqtt\u0026#34;\u0026#39; | \u0026#39;\u0026#34;servers\u0026#34;\u0026#39; | \u0026#39;\u0026#34;slos\u0026#34;\u0026#39; | \u0026#39;\u0026#34;stomp\u0026#34;\u0026#39; | \u0026#39;\u0026#34;stomps\u0026#34;\u0026#39; | \u0026#39;\u0026#34;string\u0026#34;\u0026#39; | \u0026#39;\u0026#34;subscribe\u0026#34;\u0026#39; | \u0026#39;\u0026#34;summary\u0026#34;\u0026#39; | \u0026#39;\u0026#34;tags\u0026#34;\u0026#39; | \u0026#39;\u0026#34;termsOfService\u0026#34;\u0026#39; | \u0026#39;\u0026#34;throughput\u0026#34;\u0026#39; | \u0026#39;\u0026#34;time_to_tail\u0026#34;\u0026#39; | \u0026#39;\u0026#34;time_to_tepair\u0026#34;\u0026#39; | \u0026#39;\u0026#34;title\u0026#34;\u0026#39; | \u0026#39;\u0026#34;traits\u0026#34;\u0026#39; | \u0026#39;\u0026#34;type\u0026#34;\u0026#39; | \u0026#39;\u0026#34;type_consistency\u0026#34;\u0026#39; | \u0026#39;\u0026#34;unit\u0026#34;\u0026#39; | \u0026#39;\u0026#34;up-to-dateness\u0026#34;\u0026#39; | \u0026#39;\u0026#34;uptime\u0026#34;\u0026#39; | \u0026#39;\u0026#34;url\u0026#34;\u0026#39; | \u0026#39;\u0026#34;value\u0026#34;\u0026#39; | \u0026#39;\u0026#34;variables\u0026#34;\u0026#39; | \u0026#39;\u0026#34;version\u0026#34;\u0026#39; | \u0026#39;\u0026#34;window\u0026#34;\u0026#39; | \u0026#39;\u0026#34;windowUnit\u0026#34;\u0026#39; | \u0026#39;\u0026#34;ws\u0026#34;\u0026#39; | \u0026#39;\u0026#34;wss\u0026#34;\u0026#39; | \u0026#39;\u0026#34;x-basePackage\u0026#34;\u0026#39; | \u0026#39;\u0026#34;x-qosMetrics\u0026#34;\u0026#39; | \u0026#39;\u0026#34;x-sla\u0026#34;\u0026#39; | \u0026#39;\u0026#34;x-title\u0026#34;\u0026#39;; ","date":"13 October 2023","permalink":"/posts/language/code-generation/asyncapi-code-generator/","section":"博客","summary":"Background # IIoT（工业物联网）架构通常是分布式和异步的，通信由事件驱动，如消息的发布（和相应的订阅）。这些异步架构提高了可扩展性和对变化的耐受性，但也引发了互操作性问题，因为架构各元素之间对消息内部结构及其分类（主题）的明确知识被稀释了。","title":"A Modeling Editor and Code Generator for message-driven architectures with AsyncAPI"},{"content":"","date":"13 October 2023","permalink":"/tags/asyncapi/","section":"Tags","summary":"","title":"Asyncapi"},{"content":"","date":"13 October 2023","permalink":"/tags/openapi/","section":"Tags","summary":"","title":"Openapi"},{"content":" OpenAPI Generator 可根据 OpenAPI yaml 规范生成代码，并支持多种语言。\n如何使用 OpenAPI # 本节介绍如何创建一个基本的 OpenAPI yaml 规范，并用它为 Spring Boot 应用程序生成服务器端代码。\nCreate OpenAPI spec # 首先要做的是为您的应用程序设计 OpenAPI 规范。您将设计一个客户 API。该 API 允许您创建一个客户，并根据其 ID 检索该客户。现实生活中的应用程序接口会更加复杂，但我们还是保持简单。\n使用 Swagger 编辑器 是设计 API 的简便方法。它会立即反馈您的规范是否有错误，并即时生成 Swagger 文档。\nOpenAPI 规范的 header 包含一些有关 API 的元数据，如标题、版本、API 运行的服务器等。标签可用于对资源进行分组，从而为您提供更多概览。\nopenapi: \u0026#34;3.0.2\u0026#34; info: title: API Customer version: \u0026#34;1.0\u0026#34; servers: - url: https://localhost:8080 tags: - name: Customer description: Customer specific data. paths 部分包含资源规范。您定义的第一个资源是创建 Customer 的资源，将通过包含 JSON 主体的 POST 方式创建。生成器将使用 operationId 为该资源创建方法名称。为简单起见，只考虑成功响应。模式指的是 JSON 主体，将在本节后面介绍。\n/customer: post: tags: - Customer summary: Create Customer operationId: createCustomer requestBody: content: application/json: schema: $ref: \u0026#39;#/components/schemas/Customer\u0026#39; responses: \u0026#39;200\u0026#39;: description: OK content: \u0026#39;application/json\u0026#39;: schema: $ref: \u0026#39;#/components/schemas/CustomerFullData\u0026#39; 第二个资源允许您检索客户。该资源也需要一个包含要检索的 customerId 的路径参数。如果 ID 不存在，将返回 NOT FOUND 的响应。\n/customer/{customerId}: get: tags: - Customer summary: Retrieve Customer operationId: getCustomer parameters: - name: customerId in: path required: true schema: type: integer format: int64 responses: \u0026#39;200\u0026#39;: description: OK content: \u0026#39;application/json\u0026#39;: schema: $ref: \u0026#39;#/components/schemas/CustomerFullData\u0026#39; \u0026#39;404\u0026#39;: description: NOT FOUND 最后，在组件部分，定义了使用的模式。除了 ID 之外，Customer 模式和 CustomerFullData 模式共享所有属性。为了提高可维护性，可以使用 allOf 属性。\ncomponents: schemas: Customer: type: object properties: firstName: type: string description: First name of the customer lastName: type: string description: Last name of the customer CustomerFullData: allOf: - $ref: \u0026#39;#/components/schemas/Customer\u0026#39; - type: object properties: customerId: type: integer description: The ID of the customer format: int64 description: Full data of the customer. 该应用程序的 OpenAPI 规范现已完成。\nCreate Spring Boot Application # 要创建 Spring Boot 应用程序，请访问 start.spring.io，选择最新稳定的 Spring Boot 版本、Java 17 并添加 Spring Web 依赖关系。下载生成的项目并将其打开到您喜欢的集成开发环境中。在 src/main/resources 目录中添加 OpenAPI 规范，名称为 customer.yml。\n您将使用 Open API Generator Maven 插件，因此请将该插件添加到 pom 文件的构建部分。由于您使用的是 Spring Boot 应用程序，因此使用 spring 作为 generatorName，并使用 inputSpec 属性设置 customer.yml 文件的路径。\n\u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.openapitools\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;openapi-generator-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.3.0\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;generate\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;inputSpec\u0026gt;${project.basedir}/src/main/resources/customer.yml\u0026lt;/inputSpec\u0026gt; \u0026lt;generatorName\u0026gt;spring\u0026lt;/generatorName\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; 执行以下命令生成代码：\n$ mvn clean compile 编译失败，出现以下错误：\npackage io.swagger.annotations does not exist package io.swagger.annotations does not exist package org.openapitools.jackson.nullable does not exist cannot find symbol 为了解决这些问题，需要在 pom 文件中添加以下依赖项：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.swagger\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;swagger-annotations\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.6.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;javax.validation\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;validation-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.0.1.Final\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.openapitools\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jackson-databind-nullable\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.2.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 再次运行编译会出现以下错误：\npackage springfox.documentation.builders does not exist package springfox.documentation.builders does not exist package springfox.documentation.service does not exist package springfox.documentation.service does not exist package springfox.documentation.spi does not exist package springfox.documentation.spring.web.paths does not exist package springfox.documentation.spring.web.paths does not exist package springfox.documentation.spring.web.plugins does not exist package springfox.documentation.swagger2.annotations does not exist 在 pom 文件中添加以下依赖项可以解决这些错误：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.springfox\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;springfox-swagger-ui\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.6.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.springfox\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;springfox-swagger2\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.6.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 仔细看看生成了什么。导航至 target/generated-sources/open-api 目录，在该目录中可以找到生成的文件。以下目录包含生成的文件：\nsrc/main/org/openapitools/api : 是 Spring 控制器的一个接口，也是一个实现 src/main/org/openapitools/configuration : 是 Swagger 文档的控制器 src/main/org/openapitools/model : 基于 API 规范的 API 模型 src/main/org/openapitools : OpenAPI2SpringBoot 是一个 SpringBootApplication 当你想运行 Spring Boot 应用程序时，你会遇到一个错误，因为 Spring Boot 无法确定它需要运行哪个 SpringBootApplication :\n$ mvn spring-boot:run 由此产生的错误是 :\nUnable to find a single main class from the following candidates [org.openapitools.OpenAPI2SpringBoot, com.mydeveloperplanet.myopenapiplanet.MyOpenApiPlanetApplication 默认情况下会生成大量代码，也许比你需要的还要多。下一段将介绍如何调整配置。\nConfigure OpenAPI plugin # 除了 OpenAPI 插件的 Maven 部分记录的所有选项外，还有许多额外的选项可在 OpenAPI 插件配置部分的 configOptions 部分进行配置。可通过在配置部分添加 configHelp 属性来显示可用选项。\n\u0026lt;configuration\u0026gt; \u0026lt;inputSpec\u0026gt;${project.basedir}/src/main/resources/customer.yml\u0026lt;/inputSpec\u0026gt; \u0026lt;generatorName\u0026gt;spring\u0026lt;/generatorName\u0026gt; \u0026lt;configHelp\u0026gt;true\u0026lt;/configHelp\u0026gt; \u0026lt;/configuration\u0026gt; 在此列表中，您将使用 interfaceOnly 属性，它只会为控制器和 API 模型生成接口。\n\u0026lt;configuration\u0026gt; ... \u0026lt;configOptions\u0026gt; \u0026lt;interfaceOnly\u0026gt;true\u0026lt;/interfaceOnly\u0026gt; \u0026lt;/configOptions\u0026gt; \u0026lt;/configuration\u0026gt; 此时，还可以删除之前添加的 Springfox 依赖项。这些都不再需要了。\n从生成的代码中还可以看到，代码是在 org.openapitools 包中生成的。你可能希望这是你自己的软件包名称，这可以通过一些基本属性来配置。通过 packageName 属性，您可以设置默认的软件包名称。不过，还必须设置 apiPackage 和 modelPackage 属性，否则这些属性仍将在 org.openapitools 包中生成。在配置部分添加以下内容。\n\u0026lt;configuration\u0026gt; .... \u0026lt;packageName\u0026gt;com.mydeveloperplanet.myopenapiplanet\u0026lt;/packageName\u0026gt; \u0026lt;apiPackage\u0026gt;com.mydeveloperplanet.myopenapiplanet.api\u0026lt;/apiPackage\u0026gt; \u0026lt;modelPackage\u0026gt;com.mydeveloperplanet.myopenapiplanet.model\u0026lt;/modelPackage\u0026gt; .... \u0026lt;/configuration\u0026gt; 生成的控制器界面如下：\n@javax.annotation.Generated(value = \u0026#34;org.openapitools.codegen.languages.SpringCodegen\u0026#34;, date = \u0026#34;2022-01-15T12:51:43.809971036+01:00[Europe/Amsterdam]\u0026#34;) @Validated @Api(value = \u0026#34;customer\u0026#34;, description = \u0026#34;the customer API\u0026#34;) public interface CustomerApi { default Optional\u0026lt;NativeWebRequest\u0026gt; getRequest() { return Optional.empty(); } /** * POST /customer : Create Customer * * @param customer (optional) * @return OK (status code 200) */ @ApiOperation(value = \u0026#34;Create Customer\u0026#34;, nickname = \u0026#34;createCustomer\u0026#34;, notes = \u0026#34;\u0026#34;, response = CustomerFullData.class, tags={ \u0026#34;Customer\u0026#34;, }) @ApiResponses(value = { @ApiResponse(code = 200, message = \u0026#34;OK\u0026#34;, response = CustomerFullData.class) }) @RequestMapping( method = RequestMethod.POST, value = \u0026#34;/customer\u0026#34;, produces = { \u0026#34;application/json\u0026#34; }, consumes = { \u0026#34;application/json\u0026#34; } ) default ResponseEntity\u0026lt;CustomerFullData\u0026gt; createCustomer(@ApiParam(value = \u0026#34;\u0026#34;) @Valid @RequestBody(required = false) Customer customer) { getRequest().ifPresent(request -\u0026gt; { for (MediaType mediaType: MediaType.parseMediaTypes(request.getHeader(\u0026#34;Accept\u0026#34;))) { if (mediaType.isCompatibleWith(MediaType.valueOf(\u0026#34;application/json\u0026#34;))) { String exampleString = \u0026#34;null\u0026#34;; ApiUtil.setExampleResponse(request, \u0026#34;application/json\u0026#34;, exampleString); break; } } }); return new ResponseEntity\u0026lt;\u0026gt;(HttpStatus.NOT_IMPLEMENTED); } ... Use Generated Code # 在应用程序中，首先要在包 domain 中创建一个 Customer 类。\npublic class Customer { private Long customerId; private String firstName; private String lastName; // Getters and setters } 创建一个 CustomerController，实现生成的 CustomerApi 接口。\n创建 Customer 是一种基本的实现方式，您可以将 Customer 添加到 HashMap 中：计算索引是键，域客户对象是值。在实际应用中，您将把客户保存到数据库中。\n检索客户时，首先要检查所请求的 ID 是否存在于 HashMap 中。找到 ID 后，Customer 域对象将转换为 Customer API 模型对象并返回给请求者。如果未找到 ID，则会返回 NOT FOUND 响应。\n@RestController public class CustomerController implements CustomerApi { private final HashMap\u0026lt;Long, com.mydeveloperplanet.myopenapiplanet.domain.Customer\u0026gt; customers = new HashMap\u0026lt;\u0026gt;(); private Long index = 0L; @Override public ResponseEntity\u0026lt;CustomerFullData\u0026gt; createCustomer(Customer apiCustomer) { com.mydeveloperplanet.myopenapiplanet.domain.Customer customer = new com.mydeveloperplanet.myopenapiplanet.domain.Customer(); customer.setCustomerId(index); customer.setFirstName(apiCustomer.getFirstName()); customer.setLastName(apiCustomer.getLastName()); customers.put(index, customer); index++; return ResponseEntity.ok(domainToApi(customer)); } @Override public ResponseEntity\u0026lt;CustomerFullData\u0026gt; getCustomer(Long customerId) { if (customers.containsKey(customerId)) { return ResponseEntity.ok(domainToApi(customers.get(customerId))); } else { return new ResponseEntity\u0026lt;\u0026gt;(HttpStatus.NOT_FOUND); } } private CustomerFullData domainToApi(com.mydeveloperplanet.myopenapiplanet.domain.Customer customer) { CustomerFullData cfd = new CustomerFullData(); cfd.setCustomerId(customer.getCustomerId()); cfd.setFirstName(customer.getFirstName()); cfd.setLastName(customer.getLastName()); return cfd; } } 运行 Spring Boot 应用程序：\n$ mvn spring-boot:run 添加 Consumer，并查找\n$ curl -i -X \u0026#39;POST\u0026#39; \\ \u0026gt; \u0026#39;http://localhost:8080/customer\u0026#39; \\ \u0026gt; -H \u0026#39;accept: application/json\u0026#39; \\ \u0026gt; -H \u0026#39;Content-Type: application/json\u0026#39; \\ \u0026gt; -d \u0026#39;{ \u0026gt; \u0026#34;firstName\u0026#34;: \u0026#34;Foo\u0026#34;, \u0026gt; \u0026#34;lastName\u0026#34;: \u0026#34;Bar\u0026#34; \u0026gt; }\u0026#39; HTTP/1.1 200 Content-Type: application/json Transfer-Encoding: chunked Date: Sat, 15 Jan 2022 11:42:47 GMT {\u0026#34;firstName\u0026#34;:\u0026#34;Foo\u0026#34;,\u0026#34;lastName\u0026#34;:\u0026#34;Bar\u0026#34;,\u0026#34;customerId\u0026#34;:0} $ curl -i -X \u0026#39;POST\u0026#39; \\ \u0026gt; \u0026#39;http://localhost:8080/customer\u0026#39; \\ \u0026gt; -H \u0026#39;accept: application/json\u0026#39; \\ \u0026gt; -H \u0026#39;Content-Type: application/json\u0026#39; \\ \u0026gt; -d \u0026#39;{ \u0026gt; \u0026#34;firstName\u0026#34;: \u0026#34;John\u0026#34;, \u0026gt; \u0026#34;lastName\u0026#34;: \u0026#34;Doe\u0026#34; \u0026gt; }\u0026#39; HTTP/1.1 200 Content-Type: application/json Transfer-Encoding: chunked Date: Sat, 15 Jan 2022 11:43:11 GMT {\u0026#34;firstName\u0026#34;:\u0026#34;John\u0026#34;,\u0026#34;lastName\u0026#34;:\u0026#34;Doe\u0026#34;,\u0026#34;customerId\u0026#34;:1} $ curl -i http://localhost:8080/customer/1 HTTP/1.1 200 Content-Type: application/json Transfer-Encoding: chunked Date: Sat, 15 Jan 2022 11:45:21 GMT {\u0026#34;firstName\u0026#34;:\u0026#34;John\u0026#34;,\u0026#34;lastName\u0026#34;:\u0026#34;Doe\u0026#34;,\u0026#34;customerId\u0026#34;:1} $ curl -i http://localhost:8080/customer/2 HTTP/1.1 404 Content-Length: 0 Date: Sat, 15 Jan 2022 11:46:18 GMT Add OpenAPI Documentation # https://springdoc.org/ \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springdoc\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;springdoc-openapi-ui\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.5.12\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 在浏览器中导航至 http://localhost:8080/swagger-ui.html，即可显示 OpenAPI 文档，并可在此处下载 OpenAPI yaml 规范。\n当你仔细查看文档时，会发现它与 Swagger 编辑器中显示的文档有所不同。springdoc 依赖项默认会从源代码生成文档，并使用生成的文档。如何配置 springdoc 以使用 customer.yml 文件？\n首先，您需要将 customer.yml 文件移至 src/main/resources/static/customer.yml 目录。这也意味着你需要更改 pom 中的 Open API 生成器配置。\n\u0026lt;configuration\u0026gt; \u0026lt;inputSpec\u0026gt;${project.basedir}/src/main/resources/static/customer.yml\u0026lt;/inputSpec\u0026gt; ... \u0026lt;/configuration\u0026gt; 在 application.properties 文件中添加以下属性\nspringdoc.swagger-ui.url=/customer.yml URL 现在显示的是您创建的 customer.yml 中定义的 API\nResources # 官方 OpenAPI Specification v3.1.0 repo https://tools.openapis.org/categories/code-generators.html Blogs Open API Server Implementation Using OpenAPI Generator Generate Server Code Using OpenAPI Generator ","date":"13 October 2023","permalink":"/posts/language/code-generation/openapi-code-generator/","section":"博客","summary":"OpenAPI Generator 可根据 OpenAPI yaml 规范生成代码，并支持多种语言。","title":"Openapi Code Generator"},{"content":"","date":"13 October 2023","permalink":"/tags/restful/","section":"Tags","summary":"","title":"Restful"},{"content":"REST 全称是 Representational State Transfer（表现层状态转化），更具体的全称是 Resource Representational State Transfer（资源表现层状态转化），具体可以见 Roy Thomas Fielding 的博士论文 https://www.ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm 这一章。\nREST 指的是一组架构约束条件和原则：\n为设计一个功能强、性能好、适宜通信的 web 应用 如果一个架构符合 REST 的约束条件和原则，我们就称它为 RESTful 结构 Resources # restful petclinic tutorial docter paper bilibili-软件体系结构-2022.7-REST Create REST APIs with JAX-RS 核心概念 # 资源（Resources） 表现层（Representation） 状态转化（State Transfer） 资源 # 网络上的一个实体，或者说是网络上的一个具体信息，任何事物，只要有被引用到的必要，它就是一个资源。\n一段文本，一张图片，一首歌曲 数据库中的一行数据 一个手机号码，某用户的个人信息 一种服务 资源标识 # 要让一个资源可以被识别，需要有个唯一标识，在Web中这个唯一标识就是URI（Uniform Resource Identifier）。例如：\nhttps://www.ex.com/software/releases/latest.tar.gz https://www.ex.com/map/roads/USA/CA/17_mile_drive https://www.ex.com/search/cs578 URI 设计原则\n易读： https://www.oschina.net/news/38119/oschina-translate-reward-plan 表达资源的层级关系： https://github.com/git/git/commit/e3ae056f87e1d675913d08/orders/2012/10 表示资源的同级关系： /git/block-sha1/sha1.h/compare/e3af72cda056f87e;bd63e61bdf38eb264 表达资源的过滤： https://github.com/git/git/pulls?q=is%3Aclosed 统一资源接口\nRESTful 架构应该遵循统一接口原则，统一接口包含了一组受限的预定义的操作，不论什么样的资源，都是通过使用相同的接口进行资源的访问。接口应该使用标准的 HTTP 方法如 GET，PUT 和 POST，并遵循这些方法的语义 如果按照HTTP方法的语义来暴露资源，那么接口将会拥有安全性和幂等性的特性 GET和HEAD请求是安全的，无论请求多少次，都不改变服务器状态 GET、HEAD、PUT和DELETE请求是幂等的，无论对资源操作多少次，结果总是一样的，后面的请求并不会产生比第一次更多的影响 GET # 获取表示，变更时获取表示（缓存）。安全且幂等。\n200: OK，表示已在响应中发出 204: 无内容，资源有空表示 301: Moved Permanently，资源的URI已被更新 303: See Other，其他（如，负载均衡） 304: not modified，资源未更改（缓存） 400: bad request，指代坏请求（如，参数错误） 404: not found，资源不存在 406: not acceptable，服务端不支持所需表示 500: internal server error，通用错误响应 503: Service Unavailable，服务端当前无法处理请求 POST # 使用服务端管理的（自动产生）的实例号创建资源，或创建子资源，部分更新资源，如果没有被修改，则不过更新资源（乐观锁）。不安全且不幂等。\n406: not acceptable，服务端不支持所需表示 409: conflict，通用冲突 412: Precondition Failed，前置条件失败（如执行条件更新时的冲突） 415: unsupported media type，接受到的表示不受支持 500: internal server error，通用错误响应 503: Service Unavailable，服务当前无法处理请求 PUT # 用客户端管理的实例号创建一个资源，通过替换的方式更新资源，如果未被修改，则更新资源（乐观锁）。不安全但幂等。\n200: OK，如果已存在资源被更改 201: created，如果新资源被创建 301: Moved Permanently，资源的URI已更改 303: See Other，其他（如，负载均衡） 400: bad request，指代坏请求 404: not found，资源不存在 DELETE # 删除资源。不安全但幂等。\n200: OK，资源已被删除 301: Moved Permanently，资源的URI已更改 303: See Other，其他，如负载均衡 400: bad request，指代坏请求 404: not found，资源不存在 409: conflict，通用冲突 500: internal server error，通用错误响应 503: Service Unavailable，服务端当前无法处理请求 指导意义 # 统一资源接口要求使用标准的HTTP方法对资源进行操作，所以URI只应该来表示资源的名称，而不应该包括资源的操作。通俗来说，URI不应该使用动作来描述。例如：\nPOST /getUser?id=1 $\\rightarrow$ GET /Uset/1 GET /newUser $\\rightarrow$ POST /User GET /updateUser $\\rightarrow$ PUT /User/1 GET /deleteUser?id=2 $\\rightarrow$ DELETE /User/2 表现 (Representation) # \u0026ldquo;资源\u0026quot;是一种信息实体，它可以有多种外在表现形式。我们把\u0026quot;资源\u0026quot;具体呈现出来的形式，叫做它的\u0026quot;表现层\u0026rdquo;（Representation）\n文本可以用txt格式表现，也可以用HTML格式、XMIL格式、JSON格式表现，甚至可以采用二进制格式 图片可以用JPG格式表现，也可以用PNG格式表示 资源表述 # URI只代表资源的实体，不代表它的形式。严格地说，有些网址最后的 .html 后缀名是不必要的，因为这个后缀名表示格式，属于 \u0026ldquo;表现层\u0026rdquo; 范畴，而URI应该只代表 \u0026ldquo;资源\u0026rdquo; 的位置。\n资源的表述包括数据和描述数据的元数据，例如，HTTP头 \u0026ldquo;Content-Type\u0026rdquo; 就是这样一个元数据属性\n客户端可以通过 Accept 头请求一种特定格式的表述，服务端则通过 Content-Type 告诉客户端资源的表述形式\n支持的表达\n~ » http get https://api.github.com/orgs/github \u0026#39;Accept: application/json\u0026#39; HTTP/1.1 200 OK Content-Type: application/json; charset=utf-8 { \u0026#34;archived_at\u0026#34;: null, \u0026#34;avatar_url\u0026#34;: \u0026#34;https://avatars.githubusercontent.com/u/9919?v=4\u0026#34;, \u0026#34;blog\u0026#34;: \u0026#34;https://github.com/about\u0026#34;, \u0026#34;company\u0026#34;: null, \u0026#34;created_at\u0026#34;: \u0026#34;2008-05-11T04:37:31Z\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;How people build software.\u0026#34;, \u0026#34;email\u0026#34;: null, \u0026#34;events_url\u0026#34;: \u0026#34;https://api.github.com/orgs/github/events\u0026#34;, \u0026#34;followers\u0026#34;: 29993, \u0026#34;following\u0026#34;: 0, \u0026#34;has_organization_projects\u0026#34;: true, \u0026#34;has_repository_projects\u0026#34;: true, \u0026#34;hooks_url\u0026#34;: \u0026#34;https://api.github.com/orgs/github/hooks\u0026#34;, \u0026#34;html_url\u0026#34;: \u0026#34;https://github.com/github\u0026#34;, \u0026#34;id\u0026#34;: 9919, \u0026#34;is_verified\u0026#34;: true, \u0026#34;issues_url\u0026#34;: \u0026#34;https://api.github.com/orgs/github/issues\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;San Francisco, CA\u0026#34;, \u0026#34;login\u0026#34;: \u0026#34;github\u0026#34;, \u0026#34;members_url\u0026#34;: \u0026#34;https://api.github.com/orgs/github/members{/member}\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;GitHub\u0026#34;, \u0026#34;node_id\u0026#34;: \u0026#34;MDEyOk9yZ2FuaXphdGlvbjk5MTk=\u0026#34;, \u0026#34;public_gists\u0026#34;: 0, \u0026#34;public_members_url\u0026#34;: \u0026#34;https://api.github.com/orgs/github/public_members{/member}\u0026#34;, \u0026#34;public_repos\u0026#34;: 477, \u0026#34;repos_url\u0026#34;: \u0026#34;https://api.github.com/orgs/github/repos\u0026#34;, \u0026#34;twitter_username\u0026#34;: null, \u0026#34;type\u0026#34;: \u0026#34;Organization\u0026#34;, \u0026#34;updated_at\u0026#34;: \u0026#34;2022-11-29T19:44:55Z\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://api.github.com/orgs/github\u0026#34; } 不支持的表达\n~ » http get https://api.github.com/orgs/github \u0026#39;Accept: text/xml\u0026#39; HTTP/1.1 415 Unsupported Media Type Content-Type: application/json; charset=utf-8 { \u0026#34;documentation_url\u0026#34;: \u0026#34;https://docs.github.com/v3/media\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Unsupported \u0026#39;Accept\u0026#39; header: \u0026#39;text/xml\u0026#39;. Must accept \u0026#39;application/json\u0026#39;.\u0026#34; } 资源链接 # 当你浏览Web网页时，从一个连接跳到一个页面，再从另一个连接跳到另外一冬页面，就是利用了超媒体的概念：把一个个把资源链接起来。\n同样，我们在表述格式里边加入链接来引导客户端：\n在Link头告诉客户端怎么访问下一页和最后一页的记录； 在响应体里用url来链接项目所有者和项目地址 ~ » http -h get https://api.github.com/orgs/github/repos HTTP/1.1 200 OK Content-Type: application/json; charset=utf-8 Link: \u0026lt;https://api.github.com/organizations/9919/repos?page=2\u0026gt;; rel=\u0026#34;next\u0026#34;, \u0026lt;https://api.github.com/organizations/9919/repos?page=16\u0026gt;; rel=\u0026#34;last\u0026#34; [ { \u0026#34;id\u0026#34;: 3222, \u0026#34;node_id\u0026#34;: \u0026#34;MDEwOlJlcG9zaXRvcnkzMjIy\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;media\u0026#34;, \u0026#34;full_name\u0026#34;: \u0026#34;github/media\u0026#34;, \u0026#34;private\u0026#34;: false, \u0026#34;owner\u0026#34;: { \u0026#34;login\u0026#34;: \u0026#34;github\u0026#34;, \u0026#34;id\u0026#34;: 9919, \u0026#34;node_id\u0026#34;: \u0026#34;MDEyOk9yZ2FuaXphdGlvbjk5MTk=\u0026#34;, \u0026#34;avatar_url\u0026#34;: \u0026#34;https://avatars.githubusercontent.com/u/9919?v=4\u0026#34;, \u0026#34;gravatar_id\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://api.github.com/users/github\u0026#34;, \u0026#34;html_url\u0026#34;: \u0026#34;https://github.com/github\u0026#34;, \u0026#34;followers_url\u0026#34;: \u0026#34;https://api.github.com/users/github/followers\u0026#34;, \u0026#34;following_url\u0026#34;: \u0026#34;https://api.github.com/users/github/following{/other_user}\u0026#34;, \u0026#34;gists_url\u0026#34;: \u0026#34;https://api.github.com/users/github/gists{/gist_id}\u0026#34;, \u0026#34;starred_url\u0026#34;: \u0026#34;https://api.github.com/users/github/starred{/owner}{/repo}\u0026#34;, \u0026#34;subscriptions_url\u0026#34;: \u0026#34;https://api.github.com/users/github/subscriptions\u0026#34;, \u0026#34;organizations_url\u0026#34;: \u0026#34;https://api.github.com/users/github/orgs\u0026#34;, \u0026#34;repos_url\u0026#34;: \u0026#34;https://api.github.com/users/github/repos\u0026#34;, \u0026#34;events_url\u0026#34;: \u0026#34;https://api.github.com/users/github/events{/privacy}\u0026#34;, \u0026#34;received_events_url\u0026#34;: \u0026#34;https://api.github.com/users/github/received_events\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Organization\u0026#34;, \u0026#34;site_admin\u0026#34;: false }, \u0026#34;html_url\u0026#34;: \u0026#34;https://github.com/github/media\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Media files for use in your GitHub integration projects\u0026#34;, \u0026#34;fork\u0026#34;: false, \u0026#34;url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media\u0026#34;, \u0026#34;forks_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/forks\u0026#34;, \u0026#34;keys_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/keys{/key_id}\u0026#34;, \u0026#34;collaborators_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/collaborators{/collaborator}\u0026#34;, \u0026#34;teams_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/teams\u0026#34;, \u0026#34;hooks_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/hooks\u0026#34;, \u0026#34;issue_events_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/issues/events{/number}\u0026#34;, \u0026#34;events_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/events\u0026#34;, \u0026#34;assignees_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/assignees{/user}\u0026#34;, \u0026#34;branches_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/branches{/branch}\u0026#34;, \u0026#34;tags_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/tags\u0026#34;, \u0026#34;blobs_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/git/blobs{/sha}\u0026#34;, \u0026#34;git_tags_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/git/tags{/sha}\u0026#34;, \u0026#34;git_refs_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/git/refs{/sha}\u0026#34;, \u0026#34;trees_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/git/trees{/sha}\u0026#34;, \u0026#34;statuses_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/statuses/{sha}\u0026#34;, \u0026#34;languages_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/languages\u0026#34;, \u0026#34;stargazers_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/stargazers\u0026#34;, \u0026#34;contributors_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/contributors\u0026#34;, \u0026#34;subscribers_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/subscribers\u0026#34;, \u0026#34;subscription_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/subscription\u0026#34;, \u0026#34;commits_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/commits{/sha}\u0026#34;, \u0026#34;git_commits_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/git/commits{/sha}\u0026#34;, \u0026#34;comments_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/comments{/number}\u0026#34;, \u0026#34;issue_comment_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/issues/comments{/number}\u0026#34;, \u0026#34;contents_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/contents/{+path}\u0026#34;, \u0026#34;compare_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/compare/{base}...{head}\u0026#34;, \u0026#34;merges_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/merges\u0026#34;, \u0026#34;archive_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/{archive_format}{/ref}\u0026#34;, \u0026#34;downloads_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/downloads\u0026#34;, \u0026#34;issues_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/issues{/number}\u0026#34;, \u0026#34;pulls_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/pulls{/number}\u0026#34;, \u0026#34;milestones_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/milestones{/number}\u0026#34;, \u0026#34;notifications_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/notifications{?since,all,participating}\u0026#34;, \u0026#34;labels_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/labels{/name}\u0026#34;, \u0026#34;releases_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/releases{/id}\u0026#34;, \u0026#34;deployments_url\u0026#34;: \u0026#34;https://api.github.com/repos/github/media/deployments\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2008-03-09T22:43:49Z\u0026#34;, \u0026#34;updated_at\u0026#34;: \u0026#34;2023-09-23T01:50:37Z\u0026#34;, \u0026#34;pushed_at\u0026#34;: \u0026#34;2015-02-27T17:31:20Z\u0026#34;, \u0026#34;git_url\u0026#34;: \u0026#34;git://github.com/github/media.git\u0026#34;, \u0026#34;ssh_url\u0026#34;: \u0026#34;git@github.com:github/media.git\u0026#34;, \u0026#34;clone_url\u0026#34;: \u0026#34;https://github.com/github/media.git\u0026#34;, \u0026#34;svn_url\u0026#34;: \u0026#34;https://github.com/github/media\u0026#34;, \u0026#34;homepage\u0026#34;: \u0026#34;https://github.com/logos\u0026#34;, \u0026#34;size\u0026#34;: 4484, \u0026#34;stargazers_count\u0026#34;: 293, \u0026#34;watchers_count\u0026#34;: 293, \u0026#34;language\u0026#34;: null, \u0026#34;has_issues\u0026#34;: false, \u0026#34;has_projects\u0026#34;: true, \u0026#34;has_downloads\u0026#34;: true, \u0026#34;has_wiki\u0026#34;: false, \u0026#34;has_pages\u0026#34;: false, \u0026#34;has_discussions\u0026#34;: false, \u0026#34;forks_count\u0026#34;: 69, \u0026#34;mirror_url\u0026#34;: null, \u0026#34;archived\u0026#34;: true, \u0026#34;disabled\u0026#34;: false, \u0026#34;open_issues_count\u0026#34;: 0, \u0026#34;license\u0026#34;: null, \u0026#34;allow_forking\u0026#34;: true, \u0026#34;is_template\u0026#34;: false, \u0026#34;web_commit_signoff_required\u0026#34;: false, \u0026#34;topics\u0026#34;: [], \u0026#34;visibility\u0026#34;: \u0026#34;public\u0026#34;, \u0026#34;forks\u0026#34;: 69, \u0026#34;open_issues\u0026#34;: 0, \u0026#34;watchers\u0026#34;: 293, \u0026#34;default_branch\u0026#34;: \u0026#34;master\u0026#34;, \u0026#34;permissions\u0026#34;: { \u0026#34;admin\u0026#34;: false, \u0026#34;maintain\u0026#34;: false, \u0026#34;push\u0026#34;: false, \u0026#34;triage\u0026#34;: false, \u0026#34;pull\u0026#34;: true } }, ... ] 状态转移（State Transfer） # 状态应该区分应用状态和资源状态，\n客户端负责维护应用状态， 而服务端维护资源状态。 客户端与服务端的交互必须是无状态的，并在每一次请求中包含处理该请求所需的一切信息。服务端不需要在请求间保留应用状态，只有在接受到实际请求的时候，服务端才会关注应用状态。这种无状态通信原则，使得服务端和中介能够理解独立的请求和响应。在多次请求中，同一客户端也不再需要依赖于同一服务器，方便实现高可扩展和高可用性的服务端。\n客户端应用状态在服务端提供的超媒体的指引下发生变迁。服务端通过超媒体告诉客户端当前状态有哪些后续状态可以进入。\n","date":"13 October 2023","permalink":"/posts/reviews/network/restful/","section":"博客","summary":"REST 全称是 \u003cstrong\u003eRe\u003c/strong\u003epresentational \u003cstrong\u003eS\u003c/strong\u003etate \u003cstrong\u003eT\u003c/strong\u003eransfer（表现层状态转化），更具体的全称是 Resource Representational State Transfer（资源表现层状态转化），具体可以见 Roy Thomas Fielding 的博士论文","title":"Restful API Tutorial"},{"content":"","date":"13 October 2023","permalink":"/tags/cross-entropy/","section":"Tags","summary":"","title":"Cross Entropy"},{"content":"案例驱动 # 通过几个简单的例子来解释和总结什么是交叉熵（Cross Entropy）以及机器学习分类问题中为什么使用交叉熵。\n第一个例子 # 假设随机从一个口袋里取硬币，口袋里有一个蓝色的，一个红色的，一个绿色的，一个橘色的。取出一个硬币之后，每次问一个问题，然后做出判断，目标是，问最少的问题，得到正确答案。其中一个最好的设计问题的策略如下：\n每一个硬币有 $\\frac{1}{4}$ 的概率被选中，$\\frac{1}{4}机率 * 2道题目 * 4颗球 = 2$，平均需要问两道题目才能找出不同颜色的球，也就是说期望值为 $2$，就是熵（entropy）。\n第二个例子 # 例子变了，变成了袋子中 $\\frac{1}{8}$ 的硬币是绿色的，$\\frac{1}{8}$ 的是橘色的，$\\frac{1}{4}$ 是红色的，$\\frac{1}{2}$ 是蓝色的，这时最优的问题的策略如下:\n$\\frac{1}{2}$ 的概率是蓝色，只需要 $1$ 个问题就可以知道是或者不是，$\\frac{1}{4}$ 的概率是红色，需要2个问题，按照这个逻辑，猜中硬币需要的问题的期望是\n$$ \\frac{1}{2}*1+\\frac{1}{4}*2+\\frac{1}{8}*3+\\frac{1}{8}*3=1.75 $$\n第三个例子 # 假设袋子中全部是蓝色的硬币，那么这时候需要 $0$ 个问题就可以猜到硬币，即 $\\log_{2}{1}=0$。 需要注意的是，只有当知道袋子中全部是蓝色的硬币的时候需要的问题是 $0$ 个。\n总结上面的例子，假设一种硬币出现的概率是 $p$，那么猜中该硬币的所需要的问题数是 $\\log_2{\\frac1{P_i}}$。例如 $p=\\frac{1}{4}，\\log_{2}{4}$ 。\n在这个问题中，问题个数的期望是\n$$ \\sum_i{p_i}*log_2{\\frac{1}{p_i}} $$\n这个式子就是熵的表达式 。简单来说，其意义就是在最优化策略下，猜到颜色所需要的问题的个数。熵代表的是随机变量或整个系统的不确定性，熵越大，随机变量或系统的不确定性就越大。\n现在已经了解了熵是什么，那么，下面解释交叉熵（cross entropy） 的含义.对于第二个例子，如果仍然使用第一个例子中的策略，如下图:\n$\\frac{1}{8}$ 的概率，硬币是橘色，需要两个问题，$\\frac{1}{2}$ 的概率是蓝色，仍然需要两个问题，也就是说，认为小球的分布为 $(\\frac{1}{4},\\frac{1}{4},\\frac{1}{4},\\frac{1}{4})$ ，这个分布就是非真实分布。平均来说，需要的问题数是 $\\frac{1}{8}*2+\\frac{1}{8}*2+\\frac{1}{4}*2+\\frac{1}{2}*2=2$ 。\n因此，在例子二中使用例子一的策略是一个比较差的策略。其中 $2$ 是这个方案中的交叉熵，而最优方案的交叉熵是 $1.75$。\n给定一个策略，交叉熵就是在该策略下猜中颜色所需要的问题的期望值。更普遍的说，交叉熵用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出成本的大小。交叉的字面意思在于：真实分布与非真实分布的交叉。给定一个方案，越优的策略，最终的交叉熵越低。具有最低的交叉熵的策略就是最优化策略，也就是上面定义的熵。因此，在机器学习中，我们需要最小化交叉熵。\n数学上来讲 # 其中，$p$ 是真正的概率，例如例子二中，橘色和绿色是 $\\frac{1}{8}$，红色是 $\\frac{1}{4}$，蓝色是 $\\frac{1}{2}$。$\\hat p$ 是错误地假设了的概率，例如，在例子二中我们错误地假设了所有的颜色的概率都是 $\\frac{1}{4}$。$p$ 和 $\\hat p$ 可能有点容易混淆。记住一点，$log$ 是用来计算在你的策略下猜中所需要的问题数，因此，$log$ 中需要的是你的预测概率 $\\hat p$ 。在决策树中，如果建立的树不是最优的，结果就是对于输出的概率分布的假设是错误地，导致的直接结果就是交叉熵很高。交叉熵不仅仅应用在决策树中，在其他的分类问题中也有应用。\n分类问题 # 在二分类问题中，标签 $y$ 是 $1$ 的似然是对于标签 $y$ 的预测 $\\hat y$ ，同样的，标签是 $0$ 的似然是 $1-\\hat y$ 。我们需要最大化似然函数，而且，由于二分类问题的特殊性，根据伯努力分布 (Bernoulli distribution)，可以把似然函数写成\n当 $y=1$ 的时候，第二项为 $1$，因此，优化的是 $\\hat y$ 当 $y=0$ 的时候，第一项为 $1$，优化的是 $1-\\hat y$ 。 对上面的似然函数取对数，结果是最大化似然函数，就是对上面表达式取负然后最小化。也是交叉熵的表达式。\n交叉熵有时候也被称为对数损失函数。注意与上边例子区别是多了个负号，上边例子是消除不确定性需要付出的成本；而现在这个加了负号的交叉熵，则是最终的目标函数。\n举例来说，假设我有 $3$ 枚硬币，正正反，记为 $(1,1,0)$ 。预测结果是 $(0.8,0.9,0.3)$，那么，交叉熵的均值是:\n$$ \\frac13(1×\\log_20.8+1×log_20.9+(1-0)×log_2(1-0.3)) $$\n假设有一个完美的算法，直接预测出了 $(1,1,0)$，那么交叉熵的结果就是 $0$。\n","date":"13 October 2023","permalink":"/posts/ai/cross-entropy/","section":"博客","summary":"案例驱动 # 通过几个简单的例子来解释和总结什么是交叉熵（Cross Entropy）以及机器学习分类问题中为什么使用交叉熵。","title":"交叉熵"},{"content":" claude chatGPT kimi ","date":"13 October 2023","permalink":"/posts/ai/llm/llm-demos/","section":"博客","summary":"现有大模型","title":"现有大模型"},{"content":"CPU 上有多个内核。如果我们想充分利用现有的这些硬件，就需要一种并发运行代码的方法。数十年来无法追踪的错误和开发人员的沮丧都表明，线程并不是解决问题的办法。不过不用担心，我们还有其他很好的选择，今天我要向你展示的就是其中之一：actor model。\nactor model # actor model 是一种处理并发计算的概念模型。它为系统组件的行为和交互方式定义了一些通用规则。\nactors # actor 是计算的原始单元。它接收 message，并根据 message进行某种计算。\n这种想法与面向对象语言（object-oriented languages）中的想法非常相似：对象接收 message（方法调用），并根据接收到的 message（我们调用的方法）进行操作。\n主要区别在于，actors 之间是完全隔离的，它们永远不会共享内存。值得注意的是，一个 actor 可以保持一个私有状态，其他 actor 永远无法直接改变该状态。\n一个 actor 不是 actor。它们是以系统的形式出现的。在 actor model 中，一切都是 actor，它们需要有地址，这样一个行为者才能向另一个 actor 发送 message。\nmailbox # 虽然多个 actor 可以同时运行，但一个 actor 会按顺序处理给定的 message。这意味着，如果你向同一个 actor 发送 3 条 message，它只会一次执行一条。要同时执行这 3 条 message，你需要创建 3 个 actor，每个 actor 发送一条 message。\nmessage 是异步发送给角色的，角色在处理另一条消息时需要将消息存储在某个地方。mailbox 就是存储这些 message 的地方。\nactor 之间通过发送异步消息进行通信。这些 message 会保存在其他 actor 的 mailbox 中，直到它们被处理。\nWhat actors do # 当 actor 收到 message 时，它可以做以下三件事中的一件：\nCreate more actors Send messages to other actors Designate what to do with the next message：指定义这个状态在收到下一条信息时的样子，行为体如何改变状态。假设我们有一个行为类似于计算器的行为体，它的初始状态是简单的数字 0。当这个行为体收到 add(1) 消息时，它不会改变自己的原始状态，而是指定在收到下一条消息时，状态将是 1。 Fault tolerance # Erlang 引入了 \u0026ldquo;let it crash\u0026rdquo; 的理念。其理念是，你不需要进行防御性编程，试图预测所有可能发生的问题，并找到处理它们的方法，因为根本不可能考虑到每一个故障点。\nErlang 所做的就是简单地让它崩溃，但让这些关键代码由某个人监管，而这个人唯一的责任就是知道当崩溃发生时该做什么（比如将代码单元重置为稳定状态），而使这一切成为可能的就是 actor model。\n每段代码都运行在一个进程中（这也是 Erlang 对其角色的基本称呼）。这个进程是完全孤立的，这意味着它的状态不会影响任何其他进程。我们有一个 \u0026ldquo;监督者\u0026rdquo;，它基本上是另一个进程（所有东西都是行为体，还记得吗？），当被监督的进程崩溃时，它会收到通知，然后可以采取一些措施。\n这就使得创建 \u0026ldquo;self heal\u0026rdquo; 系统成为可能，也就是说，如果一个行为体由于某种原因进入了异常状态并崩溃，那么监管者就可以采取一些措施，尝试将其恢复到一致的状态（有多种策略可以做到这一点，最常见的就是以初始状态重新启动行为体）。\nActor Model For IoT # 物联网（IoT）由许多节点组成，通常功能有限。通过互联网协议标准进行通信的小型软件组件通常在机器之间形成高度分布式的工作流程，人与机器之间的互动极少。一般的应用场景包括监控环境条件等数据的传感器。复杂的应用则使用传感器和执行器，例如：家庭自动化和健康数据跟踪。这些系统使机器能够将数据上传到互联网服务器。因此，它们可以随时随地跟踪数据。\n典型 IoT 系统的主要特点之一是涉及大量受管设备，每个设备的内部状态都在不断变化。在许多情况下，这些设备都是在一些简单的网络协议上运行的原始硬件。这种 \u0026ldquo;极简\u0026rdquo; 要求与 actor model 非常吻合，因为 actor model 的基本原则之一就是将业务逻辑分解成最小的任务，由各个 actor 来处理。\nactor 具有 delivery guarantees 和 isolation 特性，非常适合物联网世界，是模拟数百万个并发连接的传感器生成实时数据的绝佳工具。它们设计轻巧，因此可以在不消耗过多计算资源的情况下进行扩展。\n以下是行动者适合物联网的特征属性：\nScalability：物联网带来了许多挑战，如何处理所有同时连接的设备产生的大量数据，并对其进行检索、汇总、分析和推送，同时保持设备的响应速度。面临的挑战包括管理高峰期接收传感器数据的巨大突发流量、批处理和实时处理这些海量数据，以及进行模拟真实世界使用模式的大规模仿真。一些物联网部署还要求后端服务管理设备，而不仅仅是吸收设备发送的数据。管理这一切的后端系统需要能够按需扩展，并具有完全的弹性。这非常适合 reactive architectures ，尤其是 Akka。\nConcurrency：物联网应用网关是系统中将本地传感器和执行器连接到云的点（例如路边站、运输过程中的车载设备或家庭自动化网关）。即使一个应用程序在传感器、执行器和云服务之间 \u0026ldquo;只转发数据\u0026rdquo;，也会有并发事件。物联网应用网关需要处理在其环境中发生的事件流和到达其接口的数据流。环境以自己的速度产生数据并要求输出。Actor model 通过消息传递实现了对来自设备的消息的高性能并发处理，从而解决了上述问题。message-processing models的优势之一是，传统的并发问题（主要是共享状态的同步）不再是问题。行为体可以保留设备内部状态或活动会话等私有状态，并在没有锁的情况下自由更新。Actor model 可确保一次只处理一条消息。\nFault Tolerance：在构建可能被数百万联网设备使用的服务时，您需要一个应对信息流的模型。您需要对设备故障、信息丢失和服务失败时的情况进行抽象。今天，我们常常认为调用堆栈是理所当然的。但是，它们发明的年代，由于多 CPU 系统并不常见，并发编程并不那么重要。调用栈不能跨线程，因此不能模拟异步调用链。 上图显示了一个严重的问题。工作线程如何处理这种情况？它很可能无法解决问题，因为它通常不知道失败任务的目的。调用者 \u0026ldquo;线程需要得到通知，但没有调用栈可以释放异常。失败通知只能通过侧通道完成，例如，在 \u0026ldquo;调用者 \u0026ldquo;线程希望得到结果的地方放置一个错误代码。如果没有这种通知，\u0026ldquo;调用者 \u0026ldquo;就永远不会收到失败通知，任务也就丢失了！这与网络系统的工作原理惊人地相似，在网络系统中，信息/请求可能在没有任何通知的情况下丢失/失败。\n有了 actor，我们可以将 actor 组织成监管层次，因此，单个 actor 的错误不会导致整个系统瘫痪。\nLightWeight：基准测试表明，Akka 模型每千兆字节堆内存可处理 250 万个角色，单机每秒可处理 5000 万条消息。\nNetwork Protocol Decoupling：利用 actor model ，我们可以利用容错功能，将代表设备的角色与底层通信协议分离开来。这样，代表设备和设备状态的角色就可以从代表通信协议的 actor 中分离出来，从而使设备 actor 免受网络错误的影响，并提高各个 actor 的功能一致性。\nNon-blocking communications：物联网应用 \u0026ldquo;必须 \u0026ldquo;具有反应性和异步性。大多数物联网应用程序都应能够处理来自设备的许多连接以及从设备中获取的所有信息。异步消息传递广泛应用于机器对机器通信。异步通信具有灵活性：应用程序可以发送一条信息，然后继续处理其他事情。actor 是唯一可寻址的，拥有自己独立的邮箱或消息队列。它们通过消息传递支持非阻塞通信，因此适合构建非阻塞和分布式计算系统。\nCustomization：所有行为体都有一个定义明确的生命周期，并配有精致的钩子，如用于生命周期逻辑控制的 preStart()、postRestart() 和 postStop()。在模拟物联网设备时，可以轻松地将自定义初始化和终止例程锚定到相应的钩子上。\nobject Device { def props(deviceType: String, mqttPubSub: ActorRef) = //... } class Device(deviceType: String, mqttPubSub: ActorRef) extends Actor { import Device._ private var opState: OpState = InitialState(deviceType) override def preStart(): Unit = //Initialize device\u0026#39;s op-state... override def postStop(): Unit = //Reset/Shutdown device... def receive = { case ReportOpState =\u0026gt; //Assemble report data with OpState mqttPubSub ! new Publish(Mqtt.topicReport, reportData) case UpdateOpState(newState) =\u0026gt; //Update opState with newState mqttPubSub ! new Publish(Mqtt.topicUpdate, updateResult) case PowerOff =\u0026gt; //Shutdown device... } } view raw 上面的片段展示了如何在 Scala/Akka 中构建一个设备角色，使用行业标准 MQTT（消息队列遥测传输）发布-订阅消息协议向订阅者发布其运行状态信息。这里的目的并不是研究如何用 Scala 或 Akka 编程，而是提供一个简单的示例，说明 Akka 角色易于理解的逻辑流程。\nResources # https://www.brianstorti.com/the-actor-model/ http://akshantalpm.github.io/Actor-Model-For-IoT/ https://www.infoworld.com/article/3209728/why-akka-and-the-actor-model-shine-for-iot-applications.html ","date":"13 October 2023","permalink":"/posts/architecture/iot/actor-model/","section":"博客","summary":"CPU 上有多个内核。如果我们想充分利用现有的这些硬件，就需要一种并发运行代码的方法。数十年来无法追踪的错误和开发人员的沮丧都表明，线程并不是解决问题的办法。不过不用担心，我们还有其他很好的选择，今天我要向你展示的就是其中之一：actor model。","title":"Actor Model"},{"content":"","date":"12 October 2023","permalink":"/tags/ai/","section":"Tags","summary":"","title":"Ai"},{"content":"Resources # Demos https://github.com/gofireflyio/aiac https://github.com/JustAIGithub/AI-Code-Convert Blogs 25 Best AI Code Generators ","date":"12 October 2023","permalink":"/posts/language/code-generation/ai-code-generators/","section":"博客","summary":"Resources # Demos https://github.","title":"AI Code Generators"},{"content":"Resources # git tutorial: https://wyag.thb.lt/ 动图展示10大Git命令: https://zhuanlan.zhihu.com/p/132573100 git intro: https://missing.csail.mit.edu/2020/version-control/ book: https://git-scm.com/book/en/v2 commit convention 规范: https://www.conventionalcommits.org/en/v1.0.0/#summary Write yourself a Git：https://wyag.thb.lt/ 如何编写Git Commit Message? # 为了创建一个有用的 revision history ，团队应该首先就 commit message convention 达成一致，至少要定义以下三点：\nStyle：标记语法Markup syntax, 流式布局wrap margins, 语法grammar, 大小写capitalization, 标点符号punctuation。把这些东西写出来，去掉猜测，让一切尽可能简单。 Content：提交消息的正文应该包含什么样的信息？不应该包含什么？ Metadata：如何引用 issue tracking IDs、pull request numbers 等？ 幸运的是，Git提交信息的规范已经有了很好的约定。事实上，很多 Git 命令的功能中就包含了这些约定。您不需要重新发明什么。只要遵循下面的七条规则，您就能像专家一样 commit message 了。\nThe seven rules of a great Git commit message\nSeparate subject from body with a blank line Limit the subject line to 50 characters Capitalize the subject line Do not end the subject line with a period Use the imperative mood in the subject line Wrap the body at 72 characters Use the body to explain what and why vs. how For example:\nSummarize changes in around 50 characters or less More detailed explanatory text, if necessary. Wrap it to about 72 characters or so. In some contexts, the first line is treated as the subject of the commit and the rest of the text as the body. The blank line separating the summary from the body is critical (unless you omit the body entirely); various tools like `log`, `shortlog` and `rebase` can get confused if you run the two together. Explain the problem that this commit is solving. Focus on why you are making this change as opposed to how (the code explains that). Are there side effects or other unintuitive consequences of this change? Here\u0026#39;s the place to explain them. Further paragraphs come after blank lines. - Bullet points are okay, too - Typically a hyphen or asterisk is used for the bullet, preceded by a single space, with blank lines in between, but conventions vary here If you use an issue tracker, put references to them at the bottom, like this: Resolves: #123 See also: #456, #789 1. Separate subject from body with a blank line # From the git commit manpage:\nThough not required, it\u0026#39;s a good idea to begin the commit message with a single short (less than 50 character) line summarizing the change, followed by a blank line and then a more thorough description. The text up to the first blank line in a commit message is treated as the commit title, and that title is used throughout Git. For example, Git-format-patch(1) turns a commit into email, and it uses the title on the Subject line and the rest of the commit in the body. 首先，并非每次提交都需要主题和正文。有时一行就够了，特别是当修改非常简单，不需要更多上下文的时候。\nFix typo in introduction to user guide 如果读者想知道错别字是什么，可以直接查 typo 本身，即使用 git show 或 git diff 或 git log -p。\n如果您在命令行提交类似的内容，使用 git commit 的 -m 选项也很方便\n$ git commit -m \u0026#34;Fix typo in introduction to user guide\u0026#34; 然而，当一个提交需要一些解释和上下文时，你需要写一个正文。例如：\nDerezz the master control program MCP turned out to be evil and had become intent on world domination. This commit throws Tron\u0026#39;s disc into MCP (causing its deresolution) and turns it back into a chess game. 使用 -m 选项编写带正文的提交信息并不容易。最好使用合适的文本编辑器来编写。\n在浏览日志时，主体与主体的分离是有好处的。以下是完整的日志记录：\n$ git log commit 42e769bdf4894310333942ffc5a15151222a87be Author: Kevin Flynn \u0026lt;kevin@flynnsarcade.com\u0026gt; Date: Fri Jan 01 00:00:00 1982 -0200 Derezz the master control program MCP turned out to be evil and had become intent on world domination. This commit throws Tron\u0026#39;s disc into MCP (causing its deresolution) and turns it back into a chess game. 现在只打印主题行 git log --oneline ：\n$ git log --oneline 42e769 Derezz the master control program 或者，按用户分组提交，同样只显示主题行，git shortlog：\n$ git shortlog Kevin Flynn (1): Derezz the master control program Alan Bradley (1): Introduce security program \u0026#34;Tron\u0026#34; Ed Dillinger (3): Rename chess program to \u0026#34;MCP\u0026#34; Modify chess program Upgrade chess program Walter Gibbs (1): Introduce protoype chess program 在Git中，主题行和正文之间的区别还有很多，但如果中间没有空行，它们都无法正常工作。\n2. Limit the subject line to 50 characters # 50个字符不是硬性限制，只是一个经验法则。将主题行保持在这一长度可确保其可读性，并迫使作者思考如何以最简洁的方式说明内容。\nTip: If you\u0026#39;re having a hard time summarizing, you might be committing too many changes at once. Strive for atomic commits (a topic for a separate post). GitHub\u0026rsquo;s UI is fully aware of these conventions. It will warn you if you go past the 50 character limit and will truncate any subject line longer than 72 characters with an ellipsis.\n3. Capitalize the subject line # This is as simple as it sounds. Begin all subject lines with a capital letter.\nFor example:\nAccelerate to 88 miles per hour Instead of:\naccelerate to 88 miles per hour 4. Do not end the subject line with a period # Trailing punctuation is unnecessary in subject lines. Besides, space is precious when you\u0026rsquo;re trying to keep them to 50 chars or less.\nExample:\nOpen the pod bay doors Instead of:\nOpen the pod bay doors. 5. Use the imperative mood in the subject line # Imperative mood just means \u0026ldquo;spoken or written as if giving a command or instruction\u0026rdquo;. A few examples:\nClean your room Close the door Take out the trash Git itself uses the imperative whenever it creates a commit on your behalf.\n例如，使用 git merge 时创建的默认信息如下\nMerge branch \u0026#39;myfeature\u0026#39; 当使用 git revert 时，\nRevert \u0026#34;Add the thing with the stuff\u0026#34; This reverts commit cc87791524aedd593cff5a74532befe7ab69ce9d. 或 点击 GitHub 拉取请求上的 Merge 按钮时：\nMerge pull request #123 from someuser/somebranch 因此，当您在命令行中编写提交信息时，您遵循的是 Git 自带的约定。例如，\nRefactor subsystem X for readability Update getting started documentation Remove deprecated methods Release version 1.0.0 这样写一开始可能会有点尴尬。我们更习惯于用指示语气说话，而指示语气则是报告事实。这就是为什么提交的信息经常读起来像这样：\nFixed bug with Y Changing behavior of X 有时承诺信息会被写成内容描述：\nMore fixes for broken stuff Sweet new API methods 为了消除任何混淆，这里有一个简单的规则，以便每次都能正确操作。\n一个正确的Git提交主题行应该能够完成以下句子：\nIf applied, this commit will your subject line here For example:\nIf applied, this commit will refactor subsystem X for readability If applied, this commit will update getting started documentation If applied, this commit will remove deprecated methods If applied, this commit will release version 1.0.0 If applied, this commit will merge pull request #123 from user/branch Remember: Use of the imperative is important only in the subject line. You can relax this restriction when you\u0026rsquo;re writing the body.\n6. Wrap the body at 72 characters # Git 不会自动换行。当您写提交信息的正文时，必须注意右边距，并手动换行。\n建议在72个字符时进行，这样Git就有足够的空间缩进文本，同时又能将所有内容保持在80个字符以内。\n7. Use the body to explain what and why vs. how # Bitcoin Core 的这个 commit 是一个很好的例子，它解释了改变的内容和原因：\ncommit eb0b56b19017ab5c16c745e6da39c53126924ed6 Author: Pieter Wuille \u0026lt;pieter.wuille@gmail.com\u0026gt; Date: Fri Aug 1 22:57:55 2014 +0200 Simplify serialize.h\u0026#39;s exception handling Remove the \u0026#39;state\u0026#39; and \u0026#39;exceptmask\u0026#39; from serialize.h\u0026#39;s stream implementations, as well as related methods. As exceptmask always included \u0026#39;failbit\u0026#39;, and setstate was always called with bits = failbit, all it did was immediately raise an exception. Get rid of those variables, and replace the setstate with direct exception throwing (which also removes some dead code). As a result, good() is never reached after a failure (there are only 2 calls, one of which is in tests), and can just be replaced by !eof(). fail(), clear(n) and exceptions() are just never called. Delete them. 看看完整的差异，想想作者花时间在此时此地提供这些上下文，为同事和未来的提交者节省了多少时间。如果他不这样做，这些内容可能会永远丢失。\n在大多数情况下，您可以省略关于如何修改的细节。在这方面，代码通常是不言自明的，如果代码非常复杂，需要用散文来解释，那就是源注释的作用。只需重点说明您首先进行修改的原因\u0026ndash;(修改前的工作方式以及有什么问题)、现在的工作方式，以及您为什么决定以这种方式解决问题。\n未来感谢您的维护者可能就是您自己！\n","date":"12 October 2023","permalink":"/posts/devoops/git/how-to-write-a-git-commit-message/","section":"博客","summary":"Resources # git tutorial: https://wyag.","title":"How to Write a Git Commit Message"},{"content":"Linux 的命令确实非常多，然而熟悉 Linux 的人从来不会因为 Linux 的命令太多而烦恼。因为我们仅仅只需要掌握常用命令，就完全可以驾驭 Linux。\n接下来，让我们一起来看看都有那些常用的 Linux 命令吧！\n一、文件目录操作 # 1.ls 命令 # ls 命令不仅可以查看 linux 文件夹包含的文件而且可以查看文件权限（包括目录、文件夹、文件权限）查看目录信息等等。\n命令格式\nls [选项][目录名] 常用参数\n-l ：列出长数据串，包含文件的属性与权限数据等 -a ：列出全部的文件，连同隐藏文件（开头为.的文件）一起列出来（常用） -d ：仅列出目录本身，而不是列出目录的文件数据 -h ：将文件容量以较易读的方式（GB，kB等）列出来 -R ：连同子目录的内容一起列出（递归列出），等于该目录下的所有文件都会显示出来 使用实例\n1.列出 home 目录下的所有文件和目录的详细资料。\nls -a -l /home ls -al /home 2.列出当前目录下所有以\u0026quot;d\u0026quot;开头的文件目录详情内容。\nls -l d* 2.cd命令 # 最基本的命令语句，其他的命令语句要进行操作，都是建立在使用 cd 命令上的。用于切换当前目录至dirName。\n命令格式\ncd [目录名] 操作案例\n1.从当前目录进入系统根目录。\ncd / 2.跳转到 home/Code 目录。\ncd /home/Code 3.pwd 命令 # 查看\u0026quot;当前工作目录\u0026quot;的完整路径。\n命令格式\npwd [选项] 常用参数\n-P :显示实际物理路径，而非使用连接（link）路径 -L :当目录为连接路径时，显示连接路径 操作案例\n1.显示当前所在路径。\npwd 4.mkdir 命令 # 用来创建指定的名称的目录，要求创建目录的用户在当前目录中具有写权限，并且指定的目录名不能是当前目录中已有的目录。\n命令格式\nmkdir [选项] 目录 常用参数\n-m, \u0026ndash;mode=模式，设定权限\u0026lt;模式\u0026gt; (类似 chmod)，而不是 rwxrwxrwx 减 umask -p, \u0026ndash;parents 可以是一个路径名称。此时若路径中的某些目录尚不存在,加上此选项后,系统将自动建立好那些尚不存在的目录,即一次可以建立多个目录; -v, \u0026ndash;verbose 每次创建新目录都显示信息 \u0026ndash;help 显示此帮助信息并退出 \u0026ndash;version 输出版本信息并退出 使用实例\n1.创建一个空目录。\nmkdir test 2.递归创建多个目录。\nmkdir test/test1 3.创建权限为777的目录。\nmkdir -m 777 test2 4.创建目录都显示信息。\nmkdir -v test4 5.rm 命令 # 删除一个目录中的一个或多个文件或目录，如果没有使用- r选项，则rm不会删除目录。如果使用 rm 来删除文件，通常仍可以将该文件恢复原状。\n命令格式\nrm [选项] 文件 常用参数\n-f, \u0026ndash;force 忽略不存在的文件，从不给出提示。 -i, \u0026ndash;interactive 进行交互式删除 -r, -R, \u0026ndash;recursive 指示rm将参数中列出的全部目录和子目录均递归地删除。 -v, \u0026ndash;verbose 详细显示进行的步骤 \u0026ndash;help 显示此帮助信息并退出 \u0026ndash;version 输出版本信息并退出 使用实例\n1.删除文件 test.txt,系统会提示是否删除。\nrm test.txt 2.强制删除 test.txt，系统不再提示。\nrm -f test.txt 3.将 test 子目录及目录中所有档案删除。\nrm -r test 6.rmdir 命令 # 该命令从一个目录中删除一个或多个子目录项，删除某目录时也必须具有对父目录的写权限。\n命令格式\nrmdir [选项] 目录 常用参数\n-p 递归删除目录dirname，当子目录删除后其父目录为空时，也一同被删除。如果整个路径被删除或者由于某种原因保留部分路径，则系统在标准输出上显示相应的信息。 -v, \u0026ndash;verbose 显示指令执行过程 使用实例\n1.删除空目录 test1，非空目录无法删除。\nrmdir test1 2.当子目录被删除后使它也成为空目录的话，则顺便一并删除\nrmdir -p test2 # test 目录下仅有 test2 7. mv 命令 # 可以用来移动文件或者将文件改名（move (rename) files）。当第二个参数类型是文件时，mv命令完成文件重命名。当第二个参数是已存在的目录名称时，源文件或目录参数可以有多个，mv命令将各参数指定的源文件均移至目标目录中。\n命令格式\nmv [选项] 源文件或目录 目标文件或目录 常用参数\n-b ：若需覆盖文件，则覆盖前先行备份 -f ：force 强制的意思，如果目标文件已经存在，不会询问而直接覆盖 -i ：若目标文件 (destination) 已经存在时，就会询问是否覆盖 -u ：若目标文件已经存在，且 source 比较新，才会更新(update) -t ： \u0026ndash;target-directory=DIRECTORY move all SOURCE arguments into DIRECTORY，即指定mv的目标目录，该选项适用于移动多个源文件到一个目录的情况，此时目标目录在前，源文件在后 使用实例\n1.将 test1.txt 重命名为 test2.txt。\nmv test1.txt test2.txt 2.移动文件 test1.txt 到目录 test2\nmv test1.txt test2 3.将文件 test1.txt、test2.txt、test3.txt 移动到目录 test3。\nmv test1.txt test2.txt test3.txt test3 8.cp 命令 # 将源文件复制至目标文件，或将多个源文件复制至目标目录。\n命令格式\ncp [选项] 源文件 目录 或 cp [选项] -t 目录 源文件 常用参数\n-t \u0026ndash;target-directory 指定目标目录 -i \u0026ndash;interactive 覆盖前询问（使前面的 -n 选项失效） -n \u0026ndash;no-clobber 不要覆盖已存在的文件（使前面的 -i 选项失效） -f \u0026ndash;force 强行复制文件或目录，不论目的文件或目录是否已经存在 -u \u0026ndash;update 使用这项参数之后，只会在源文件的修改时间较目的文件更新时，或是对应的目的文件并不存在，才复制文件 使用实例\n1.复制文件 test1.txt 到 test1 目录\ncp test1.txt test1 # 若文件存在，会提示是否覆盖。若不存在直接完成复制 复制 test1 整个目录到 test2\ncp -a test1 test2 9.touch 命令 # touch命令参数可更改文档或目录的日期时间，包括存取时间和更改时间。\n命令格式\ntouch [选项] 文件 常用参数\n-a 或\u0026ndash;time=atime或\u0026ndash;time=access或\u0026ndash;time=use 只更改存取时间 -c 或\u0026ndash;no-create 不建立任何文档 -d 使用指定的日期时间，而非现在的时间 -f 此参数将忽略不予处理，仅负责解决BSD版本touch指令的兼容性问题 -m 或\u0026ndash;time=mtime或\u0026ndash;time=modify 只更改变动时间 -r 把指定文档或目录的日期时间，统统设成和参考文档或目录的日期时间相同 -t 使用指定的日期时间，而非现在的时间 使用实例\n1.创建不存在的文件test.txt\ntouch test.txt 2.更新 test.txt 的实践和 test1.txt 时间戳相同\ntouch -r test.txt test1.txt 10.cat 命令 # 用来显示文件内容，或者将几个文件连接起来显示，或者从标准输入读取内容并显示，它常与重定向符号配合使用。\n命令格式\ncat [选项] [文件] 常用参数\n-A, \u0026ndash;show-all 等价于 -vET -b, \u0026ndash;number-nonblank 对非空输出行编号 -e 等价于 -vE -E, \u0026ndash;show-ends 在每行结束处显示 $ -n, \u0026ndash;number 对输出的所有行编号,由1开始对所有输出的行数编号 -s, \u0026ndash;squeeze-blank 有连续两行以上的空白行，就代换为一行的空白行 -t 与 -vT 等价 -T, \u0026ndash;show-tabs 将跳格字符显示为 ^I -u (被忽略) -v, \u0026ndash;show-nonprinting 使用 ^ 和 M- 引用，除了 LFD 和 TAB 之外 使用实例\n1.把 test.log 的文件内容加上行号后输入 test1.log 这个文件里。\ncat -n test.log test1.log 将 test.log 的文件内容反向显示。\ntac test.log 11.nl 命令 # 输出的文件内容自动的加上行号！其默认的结果与 cat -n 有点不太一样， nl 可以将行号做比较多的显示设计，包括位数与是否自动补齐 0 等等的功能。\n命令格式\nnl [选项] [文件] 常用参数\n-b ：指定行号指定的方式，主要有两种： -b a ：表示不论是否为空行，也同样列出行号(类似 cat -n) -b t ：如果有空行，空的那一行不要列出行号(默认值) -n ：列出行号表示的方法，主要有三种： -n ln ：行号在萤幕的最左方显示 -n rn ：行号在自己栏位的最右方显示，且不加 0 -n rz ：行号在自己栏位的最右方显示，且加 0 -w ：行号栏位的占用的位数 使用实例\n用 nl 列出 test.log 的内容。\nnl test.log 用 nl 列出 test.log 的内容，空本行也加上行号。\nnl -b a test.log 12.more 命令 # more 命令和 cat 的功能一样都是查看文件里的内容，但有所不同的是more可以按页来查看文件的内容，还支持直接跳转行等功能。\n命令格式\nmore [-dlfpcsu ] [-num ] [+/ pattern] [+ linenum] [file ... ] 常用参数\n+n 从笫n行开始显示 -n 定义屏幕大小为n行 +/pattern 在每个档案显示前搜寻该字串（pattern），然后从该字串前两行之后开始显示 -c 从顶部清屏，然后显示 -d 提示Press space to continue，'q' to quit（按空格键继续，按q键退出），禁用响铃功能 -l 忽略Ctrl+l（换页）字符 -p 通过清除窗口而不是滚屏来对文件进行换页，与-c选项相似 -s 把连续的多个空行显示为一行 -u 把文件内容中的下画线去掉 操作指令\nEnter：向下n行，需要定义。默认为1行 Ctrl+F：向下滚动一屏 空格键：向下滚动一屏 Ctrl+B：返回上一屏 = ：输出当前行的行号 ：f ：输出文件名和当前行的行号 V ：调用vi编辑器 !命令 ：调用Shell，并执行命令 q ：退出more 使用实例\n1.显示文件 test.log 第3行起内容。\nmore +3 test.log 2.从文件 test.log 查找第一个出现\u0026quot;day3\u0026quot;字符串的行，并从该处前2行开始显示输出。\nmore +/day3 test.log 设置每屏显示行数\nmore -5 test.log 13.less 命令 # less 与 more 类似，但使用 less 可以随意浏览文件，而 more 仅能向前移动，却不能向后移动，而且 less 在查看之前不会加载整个文件。\n命令格式\nless [参数] 文件 常用参数\n-b \u0026lt;缓冲区大小\u0026gt; 设置缓冲区的大小 -e 当文件显示结束后，自动离开 -f 强迫打开特殊文件，例如外围设备代号、目录和二进制文件 -g 只标志最后搜索的关键词 -i 忽略搜索时的大小写 -m 显示类似more命令的百分比 -N 显示每行的行号 -o \u0026lt;文件名\u0026gt; 将less 输出的内容在指定文件中保存起来 -Q 不使用警告音 -s 显示连续空行为一行 -S 行过长时间将超出部分舍弃 -x \u0026lt;数字\u0026gt; 将\u0026quot;tab\u0026quot;键显示为规定的数字空格 操作命令\n/字符串：向下搜索\u0026quot;字符串\u0026quot;的功能 ?字符串：向上搜索\u0026quot;字符串\u0026quot;的功能 n：重复前一个搜索（与 / 或 ? 有关） N：反向重复前一个搜索（与 / 或 ? 有关） b 向后翻一页 d 向后翻半页 h 显示帮助界面 Q 退出less 命令 u 向前滚动半页 y 向前滚动一行 空格键 滚动一行 回车键 滚动一页 [pagedown]： 向下翻动一页 [pageup]： 向上翻动一页 使用实例\n1.查看文件 test.log。\nless test.log 14.head 命令 # head 用来显示档案的开头至标准输出中，默认 head 命令打印其相应文件的开头 10 行。\n命令格式\nhead [参数] [文件] 常用参数\n-q 隐藏文件名 -v 显示文件名 -c\u0026lt;字节\u0026gt; 显示字节数 -n\u0026lt;行数\u0026gt; 显示的行数 使用实例\n1.显示文件 test.log 的前 5 行\nhead -n 5 test.log 2.显示文件 test.log 前 20 个字节\nhead -c 20 test.log 15.tail 命令 # 显示指定文件末尾内容，不指定文件时，作为输入信息进行处理。常用查看日志文件。\n命令格式\ntail [必要参数] [选择参数] [文件] 常用参数\n-f 循环读取 -q 不显示处理信息 -v 显示详细的处理信息 -c\u0026lt;数目\u0026gt; 显示的字节数 -n\u0026lt;行数\u0026gt; 显示行数 \u0026ndash;pid=PID 与-f合用,表示在进程ID,PID死掉之后结束. -q, \u0026ndash;quiet, \u0026ndash;silent 从不输出给出文件名的首部 -s, \u0026ndash;sleep-interval=S 与-f合用,表示在每次反复的间隔休眠S秒 使用实例\n1.显示文件 test.log 最后 5 行内容。\ntail -n 5 test.log 2.循环查看文件内容\ntail -f test.log 二、文件查找 # 16.which 命令 # which指令会在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。\n命令格式\nwhich 可执行文件名称 常用参数\n-n 指定文件名长度，指定的长度必须大于或等于所有文件中最长的文件名 -p 与-n参数相同，但此处的包括了文件的路径 -w 指定输出时栏位的宽度 -V 显示版本信息 使用实例\n1.查找文件、显示命令路径。\nwhich pwd 用 which 去找出 which\nwhich which 17.whereis 命令 # whereis命令是定位可执行文件、源代码文件、帮助文件在文件系统中的位置。\n命令格式\nwhereis [-bmsu] [BMS 目录名 -f ] 文件名 常用参数\n-b 定位可执行文件 -m 定位帮助文件 -s 定位源代码文件 -u 搜索默认路径下除可执行文件、源代码文件、帮助文件以外的其它文件 -B 指定搜索可执行文件的路径 -M 指定搜索帮助文件的路径 -S 指定搜索源代码文件的路径 使用实例\n1.将和 svn 文件相关的文件都查找出来。\nwhereis svn 2.只将二进制文件查找出来。\nwhereis -b svn 18.locate 命令 # 可以很快速的搜寻档案系统内是否有指定的档案。\n命令格式\nlocate [选择参数] [样式] 常用参数\n-e 将排除在寻找的范围之外。 -1 如果 是 1．则启动安全模式。在安全模式下，使用者不会看到权限无法看到 的档案。这会始速度减慢，因为 locate 必须至实际的档案系统中取得档案的 权限资料。 -f 将特定的档案系统排除在外，例如我们没有到理要把 proc 档案系统中的档案 放在资料库中。 -q 安静模式，不会显示任何错误讯息。 -n 至多显示 n个输出。 -r 使用正规运算式 做寻找的条件。 -o 指定资料库存的名称。 -d 指定资料库的路径 使用实例\n1.查找和 pwd 相关的所有文件。\nlocate pwd 搜索etc 目录下，所有以 m 开头的文件。\nbash复制代码locate /etc/m 19.find 命令 # 主要作用是沿着文件层次结构向下遍历，匹配符合条件的文件，并执行相应的操作。\n命令格式\nfind [选项] [搜索路径] [表达式] 常用参数\n-print find 命令将匹配的文件输出到标准输出 -exec find 命令对匹配的文件执行该参数所给出的 shell 命令\n-name 按照文件名查找文件 -type 查找某一类型的文件 使用实例\n1.打印当前目录文件目录列表。\nfind . -print 2.打印当前目录下所有不以.txt 结尾的文件名。\nfind . ! -name \u0026#34;*.txt\u0026#34; 3.打印当前目录下所有权限为 777 的 php 文件。\nfind . -type f -name \u0026#34;*.php\u0026#34; -perm 777 4.找到当前目录下所有 php 文件，并显示其详细信息。\nfind . -name \u0026#34;*.php\u0026#34; -exec ls -l {} \\; 5.查找当前目录下所有 c 代码文件，统计总行数。\nfind . -type f -name \u0026#34;*.c\u0026#34; | xargs wc -l xargs 命令可以从标准输入接收输入，并把输入转换为一个特定的参数列表。\n命令格式\ncommand | xargs [选项] [command] xargs 命令应该紧跟在管道操作符之后，因为它以标准输入作为主要的源数据流。\n常用参数\n-n 指定每行最大的参数数量 -d 指定分隔符 三、文件打包上传和下载 # 20.tar 命令 # 用来压缩和解压文件。tar本身不具有压缩功能。他是调用压缩功能实现的。\n命令格式\ntar [必要参数] [选择参数] [文件] 常用参数\n必要参数\n-A 新增压缩文件到已存在的压缩 -B 设置区块大小 -c 建立新的压缩文件 -d 记录文件的差别 -r 添加文件到已经压缩的文件 -u 添加改变了和现有的文件到已经存在的压缩文件 -x 从压缩的文件中提取文件 -t 显示压缩文件的内容 -z 支持gzip解压文件 -j 支持bzip2解压文件 -Z 支持compress解压文件 -v 显示操作过程 -l 文件系统边界设置 -k 保留原有文件不覆盖 -m 保留文件不被覆盖 -W 确认压缩文件的正确性 可选参数\n-b 设置区块数目 -C 切换到指定目录 -f 指定压缩文件 \u0026ndash;help 显示帮助信息 \u0026ndash;version 显示版本信息 使用实例\n1.将文件打全部打包成tar包。\ntar -cvf test.tar test.log # 仅打包，不压缩！ tar -zcvf test.tar.gz test.log # 打包后，以 gzip 压缩 tar -zcvf test.tar.bz2 test.log # 打包后，以 bzip2 压缩 2.将 tar 包解压缩\ntar -zxvf test.tar.gz 21.gzip 命令 # 使用广泛的压缩程序，文件经它压缩过后，其名称后面会多出\u0026quot;.gz\u0026quot;的扩展名。\n命令格式\ngzip [参数] [文件或者目录] 常用参数\n-a或\u0026ndash;ascii 使用ASCII文字模式。 -c或\u0026ndash;stdout或\u0026ndash;to-stdout 把压缩后的文件输出到标准输出设备，不去更动原始文件。 -d或\u0026ndash;decompress或\u0026mdash;-uncompress 解开压缩文件。 -f或\u0026ndash;force 强行压缩文件。不理会文件名称或硬连接是否存在以及该文件是否为符号连接。 -h或\u0026ndash;help 在线帮助。 使用实例\n1.把 test1 目录下的每个文件压缩成.gz 文件。\ngzip * 四、文件权限设置 # 22.chmod 命令 # 用于改变linux系统文件或目录的访问权限。\n命令格式\nchmod [-cfvR] [--help] [--version] mode file 常用参数\n必要参数\n-c 当发生改变时，报告处理信息 -f 错误信息不输出 -R 处理指定目录以及其子目录下的所有文件 -v 运行时显示详细处理信息 选择参数\n\u0026ndash;reference=\u0026lt;目录或者文件\u0026gt; 设置成具有指定目录或者文件具有相同的权限 \u0026ndash;version 显示版本信息 \u0026lt;权限范围\u0026gt;+\u0026lt;权限设置\u0026gt; 使权限范围内的目录或者文件具有指定的权限 \u0026lt;权限范围\u0026gt;-\u0026lt;权限设置\u0026gt; 删除权限范围的目录或者文件的指定权限 \u0026lt;权限范围\u0026gt;=\u0026lt;权限设置\u0026gt; 设置权限范围内的目录或者文件的权限为指定的值 权限范围\nu ：目录或者文件的当前的用户 g ：目录或者文件的当前的群组 o ：除了目录或者文件的当前用户或群组之外的用户或者群组 a ：所有的用户及群组 权限代号\nr：读权限，用数字4表示 w：写权限，用数字2表示 x：执行权限，用数字1表示 -：删除权限，用数字0表示 使用实例\n1.增加文件所有用户组可执行权限\nchmod a+x test.log 删除所有用户的可执行权限\nchmod a-x test.log 23.chgrp 命令 # 可采用群组名称或群组识别码的方式改变文件或目录的所属群组。\n命令格式\nchgrp [选项] [组] [文件] 常用参数\n必要参数\n-c 当发生改变时输出调试信息 -f 不显示错误信息 -R 处理指定目录以及其子目录下的所有文件 -v 运行时显示详细的处理信息 \u0026ndash;dereference 作用于符号链接的指向，而不是符号链接本身 \u0026ndash;no-dereference 作用于符号链接本身 选择参数\n\u0026ndash;reference=\u0026lt;文件或者目录\u0026gt; \u0026ndash;help 显示帮助信息 \u0026ndash;version 显示版本信息 使用实例\n1.改变文件的群组属性\nchgrp -v bin test.log 2.改变文件test1.log 的群组属性，使得文件test1.log的群组属性和参考文件test.log的群组属性相同\nchgrp --reference=test.log test1.log 24.chown 命令 # 通过chown改变文件的拥有者和群组。\n命令格式\nchown [选项] [所有者] [:[组]] 文件 常用参数\n必要参数\n-c 显示更改的部分的信息 -f 忽略错误信息 -h 修复符号链接 -R 处理指定目录以及其子目录下的所有文件 -v 显示详细的处理信息 -deference 作用于符号链接的指向，而不是链接文件本身 选择参数\n\u0026ndash;reference=\u0026lt;目录或文件\u0026gt; 把指定的目录/文件作为参考，把操作的文件/目录设置成参考文件/目录相同拥有者和群组 \u0026ndash;from=\u0026lt;当前用户：当前群组\u0026gt; 只有当前用户和群组跟指定的用户和群组相同时才进行改变 \u0026ndash;help 显示帮助信息 \u0026ndash;version 显示版本信息 使用实例\n1.改变拥有者和群组\nchown mail:mail test.log 五、磁盘存储 # 25.df 命令 # 显示指定磁盘文件的可用空间。\n命令格式 # df [选项] [文件] 常用参数\n必要参数\n-a 全部文件系统列表 -h 方便阅读方式显示 -H 等于\u0026rsquo;-h\u0026rsquo;，但是计算式，1K=1000，而不是1K=1024 -i 显示inode信息 -k 区块为1024字节 -l 只显示本地文件系统 -m 区块为1048576字节 \u0026ndash;no-sync 忽略 sync 命令 -P 输出格式为POSIX \u0026ndash;sync 在取得磁盘信息前，先执行sync命令 -T 文件系统类型 选择参数\n\u0026ndash;block-size=\u0026lt;区块大小\u0026gt; 指定区块大小 -t\u0026lt;文件系统类型\u0026gt; 只显示选定文件系统的磁盘信息 -x\u0026lt;文件系统类型\u0026gt; 不显示选定文件系统的磁盘信息 \u0026ndash;help 显示帮助信息 \u0026ndash;version 显示版本信息 使用实例\n1.显示指定磁盘使用情况\ndf -t ext3 du 命令 显示每个文件和目录的磁盘使用空间。\n命令格式\ndu [选项] [文件] 常用参数\n-a或-all 显示目录中个别文件的大小。 -b或-bytes 显示目录或文件大小时，以byte为单位。 \u0026ndash; -c或\u0026ndash;total 除了显示个别目录或文件的大小外，同时也显示所有目录或文件的总和。 -k或\u0026ndash;kilobytes 以KB(1024bytes)为单位输出。 -m或\u0026ndash;megabytes 以MB为单位输出。 -s或\u0026ndash;summarize 仅显示总计，只列出最后加总的值。 -h或\u0026ndash;human-readable 以K，M，G为单位，提高信息的可读性。 -x或\u0026ndash;one-file-xystem 以一开始处理时的文件系统为准，若遇上其它不同的文件系统目录则略过。 -L\u0026lt;符号链接\u0026gt;或\u0026ndash;dereference\u0026lt;符号链接\u0026gt; 显示选项中所指定符号链接的源文件大小。 -S或\u0026ndash;separate-dirs 显示个别目录的大小时，并不含其子目录的大小。 -X\u0026lt;文件\u0026gt;或\u0026ndash;exclude-from=\u0026lt;文件\u0026gt; 在\u0026lt;文件\u0026gt;指定目录或文件。 \u0026ndash;exclude=\u0026lt;目录或文件\u0026gt; 略过指定的目录或文件。 -D或\u0026ndash;dereference-args 显示指定符号链接的源文件大小。 -H或\u0026ndash;si 与-h参数相同，但是K，M，G是以1000为换算单位。 -l或\u0026ndash;count-links 重复计算硬件链接的文件。 使用实例\n1.显示指定目录或文件所占空间\ndu test # 目录 du test.log # 文件 六、性能监控和优化命令 # 27.top 命令 # 显示当前系统正在执行的进程的相关信息，包括进程ID、内存占用率、CPU占用率等。\n命令格式\ntop [参数] 常见参数\n-b 批处理 -c 显示完整的治命令 -I 忽略失效过程 -s 保密模式 -S 累积模式 -i\u0026lt;时间\u0026gt; 设置间隔时间 -u\u0026lt;用户名\u0026gt; 指定用户名 -p\u0026lt;进程号\u0026gt; 指定进程 -n\u0026lt;次数\u0026gt; 循环显示的次数 使用实例\n显示进程信息\ntop 28.free 命令 # 显示系统使用和空闲的内存情况，包括物理内存、交互区内存(swap)和内核缓冲区内存。\n命令格式\nfree [参数] 常见参数\n-b 以Byte为单位显示内存使用情况 -k 以KB为单位显示内存使用情况 -m 以MB为单位显示内存使用情况 -g 以GB为单位显示内存使用情况 -o 不显示缓冲区调节列 -s \u0026lt;间隔秒数\u0026gt; 持续观察内存使用状况 -t 显示内存总和列。 -V 显示版本信息。 使用实例\n1.显示内存情况。\nfree free -g #以GB为单位 free -m #以MB为单位 29.vmstat # 用来显示虚拟内存的信息。\n命令格式\nvmstat [-a] [-n] [-S unit] [delay [ count]] vmstat [-s] [-n] [-S unit] vmstat [-m] [-n] [delay [ count]] vmstat [-d] [-n] [delay [ count]] vmstat [-p disk partition] [-n] [delay [ count]] vmstat [-f] vmstat [-V] 常见参数\n-a：显示活跃和非活跃内存 -f：显示从系统启动至今的fork数量 -m：显示slabinfo -n：只在开始时显示一次各字段名称 -s：显示内存相关统计信息及多种系统活动数量 delay：刷新时间间隔。如果不指定，只显示一条结果 count：刷新次数。如果不指定刷新次数，但指定了刷新时间间隔，这时刷新次数为无穷 -d：显示磁盘相关统计信息 -p：显示指定磁盘分区统计信息 -S：使用指定单位显示。参数有 k 、K 、m 、M ，分别代表1000、1024、1000000、1048576字节（byte）。默认单位为K（1024 bytes） 使用实例\n1.显示活跃和非活跃内存。\nvmstat -a 5 5 # 5秒时间内进行5次采样 30.lostat 命令 # 通过iostat方便查看CPU、网卡、tty设备、磁盘、CD-ROM 等等设备的活动情况, 负载信息。\n命令格式\niostat [参数] [时间] [次数] 常见参数\n-C 显示CPU使用情况 -d 显示磁盘使用情况 -k 以 KB 为单位显示 -m 以 M 为单位显示 -N 显示磁盘阵列(LVM) 信息 -n 显示NFS 使用情况 -p[磁盘] 显示磁盘和分区的情况 -t 显示终端和CPU的信息 -x 显示详细信息 使用实例\n1.定时显示所有信息。\niostat 2 3 #每隔 2秒刷新显示，且显示3次 31.lsof 命令 # 用于查看你进程开打的文件，打开文件的进程，进程打开的端口(TCP、UDP)。\n命令格式\nlsof [参数] [文件] 常见参数\n-a 列出打开文件存在的进程 -c\u0026lt;进程名\u0026gt; 列出指定进程所打开的文件 -g 列出GID号进程详情 -d\u0026lt;文件号\u0026gt; 列出占用该文件号的进程 +d\u0026lt;目录\u0026gt; 列出目录下被打开的文件 +D\u0026lt;目录\u0026gt; 递归列出目录下被打开的文件 -n\u0026lt;目录\u0026gt; 列出使用NFS的文件 -i\u0026lt;条件\u0026gt; 列出符合条件的进程。（4、6、协议、:端口、 @ip ） -p\u0026lt;进程号\u0026gt; 列出指定进程号所打开的文件 -u 列出UID号进程详情 使用实例\n1.查看谁正在使用bash文件，也就是说查找某个文件相关的进程。\nlsof /bin/bash 七、网络命令 # 32.ifconfig 命令 # ifconfig 命令用来查看和配置网络设备。\n命令格式\nifconfig [网络设备] [参数] 常见参数\nup 启动指定网络设备/网卡 down 关闭指定网络设备/网卡。 arp 设置指定网卡是否支持ARP协议 -promisc 设置是否支持网卡的promiscuous模式，如果选择此参数，网卡将接收网络中发给它所有的数据包 -allmulti 设置是否支持多播模式，如果选择此参数，网卡将接收网络中所有的多播数据包 -a 显示全部接口信息 -s 显示摘要信息（类似于 netstat -i） add 给指定网卡配置IPv6地址 del 删除指定网卡的IPv6地址 使用实例\n1.启动关闭指定网卡\nifconfig eth0 up ifconfig eth0 down 2.用ifconfig修改MAC地址\nifconfig eth0 hw ether 00:AA:BB:CC:DD:EE 33.route 命令 # Route命令是用于操作基于内核ip路由表，它的主要作用是创建一个静态路由让指定一个主机或者一个网络通过一个网络接口，如eth0。\n命令格式\nroute [-f] [-p] [Command [Destination] [mask Netmask] [Gateway] [metric Metric]] [if Interface]] 常见参数\n-c 显示更多信息 -n 不解析名字 -v 显示详细的处理信息 -F 显示发送信息 -C 显示路由缓存 -f 清除所有网关入口的路由表。 -p 与 add 命令一起使用时使路由具有永久性。 add:添加一条新路由。 del:删除一条路由。 -net:目标地址是一个网络。 -host:目标地址是一个主机。 netmask:当添加一个网络路由时，需要使用网络掩码。 gw:路由数据包通过网关。注意，你指定的网关必须能够达到。 metric：设置路由跳数。 Command 指定您想运行的命令 (Add/Change/Delete/Print)。 Destination 指定该路由的网络目标。 使用实例\n1.显示当前路由\nroute route -n 2.添加网关/设置网关\nroute add -net 224.0.0.0 netmask 240.0.0.0 dev eth0 ping 命令 确定网络和各外部主机的状态；跟踪和隔离硬件和软件问题；测试、评估和管理网络。\n命令格式\nping [参数] [主机名或IP地址] 常见参数\n-d 使用Socket的SO_DEBUG功能 -f 极限检测。大量且快速地送网络封包给一台机器，看它的回应 -n 只输出数值 -q 不显示任何传送封包的信息，只显示最后的结果 -r 忽略普通的Routing Table，直接将数据包送到远端主机上。通常是查看本机的网络接口是否有问题 -R 记录路由过程 -v 详细显示指令的执行过程 -c 数目：在发送指定数目的包后停止 -i 秒数：设定间隔几秒送一个网络封包给一台机器，预设值是一秒送一次 -I 网络界面：使用指定的网络界面送出数据包 -l 前置载入：设置在送出要求信息之前，先行发出的数据包 -p 范本样式：设置填满数据包的范本样式 -s 字节数：指定发送的数据字节数，预设值是56，加上8字节的ICMP头，一共是64ICMP数据字节 -t 存活数值：设置存活数值TTL的大小 使用实例\nping 网关\nping -b 192.168.120.1 35.traceroute 命令\n让你追踪网络数据包的路由途径，预设数据包大小是40Bytes，用户可另行设置。\n命令格式\ntraceroute [参数] [主机] 常见参数\n-d 使用Socket层级的排错功能 -f 设置第一个检测数据包的存活数值TTL的大小 -F 设置勿离断位 -g 设置来源路由网关，最多可设置8个 -i 使用指定的网络界面送出数据包 -I 使用ICMP回应取代UDP资料信息 -m 设置检测数据包的最大存活数值TTL的大小 -n 直接使用IP地址而非主机名称 -p 设置UDP传输协议的通信端口 -r 忽略普通的Routing Table，直接将数据包送到远端主机上 -s 设置本地主机送出数据包的IP地址 -t 设置检测数据包的TOS数值 -v 详细显示指令的执行过程 -w 设置等待远端主机回报的时间 -x 开启或关闭数据包的正确性检验 使用实例\n1.traceroute 用法简单、最常用的用法\ntraceroute www.baidu.com 跳数设置\ntraceroute -m 10 www.baidu.com 36.netstat 命令 # 用于显示与IP、TCP、UDP和ICMP协议相关的统计数据，一般用于检验本机各端口的网络连接情况。\n命令格式\nnetstat [-acCeFghilMnNoprstuvVwx] [-A\u0026lt;网络类型\u0026gt;] [--ip] 常见参数\n-a或-all 显示所有连线中的Socket -A\u0026lt;网络类型\u0026gt;或-\u0026lt;网络类型\u0026gt; 列出该网络类型连线中的相关地址 -c或-continuous 持续列出网络状态 -C或-cache 显示路由器配置的快取信息 -e或-extend 显示网络其他相关信息 -F或-fib 显示FIB -g或-groups 显示多重广播功能群组组员名单 -h或-help 在线帮助 -i或-interfaces 显示网络界面信息表单 -l或-listening 显示监控中的服务器的Socket -M或-masquerade 显示伪装的网络连线 -n或-numeric 直接使用IP地址，而不通过域名服务器 -N或-netlink或-symbolic 显示网络硬件外围设备的符号连接名称 -o或-timers 显示计时器 -p或-programs 显示正在使用Socket的程序识别码和程序名称 -r或-route 显示Routing Table -s或-statistice 显示网络工作信息统计表 -t或-tcp 显示TCP传输协议的连线状况 -u或-udp 显示UDP传输协议的连线状况 -v或-verbose 显示指令执行过程 -V或-version 显示版本信息 -w或-raw 显示RAW传输协议的连线状况 -x或-unix 此参数的效果和指定\u0026quot;-A unix\u0026quot;参数相同 -ip或-inet 此参数的效果和指定\u0026quot;-A inet\u0026quot;参数相同 使用实例\n列出所有端口\nnetstat -a 37.telnet 命令 # 执行telnet指令开启终端机阶段作业，并登入远端主机。\n命令格式\ntelnet [参数] [主机] 常见参数\n-8 允许使用8位字符资料，包括输入与输出 -a 尝试自动登入远端系统 -b\u0026lt;主机别名\u0026gt; 使用别名指定远端主机名称 -c 不读取用户专属目录里的.telnetrc文件 -d 启动排错模式 -e\u0026lt;脱离字符\u0026gt; 设置脱离字符 -E 滤除脱离字符 -f 此参数的效果和指定\u0026quot;-F\u0026quot;参数相同 使用实例\n1.远程服务器无法访问\ntelnet 192.168.120.206 八、其他命令 # 38.ln 命令 # 为某一个文件在另外一个位置建立一个同步的链接.当我们需要在不同的目录，用到相同的文件时，我们不需要在每一个需要的目录下都放一个必须相同的文件，我们只要在某个固定的目录，放上该文件，然后在 其它的目录下用ln命令链接（link）它就可以，不必重复的占用磁盘空间。\n命令格式\nln [参数] [源文件或目录] [目标文件或目录] 常用参数\n必要参数\n-b 删除，覆盖以前建立的链接 -d 允许超级用户制作目录的硬链接 -f 强制执行 -i 交互模式，文件存在则提示用户是否覆盖 -n 把符号链接视为一般目录 -s 软链接(符号链接) -v 显示详细的处理过程 选择参数\n-S -S\u0026lt;字尾备份字符串\u0026gt; 或 --suffix=\u0026lt;字尾备份字符串\u0026gt; -V -V\u0026lt;备份方式\u0026gt; 或 --version-control=\u0026lt;备份方式\u0026gt;\n使用实例\n1.为 test.log文件创建软链接linktest\nln -s test.log linktest 2.为 test.log创建硬链接lntest。\nln test.log lntest 39.diff 命令 # 比较单个文件或者目录内容。\n命令格式\ndiff [参数] [文件1或目录1] [文件2或目录2] 常用参数\n-c 上下文模式，显示全部内文，并标出不同之处 -u 统一模式，以合并的方式来显示文件内容的不同 -a 只会逐行比较文本文件 -N 在比较目录时，若文件 A 仅出现在某个目录中，预设会显示：Only in 目录。若使用 -N 参数，则 diff 会将文件 A 与一个空白的文件比较 -r 递归比较目录下的文件 使用实例\n1.显示 test1.txt 和 test2.txt 两个文件差异。\ndiff test1.txt test2.txt 40.grep 命令 # 一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹 配的行打印出来。\n命令格式\ngrep [option] pattern file 常用参数\n-c 计算找到\u0026rsquo;搜寻字符串\u0026rsquo;（即 pattern）的次数 -i 忽略大小写的不同，所以大小写视为相同 -n 输出行号 -v 反向选择，打印不匹配的行 -r 递归搜索 \u0026ndash;color=auto 将找到的关键词部分加上颜色显示 使用实例\n1.将 /etc/passwd 文件中出现 root 的行取出来，关键词部分加上颜色显示。\ngrep \u0026#34;root\u0026#34; /etc/passwd --color=auto cat /etc/passwd | grep \u0026#34;root\u0026#34; --color=auto 2.将 /etc/passwd 文件中没有出现 root 和 nologin 的行取出来。\ngrep -v \u0026#34;root\u0026#34; /etc/passwd | grep -v \u0026#34;nologin\u0026#34; 41.wc 命令 # 用来显示文件所包含的行、字和字节数。\n命令格式\nwc [选项] [文件] 常用参数\n-c 统计字节数 -l 统计行数 -m 统计字符数，这个标志不能与 -c 标志一起使用 -w 统计字数，一个字被定义为由空白、跳格或换行字符分隔的字符串 -L 打印最长行的长度 使用实例\n1.统计文件的字节数、行数和字符数。\nwc -c test.txt wc -l test.txt wc -m test.txt 2.统计文件的字节数、行数和字符数，只打印数字，不打印文件名。\ncat test.txt | wc -c cat test.txt | wc -l cat test.txt | wc -m 42.ps 命令 # 用来显示当前进程的状态。\n命令格式\nps[参数] 常用参数\na 显示所有进程 -a 显示同一终端下的所有程序 -A 显示所有进程 c 显示进程的真实名称 -N 反向选择 -e 等于-A e 显示环境变量 f 显示程序间的关系 -H 显示树状结构 r 显示当前终端的进程 T 显示当前终端的所有程序 u 指定用户的所有进程 -au 显示较详细的资讯 -aux 显示所有包含其他使用者的行程 -C\u0026lt;命令\u0026gt; 列出指定命令的状况 \u0026ndash;lines\u0026lt;行数\u0026gt; 每页显示的行数 \u0026ndash;width\u0026lt;字符数\u0026gt; 每页显示的字符数 使用实例\n1.显示所有进程信息。\nps -A 显示指定用户信息。\nps -u root 显示所有进程信息，连同命令行。\nps -ef 43.watch 命令\n可以将命令的输出结果输出到标准输出设备，多用于周期性执行命令/定时执行命令。\n命令格式\nwatch [参数] [命令] 常用参数\n-n或\u0026ndash;interval watch缺省每2秒运行一下程序，可以用-n或-interval来指定间隔的时间。 -d或\u0026ndash;differences 用-d或\u0026ndash;differences 选项watch 会高亮显示变化的区域。 而-d=cumulative选项会把变动过的地方(不管最近的那次有没有变动)都高亮显示出来。 -t 或-no-title 会关闭watch命令在顶部的时间间隔,命令，当前时间的输出。 -h, \u0026ndash;help 查看帮助文档 使用实例\n1.每隔一秒高亮显示网络链接数的变化情况\nwatch -n 1 -d netstat -ant 2.每隔一秒高亮显示http链接数的变化情况\nwatch -n 1 -d \u0026#39;pstree|grep http\u0026#39; 44.at 命令 # 在一个指定的时间执行一个指定任务，只能执行一次。（需开启atd进程）\n命令格式\nat [参数] [时间] 常用参数\n-m 当指定的任务被完成之后，将给用户发送邮件，即使没有标准输出 -I atq的别名 -d atrm的别名 -v 显示任务将被执行的时间 -c 打印任务的内容到标准输出 -V 显示版本信息 -q\u0026lt;列队\u0026gt; 使用指定的列队 -f\u0026lt;文件\u0026gt; 从指定文件读入任务而不是从标准输入读入 -t\u0026lt;时间参数\u0026gt; 以时间参数的形式提交要运行的任务 使用实例\n1.3天后的下午5点执行/bin/ls\nat 5pm+3 days at\u0026gt; /bin/ls at\u0026gt; \u0026lt;EOT\u0026gt; 45.crontab 命令 # 在固定的间隔时间执行指定的系统指令或 shell script脚本。时间间隔的单位可以是分钟、小时、日、月、周及以上的任意组合。(需开启crond服务)\n命令格式\ncrontab [-u user] file 或 crontab [-u user] [ -e | -l | -r ] 常用参数\n-u user：用来设定某个用户的crontab服务，例如，-u ixdba表示设定ixdba用户的crontab服务，此参数一般有root用户来运行。 file：file是命令文件的名字,表示将file做为crontab的任务列表文件并载入crontab。如果在命令行中没有指定这个文件，crontab命令将接受标准输入（键盘）上键入的命令，并将它们载入crontab。 -e：编辑某个用户的crontab文件内容。如果不指定用户，则表示编辑当前用户的crontab文件。 -l：显示某个用户的crontab文件内容，如果不指定用户，则表示显示当前用户的crontab文件内容。 -r：从/var/spool/cron目录中删除某个用户的crontab文件，如果不指定用户，则默认删除当前用户的crontab文件。 -i：在删除用户的crontab文件时给确认提示。 使用实例\n1.列出 crontab 文件。\ncrontab -l 2.编辑crontab 文件。\ncrontab -e Crontab 任务实例\n1.每1分钟执行一次command\n* * * * * command 2.每小时的第3和第15分钟执行\n3,15 * * * * command 3.在上午8点到11点的第3和第15分钟执行\n3,15 8-11 * * * command ","date":"12 October 2023","permalink":"/posts/reviews/os/linux-instructions/","section":"博客","summary":"Linux 的命令确实非常多，然而熟悉 Linux 的人从来不会因为 Linux 的命令太多而烦恼。因为我们仅仅只需要掌握常用命令，就完全可以驾驭 Linux。","title":"45 个常用Linux 命令，让你轻松玩转Linux！"},{"content":"","date":"12 October 2023","permalink":"/tags/chatgpt/","section":"Tags","summary":"","title":"Chatgpt"},{"content":"Six strategies for getting better results # Write clear instructions # GPT 无法读懂你的心思。如果产出太长，请要求简短回复。如果结果太简单，要求专家级的写作。如果您不喜欢格式，请演示您希望看到的格式。GPT 越少需要猜测你想要什么，你就越有可能得到它。\n在您的询问中包含详细信息，以获得更多相关答案：为了得到高度相关的回复，请确保请求提供了任何重要的细节或上下文。否则，您就只能让模型来猜测您的意思了。 要求模特采用一个角色：系统信息可用于指定模型在回复中使用的角色。 使用分隔符清楚标明输入内容的不同部分：三引号、XML 标记、章节标题等分隔符可以帮助划分需要区别对待的文本部分。 指定完成任务所需的步骤：有些任务最好以一连串的步骤来指定。明确写出这些步骤可以让模型更容易地遵循它们。 举例说明：提供适用于所有示例的一般说明通常比通过示例演示任务的所有排列组合更有效，但在某些情况下，提供示例可能更容易。例如，如果您打算让模型复制一种难以明确描述的回应用户询问的特定风格，这就是所谓的 \u0026ldquo;少量 \u0026ldquo;提示。这就是所谓的 \u0026ldquo;少量 \u0026ldquo;提示。 指定所需的输出长度：您可以要求模型生成具有给定目标长度的输出。可以用字数、句数、段落数、要点数等来指定目标输出长度。但请注意，指示模型生成特定字数的精确度并不高。模型可以更可靠地生成具有特定段落数或要点数的输出结果。 Provide reference text # GPT 可以自信地编造虚假答案，尤其是在被问及深奥的话题或引用和 URL 时。就像一张笔记能帮助学生在考试中取得更好的成绩一样，为 GPT 提供参考文本也能帮助他们在作答时减少无中生有的情况。\n指导模型使用参考文本作答：如果我们能为模型提供与当前查询相关的可信信息，那么我们就可以指示模型使用所提供的信息来撰写答案。 指导范例引用参考文献回答问题：如果输入内容中已经补充了相关知识，那么就可以直接要求模型通过引用所提供文档中的段落来为其答案添加引文。请注意，输出中的引用可以通过所提供文档中的字符串匹配进行编程验证。 Split complex tasks into simpler subtasks # 在软件工程中，将一个复杂的系统分解成一系列模块化组件是一种很好的做法，提交给 GPT 的任务也是如此。复杂任务的错误率往往高于简单任务。此外，复杂任务通常可以重新定义为较简单任务的工作流程，其中前期任务的输出被用于构建后期任务的输入。\n使用意图分类来确定与用户查询最相关的指令：对于需要大量独立指令集来处理不同情况的任务，首先对查询类型进行分类，并利用该分类来确定需要哪些指令，可能会有所帮助。这可以通过定义固定类别和硬编码与处理特定类别任务相关的指令来实现。这一过程也可以递归应用，将任务分解为一系列阶段。这种方法的优势在于，每次查询只包含执行任务下一阶段所需的指令，与使用单次查询执行整个任务相比，错误率更低。这还可以降低成本，因为运行较大的提示需要花费更多的成本。 对于需要冗长对话的对话应用程序，总结或过滤之前的对话：由于 GPT 的上下文长度是固定的，因此用户和助手之间的对话（整个对话都包含在上下文窗口中）不可能无限期地进行下去。解决这个问题有多种变通方法，其中之一就是总结对话中的前几轮对话。一旦输入的大小达到预定的阈值长度，就会触发一个对部分对话进行总结的查询，而之前对话的总结可以作为系统消息的一部分。或者，也可以在整个对话过程中在后台异步总结之前的对话。 对长文档进行分块摘要，并递归构建完整摘要：由于 GPT 有固定的上下文长度，因此在单次查询中，GPT 无法用于摘要长度超过上下文长度减去生成摘要长度的文本。 Give GPTs time to \u0026ldquo;think\u0026rdquo; # 如果要求你用 17 乘以 28，你可能不会马上知道，但花点时间还是能算出来的。同样，GPT 学生在试图立即回答而不是花时间推理出答案时，会犯更多的推理错误。在回答问题之前，要求学生进行一连串的推理，可以帮助 GPT 学生更可靠地推理出正确答案。\n在匆忙得出结论之前，指示模型自己找出解决方案：如果我们明确指示模型在得出结论之前先从第一性原理进行推理，会得到更好的结果。例如，假设我们想要一个模型来评估学生对数学问题的解答。最明显的方法是简单地问模型学生的解法是否正确。 使用内心独白或一系列查询来隐藏模型的推理过程：前面的策略表明，在回答具体问题之前，模型有时必须对问题进行详细推理。对于某些应用，模型得出最终答案的推理过程不宜与用户共享。例如，在辅导应用中，我们可能希望鼓励学生自己找出答案，但模型对学生解决方案的推理过程可能会向学生透露答案。内心独白是一种可以用来缓解这种情况的策略。内心独白的原理是指示模型将输出结果中不对用户公开的部分转化为结构化格式，以便于解析。然后，在向用户展示输出结果之前，先对输出结果进行解析，只让部分输出结果可见。 Use external tools # 向 GPT 提供其他工具的输出结果，弥补 GPT 的不足。例如，文本检索系统可以告诉 GPT 相关文档的信息。代码执行引擎可以帮助 GPT 进行数学运算和运行代码。如果某项任务可以通过工具而不是 GPT 更可靠或更高效地完成，那么就将其卸载，以获得两者的最佳效果。\n利用嵌入式搜索实现高效知识检索：如果将外部信息源作为输入的一部分，模型可以利用外部信息源。这可以帮助模型生成更多信息和最新回复。例如，如果用户询问有关特定电影的问题，那么在模型输入中添加有关电影的高质量信息（如演员、导演等\u0026hellip;\u0026hellip;）可能会很有用。嵌入可用于实现高效的知识检索，以便在运行时将相关信息动态添加到模型输入中。文本嵌入是一个可以衡量文本字符串之间相关性的向量。相似或相关的字符串会比不相关的字符串靠得更近。这一事实以及快速向量搜索算法的存在，意味着嵌入可以用来实现高效的知识检索。特别是，文本语料库可以分割成若干块，每个块都可以嵌入和存储。然后，可以嵌入给定的查询，并执行矢量搜索，从语料库中找到与查询最相关的嵌入文本块（即在嵌入空间中最接近的文本块）。 使用代码执行来执行更精确的计算或调用外部应用程序接口：不能依靠 GPT 自行准确执行算术运算或长时间计算。在需要的情况下，可以指示模型编写和运行代码，而不是自己进行计算。特别是，可以指示模型将需要运行的代码放入指定格式（如三重回溯）中。产生输出后，可提取并运行代码。最后，如有必要，可将代码执行引擎（即 Python 解释器）的输出作为下一次查询的模型输入。 让模型访问特定功能：聊天完成 API 允许在请求中传递函数描述列表。这样，模型就能根据提供的模式生成函数参数。生成的函数参数由 API 以 JSON 格式返回，可用于执行函数调用。然后，函数调用提供的输出可以在下一个请求中反馈到模型中，以结束循环。这是使用 GPT 模型调用外部函数的推荐方式。 Test changes systematically # 如果能对性能进行测量，提高性能就会变得更容易。在某些情况下，对提示符的修改会在一些孤立的示例上取得更好的性能，但在更具代表性的示例集上却会导致整体性能下降。因此，为了确保修改对性能的净积极影响，可能有必要定义一个综合测试套件（也称为 \u0026ldquo;评估\u0026rdquo;）。\nResources # https://platform.openai.com/docs/guides/gpt-best-practices https://github.com/mattnigh/ChatGPT3-Free-Prompt-List https://style.mla.org/citing-generative-ai/ ","date":"12 October 2023","permalink":"/posts/ai/chatgpt-guide/","section":"博客","summary":"Six strategies for getting better results # Write clear instructions # GPT 无法读懂你的心思。如果产出太长，请要求简短回复。如果结果太简单，要求专家级的写作。如果您不喜欢格式，请演示您希望看到的格式。GPT 越少需要猜测你想要什么，你就越有可能得到它。","title":"chatGPT 使用指南"},{"content":"","date":"12 October 2023","permalink":"/tags/linux/","section":"Tags","summary":"","title":"Linux"},{"content":"Resources # github：https://github.com/fatedier/frp document：https://gofrp.org/docs/ finalshell：https://sourceforge.net/projects/finalshell/ vscode remote ssh：https://code.visualstudio.com/docs/remote/ssh 下面给出一些blog，都详细写了如何使用frp搭建内网穿透，在本文中就不再赘述。\n使用frp进行内网穿透：https://sspai.com/post/52523 基于frp docker 进行内网穿透：https://izhaong.com/pages/b387de/ CentOS7下通过frp做内网穿透：https://blog.fengdis.com/2019/12/25/CentOS%E4%B8%8B%E9%80%9A%E8%BF%87frp%E5%81%9A%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/ 这一篇blog的05节写了遇到的常见问题，这也是本文关心的。\n常见问题：https://www.derrors.cn/index.php/it-tech/frp.html Questions # 大部分都是网络端口上的问题，下面先给出一张frp的原理图。\nssh: connect to host xx.xx.xx.xx port xx: Operation timed out\n使用ssh连接时，连接超时 原因：服务器防火墙未开放frp配置中对应的remote_port端口； 解决：在服务器的防火墙中开放相应端口。 ssh: connect to host xx.xx.xx.xx port xx: Connection refused\n连接被拒绝 原因：服务器防火墙未开放frp配置中对应的server_port端口； 解决：在服务器的防火墙中开放相应端口。 当然云服务器端，也会有安全组或者防火墙，需要把相应的都开起来\n#开放端口 firewall-cmd --zone=public --add-port=7000/tcp --permanent firewall-cmd --zone=public --add-port=6000/tcp --permanent #查看开放端口列表 firewall-cmd --permanent --zone=public --list-ports #防火墙reload firewall-cmd --reload firewalld 拓展 # 这边很多的问题都跟防火墙有关系，这边给出 firewalld 的相关指令。\nsource: 根据源地址过滤（优先级最高） interface: 根据网卡过滤（优先级次高） service: 根据服务名过滤 port: 根据端口过滤 icmp-block: icmp 报文过滤，按照 icmp 类型配置 masquerade: ip 地址伪装 forward-port: 端口转发 rule: 自定义规则 # 查看是否开启 systemctl status firewalld.service # 打开防火墙 systemctl start firewalld.service # 停用防火墙 systemctl disable firewalld # 禁用防火墙 systemctl stop firewalld.service # 开机启动 systemctl enable firewalld # 取消开机启动 systemctl disable firewalld # 查看运行状态 firewall-cmd --state # 查看接口信息 firewall-cmd --list-all # 更新防火墙规则方法1:无需断开连接，动态更改规则 firewall-cmd --reload # 更新防火墙规则方法2:断开连接，以重启的方式更改规则 firewall-cmd --complete-reload # 查看帮助 firewall-cmd --help --zone=NAME # 指定 Zone --permanent # 为永久生效 --timeout=seconds # 持续一段时间，到期后自动移除，经常用于调试，且不能与 --permanent 同时使用 # 追加一个8181端口，永久有效 firewall-cmd --add-port=8181/tcp --permanent # 追加一段端口范围 firewall-cmd --add-port=6000-6600/tcp # 开放 ftp 服务 firewall-cmd --add-service=ftp # 添加eth0 接口至 public 信任等级，永久有效 firewall-cmd --zone=public --add-interface=eth0 --permanent # 关闭防火墙 sudo systemctl stop firewalld # 关闭端口 sudo firewall-cmd --remove-port=3000/tcp --permanent # 配置 public zone 的端口转发 firewall-cmd --zone=public --add-masquerade # 然后转发 tcp 22 端口至 9527 firewall-cmd --zone=public --add-forward-port=port=22:proto=tcp:toport=9527 # 转发 22 端口数据至另一个 ip 的相同端口上 firewall-cmd --zone=public --add-forward-port=port=22:proto=tcp:toaddr=192.168.1.123 # 转发 22 端口数据至另一 ip 的 9527 端口上 firewall-cmd --zone=public --add-forward-port=port=22:proto=tcp:toport=9527:toaddr=192.168.1.100 # IP 封禁 firewall-cmd --permanent --add-rich-rule=\u0026#34;rule family=\u0026#39;ipv4\u0026#39; source address=\u0026#39;192.168.1.123\u0026#39; reject\u0026#34; # 通过 ipset 来封禁 ip firewall-cmd --permanent --zone=public --new-ipset=blacklist --type=hash:ip firewall-cmd --permanent --zone=public --ipset=blacklist --add-entry=192.168.1.123 # 封禁网段 firewall-cmd --permanent --zone=public --new-ipset=blacklist --type=hash:net firewall-cmd --permanent --zone=public --ipset=blacklist --add-entry=192.168.1.0/24 # 倒入 ipset 规则 blacklist，然后封禁 blacklist firewall-cmd --permanent --zone=public --new-ipset-from-file=/path/blacklist.xml firewall-cmd --permanent --zone=public --add-rich-rule=\u0026#39;rule source ipset=blacklist drop\u0026#39; ","date":"12 October 2023","permalink":"/posts/skills/frp-nat-traversal/","section":"博客","summary":"Frp是一款开源的内网穿透工具，可实现将内部网络服务暴露给外部网络访问。它通过服务器端和客户端配合工作，服务器端位于外网，客户端位于内网，通过建立反向代理通道实现数据传输。Frp支持TCP、UDP、HTTP等协议，具有简单易用、配置灵活、跨平台等特点，常用于搭建家庭服务器、内网访问、远程调试等场景。","title":"Frp Nat Traversal"},{"content":"","date":"12 October 2023","permalink":"/tags/nat-traversal/","section":"Tags","summary":"","title":"Nat Traversal"},{"content":"","date":"12 October 2023","permalink":"/tags/telosys/","section":"Tags","summary":"","title":"Telosys"},{"content":"Resources # url: https://www.telosys.org/ tutorial: https://tomassetti.me/telosys-code-generation-tool/ ","date":"12 October 2023","permalink":"/posts/language/code-generation/telosys-code-generation-tool/","section":"博客","summary":"Resources # url: https://www.","title":"Telosys: a Code Generation Tool by Laurent Guerin"},{"content":"一、书名和作者 # 书名：《人月神话》 作者：布鲁克斯(FrederickP.Brooks.Jr.) 二、书籍概览 # 主要论点和结构 《人月神话》是一本旨在深入探讨软件工程中的管理和工程问题的经典著作。本书强调了软件开发过程中的复杂性和挑战，尤其是在大规模项目中。书中还探讨了许多经典观点，如\u0026quot;人月神话\u0026quot;、\u0026ldquo;二八定律\u0026quot;和\u0026quot;沟通成本\u0026rdquo;，为软件行业的专业人员提供了宝贵的见解和管理原则，使他们能够更好地理解和应对软件项目的挑战。\n目标读者和应用场景 该书的目标读者包括软件工程师、项目经理、团队领导和决策者，以及任何对软件开发过程感兴趣的人。对于软件开发工程师来说，这本书提供了宝贵的洞察，帮助他们更好地理解项目管理和团队协作的挑战；对于项目经理来说，本书提供了管理大型软件项目所需的关键原则和策略；领导小型或大型软件团队的人员可以从本书中获得关于如何优化团队协作、提高效率和管理项目的方法；即使不是专业人员，任何对软件开发过程感兴趣的人都可以从本书中获得对软件工程领域的深入了解，从而更好地理解和评估不同软件项目。总的来说，《人月神话》适用于各种软件项目，无论是大规模的企业级项目还是小规模的个人项目。\n三、核心观点与主题 # 1. 人月神话\n人月神话的产生 《人月神话》的核心观点之一是关于\u0026quot;人月神话\u0026quot;本身的产生。这一概念源自于普遍存在的一种误解，即认为增加项目开发的人员数量会自动缩短项目完成时间。作者布鲁克斯解释了这种误解的根源，即对软件工程的特殊性和复杂性的不理解。这种误解在早期的计算机领域中非常普遍，导致了一些项目的失败和项目时间的延长。\n后果和启发 项目增加人员后出现的管理问题和沟通成本的急剧上升，最后导致了项目的失败，包括延期、成本超支和低质量交付等。这些后果为软件开发的实践带来了极大的挑战，但也激发了对更好方法的追求。软件工程领域需要更多的规划、需求管理和团队协作，以避免人员增加引发的问题。\n实例或案例 一个鲜明的案例是IBM的OS/360项目，该项目是为了开发一种崭新的操作系统。初期，这个项目规模宏大，聚集了大量人员资源，充满了雄心壮志，然而，很快就陷入了严重的延期和质量问题的泥淖。在这个项目中，管理层采取了一种常见的措施，即试图通过增加项目开发的人员数量来加快进度。然而，结果却截然不同于期望。\n2. 二八定律\n二八定律的阐述 本书的第二个重要主题是\u0026quot;二八定律\u0026quot;，它强调了在软件开发中常见的现象，即80%的工作通常需要80%的时间，而剩下的20%工作同样需要80%的时间。这一定律揭示了工作任务的不均衡性，以及为什么某些部分的工作似乎总是比预期需要更多的时间。作者详细探讨了这一定律的背后原因，以及它在软件工程中的应用。\n重要任务的优先性 项目中的关键任务和非关键任务应当被明智地区分开来。关键任务往往占据大部分时间和资源，因此它们的规划和执行至关重要。这个观点呼吁项目管理者和团队要明智地设置优先级，确保关键任务首先得到充分关注，以确保项目能够按计划顺利进行。\n实例或案例 一个生动的例子是在软件开发项目中的功能开发和测试。根据二八定律，80%的开发工作可能会占用80%的时间，但剩下的20%的时间可能都被用于测试和调试。这种情况表明，关键任务（测试）常常被放在项目的后期，从而导致项目延期和问题的累积。通过理解这一现象，团队可以更好地规划项目，提前考虑到测试和质量保证，从而避免在后期因紧急问题而忙乱无序。这个案例强调了二八定律的实际应用，以提高项目的效率和成功率。\n3. 沟通成本\n沟通成本的重要性 这本书的第三个主题关注了\u0026quot;沟通成本\u0026quot;的概念。沟通在软件开发项目中是至关重要的，因为团队成员需要共同合作、协调工作和共享信息。然而，随着团队规模的增大，沟通的复杂性也随之增加。所以为了有效地合作，必须投入时间和精力来解决沟通问题。\n沟通成本的增加 随着团队规模的增加，沟通成本的急剧上升。当团队规模庞大时，需要花更多的时间来协调、汇报和共享信息。这不仅仅是人员增加导致的问题，还包括了更多的管理层次、更多的会议和文档。这会消耗时间和资源，导致项目时间表的延迟。\n实例或案例 在大型软件开发项目中，特别是在跨地理位置分布的全球团队中，沟通成本的急剧上升。团队成员分布在不同的时区，可能使用不同的语言和文化，这会增加沟通的困难。管理层必须花更多的时间来协调跨团队合作，编写文档以确保信息传递清晰，以及组织跨地域的会议。这些额外的沟通成本不仅会影响项目进度，还可能导致误解和沟通失败。通过理解沟通成本的重要性和增加，团队可以采取更有效的沟通策略，包括利用技术工具、清晰的沟通计划和团队培训，以减轻这一问题带来的负面影响。这个案例强调了如何通过降低沟通成本来提高项目的成功机会。\n4. 团队工作\n团队工作的重要性 软件开发项目往往需要多个团队成员之间的有效合作，包括程序员、测试人员、设计师和管理者，团队协作的不可或缺，才能保证项目成功完成。\n团队协作所面临的挑战 随着团队规模的扩大，不同成员之间的协调和沟通变得更加困难。这可能导致沟通失误、工作分配的混乱和项目的延期。有效的团队协作不仅涉及技术层面，还需要关注人际关系和沟通技巧。\n实例或案例 考虑一个涉及多个团队的复杂项目，每个团队负责不同的模块或组件。如果团队之间的协调和沟通不顺畅，可能会导致不同部分之间的不一致，甚至出现集成问题。\n四、亮点与启发 # 最有影响的观点或实例 在《人月神话》中，最有影响的观点之一是关于\u0026quot;人月神话\u0026quot;本身。这一观点深刻地揭示了在软件开发项目中的一个普遍误解，即增加项目开发的人员数量会缩短项目时间。通过生动的IBM的OS/360项目的案例，作者清晰地展示了增加人员数量并不总是解决方案，反而可能导致更多的管理和沟通成本，从而延长项目时间表。这个观点对软件工程领域产生了深远的影响，提醒我们要谨慎处理人员规模的增长，强调了规划、管理和沟通的重要性。\n另一个关键观点是\u0026quot;二八定律\u0026quot;，它解释了为什么80%的工作通常需要80%的时间，而剩下的20%同样需要80%的时间。这一定律强调了项目中关键任务的优先性和规划的必要性。通过理解这一观点，团队可以更好地分配资源和精力，确保项目关键任务的顺利执行，从而避免时间表的延迟和资源浪费。\n对个人或专业发展的启示 它提醒我们要对软件工程项目的复杂性和挑战有充分的认识。软件开发不同于传统工程，它涉及到人、技术和管理的多层次交互。因此，我们需要谨慎规划、有效沟通和管理，以确保项目的成功。此外，书中的案例和观点强调了团队协作的不可或缺性。无论是在大型企业项目还是小型团队中，团队成员之间的合作和协调至关重要。这启示我们要发展良好的团队协作技能，倾听他人的意见，学会解决冲突，以实现共同的目标。通过《人月神话》，我们能够深入理解软件工程的本质，从中汲取宝贵的经验教训，不仅提高专业素养，还能应用于各种项目和团队，推动软件工程领域的不断进步。\n五、批评与局限性 # 任何有争议、模糊或过时的信息 尽管《人月神话》包含了许多宝贵的观点和经验教训，但也存在一些有争议、模糊或过时的信息。首先，书中的一些案例和观点可能仅适用于特定的历史背景，因为软件工程领域在书写时已经发生了巨大的变化。例如，书中提到的硬件和软件环境可能与现代技术和工具有很大不同，因此某些观点可能已经过时。此外，一些观点可能在不同背景下产生争议。例如，在某些敏捷开发项目中，强调小团队、快速迭代和自组织可能与书中的一些建议相悖。因此，读者需要谨慎评估书中的观点，以确保其适用于其具体的项目和环境。\n可能的不足或缺陷 一个潜在的不足是书中强调的某些问题可能过于简化了复杂的软件工程现实。例如，书中提到的\u0026quot;人月神话\u0026quot;观点虽然有其价值，但它可能过于一概而论。在实际项目中，项目规模、团队结构和技术要求各不相同，因此不同项目可能会有不同的最佳实践。这种简化可能导致读者忽视了项目的特定需求。此外，书中强调的一些建议和技巧可能需要更多的上下文和实际操作指南。读者可能需要额外的资源来理解如何具体应用这些原则。因此，书中的一些内容可能缺乏具体的实施细节，这可能对一些读者而言是不足之处。\n六、实际应用和拓展 # 在实际工作 / 学习中如何应用这些概念 《人月神话》中的概念对实际工作和学习有重要意义。首先，对于软件工程领域的专业人士，书中的观点提供了宝贵的指导，如如何有效地管理项目、规划资源、协调团队和降低沟通成本。对于项目经理、团队领导和决策者，这些观点有助于更好地理解软件项目的特殊性和复杂性，从而提高项目的成功机会。\n其次，这些概念也适用于其他领域，特别是项目管理领域。无论是在制造业、医疗保健、建筑业还是任何需要团队合作和资源管理的领域，书中的原则都具有通用性。学习如何应对复杂性、规划和协调资源以及降低沟通成本对于任何项目的成功都是至关重要的。\n对未来研究或实践的建议 随着技术的不断发展，需要考虑新兴技术对软件工程和项目管理的影响。例如，人工智能、云计算和大数据等新技术如何改变项目的性质和需求。\n其次，可以深入研究如何应对全球化和跨文化团队合作的挑战。随着全球化趋势的加强，团队成员可能分布在不同国家和文化中，如何有效协作和沟通将成为一个重要的研究领域。\n七、总结与评价 # 对书籍的整体评价 《人月神话》是一本经典的软件工程管理著作，提供了深刻的洞察和宝贵的经验教训。它以清晰、易懂的语言讨论了软件开发中的复杂性和挑战，强调了管理和工程方面的重要性。这本书的长期影响力可见一斑，许多软件专业人士将其视为必读之作。\n书籍的长处和短处 长处：\n经典观点： 书中的观点，如\u0026quot;人月神话\u0026quot;和\u0026quot;二八定律\u0026quot;，具有深远的影响，为软件工程管理提供了宝贵的指导。 实际建议： 书中提供了许多实际的管理建议和案例，读者可以在实际项目中应用。 通俗易懂： 作者以平易近人的语言阐释了复杂的概念，使其对广大读者更容易理解。 跨学科性： 书中的原则和观点不仅适用于软件工程领域，还适用于其他项目管理领域。 短处：\n部分过时观点： 由于书写时间较早，某些观点和案例已经过时，需要根据现代技术和实践进行审慎评估。 不足的实际操作指南： 有些观点可能需要更多的实际操作指南，以帮助读者更好地应用。 ","date":"12 October 2023","permalink":"/read/%E4%BA%BA%E6%9C%88%E7%A5%9E%E8%AF%9D/","section":"阅读","summary":"旨在深入探讨软件工程中的管理和工程问题的经典著作。本书强调了软件开发过程中的复杂性和挑战，尤其是在大规模项目中。书中还探讨了许多经典观点，如\u0026quot;人月神话\u0026quot;、\u0026ldquo;二八定律\u0026quot;和\u0026quot;沟通成本\u0026rdquo;，为软件行业的专业人员提供了宝贵的见解和管理原则，使他们能够更好地理解和应对软件项目的挑战。","title":"《人月神话》阅读笔记"},{"content":"什么是 DevOps # 什么是 DevOps？DevOps 集文化理念、实践和工具于一身，它强调团队授权、跨团队沟通和协作以及技术自动化，其最终目标是优化质量和交付。\nDevOps 理念，旨在打破开发工程师和运维工程师的壁垒，强调两个团队合而为一，在产品的整个生命周期（从开发、测试、部署再到运维、运营）内相互协作，工程师不再限于单一职能。\nDevOps 始于 2007 年左右，当时的开发和运维对传统的软件开发模式提出了担忧：在这种模式下，编写代码的开发人员与负责部署的运维人员分开工作。 DevOps 一词是开发（development）和运维（operations）这两个词的组合，反映了将二者合而为一的过程。\nDevOps 如何工作 # DevOps 团队包括在整个产品生命周期中协同工作的开发人员和运维人员，以提高软件部署的速度和质量。这是一种新的工作方式，一种文化转变，对团队及其工作的组织具有重要意义。\n在 DevOps 模型下，开发和运维团队不再 \u0026ldquo;孤立\u0026rdquo;。有时，这两个团队甚至会合并为一个团队，工程师在整个应用程序生命周期中工作，需要具备从开发、测试到部署和运维的复合型能力。\nDevOps 团队使用工具来自动化和优化流程，这有助于提高可靠性。 DevOps 工具链可帮助团队处理重要的 DevOps 基础知识，包括持续集成、持续交付、自动化和协作。\nDevOps 价值观也适用于开发以外的团队。如果 QA、安全团队也和开发、运维团队紧密地结合在一起，贯穿产品的整个生命周期。此时，安全成为了所有 DevOps 团队成员的工作重点，此时可以称为为 \u0026ldquo;DevSecOps\u0026rdquo;。\nDevOps 的生命周期 # 由于 DevOps 的连续性，可以使用无限循环来展示 DevOps 生命周期的各个阶段是如何相互关联的。尽管看起来是按顺序流动的，但循环象征着在整个生命周期中始终保持持续迭代。\nDevOps 生命周期由六个阶段组成，分别代表开发和运维所需的流程、功能和工具。在每个阶段，团队协作和沟通以保持一致性、速度和质量。\nDevOps 的优势 # 速度：应用 DevOps 可以更频繁地发布可交付成果，并且质量和稳定性也更高。高效的迭代，可以根据客户和市场反馈进行快速响应，以适应市场变化，有效推动业务发展。 促进协作：DevOps 的基础是开发和运维之间的协作文化，两个团队紧密协作，共同承担诸多责任，并将各自的工作流程相互融合。这有助于减少效率低下的工作，同时节约大家的时间。 快速发布：提高发布的频率和速度，以便能够更快速地进行创新并完善产品。您发布新功能和修复错误的速度越快，就越能快速地响应客户需求并建立竞争优势。持续集成和持续交付是自动执行软件发布流程（从构建到部署）的两项实践经验。 可靠性：持续集成和持续部署等实践可检验程序变更后，功能是否正常，是否安全，从而提高软件产品的交付质量。监控和日志记录可以帮助团队实时了解服务当前的运行状态。 规模：大规模运行和管理您的基础设施及开发流程。自动化和一致性可在降低风险的同时，帮助您有效管理复杂或不断变化的系统。例如，基础设施即代码能够帮助您以一种可重复且更有效的方式来管理部署、测试和生产环境。 安全性：通过将自动实施的合规性策略、精细控制和配置管理技术集成到敏捷开发和 DevOps 工作流程中，使得产品内置了安全性。 DevOps 工具 # DevOps 各生命周期阶段都有合适的工具可以作为解决方案。它们通过提高协作效率、减少上下文切换、引入自动化以及实现可监控来全方位增强 DevOps 实践。\nDevOps 工具链通常遵循两种模式：完整解决方案或开放式工具链。\n完整解决方案实现了端到端的交付，流程很完备，但是一般难以兼容、集成第三方工具。 开放式工具链允许使用不同的工具进行定制。 这两种方法各有利弊。\n这里列举一些常见的 DevOps 工具：\n项目管理： Jira 文档管理： Confluence 代码管理： Gitlab、 Github CI/CD： Gitlab、 Jenkins 容器 Docker 将应用程序与该程序的依赖，打包在一个文件里面。运行这个文件，就会生成一个虚拟容器。程序在这个虚拟容器里运行，就好像在真实的物理机上运行一样。有了 Docker，就不用担心环境问题。 Kubernetes 是谷歌开源的容器集群管理系统 是用于自动部署，扩展和管理 Docker 应用程序的开源系统，简称 K8S。 日志 ELK 技术栈，通过数据采集工具 Logstack、Beats 套件、日志存储、解析服务 ElasticSearch、日志可视化工具 Kibnana，形成了一套完整的端到端日志解决方案，深受业界好评。 监控 ELK 的技术栈比较成熟，应用范围也比较广，除了可用作监控系统外，还可以用作日志查询和分析。 Prometheus 的独特之处在于它采用了拉数据的方式，对业务影响较小，同时也采用了时间序列数据库存储，而且支持独有的 PromQL 查询语言，功能强大而且简洁。 Grafana 是流行的监控数据分析和可视化套件。 Graphite 是基于时间序列数据库存储的监控系统，并且提供了功能强大的各种聚合函数比如 sum、average、top5 等可用于监控分析，而且对外提供了 API 也可以接入其他图形化监控系统如 Grafana。 链路追踪 Zipkin：Zipkin 是 Twitter 开源的调用链分析工具，目前基于 spring-cloud-sleuth 得到了广泛的使用，特点是轻量，使用、部署简单。 Pinpoint：是韩国人开源的基于字节码注入的调用链分析，以及应用监控分析工具。特点是支持多种插件，UI 功能强大，接入端无代码侵入。 SkyWalking：是本土开源的基于字节码注入的调用链分析，以及应用监控分析工具。特点是支持多种插件，UI 功能较强，接入端无代码侵入。目前已加入 Apache 孵化器。 CAT：CAT 是美团点评开源的基于编码和配置的调用链分析，应用监控分析，日志采集，监控报警等一系列的监控平台工具。 负载均衡 Nginx 可以作为四层或七层负载均衡器。 LVS 可以作为四层负载均衡器。其负载均衡的性能要优于 Nginx。 HAProxy 可以作为 HTTP 和 TCP 负载均衡器。 F5 作为硬件负载均衡 A10 作为硬件负载均衡 网关 Kong 是一个云原生、快速、可扩展和分布式的微服务抽象层（也称为 API 网关，API 中间件）。 Zuul 是 Netflix 开源的一个 API 网关，Zuul 在云平台上提供动态路由，监控，弹性，安全等边缘服务的框架。 告警：短信、邮件、企业聊天软件、OA 参考资料 # 【Youtube 视频】What is DevOps? - In Simple English 【Youtube 视频】DevOps In 5 Minutes DevOps: Breaking the development-operations barrier ","date":"12 October 2023","permalink":"/posts/devoops/overview/","section":"博客","summary":"DevOps 集文化理念、实践和工具于一身，它强调团队授权、跨团队沟通和协作以及技术自动化，其最终目标是优化质量和交付","title":"DevOps 简介"},{"content":"数组 # 【LeetCode 135】 分发糖果 【LeetCode 6】Z 字形变换 动态规划 # 【LeetCode 55】跳跃游戏 【LeetCode 72】编辑距离 【LeetCode 115】不同的子序列 【LeetCode 124】二叉树中的最大路径和 【LeetCode 174】地下城游戏 【LeetCode 188】买卖股票的最佳时机IV 【LeetCode 198】打家劫舍 【LeetCode 213】打家劫舍II 【LeetCode 233】数字1的个数 【LeetCode 300】最长递增子序列 【LeetCode 309】最佳买卖股票时机含冷冻期 【LeetCode 312】戳气球 【LeetCode 337】打家劫舍III 【LeetCode 354】俄罗斯套娃信封问题 【LeetCode 368】最大整除子集 【LeetCode 376】摆动序列 【LeetCode 390】消除游戏 【LeetCode 446】等差数列划分 II - 子序列 【LeetCode 583】两个字符串的删除操作 【LeetCode 629】K个逆序对数组 【LeetCode 673】最长递增子序列的个数 【LeetCode 689】三个无重叠子数组的最大和 【LeetCode 714】买卖股票的最佳时机含手续费 【LeetCode 740】删除并获得点数 【LeetCode 907】子数组的最小值之和 【LeetCode 943】最短超级串 【LeetCode 978】最长湍流子数组 【LeetCode 1031】两个非重叠子数组的最大和 【LeetCode 1035】不相交的线 【LeetCode 1039】多边形三角剖分的最低得分 【LeetCode 1143】最长公共子序列 【LeetCode 1186】删除一次得到子数组最大和 【LeetCode 1218】最长定差子序列 【LeetCode 1473】粉刷房子 III 【LeetCode 1713】得到子序列的最少操作次数 【LeetCode 系列】买卖股票的最佳时机 【LeetCode 面试题 08.11】硬币 贪心算法 # 【LeetCode 55】跳跃游戏 【LeetCode 121】买卖股票的最佳时机 【LeetCode 122】买卖股票的最佳时机II 【LeetCode 123】买卖股票的最佳时机III 【LeetCode 42】接雨水 【LeetCode 135】分发糖果 【LeetCode 229】多数元素 II 【LeetCode 330】按要求补齐数组 【LeetCode 376】摆动序列 【LeetCode 495】提莫攻击 【LeetCode 556】下一个更大元素III 【LeetCode 861】反转矩阵后的得分 【LeetCode 926】将字符串翻转到单调递增 【LeetCode 927】三等分 【LeetCode 1053】交换一次的先前排列 【LeetCode 1111】有效括号的嵌套深度 数学技巧 # 【LeetCode 69】x的平方根 【LeetCode 233】数字1的个数 【LeetCode 319】灯泡开关 【LeetCode 357】计算各个位数不同的数字个数 【LeetCode 470】用Rand7()实现Rand10() 【LeetCode 523】连续的子数组和 【LeetCode 672】灯泡开关II 【LeetCode 829】连续整数求和 【LeetCode 1006】笨阶乘 【LeetCode 1227】飞机座位分配概率 【LeetCode 1250】检查「好数组」 【LeetCode 1363】形成三的最大倍数 滑动窗口 # 【LeetCode 3】无重复字符的最长子串 【LeetCode 16】最接近的三数之和 【LeetCode 42】接雨水 【LeetCode 76】最小覆盖子串 【LeetCode 88】合并两个有序数组 【LeetCode 287】寻找重复数 【LeetCode 328】奇偶链表 【LeetCode 438】找到字符串中所有字母异位词 【LeetCode 567】字符串的排列 【LeetCode 658】找到K个最接近的元素 【LeetCode 881】救生艇 【LeetCode 992】K个不同整数的子数组 【LeetCode 1004】最大连续1的个数III 【LeetCode 1248】统计优美子数组 哈希算法 # 【LeetCode 41】缺失的第一个正数 【LeetCode 128】最长连续序列 【LeetCode 825】适龄的朋友 【LeetCode 846】一手顺子 【LeetCode 992】K个不同整数的子数组 【LeetCode 1248】统计优美子数组 二分算法 # 【LeetCode 153】寻找旋转排序数组中的最小值 【LeetCode 287】寻找重复数 【LeetCode 556】下一个更大元素III 【LeetCode 658】找到K个最接近的元素 【LeetCode 668】乘法表中第k小的数 【LeetCode 719】找出第k小的距离对 【LeetCode 825】适龄的朋友 【LeetCode 875】爱吃香蕉的珂珂 单调栈（队列） # 【LeetCode 42】接雨水 【LeetCode 135】分发糖果 【LeetCode 239】滑动窗口最大值 【LeetCode 503】下一个更大元素II 【LeetCode 907】子数组的最小值之和 位运算 # 【LeetCode 137】只出现一次的数字II 【LeetCode 672】灯泡开关II 【LeetCode 810】黑板异或游戏 【LeetCode 861】翻转矩阵后的得分 【LeetCode 面试题 17.19】消失的两个数字II 二叉树 # 【LeetCode 99】恢复二叉搜索树 【LeetCode 124】二叉树中的最大路径和 【LeetCode 199】二叉树的右视图 【LeetCode 337】打家劫舍III 【LeetCode 面试题 04.06】后继者 并查集 # 【LeetCode 128】最长连续序列 【LeetCode 684】冗余连接 【LeetCode 685】冗余连接II ","date":"12 October 2023","permalink":"/posts/reviews/%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7/leetcode/","section":"博客","summary":"Leetcode刷题记录","title":"Leetcode"},{"content":"DSL 和 DSL 工具的一个重要方面是代码生成。DSL 本身在形式化、指定和交流内容方面具有优势，因为它们具有特定领域的性质。但是，如果能从指定的内容中推导出实现代码，就能大大提高工作效率。\nResources # blogs https://www.typefox.io/blog/code-generation-for-langium-based-dsls/ https://www.typefox.io/blog/code-generation-for-langium-based-dsls-2 https://www.typefox.io/blog/code-generation-for-langium-based-dsls-3/ github repo: https://github.com/TypeFox/langium-in-browser-codegen-example/tree/main https://github.com/eclipse-langium/langium/blob/main/examples/arithmetics 运行示例 # 本帖中的运行示例使用 Langium 的 Arithmetics 示例实现。Arithmetics 的 grammar 见 arithmetics.langium\n代码生成器的输入示例如下：\nMODULE priceCalculator DEF materialPerUnit: 100; DEF laborPerUnit: 200; DEF costPerUnit: materialPerUnit + laborPerUnit; DEF expectedNoOfSales: 200; DEF costOfGoodsSold: expectedNoOfSales * costPerUnit; DEF generalExpensesAndSales: 10000; DEF desiredProfitPerUnit: 50; DEF netPrice: (costOfGoodsSold + generalExpensesAndSales) / expectedNoOfSales + desiredProfitPerUnit; DEF vat: 0.15; DEF calcGrossListPrice(net, tax): net / (1 - tax); calcGrossListPrice(netPrice, vat); 本模块介绍一种非常简单的产品价格计算方法。它包括给变量分配常量值和计算值。最后，一个名为 calcGrossListPrice 的函数被调用，参数是之前定义的 netPrice 和 tax。下图展示了 Langium 在解析输入时创建的抽象语法树（AST）。\n现在，让我们将其转化为纯 JavaScript 代码。为了合成所需的代码段，生成器需要访问 AST 并检查相应的部分。让我们通过一个纯 JavaScript 模板来定义生成器的入口函数，如下所示，它会贡献一些静态框架代码：\nfunction generateModule(root: Module): string { return ` ········\u0026#34;use strict\u0026#34;; ········(() =\u0026gt; { ········ ${generateModuleContent(root)} ········}) `; } 让我们也定义 generateModuleContent(Module) 并按如下方式实现它，由于需要循环，所以这次使用经典的字符串连接：\nfunction generateModuleContent(module: Module): string { let result = `let ${lastComputableExpressionValueVarName};\\n`; for (const s of module.statements) { result += generateStatement(s) + \u0026#39;\\n\u0026#39;; } result += `\\n` result += `return ${lastComputableExpressionValueVarName};`; return result; } 问题 1：对于多行模板文字，生成的代码将包含由 ········ 在 generateModule() 中指示的空白。我添加了空白，以使生成器符合我们的格式规则。\n缺点：会使生成结果变得混乱。\n问题 2：访问列表时，我们必须在每个语句的生成片段后插入换行符。此外，我们还必须注意 for 循环前后的换行符。最后，还有 \\n 与 \\r\\n 的问题。\n虽然这个问题在这里很简单，但如果出现有条件附加的代码段或者连续多个循环，就会变得相当困难。\n问题 3：generateModuleContent() 中的字符串连接没有注意 generateModule() 中调用该函数之前的缩进。\n生成的代码将如下所示，具体取决于 generateStatement() 的实现：\n········\u0026#34;use strict\u0026#34;; ········(() =\u0026gt; { ········ let lastComputableExpressionValue; const materialPerUnit = lastComputableExpressionValue = 100; const laborPerUnit = lastComputableExpressionValue = 200; . . . return lastComputableExpressionValue; ········}) .... 这个示例很好地说明了生成代码中的缩进是如何出错的。周围的静态代码缩进了，但不应该缩进，而括弧中的语句没有缩进，但应该缩进。\nSolution A: Smart tagged templates # Solution A：Langium 提供了一个名为 expandToString 的标签函数，可智能处理空白。\n在 generateModule(Module) 第 2 行的开头回车之前直接插入 expandToString 引用，可将后续模板转换为标记模板，请参见 generateModule2(Module)：\nimport { expandToString } from \u0026#39;langium\u0026#39;; function generateModule2(root: Module): string { return expandToString` ········\u0026#34;use strict\u0026#34;; ········(() =\u0026gt; { ········ ${generateModuleContent(root)} ········}) `; } 这样就得到了下面的生成结果：\n\u0026#34;use strict\u0026#34;; (() =\u0026gt; { let lastComputableExpressionValue; const materialPerUnit = lastComputableExpressionValue = 100; const laborPerUnit = lastComputableExpressionValue = 200; . . . return lastComputableExpressionValue; }) expandToString 实现以下这些功能：\n在模板的所有非空行中识别和修剪共同的前导空格 确定用 ${} 包装的表达式的偏移量 修剪 single leading and trailing line breaks 合并模板内的换行符 因此，\n功能 1 删除了生成模块 2(Module) 中由 ········ 表示的空白，这使得静态代码从偏移量 0 开始，即生成时没有任何缩进。 功能 2 将 ${generateModuleContent(root)} 行内的额外缩进 (␣␣) 应用到替换字符串中的每一行。在我们的示例中，这将产生正确缩进的语句实现片段，而缩进只需指定一次。 功能 3 丢弃了紧随开头回车符之后的初始换行符，以及包括结尾回车符缩进在内的尾部换行符。这与生成器入口函数（如 generateModule2(Module)）关系不大，但与从其他标记模板（如 generateModuleContent(Module)）中调用的生成器函数（如 generateModuleContent(Module)）非常相关，因为周围的换行符将由调用模板决定。最后但并非最不重要的一点是， 功能 4 使所有换行符都与系统换行符一致。这一点非常可取，因为生成的代码通常会被持久化到磁盘上，并希望与平台保持一致。 现在，让我们再来看看 generateModuleContent(Module) 模块：\nfunction generateModuleContent(module: Module): string { let result = `let ${lastComputableExpressionValueVarName};\\n`; for (const s of module.statements) { result += generateStatement(s) + \u0026#39;\\n\u0026#39;; } result += `\\n` result += `return ${lastComputableExpressionValueVarName};`; return result; } 将循环重写为 map;join 表达式后，我们就可以使用标记模板和 expandToString 来实现字符串连接，如下所示：\nfunction generateModuleContent2(module: Module): string { return expandToString` let ${lastComputableExpressionValueVarName}; ${ module.statements.map(generateStatement).join(\u0026#39;\\n\u0026#39;) } return ${lastComputableExpressionValueVarName}; `; } 连接操作中的分隔符会被功能 4 expandToString 处理，如果在 MS Windows 机器上执行，它会用 \\r\\n 替换单个 \\n。\n我们上面的价格计算示例的整个输出结果可能如下，我在这里跳过了缺失的生成器部分。\n\u0026#34;use strict\u0026#34;; (() =\u0026gt; { let lastComputableExpressionValue; const materialPerUnit = lastComputableExpressionValue = 100; const laborPerUnit = lastComputableExpressionValue = 200; const expectedNoOfSales = lastComputableExpressionValue = 200; const costPerUnit = lastComputableExpressionValue = materialPerUnit + laborPerUnit; const costOfGoodsSold = lastComputableExpressionValue = expectedNoOfSales * costPerUnit; const generalExpensesAndSales = lastComputableExpressionValue = 10000; const desiredProfitPerUnit = lastComputableExpressionValue = 50; const netPrice = lastComputableExpressionValue = ((costOfGoodsSold + generalExpensesAndSales) / expectedNoOfSales) + desiredProfitPerUnit; const vat = lastComputableExpressionValue = 0.15; const calcGrossListPrice = (net, tax) =\u0026gt; net / (1 - tax); lastComputableExpressionValue = calcGrossListPrice( netPrice, vat ); return lastComputableExpressionValue; }) 除了普通关键字、标识符和运算符的连接外，我的生成器还插入了典型的括号复合表达式，比如在计算 netPrice 的值时。此外，像 calcGrossListPrice 这样的函数调用会在多行中生成，从而使参数更易于阅读。\n结论：如果我们想使用 JavaScript 模板表达式而不是普通的字符串连接来实现代码生成器，如果我们想获得正确格式化的生成代码以及正确格式化的模板，那么 expandToString 将为我们提供极大的帮助。\n备注：重要的是要保持模板行缩进一致，特别是不要混合使用制表符和空格！VS 代码提供了一个显示空白字符的便捷选项，名为 Toggle Render Whitespace。\nSolution B: two stage code generation # 试想一下，如果某些行后没有添加内容，您希望跳过这些行的换行符。试想一下，您需要对代码片段的缩进进行配置，或者需要对生成的代码进行后处理和调整，以满足特定条件。在生成 Java 或 JavaScript 等语言时，可以考虑添加导入子句，同时在代码中添加符号引用。生成丰富的表达式语法也可能需要比纯字符串更多的抽象。最后但并非最不重要的一点是，我们可能希望将生成的代码段与它们在文本中代表的源定义区域关联起来。这样的要求需要一种不同的方法。\n在本部分中，将重点介绍两阶段代码生成方法，并展示如何将其与 Solution A 中使用的 Tagged Templates 整合在一起。\nGeneration tree # 要满足上述要求，一种可行的方法是将生成任务一分为二，并使用比字符串更具表现力的数据结构来捕获中间结果。任务 1 建立待生成代码的描述，任务 2 则渲染所需的输出结果。\n在我们的日常实践中，事实证明树状数据结构非常有用。我们定义了以下数据类型的联盟，并将其称为 Generated 类型：\ntype Generated = string | GeneratorNode | undefined; type GeneratorNode = CompositeGeneratorNode | IndentNode | NewLineNode; 生成任务 1 的结果可能已经是字符串类型，例如，如果结果非常短。通常，它的类型是 GeneratorNode。此外，它还可能是 undefined 的。这在顶层没有太大意义，但在将模板的部分内容转移到子例程时却非常有用。未定义的可能结果允许这些函数向其调用者发出信号，表明该函数不会生成任何东西，这与空字符串等其他东西不同。\nCompositeGeneratorNode 实现了复合设计模式。该类型的实例是容器，可容纳一系列其他字符串和生成器节点。IndentNode 是 CompositeGeneratorNode 的特化，提供缩进信息。NewLineNode 的实例用于描述换行，它们的严格程度是可参数化的。\n在早期的 Langium 中，我们通过以编程方式合成生成器描述来构建代码生成器，例如 Langium CLI 中包含的描述。这样一来，代码生成器的实现就会被大量的 node.append(...) 或 node.children.push(...) 指令所支配，而所需生成的代码结构很快就会被混淆。\n通过 tagged templates # 在 Langium v1.0 中，发布了另一个名为 expandToNode 的标签函数，也就是我们的解决方案 B。请回顾算术语言示例中的 generateModule2 示例：\nfunction generateModule2(root: Module): string { return expandToString` ········\u0026#34;use strict\u0026#34;; ········(() =\u0026gt; { ········ ${generateModuleContent(root)} ········}) `; } 将标签函数替换为 expandToNode 并将返回类型更改为 Generated，就可以轻松将其转换为两阶段生成。\nfunction generateModule3(root: Module): Generated { return expandToNode` ········\u0026#34;use strict\u0026#34;; ········(() =\u0026gt; { ········ ${generateModuleContent2(root)} ········}) `; } 与 expandToString 一样，模板中会自动删除 ········ 所指示的缩进。此外，还省略了开头 \\n 后的初始换行，以及结尾 \\n 前的换行和随后的空白。\n然后，必须将 generateModule3(Module) 的结果转换为字符串，这就是我上文提到的生成任务 2。为此，Langium 提供了名为 toString(unknown) 的函数。如果调用 toString 时使用了 GeneratorNode 类型的参数，它就会将该参数转换为字符串，否则就会委托 JavaScript 的默认字符串构造函数来处理。\n现在让我们看看 generateModuleContent2(Module) 的实现，这也是上次的内容：\nfunction generateModuleContent2(module: Module): string { return expandToString` let ${lastComputableExpressionValueVarName}; ${ module.statements.map(generateStatement).join(\u0026#39;\\n\u0026#39;) } return ${lastComputableExpressionValueVarName}; `; } 同样，我替换了上面的标记函数和返回类型。不过，我们并不想立即将语句元素的生成结果连接成一个字符串。相反，我们想为每个元素创建生成描述，并将其包含在该模板的结果中。为此，Langium 提供了 joinToNode() 函数。该函数的使用方法将在 generateModuleContent3(Module) 中进行说明：\nfunction generateModuleContent3(module: Module): Generated { return expandToNode` let ${lastComputableExpressionValueVarName}; ${ joinToNode(module.statements, generateStatement, { appendNewLineIfNotEmpty: true }) } return ${lastComputableExpressionValueVarName}; `; } joinToNode 的第一个参数是一个要访问的元素集合、一个为每个元素创建生成描述的函数，以及一个可选的配置对象，用于确定分隔符或注册其他回调（如 element filter 和 prefix/suffix 提供程序）。如果输入集合为空，或者所有元素都没有生成，joinToNode 也不会返回任何结果，实际上用 undefined 来表示。\n为什么要区分 undefined ？ # tl;dr：expandToNode 可以将换行符配置为可省略。如果某行的最后一个替换是未定义的或 GeneratorNode 类型的对象，它就会这样做。如果该行的剩余部分只包含空白字符，则整行将被省略，同时呈现所需的输出结果。\nlet lastComputableExpressionValueVarName return lastComputableExpressionValueVarName; 调用 joinToNode(\u0026hellip;) 没有任何结果。不过，它的尾部换行符会被附加到生成的代码中，并产生第一个空行。然后，我们在模板中请求的空行也会被附加到生成的代码中，这样就连续生成了两行空行。不过，我个人（也许你也一样）更倾向于省略包含 joinToNode(\u0026hellip;) 调用的整行，即忽略替换后的换行。为了实现这一首选行为，expandToNode 会检查每一行是否有占位符/替换。如果包含替换，则按以下方式评估最后一个替换的值：\n如果替换值未定义或属于 GeneratorNode 类型，则配置该行的终端 NewLineNode，使其仅在前一行为非空时才显示为换行符。否则，配置 NewLineNode 为无条件换行。\n在我们的例子中，generateModuleContent3(Module) 的语句列表为空，这意味着我们将在第 1 行末尾得到一个换行符，因为该行至少包含静态字符串 let，即非空字符串。准确地说，无论其配置如何，添加到生成描述中的 NewLineNode 都会导致换行。第 2 行的占位符将解析为 undefined 的 \u0026ldquo;值\u0026rdquo;。因此，随后代表第 2 行末尾换行符的 NewLineNode 将被标记为 ifNotEmpty，如上所述。在稍后的字符串呈现过程中（任务 2），第 2 行将被评估为空，从而使结束符 NewLineNode 呈现为空。\n第 3 行仅包含一个换行符（不包含任何替换），并导致在生成描述中无条件添加一个 NewLineNode。第 4 行要求在生成说明中添加 return- 以及 lastComputableExpressionValueVarName 内容的字符串值。由于模板将在下一行关闭，因此结束符将被忽略。\n这种方法还允许对仅包含空白和可能导致 undefined 的替换的行强制执行无条件换行。只需将 ??'' 到（最后一个）替换内容中，或者在行尾再添加一个类似 ${''} 的替换。expandToNode 就会插入一个无条件的 NewLineNode。顺便说一下：后一个选项也适用于包含可能为空的 CompositeGeneratorNodes 的替换。\nBenefits # 函数 expandToNode 返回 CompositeGeneratorNode 的一个实例，代表某段文本的生成描述。此类对象可任意组合，也可随意操作。元素可以添加、删除或改变顺序。此外，由于复合生成器节点（CompositeGeneratorNode）所描述的某些文本片段的具体缩进最终是在其跨容器（任务 2）的文本渲染时确定的，因此父节点和某些子节点的创建和组合可能完全独立于彼此。一个子节点甚至可能包含在同一生成描述中不同缩进级别的不同位置。此外，在要连接的字符串模板或表达式中，不再需要硬编码的换行符。\n此外，生成器实现可以在基于标记模板的实现风格和基于普通方法调用的风格之间来回切换，这取决于哪种风格最适合。由于 CompositeGeneratorNode 定义了更多的方便方法，因此这两者之间的界限并不明显。下面将提到其中一些方法，有关它们的精确定义，请参阅 Langium 代码库：\nappend(\u0026hellip;Generated[]) appendNewLine() appendNewLineIfNotEmpty() appendIf(boolean, \u0026hellip;Generated[]) appendTemplate\u0026lt;template content\u0026gt; appendTemplateIf(boolean)\u0026lt;template content\u0026gt; indent(Generated[]) … 在某些情况下，这种方式可能更好。\nfunction generateModuleContent3(module: Module): Generated { return expandToNode` let ${lastComputableExpressionValueVarName}; `.appendNewLine() .appendIf(module.statements.length !== 0, joinToNode(module.statements, generateStatement, { appendNewLineIfNotEmpty: true }) ).appendTemplate` return ${lastComputableExpressionValueVarName}; `; } The avigation between DSL source and generated code # 在 Solution A 和 Solution B 中，已经使 TypeScript 和 JavaScript 中的代码生成变得简单且可扩展，现在是时候来讨论一些实际问题了，即如何处理生成的代码，而不是纯粹的字符串段连接。\n这包括在集成开发环境中导航生成的工件及其相应的源代码（例如，用于手动审查），以及在调试生成的代码时自动切换到基于 DSL 的源代码。为了在基于 DSL 的开发工具中启用这些功能，代码生成器需要收集数据，了解哪些源定义生成了哪些代码。\n用下面的截图来说明 DSL 源代码和生成代码之间的来回导航。DSL 工具的用户可能想了解代码生成器为某个专用语句生成了什么代码。DSL 开发工具可能会提供这样的审查工具，例如通过选择敏感的上下文菜单项，如第一张截图所示。当然，也可以进行其他集成：在生成的工件中，可能有多个地方会受到某个语句或定义的影响。\n另一方面，用户可能希望或需要调查为什么生成器会将某些语句放入生成的工件中，即源代码中的哪些定义。如第二张截图所示，如果有机会让开发工具说明生成代码中某些语句的原因或来源，可能会简化此类任务。\n除了这类静态代码分析外，还希望运行生成的代码，在某个入口点设置断点，并通过逐步浏览 DSL 编写的源代码来逐步实现，如下图所示。\n在这里，一个装有 Langium Arithmetics 示例语言的 Monaco editor 被添加到了一个普通网站上，并输入了在 Solution A 中介绍的正在运行的示例脚本。基于 Langium 的语言服务器已经处理了输入，确定没有验证错误，并调用了生成器。然后对获得的 JavaScript 代码和相应的源映射进行评估。源映射是根据 JavaScript 代码生成过程中捕获的跟踪数据创建的。\n获取追踪数据 # 为了实现上述功能，我们需要捕获跟踪数据，将源数据中的相关文本区域与 generated artifacts 中的相应文本区域关联起来。在此，我们假定源数据是以人类可读文本的形式（通常是根据某种 DSL）编制的，并保存在 disc 上的文件中（至少与某个 URI 相关联），而 generated artifacts 则假定由 a stream of characters 组成。\n回顾本系列的第二部分，我们将代码生成任务分为两项：\nthe composition of a generation description the rendering of the description into the desired text 引入了一种树形数据结构来捕获描述，它由几种不同的数据类型组成，这些数据类型都归属于联合类型 GeneratorNode。既然已经引入了这样一种专用数据结构，就可以根据自己的喜好为这些数据添加额外的信息。还记得上次的模板标签函数 expandToNode，它在任务（1）中为给定的 JavaScript 模板文字建立了 GeneratorNode 实例，以及生成器函数 generateModuleContent3(Module)：\nfunction generateModuleContent3(module: Module): Generated { return expandToNode` let ${lastComputableExpressionValueVarName}; ${ joinToNode(module.statements, generateStatement, { appendNewLineIfNotEmpty: true }) } return ${lastComputableExpressionValueVarName}; `; } 无论所提供的模块中定义了哪些语句，所包含的模板都会生成静态代码，而生成的输出则代表模块所包含的语句。这些语句的生成由函数 generateStatement(Statement) 完成，该函数提供给第 4 行的 joinToNode(\u0026hellip;) 调用。因此，模板第 3 行、第 5 行和第 6 行的内容只能与 module 相关联，因为这是它们被添加到输出中的原因。与此相反，generateStatement(Statement) 产生的输出可以与 module 关联，因为这些语句包含在 module 中，但更具体地说，它们应该与 module.statements 中包含的相应 Statement 实例关联。为了实现这两个目的，Langium 提供了以下函数：\nexpandTracedToNode\u0026lt;T extends AstNode\u0026gt;(T, Properties\u0026lt;T\u0026gt;?, number?) joinTracedToNode\u0026lt;T extends AstNode\u0026gt;(T, Properties\u0026lt;T\u0026gt;?) 我们可以使用这些函数捕获所需的跟踪数据，并重写 generateModuleContent3 如下：\nfunction generateModuleContent4(module: Module): Generated { return expandTracedToNode(module)` let ${lastComputableExpressionValueVarName}; ${ joinTracedToNode(module, \u0026#39;statements\u0026#39;)(module.statements, generateStatement, { appendNewLineIfNotEmpty: true }) } return ${lastComputableExpressionValueVarName}; `; } 请注意，这两个函数都会再次返回函数。返回函数的签名与 expandToNode 和 joinToNode 的签名完全一致。因此，expandTracedToNode(module) 的结果是一个将模板字面意义转换为标记模板的标记函数。它在内部委托给 expandToNode，并在组成的 GeneratorNode 中注释了模块是相应源对象的信息。\n同样的原理也适用于 joinTracedToNode(模块, \u0026lsquo;语句\u0026rsquo;)。它返回一个与 joinToNode(\u0026hellip;) 接口相同的函数。第 4 行中对 generateModuleContent4(Module) 的调用是指：对 module.statements 中的每个元素应用 generateStatement(Statement)，为生成的每个 GeneratorNode 注释跟踪信息，说明生成的部分代表父对象模块中名为 statements 的属性（集合）的第 i 个元素，将所有这些生成器节点添加到一个容器 GeneratorNode 中，并为该容器注释信息，说明生成的部分代表源对象模块中 statements 属性的全部内容。\n追踪数据剖析 # Langium 会在生成任务（2）中对跟踪数据进行评估和计算。在这种情况下，函数 toStringAndTrace(GeneratorNode) 将取代 Langium 的 toString(unknown)。它返回一个形状为 { text: string, trace：traceRegion }，其中 text 是希望生成的文本，trace 是描述嵌套跟踪区域的复合结构，将生成文本中的区域与源文件中的区域关联起来。数据类型 TraceRegion 的定义如下：\ninterface TraceRegion { sourceRegion?: TextRegion; targetRegion: TextRegion; children?: TraceRegion[]; } interface TextRegion { fileURI?: string; offset: number; end: number; length?: number; range?: Range; } 假定源数据是 Langium 通过解析 DSL 语法表述的文本而创建的有效 AST 元素，AstNode 实例就会被注释为代表相应具体语法节点的对象。后者反过来又提供了其 DSL 文档中的起始和结束位置以及文档的文件 URI。通过这些信息，toStringAndTrace(GeneratorNode) 计算出生成器节点的源区域。在文本渲染过程中，通过记录生成器节点生成文本的开始和结束位置，计算出相应的目标区域。此时，目标 TextRegion 的 fileURI 属性永远不会被设置，因为此时还不知道生成的文本是否会被写入某个文件，如果是，文件的 URI 可能是什么。\n让我们来看看以下输入和相应输出的简化示例：\nModule priceCalculator DEF materialPerUnit: 100; DEF laborPerUnit: 200; let lastComputableExpressionValue; const materialPerUnit = lastComputableExpressionValue = 100; const laborPerUnit = lastComputableExpressionValue = 200; return lastComputableExpressionValue; 下面的截图中展示了所得到的轨迹区域：\n玫瑰色背景所限定的区域代表跟踪所描述的根跟踪区域，该区域来自 generateModuleContent4(module) 所返回的生成器节点。 淡黄色矩形表示的区域来自 generateModuleContent4(Module) 。第 4 行中调用 joinTracedToNode(\u0026hellip;)(\u0026hellip;) 生成器节点生成的区域。源区域等于对象模块属性 \u0026ldquo;语句 \u0026ldquo;中所有元素定义的 \u0026ldquo;边界框\u0026rdquo;，目标区域等于第 4 行中 joinTracedToNode(\u0026hellip;)(\u0026hellip;) 调用 generateStatement(Statement) 的结果所描述的所有文本片段的 \u0026ldquo;边界框\u0026rdquo;，加上 { appendNewLineIfNotEmpty: true } 所要求的插入分隔线。包含这些源文本区域和目标文本区域描述的 TraceRegion 实例可通过根跟踪对象的子属性（即 trace.children[0]）访问。 蓝色背景区域表示跟踪区域，包括对象模块属性 \u0026ldquo;statements \u0026ldquo;条目 0 定义所涉及的源文本区域，以及执行 generateStatement(module.statements[0])后返回的生成器节点所描述的目标区域。跟踪区域描述对象可通过 trace.children[0].children[0] 访问。同样的情况也适用于绿色背景区域，但它们表示 module.statements 的条目 1 的定义和生成文本。该跟踪区域描述可通过 trace.children[0].children[1] 访问。 实际上，这种深度的跟踪数据捕获并不是终点。如果我们继续将 expandTracedToNode(\u0026hellip;) 应用于在 generateStatement(Module) 中要区分的所有特殊情况，我们就会得到完全深度解析和细粒度的跟踪区域，直至每个标识符、运算符和数字字面。\n将跟踪数据转换为 JavaScript 源映射 # 如今的浏览器和 VS Code 都支持源映射的概念，以便于调试已编译、转译或最小化的代码。源映射可以作为单独文件附加到生产的 JavaScript 代码中，甚至可以内联到生产的代码中（这通常会大大增加要传输的代码）。因此，为了实现能够调试用算术 DSL 编写的脚本这一目标，我们不仅需要捕获跟踪信息，还需要使用它们来合成符合源映射格式的数据。好消息是我们不需要完全靠自己。https://npmjs.com 上发布的 source-map 软件包可以帮我们完成大部分工作。\n在 langium-in-browser-codegen-example GitHub 代码库中，我实现了源地图数据的组合，并将其内联到生成的 JavaScript 代码中。如果内联，源地图数据必须进行 base64 编码\u0026ndash;这意味着我们基本上没有机会审查我们实际生成的内容。不过，sokra 和其他一些好心人建立了一个工具 https://sokra.github.io/source-map-visualization/。它允许我们上传生成的代码，包括源地图数据（或将源地图数据作为单独文件上传）。下面，我添加了一张所提供的可视化截图。原始页面甚至允许通过将鼠标悬停在某个区域上，观察其对应区域的高亮度（如果有的话），以交互方式查看源区域和目标区域。\n","date":"12 October 2023","permalink":"/posts/language/code-generation/code-generation-for-langium-based-dsls/","section":"博客","summary":"DSL 和 DSL 工具的一个重要方面是代码生成。DSL 本身在形式化、指定和交流内容方面具有优势，因为它们具有特定领域的性质。但是，如果能从指定的内容中推导出实现代码，就能大大提高工作效率。","title":"Code generation for Langium-based DSLs"},{"content":"","date":"12 October 2023","permalink":"/tags/template-engine/","section":"Tags","summary":"","title":"Template Engine"},{"content":"模板引擎 # 模板引擎（也称为模板处理器或模板解析器）是设计用于将模板与数据模型结合起来以生成结果文档的软件，编写模板所用的语言称为模板语言或模板语言。模板引擎通常作为 Web 模板系统或应用程序框架的一部分，也可以用作预处理器或过滤器。流行的模板引擎包括 Ejs、Jade、Pug、Mustache、HandlebarsJS、Jinja2 和 Blade。\n模板引擎如何工作 # 上图说明了模板引擎的所有基本元素和处理流程。\n使用模板引擎构建服务器端应用程序时，模板引擎会将模板文件中的变量替换为实际值，并将此值显示给客户端。这样，我们就能更轻松地快速构建应用程序。\n使用 expressJS 和 ejs 模板引擎的示例 # 对于使用 NodeJS 运行时编写的服务器端应用程序，可以使用模板引擎。\n以下步骤演示了模板引擎如何使用 expressJs 和 ejs 模板引擎工作。下面的示例在网页上渲染用户数据。\n步骤 1：安装 express 和 ejs 模板引擎\n安装 ejs 模板引擎和 express 框架，\nnpm install express ejs 步骤 2：设置视图引擎\nconst express = require(\u0026#34;express\u0026#34;) const app = express(); // Set the View Engine or Template Engine app.set(\u0026#39;view engine\u0026#39;, \u0026#39;ejs\u0026#39;); app.listen(3000) 在上面的代码中，我们创建了 express 应用程序。该应用程序通过 3000 端口监听。\napp.set('view engine', 'ejs'); 告诉我们的 express 应用程序，我们要使用 EJS 作为模板引擎。\n步骤 3：设置视图文件夹\n创建一个名为 view 的文件夹。视图文件夹应包含我们的模板。其中一个模板是 index.ejs，它将生成我们的首页。第二个模板是 user.ejs，用于从服务器端传递用户数据，并立即在网页上呈现。\nindex.js \u0026gt;view index.ejs user.ejs 步骤 4：设置 routes\n让我们为主页和用户页面创建routes。\n请注意下面的 res.render() 方法。这就是在 expressJS 中渲染模板的方法。\napp.get(\u0026#39;/\u0026#39;, function (req, res) { res.render(\u0026#34;index\u0026#34;); }) app.get(\u0026#34;/user\u0026#34;, function(req,res){ const user = { name: \u0026#34;Theodore Kelechukwu O.\u0026#34;, stack: \u0026#34;MERN\u0026#34;, email: \u0026#34;theodoreonyejiaku@gmail.com\u0026#34;, hubby: [\u0026#34;singing\u0026#34;, \u0026#34;playing guitar\u0026#34;, \u0026#34;reading\u0026#34;, \u0026#34;philosoph\u0026#34;] } res.render(\u0026#34;user\u0026#34;, {user}); }) 正如我们所见，访问默认路由\u0026quot;\u0026quot;时，会显示或渲染 index.ejs 页面。同时，\u0026quot;\\user \u0026ldquo;会显示 user.ejs 页面。\n我们将用户对象传递给 render 对象，以便将用户属性传递给网页并进行渲染。\n步骤 5：模板化我们的视图文件\n现在，我们已经从服务器端传递了用户数据，我们需要立即在前端或网页上显示这些数据。\nindex.ejs\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;This is the title\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Welcome to Template Engines\u0026lt;/p\u0026gt; \u0026lt;a href=\u0026#34;/user\u0026#34;\u0026gt;View User\u0026lt;/a\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; user.ejs\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;This is the title\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to User Details\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Name:\u0026lt;/b\u0026gt; \u0026lt;%= user.name %\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Email:\u0026lt;/b\u0026gt; \u0026lt;%= user.email %\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Stack:\u0026lt;/b\u0026gt; \u0026lt;%= user.stack %\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;u\u0026gt;\u0026lt;b\u0026gt;Hubbies\u0026lt;/b\u0026gt;\u0026lt;/u\u0026gt; \u0026lt;% user.hubby.forEach(hubby =\u0026gt;{ %\u0026gt; \u0026lt;li\u0026gt;\u0026lt;%= hubby %\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;% })%\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 注意显示值的 \u0026lt;%= variable %\u0026gt; 模式。这是在 ejs 中使用的方式。还要注意 user.forEach(); 这是为了显示模板引擎有多么强大。\nResources # https://en.wikipedia.org/wiki/Template_processor https://www.educative.io/answers/what-are-template-engines ","date":"12 October 2023","permalink":"/posts/language/code-generation/template-engine/","section":"博客","summary":"模板引擎 # 模板引擎（也称为模板处理器或模板解析器）是设计用于将模板与数据模型结合起来以生成结果文档的软件，编写模板所用的语言称为模板语言或模板语言。模板引擎通常作为 Web 模板系统或应用程序框架的一部分，也可以用作预处理器或过滤器。流行的模板引擎包括 Ejs、Jade、Pug、Mustache、HandlebarsJS、Jinja2 和 Blade。","title":"Template Engine"},{"content":"","date":"12 October 2023","permalink":"/tags/blog/","section":"Tags","summary":"","title":"Blog"},{"content":"今天花了一点时间搭建了自己的GitHub的博客，当然咯，试验阶段总会发生很多乱七八糟的问题，记录下处理问题过程中几个比较 nice 的 blog\nResources # 系列文章，用hugo的PaperMod Theme 建站: https://www.sulvblog.cn/posts/blog/ 系列文章，用hugo的Blowfish Theme 建站: https://blowfish.page/docs/ 博客使用 hugo 和 ox-hugo 写博客，也是用的Blowfish Theme: https://blog.opsnull.com/emacs/write-blog-using-hugo-and-ox-hugo Hugo + GitHub Action，搭建你的博客自动发布系统: https://www.pseudoyu.com/zh/2022/05/29/deploy_your_blog_using_hugo_and_github_action/ PaperMod主题优化： https://kdjlyy.cn/posts/site/hugo-papermod-optimization https://dvel.me/posts/hugo-papermod-config/ 在博客中加入数学公式 MathJax 3全新版本配置指南 官方提供的从mathjax2to3的转换工具 MathJax常见问题 ","date":"12 October 2023","permalink":"/posts/devoops/git/how-to-build-github-blog-with-hugo/","section":"博客","summary":"今天花了一点时间搭建了自己的GitHub的博客，当然咯，试验阶段总会发生很多乱七八糟的问题，记录下处理问题过程中几个比较 nice 的 blog","title":"How to Build Github Blog With Hugo"},{"content":"","date":"12 October 2023","permalink":"/tags/hugo/","section":"Tags","summary":"","title":"Hugo"},{"content":"为什么要用代码生成 # productivity：使用代码生成，只需编写一次 generator ，就可以根据需要多次重复使用。向 generator 提供特定输入并调用它比手动编写代码要快得多，因此代码生成可以节省时间。 Simplification：通过代码生成，你可以从一些抽象的描述中生成代码。需要维护的部分变成了 generator 的输入部分，该部分通常是代码的描述，而不是代码本身，与整个生成的代码相比，该描述通常更容易分析和检查。 Portability：一旦你有了为某种语言或框架生成代码的程序，你就可以简单地更改 generator ，并以不同的语言或框架为目标。您还可以同时针对多个平台。 例如，使用解析器生成器，您可以获得 C#、Java 和 C++ 的 parser。 另一个例子：您可能会编写一个 UML 图表，然后使用代码生成器用 C# 创建一个骨架类，并用 SQL 代码为 MySQL 创建一个数据库。因此，相同的抽象描述可用于生成不同类型的工件。 Consistency：有了代码生成，你总能得到你所期望的代码。生成的代码是根据相同的原则设计的，命名规则等也是一致的。当然，除了生成器中的 bug 之外，代码总是能按照你所期望的方式运行，代码质量始终如一。如果用手工编写代码，不同的开发人员可能会使用不同的风格，即使是最重复的代码也会偶尔出现错误。 为什么不要用代码生成 # Maintenance：当您使用代码生成工具时，您的代码就会依赖于它。代码生成工具必须得到维护。如果你创建了它，你就必须不断更新它；如果你只是使用现有的工具，你就必须希望有人继续维护它，或者你必须自己接手。因此，代码生成的优势并不是免费的。如果你没有或找不到合适的能力来维护代码生成器，风险就会更大。 Complexity：自动生成的代码往往比手工编写的代码更复杂。有时，这与将不同部分连接在一起所需的胶水代码有关，或者与生成器支持的用例多于您所需的用例有关。在第二种情况下，生成的代码可以做比你想要的更多的事情，但这并不一定是一种优势。生成代码的优化程度肯定也不如手工编写的代码。有时这种差异很小，并不明显，但如果您的应用程序需要尽可能地提高性能，那么生成的代码对您来说可能并不是最佳选择。 如何使用代码生成? # 根据具体情况，代码生成既可以提高工作效率，也可以成为开发过程中的重要组成部分。许多现代集成开发环境就是一个有用的例子：只需点击一个按钮，就能创建一个骨架类来实现接口或类似功能。你完全可以自己编写这样的代码，只不过会浪费一些时间来完成琐碎的任务。\n设计代码生成流水线的方法有很多种。基本上，我们需要定义两个要素：\nInput：用于生成代码的信息来自何处。 Output：如何获得生成的代码。 您也可以在输入和输出之间设置转换步骤。这些步骤可以简化输出层，并使输入和输出更加独立。\nPossible Inputs\nA DSL：例如，我们可以使用 ANTLR 来描述一种语言的语法。由此，我们可以生成一个解析器。 code in other formats：数据库模式。根据数据库模式，我们可以生成 DAO。 wizards：它们允许向用户询问信息。 reverse engineering：可通过处理复杂的代码工件获得信息。 data sources：比如一个DB，一个csv文件或者一个电子表格。 Possible Outputs\ntemplate engine：大多数网络程序员都知道模板引擎，它用于在 HTML UI 中填充数据。 code building APIs：例如，Javaparser 可用于以编程方式创建 Java 文件。 Some Pipelines\n现在让我们来检查一些 pipelines：\nparser generation：本网站的读者一定很熟悉 ANTLR 和其他此类从形式语法自动生成解析器的工具。在这种情况下，输入是一个 DSL，输出则是使用 template engine 生成的。 model driven design：集成开发环境或独立集成开发环境的插件，可以描述应用程序的模型，有时还提供图形界面，并据此生成整个应用程序或仅生成其骨架。 database-related code：这种用法可视为模型驱动设计和模板引擎的产物。通常，程序员会定义一个数据库模式，并据此生成整个 CRUD 应用程序或处理数据库的代码。也有一些工具可以执行相反的过程：根据现有数据库创建数据库模式或处理数据库的代码。 meta-programming languages：这些语言组包括可对程序代码进行近乎完全操作的语言，源代码只是另一种可操作的数据结构。 ad hoc applications：这一类包括所有内容：从为处理一件事情而设计的工具到企业环境中使用的临时系统，这些系统可以根据正式的自定义描述生成整个应用程序。这些应用程序通常是特定工作流程的一部分。例如，客户使用图形界面描述一个应用程序，一个临时系统会生成支持该应用程序的数据库模式，另一个系统会生成 CRUD 界面等。 IDE generated code：许多静态类型语言需要编写大量的模板代码，而集成开发环境通常可以生成其中的一部分：为要实现的方法提供存根的类、标准的等值、hashCode 和 toString 方法、所有现有属性的获取器和设置器。 代码生成工具 # 模板引擎 # 模板引擎组 (Template Engine) 可能是最著名和最常用的。模板引擎基本上就是一个能理解简单模板语言的迷你编译器。模板文件包含可由模板引擎解释的特殊符号。它能做的最简单的事情就是用运行时给出的适当数据替换这些特殊符号。大多数模板引擎还支持简单的流程控制命令（如 for 循环、if-else 语句），允许用户描述简单的结构。\n有很多例子，让我们来看两个代表大多数模板引擎行为方式的例子。\nJinja2 # Jinja2 是一个广泛使用的 Python 模板引擎。它能做所有模板引擎都能做的事情：根据提供的数据创建独一无二的文档。 它支持模块化模板、控制流、变量等。不过，它也有强大的安全措施：HTML 转义系统和沙箱环境，可以控制对危险属性的访问。\n\u0026lt;title\u0026gt;{% block title %}{% endblock %}\u0026lt;/title\u0026gt; \u0026lt;ul\u0026gt; {% for user in users %} \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;{{ user.url }}\u0026#34;\u0026gt;{{ user.username }}\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; {% endfor %} \u0026lt;/ul\u0026gt; Jinja2 特别支持生成 HTML 页面，这也是最常用的功能。不过，它也可用于创建其他类型的文件。\nPug # Pug 是一个深受 Haml 影响的高性能模板引擎，使用 JavaScript 实现，适用于 Node.js 和浏览器。在许多方面，Pug 与许多其他模板引擎一样：它支持模块化模板、控制流等。不同的是，Pug 看起来像 DSL，而且只适用于 HTML。因此，Pug 模板看起来非常简洁。\ndoctype html html(lang=\u0026#34;en\u0026#34;) head title= pageTitle script(type=\u0026#39;text/javascript\u0026#39;). if (foo) bar(1 + 5) body h1 Pug - node template engine #container.col if youAreUsingPug p You are amazing else p Get on it! p. Pug is a terse and simple templating language with a strong focus on performance and powerful features. 解析器生成器 # 解析器生成器 (Parser Generation) 是一种自动快速创建语言解析器的工具。它们非常成功且富有成效，因为人们已经对语言解析问题进行了广泛的研究。因此，有一些解决方案可以保证解析人们需要解析的大多数语言。\nANTLR # ANTLR 可能是使用最多的解析器生成器。这意味着有很多示例。然而，庞大社区的真正附加价值在于大量可用的语法。\nANTLR 的输入是语法：对语言的正式描述。解析器的输出是一棵解析树：一种包含源代码的结构，其转换方式便于程序的其他部分使用。ANTLR 还提供了两种走解析树的方法：访问者和监听者。第一种适用于需要对解析树中的元素进行操作或交互的情况，而第二种则适用于只需要在规则匹配时做一些事情的情况。\ngrammar simple; basic : NAME \u0026#39;:\u0026#39; NAME ; NAME : [a-zA-Z]* ; COMMENT : \u0026#39;/*\u0026#39; .*? \u0026#39;*/\u0026#39; -\u0026gt; skip ; 模型驱动设计 # 这些通常是集成开发环境的插件或独立的集成开发环境，可以通过图形界面描述应用程序的模型，并由此生成应用程序的骨架。之所以会出现这种情况，是因为模型驱动设计的基础是抽象模型，可以用 UML 图表或 DSL 来定义。一旦程序的主要特征可以根据模型进行描述，那么就有可能自动生成该程序的表示法。这种代码中的模型表示法会自动生成结构，但行为通常必须由开发人员自己直接实现。\nAcceleo # Acceleo 3 是一款实现 OMG 模型到文本规范的代码生成器。它为开发人员提供了高质量代码生成集成开发环境所应具备的大部分功能：简单的语法、高效的代码生成、先进的工具以及与 JDT 不相上下的功能。Acceleo 可帮助开发人员处理代码生成器的生命周期。得益于基于原型的方法，您可以从现有原型的源代码中快速、轻松地创建第一个生成器，然后利用 Acceleo 工具的所有功能（如重构工具），您可以轻松地改进生成器，实现完整的代码生成器。\nAcceleo 的工作：实施模型驱动设计原则。但缺少的是对 Acceleo 工作体验的描述。Acceleo 基本上是一个 Eclipse 插件，它为您提供了一个工具，可以根据您指定的模板，从 EMF 模型开始创建 Java 代码。EMF 模型可以通过不同方式定义：UML 图表或自定义 DSL。\nUmple # Umple 是一种建模工具和编程语言系列，可实现作者所说的面向模型的编程。它在面向对象编程语言（如 Java、C++、PHP 和 Ruby）中添加了关联、属性和状态机等抽象概念，这些抽象概念源自 UML。Umple 还可用于以文本方式创建 UML 类图和状态图。\nUmple 是一种将 UML 模式与传统编程语言结构化结合的工具。它的诞生是为了简化模型驱动开发的过程，而传统的模型驱动开发需要特定而复杂的工具。它本质上是一种编程语言，支持 UML（类和状态）图定义模型的功能。然后，Umple 代码会被其编译器转换为 Java 或 PHP 等传统语言。\nUmple 可以有多种用法：\n可用于以文本方式描述 UML 图表 可与传统语言结合使用，作为该目标语言的预处理器或扩展程序。Umple 编译器在目标语言中转换 Umple 代码，并保持现有目标语言不变。 由于其对 UML 状态机的大量支持，它可以作为状态机生成器使用。根据 Umple 对状态机的描述，可以生成许多目标语言的实现。 class Student {} class CourseSection {} class Registration { String grade; * -- 1 Student; * -- 1 CourseSection; } 下面的 Umple 代码描述的是一个状态机。\nclass GarageDoor { status { Open { buttonOrObstacle -\u0026gt; Closing; } Closing { buttonOrObstacle -\u0026gt; Opening; reachBottom -\u0026gt; Closed; } Closed { buttonOrObstacle -\u0026gt; Opening; } Opening { buttonOrObstacle -\u0026gt; HalfOpen; reachTop -\u0026gt; Open; } HalfOpen { buttonOrObstacle -\u0026gt; Opening; } } } Telosys # Telosys 设计用于生成所有管道和重复代码。它不需要使用 UML，但允许用户从起始数据库或使用 DSL 生成模型。它是一个有趣且易于使用的解决方案，还以 Eclipse 插件的形式提供 IDE 支持。\nurl：https://tomassetti.me/telosys-code-generation-tool/ 数据库相关代码 # 这一切都围绕着一个数据库模式展开，而代码就是从这个模式中生成的，或者是从一个数据库中生成一个模式。之所以可以使用这些生成器，有两个原因：\n关系数据库支持与之交互的标准语言（SQL） 编程语言中存在与数据库交互的广泛模式和库 这两个原因保证了在编程语言和包含程序所需数据的数据库之间创建标准胶合代码成为可能。在实践中，数据库模式可以作为一个简单的模型，用来生成代码。\n许多框架或集成开发环境都包含从类生成数据库模式的基本工具，反之亦然，生成与数据库表交互的类。在本节中，我们将看到一个可以做更多事情的工具示例。\nCelerio # Celerio 是面向数据应用程序的代码生成工具。\nCelerio 是一款 Java 工具，其中包括一个数据库提取器，用于从现有数据库中获取数据库模式。然后，它将生成的模式与配置文件结合起来，然后启动模板引擎，以创建整个应用程序。提取的数据库模式为 XML 格式。\nDomain Specific Language（DSL） # DSL 是以正规化方式捕捉业务逻辑的好方法。之后，需要以某种方式执行这些逻辑。虽然有时会使用解释器和编译器来执行 DSL，但代码生成器却经常被使用。通过这种方式，DSL 可以被翻译成已经存在编译器的语言，如 Java 或 C#。\n现在，可以使用语言工作台来构建 DSL，语言工作台是专门为设计和实现 DSL 而设计的集成开发环境。语言工作台之所以有用，是因为它们还能以较低的成本为 DSL 定义编辑器和其他支持工具。这一点非常重要，因为非开发人员也可以使用 DSL，他们需要定制的编辑器来利用语言的功能，或者根本无法使用普通的文本编辑器。除其他功能外，语言工作台通常还集成了代码生成功能。让我们来看几个例子。\nJetBrains MPS\nJetBrains MPS 是基于项目编辑器的语言工作台。您可以用它创建一个或多个 DSL。它还可用于扩展现有语言。例如，mbeddr 就是基于 JetBrains MPS 的 C 语言扩展，用于改进嵌入式编程。\n所谓投影式编辑器，是指 MPS 会保留数据的基本结构，并以易于编辑的形式显示出来。这个概念可能有点难以理解。想想传统的编程：你写出源代码，然后编译器将源代码转换为逻辑表示，即解析树。编译器使用这种表示法来执行一些操作，如优化或将其转换为机器代码来执行。使用项目编辑器，您可以直接处理逻辑表示：解析树。不过，您只能按照编辑器（MPS）允许的方式对其进行修改。\n这样做的主要后果是，当使用 JetBrains MPS 创建 DSL 时，您需要整个集成开发环境及其所有功能和功能。您可以获得语法高亮、代码自动补全、项目管理等功能。\n不过，这种方法的优势在于，您可以创建一个使用任何形式的输入来修改代码的 DSL，因此您可以创建一个图形编辑器、一个表格输入，甚至是普通文本。这一优势使得创建非程序员也能使用的 DSL 特别有用。\nXtext # Xtext 是一种语言工作台，构建于 Eclipse 和 Eclipse Modeling Framework 之上。它可用于设计文本 DSL 并为其获取编辑器。 从功能上讲，Xtext 是不同工具（如用于解析的 ANTLR、用于用户界面的 Eclipse 等）的组合，用于生成 DSL。\nJulia # 让我们看看 Julia 中宏的示例，Julia 是一种受 Lisp 启发的语言，它的语法更易于理解。\njulia\u0026gt; macro twostep(arg) println(\u0026#34;I execute at parse time. The argument is: \u0026#34;, arg) return :(println(\u0026#34;I execute at runtime. The argument is: \u0026#34;, $arg)) end @twostep (macro with 1 method) julia\u0026gt; ex = macroexpand( :(@twostep :(1, 2, 3)) ); julia\u0026gt; ex # the macro itself :((println)(\u0026#34;I execute at runtime. The argument is: \u0026#34;, $(Expr(:copyast, :($(QuoteNode(:((1, 2, 3))))))))) julia\u0026gt; eval(ex) # execution of the macro I execute at runtime. The argument is: (1, 2, 3) 可以看出，执行宏和执行宏返回的表达式是两码事。\n这个非常强大的功能可以用于代码生成：你不需要外部工具来创建模板代码，你可以从内部创建。在下面摘自 Julia 文档的示例中，你可以看到它是如何定义一系列新的三元运算符的。\nfor op = (:+, :*, :\u0026amp;, :|, :$) eval(:(($op)(a,b,c) = ($op)(($op)(a,b),c))) end 代码利用已定义的二元运算符定义了这些新的三元运算符：\n在前两个元素之间进行基本的二进制运算 然后在第一个运算结果和第三个元素之间再次进行运算 请注意，Julia 的标准语法与传统语言类似：没有奇怪的括号，表达式正常等。然而，当您使用元编程功能时，您将使用类似 Lisp 的内部语法。\n这只是冰山一角，你可以查阅 Julia 手册，进一步了解元编程的强大功能。\nRacket # 如果你想在元编程方面做得更多，可以使用 Racket，这是一种受 Lisp 和 Scheme（另一种受 Lisp 影响的语言）启发的语言。\nRacket 同时是一种语言和一个平台，它被设计成一种可以定义其他语言的语言。因此，它甚至可以使用比宏更强大的元编程功能。Racket 可以定义全新的语言，改变基本语言的语法。它之所以能做到这一点，基本上是因为它允许你改变解析本身。\nRacket 的传统语法类似 Lisp。\n(define (four p) (define two-p (hc-append p p)) (vc-append two-p two-p)) 你可以改变它，例如，你可以创建一种语言来定义文档：Scribble\n#lang scribble/base @title{On the Cookie-Eating Habits of Mice} If you give a mouse a cookie, he\u0026#39;s going to ask for a glass of milk. 该语言允许您创建 HTML、PDF 等文件。您可以在语言中定义结构，然后生成所需的任何输出。\n这是一个与元编程和 DSL 相匹配的全新层次：您可以使用类似 DSL 的易用界面轻松创建自定义生成器。当目标受众是其他开发人员时，可以采用这种方法。这是因为您虽然获得了一种功能强大的语言，但它仅仅是一种语言而已。如果使用语言工作台，您就可以拥有一整套强大的编辑工具，帮助普通用户使用语言。\nAd-Hoc Applications # 这一类包括所有内容：从为处理一件事情而设计的工具到在企业环境中使用的临时系统，这些系统可以根据正式的自定义描述生成整个应用程序。这些应用程序通常是特定工作流程的一部分。例如，客户使用图形界面描述一个应用程序，一个临时系统会生成支持该应用程序的数据库模式，另一个系统会生成 CRUD 界面等。\n这不是一个正确定义的类别，而是一个总括类别，包括不属于特定组别的所有内容。这意味着这组程序没有标准结构。这也证明了代码生成的多功能性：如果你能创建一个问题模型或描述，那么你就能用代码生成来解决问题。当然，你还必须了解解决一般问题和创建代码生成工具是否有意义，还是直接解决问题更好。\n在本节中，我们将讨论两种工具：CMake 是一款开发工具，而 Yeoman 则是一款脚手架工具。前者主要是生成配置文件：为其他软件提供支持的软件。第二种工具简化了开发人员的工作，提供了一种创建即用项目的方法，可针对特定软件平台、库或需求进行优化。\nCMake # CMake 是一个开源、跨平台的工具系列，用于构建、测试和打包软件。\nCMake 包括三个开发工具，用于帮助开发 C 和 C++。主要工具旨在为不同平台和工具链生成构建文件（即 makefile 和项目文件）。例如，它可以生成 Linux 的 makefile 和 Visual Studio 项目文件。\nCMake 不是编译器。用户以 CMake 格式定义项目结构，然后该工具会生成传统构建过程中使用的普通构建文件。\nCMake 文件看起来像一系列命令/宏，用于为编译器设置选项/标志、链接库、执行自定义命令等。\ncmake_minimum_required(VERSION 2.8) project(\u0026#34;Antlr-cpp-tutorial\u0026#34;) [..] if (NOT WIN32) set(CMAKE_CXX_FLAGS \u0026#34;-Wdeprecated -Wno-attributes\u0026#34; ) endif() [..] if(APPLE) add_custom_command(TARGET antlr4-tutorial POST_BUILD COMMAND ${CMAKE_COMMAND} -E copy_if_different \u0026#34;${PROJECT_SOURCE_DIR}/libs/antlr4-runtime.dylib\u0026#34; $\u0026lt;TARGET_FILE_DIR:antlr4-tutorial\u0026gt;) endif() Yeoman # Yeoman 是一个通用的脚手架系统，可以创建任何类型的应用程序。\n如今，要成为一名优秀的程序员，意味着不仅仅要知道如何编码。你需要了解你所使用的每种工具的最佳实践，并记住每次都要执行它们。编写代码本身就已经很困难了，如果还需要正确编写配置文件和使用正确的项目结构，那就更难了。这就是像 Yeoman 这样的工具的用武之地：它是一款脚手架工具，只需一条命令就能生成一个新项目，并立即实施所有最佳实践。\nYeoman 的核心是一个生成器生态系统，开发人员可以在此基础上构建自己的模板。该工具非常受欢迎，已有数千个模板可供使用。\nYeoman 是一款 JavaScript 应用程序，因此编写生成器只需编写 JavaScript 代码并使用提供的 API 即可。工作流程也非常简单：向用户询问项目信息（如名称），收集配置信息，然后生成项目。\n以下代码展示了生成器的部分示例，用于创建 Yeoman 模板。\nfunction makeGeneratorName(name) { name = _.kebabCase(name); name = name.indexOf(\u0026#39;generator-\u0026#39;) === 0 ? name : \u0026#39;generator-\u0026#39; + name; return name; } module.exports = class extends Generator { initializing() { this.props = {}; } prompting() { return askName( { name: \u0026#39;name\u0026#39;, message: \u0026#39;Your generator name\u0026#39;, default: makeGeneratorName(path.basename(process.cwd())), filter: makeGeneratorName, validate: str =\u0026gt; { return str.length \u0026gt; \u0026#39;generator-\u0026#39;.length; } }, this ).then(props =\u0026gt; { this.props.name = props.name; }); } [..] writing() { const pkg = this.fs.readJSON(this.destinationPath(\u0026#39;package.json\u0026#39;), {}); const generatorGeneratorPkg = require(\u0026#39;../package.json\u0026#39;); [..] this.fs.writeJSON(this.destinationPath(\u0026#39;package.json\u0026#39;), pkg); } conflicts() { this.fs.append(this.destinationPath(\u0026#39;.eslintignore\u0026#39;), \u0026#39;**/templatesn\u0026#39;); } install() { this.installDependencies({ bower: false }); } }; Resources # https://tomassetti.me/code-generation/ ","date":"12 October 2023","permalink":"/posts/language/code-generation/a-guide-to-code-generation/","section":"博客","summary":"为什么要用代码生成 # productivity：使用代码生成，只需编写一次 generator ，就可以根据需要多次重复使用。向 generator 提供特定输入并调用它比手动编写代码要快得多，因此代码生成可以节省时间。 Simplification：通过代码生成，你可以从一些抽象的描述中生成代码。需要维护的部分变成了 generator 的输入部分，该部分通常是代码的描述，而不是代码本身，与整个生成的代码相比，该描述通常更容易分析和检查。 Portability：一旦你有了为某种语言或框架生成代码的程序，你就可以简单地更改 generator ，并以不同的语言或框架为目标。您还可以同时针对多个平台。 例如，使用解析器生成器，您可以获得 C#、Java 和 C++ 的 parser。 另一个例子：您可能会编写一个 UML 图表，然后使用代码生成器用 C# 创建一个骨架类，并用 SQL 代码为 MySQL 创建一个数据库。因此，相同的抽象描述可用于生成不同类型的工件。 Consistency：有了代码生成，你总能得到你所期望的代码。生成的代码是根据相同的原则设计的，命名规则等也是一致的。当然，除了生成器中的 bug 之外，代码总是能按照你所期望的方式运行，代码质量始终如一。如果用手工编写代码，不同的开发人员可能会使用不同的风格，即使是最重复的代码也会偶尔出现错误。 为什么不要用代码生成 # Maintenance：当您使用代码生成工具时，您的代码就会依赖于它。代码生成工具必须得到维护。如果你创建了它，你就必须不断更新它；如果你只是使用现有的工具，你就必须希望有人继续维护它，或者你必须自己接手。因此，代码生成的优势并不是免费的。如果你没有或找不到合适的能力来维护代码生成器，风险就会更大。 Complexity：自动生成的代码往往比手工编写的代码更复杂。有时，这与将不同部分连接在一起所需的胶水代码有关，或者与生成器支持的用例多于您所需的用例有关。在第二种情况下，生成的代码可以做比你想要的更多的事情，但这并不一定是一种优势。生成代码的优化程度肯定也不如手工编写的代码。有时这种差异很小，并不明显，但如果您的应用程序需要尽可能地提高性能，那么生成的代码对您来说可能并不是最佳选择。 如何使用代码生成?","title":"A Guide to Code Generation"},{"content":"","date":"1 October 2023","permalink":"/todo/","section":"WFUing","summary":"","title":"TODO"},{"content":"1005 Spell It Right # 0、题目 # Given a non-negative integer $N$, your task is to compute the sum of all the digits of $N$, and output every digit of the sum in English.\nInput Specification: # Each input file contains one test case. Each case occupies one line which contains an $N$ ($≤10^{100}$).\nOutput Specification: # For each test case, output in one line the digits of the sum in English words. There must be one space between two consecutive words, but no extra space at the end of a line.\nSample Input: # 12345 Sample Output: # one five 1、大致题意 # 给定一个非负整数 $N$ ，计算整数 $N$ 的所有位数的和 $sum$ ，然后用英文输出 $sum$ 每一位对应的英文单词。\n2、基本思路 # 计算整数 $N$（用 $string$ 型存储，方便遍历）的每一位数字的和 $sum$（ $int$型）。+ 输出 $sum$ 每一位数字对应的英文单词，把 $sum$ 转换为 $string$ 型（方便遍历），一一对应输出对应的英文单词。一一对应可以用字符串数组实现，$0-9$ 的下标对应 $0-9$ 的英文单词。 3、解题过程 # 3.1 坑点 0 # 有了思路就飞快地写代码\n#include\u0026lt;iostream\u0026gt; #include\u0026lt;algorithm\u0026gt; using namespace std; string n; string a[10]={\u0026#34;zero\u0026#34;,\u0026#34;one\u0026#34;,\u0026#34;two\u0026#34;,\u0026#34;three\u0026#34;,\u0026#34;four\u0026#34;,\u0026#34;five\u0026#34;,\u0026#34;six\u0026#34;,\u0026#34;seven\u0026#34;,\u0026#34;eight\u0026#34;,\u0026#34;nine\u0026#34;}; int b[1000]; int main(){ cin\u0026gt;\u0026gt;n; int ans=0,c=0; int size=n.size(); for(int i=0;i\u0026lt;size;i++){ ans+=n[i]-\u0026#39;0\u0026#39;; } while(ans\u0026gt;0){ b[c++]=ans%10; ans/=10; } if(c\u0026gt;=1){ cout\u0026lt;\u0026lt;a[b[c-1]]; } for(int i=c-2;i\u0026gt;=0;i--){ cout\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;a[b[i]]; } } 17分，用例2错误。还是用白盒测试边界值法，想特殊值。以下就是 用例2 的坑点\n输入 0 输出 zero 3.2 AC代码 # #include\u0026lt;iostream\u0026gt; #include\u0026lt;algorithm\u0026gt; using namespace std; string n; string a[10]= {\u0026#34;zero\u0026#34;,\u0026#34;one\u0026#34;,\u0026#34;two\u0026#34;,\u0026#34;three\u0026#34;,\u0026#34;four\u0026#34;,\u0026#34;five\u0026#34;,\u0026#34;six\u0026#34;,\u0026#34;seven\u0026#34;,\u0026#34;eight\u0026#34;,\u0026#34;nine\u0026#34;}; int b[1000]; int main() { cin\u0026gt;\u0026gt;n; int ans=0,c=0; int size=n.size(); for(int i=0; i\u0026lt;size; i++) { ans+=n[i]-\u0026#39;0\u0026#39;; } if(ans==0) { cout\u0026lt;\u0026lt;a[0]; } else { while(ans\u0026gt;0) { b[c++]=ans%10; ans/=10; } if(c\u0026gt;=1) { cout\u0026lt;\u0026lt;a[b[c-1]]; } for(int i=c-2; i\u0026gt;=0; i--) { cout\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;a[b[i]]; } } } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1005spellitright/","section":"博客","summary":"1005 Spell It Right # 0、题目 # Given a non-negative integer $N$, your task is to compute the sum of all the digits of $N$, and output every digit of the sum in English.","title":"1005SpellItRight"},{"content":"1006 Sign In and Sign Out # 0、题目 # At the beginning of every day, the first person who signs in the computer room will unlock the door, and the last one who signs out will lock the door. Given the records of signing in’s and out’s, you are supposed to find the ones who have unlocked and locked the door on that day.\nInput Specification: # Each input file contains one test case. Each case contains the records for one day. The case starts with a positive integer M, which is the total number of records, followed by M lines, each in the format:\nID_number Sign_in_time Sign_out_time where times are given in the format HH:MM:SS, and ID_number is a string with no more than 15 characters.\nOutput Specification: # For each test case, output in one line the ID numbers of the persons who have unlocked and locked the door on that day. The two ID numbers must be separated by one space.\nNote: It is guaranteed that the records are consistent. That is, the sign in time must be earlier than the sign out time for each person, and there are no two persons sign in or out at the same moment.\nSample Input: # 3 CS301111 15:30:28 17:00:10 SC3021234 08:00:00 11:25:25 CS301133 21:45:00 21:58:40 Sample Output: # SC3021234 CS301133 1、大致题意 # 输入每个人的名字和进门、出门的时间，要求找到开门（最先进门）和锁门（最后出门）的人。\n2、基本思路 # 实现一个 employee的结构体，里面用 string存储 ID numbers，用 int存储 HH:MM:SS，然后用 sort大法\n3、 解题过程 # 3.1 使用 scanf读入 string # c语言中没有 string类型，直接用 scanf读入 string类型是不正确的。那么如何 使用 scanf读入 string\nscanf是标准输入流，没有缓存区，需要预先分配空间，而 cin是输入流，它使用了缓冲区。如果要使用 scanf读入字符串，那就一定要事先为它申请足够的内存空间。\n#include\u0026lt;iostream\u0026gt; #include\u0026lt;cstdio\u0026gt; #include\u0026lt;cstring\u0026gt; using namespace std; int main(){ string a,b; a.resize(8) ; b.resize(8) ; scanf(\u0026#34;%s%s\u0026#34;,\u0026amp;a[0],\u0026amp;b[0]) ; printf(\u0026#34;%s=%s\\n\u0026#34;,a.c_str(),b.c_str()) ; return 0; } 输入：fgdsfsa 234sfa 输出：fgdsfsa=234sfa 如果要对结构体的 string用 scanf的话，在结构体中需要初始化申请内存空间\n#include\u0026lt;iostream\u0026gt; #include\u0026lt;cstdio\u0026gt; #include\u0026lt;cstring\u0026gt; using namespace std; struct id { string name ; id() { name .resize (8) ; } } v[2]; int main() { scanf(\u0026#34;%s%s\u0026#34;,\u0026amp;v[0].name[0],\u0026amp;v[1].name[0]); printf(\u0026#34;%s=%s\\n\u0026#34;,v[0].name.c_str(),v[1].name.c_str()) ; return 0; } 3.2 AC代码 # #include\u0026lt;iostream\u0026gt; #include\u0026lt;algorithm\u0026gt; #include\u0026lt;vector\u0026gt; using namespace std; struct employee { string ID_number; int HH,MM,SS; //Sign_in_time int hh,mm,ss; //Sign_out_time } ; int n; vector\u0026lt;employee\u0026gt; emp; employee temp; int cmp1(employee a,employee b) { if(a.HH!=b.HH) return a.HH\u0026lt;b.HH; else if(a.MM!=b.MM) return a.MM\u0026lt;b.MM; else if(a.SS!=b.SS) return a.SS\u0026lt;=b.SS; } int cmp2(employee a,employee b) { if(a.hh!=b.hh) return a.hh\u0026gt;b.hh; else if(a.mm!=b.mm) return a.mm\u0026gt;b.mm; else if(a.ss!=b.ss) return a.ss\u0026gt;=b.ss; } int main() { cin\u0026gt;\u0026gt;n; for(int i=0; i\u0026lt;n; i++) { cin\u0026gt;\u0026gt;temp.ID_number; scanf(\u0026#34;%d:%d:%d%d:%d:%d\u0026#34;,\u0026amp;temp.HH,\u0026amp;temp.MM,\u0026amp;temp.SS,\u0026amp;temp.hh,\u0026amp;temp.mm,\u0026amp;temp.ss); emp.push_back(temp); } sort(emp.begin(),emp.end(),cmp1); cout\u0026lt;\u0026lt;emp[0].ID_number; sort(emp.begin(),emp.end(),cmp2); cout\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;emp[0].ID_number; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1006signinandsignout%E6%A8%A1%E6%8B%9F%E6%8E%92%E5%BA%8F/","section":"博客","summary":"1006 Sign In and Sign Out # 0、题目 # At the beginning of every day, the first person who signs in the computer room will unlock the door, and the last one who signs out will lock the door.","title":"1006SignInandSignOut模拟排序"},{"content":"1007 Maximum Subsequence Sum # 0、题目 # Given a sequence of $K$ integers ${ N_1, N_2, \u0026hellip;, N_K }$. A continuous subsequence is defined to be ${ N_i, N_{i+1}, \u0026hellip;, N_j}$ where $1≤i≤j≤K$. The Maximum Subsequence is the continuous subsequence which has the largest sum of its elements. For example, given sequence ${ -2, 11, -4, 13, -5, -2 }$, its maximum subsequence is ${ 11, -4, 13 }$ with the largest sum being 20.\nNow you are supposed to find the largest sum, together with the first and the last numbers of the maximum subsequence.\nInput Specification: # Each input file contains one test case. Each case occupies two lines. The first line contains a positive integer $K$ (≤10000). The second line contains $K$ numbers, separated by a space.\nOutput Specification: # For each test case, output in one line the largest sum, together with the first and the last numbers of the maximum subsequence. The numbers must be separated by one space, but there must be no extra space at the end of a line. In case that the maximum subsequence is not unique, output the one with the smallest indices $i$ and $j$ (as shown by the sample case). If all the $K$ numbers are negative, then its maximum sum is defined to be 0, and you are supposed to output the first and the last numbers of the whole sequence.\nSample Input: # 10 -10 1 2 3 4 -5 -23 3 7 -21 Sample Output: # 10 1 4 1、大致题意 # 找出一组数中和最大的子数组（数组的头和尾没有限制），并输出该子数组的和、子数组的第一个元素在原数组中的下标、子数组的最后一个元素在原数组中的下标\n2、基本思路 # 使用动态规划来解题，dp[i]保存的是到 $i$ 的位置的最大的值，pre[i]记录子数组开始的下标。然后就是简单选与不选的问题：\n如果选：\ndp[i] =dp[i-1]+num[i]+ pre[i] = pre[i - 1]，一直保持着开始选的那个位置\n如果不选：\ndp[i] = num[i]+ pre[i] = i\n3、解题过程 # 3.1 坑点 # 这道题的坑点其实在题目里面已经告诉我们了。\nIf all the $K$ numbers are negative, then its maximum sum is defined to be 0, and you are supposed to output the first and the last numbers of the whole sequence.\n就是上面的这句话，应该是 测试点4在测这个坑点\n3.2 AC代码 # #include\u0026lt;iostream\u0026gt; #include\u0026lt;cstdio\u0026gt; #include\u0026lt;cstring\u0026gt; using namespace std; const int maxn=10010; int k,a[maxn],dp[maxn],pre[maxn]; int maxindex,maxvalue; int main() { scanf(\u0026#34;%d\u0026#34;,\u0026amp;k); for(int i=1; i\u0026lt;=k; i++) { scanf(\u0026#34;%d\u0026#34;,\u0026amp;a[i]); } dp[1]=a[1]; pre[1]=1; for(int i=2; i\u0026lt;=k; i++) { if(dp[i-1]+a[i]\u0026gt;a[i]) { pre[i]=pre[i-1]; dp[i]=dp[i-1]+a[i]; } else { pre[i]=i; dp[i]=a[i]; } } maxindex=1; maxvalue=-1; for(int i=1; i\u0026lt;=k; i++) { if(dp[i]\u0026gt;maxvalue) { maxvalue=dp[i]; maxindex=i; } } if (maxvalue==-1) { cout\u0026lt;\u0026lt;\u0026#34;0\u0026#34;\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;a[1]\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;a[k]; } else { cout\u0026lt;\u0026lt;maxvalue\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;a[pre[maxindex]]\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;a[maxindex]; } return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1007maximumsubsequencesum%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/","section":"博客","summary":"1007 Maximum Subsequence Sum # 0、题目 # Given a sequence of $K$ integers ${ N_1, N_2, \u0026hellip;, N_K }$.","title":"1007MaximumSubsequenceSum动态规划"},{"content":"1008 Elevator # 0、题目 # The highest building in our city has only one elevator. A request list is made up with $N$ positive numbers. The numbers denote at which floors the elevator will stop, in specified order. It costs 6 seconds to move the elevator up one floor, and 4 seconds to move down one floor. The elevator will stay for 5 seconds at each stop.\nFor a given request list, you are to compute the total time spent to fulfill the requests on the list. The elevator is on the 0th floor at the beginning and does not have to return to the ground floor when the requests are fulfilled.\nInput Specification: # Each input file contains one test case. Each case contains a positive integer $N$, followed by $N$ positive numbers. All the numbers in the input are less than 100.\nOutput Specification: # For each test case, print the total time on a single line.\nSample Input: # 3 2 3 1 Sample Output: # 41 1、题目大意 # 给出电梯上下楼层，根据规则计算运行时间\n2、大致思路 # 简单题\n3、AC代码 # #include\u0026lt;iostream\u0026gt; #include\u0026lt;cstdio\u0026gt; #include\u0026lt;cstring\u0026gt; using namespace std; const int maxn=110; int n,a[maxn],ans; int main() { scanf(\u0026#34;%d\u0026#34;,\u0026amp;n); a[0]=0; ans=0; for(int i=1; i\u0026lt;=n; i++) { scanf(\u0026#34;%d\u0026#34;,\u0026amp;a[i]); } for(int i=1; i\u0026lt;=n; i++) { int tmp=0; if(a[i]\u0026gt;=a[i-1]){ tmp=(a[i]-a[i-1])*6; }else{ tmp=(a[i-1]-a[i])*4; } tmp+=5; ans+=tmp; } cout\u0026lt;\u0026lt;ans; return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1008elevator/","section":"博客","summary":"1008 Elevator # 0、题目 # The highest building in our city has only one elevator.","title":"1008Elevator"},{"content":"1009 Product of Polynomials # 0、题目 # This time, you are supposed to find $A×B$ where $A$ and $B$ are two polynomials.\nInput Specification: # Each input file contains one test case. Each case occupies 2 lines, and each line contains the information of a polynomial:\n$K N_1 a_{N_1} N_2 a_{N_2} \u0026hellip; N_K a_{N_K}$\nwhere $K$ is the number of nonzero terms in the polynomial, $N_i$ and $a_{N_i}$ ($i=1,2,⋯,K$) are the exponents and coefficients, respectively. It is given that $1≤K≤10$, $0≤NK\u0026lt;⋯\u0026lt;N2\u0026lt;N1≤1000$.\nOutput Specification: # For each test case you should output the product of $A$ and $B$ in one line, with the same format as the input. Notice that there must be NO extra space at the end of each line. Please be accurate up to 1 decimal place.\nSample Input: # 2 1 2.4 0 3.2 2 2 1.5 1 0.5 Sample Output: # 3 3 3.6 2 6.0 1 1.6 1、大致题目 # 多项式相乘，求项数和系数\n2、解题思路 # 多项式相乘，可能因为学过计组，感觉也就这么回事，特别是之前有一道 1002 A+B for Polynomials ，导致基本看不出啥难点。\n3、AC代码 # #include\u0026lt;iostream\u0026gt; #include\u0026lt;map\u0026gt; #include\u0026lt;cmath\u0026gt; #include\u0026lt;iomanip\u0026gt; using namespace std; int K; map\u0026lt;int,double,greater\u0026lt;int\u0026gt; \u0026gt;ma1,ma2,ma; int a; double b; int main() { ma.clear(); ma1.clear(); ma2.clear(); cin\u0026gt;\u0026gt;K; for(int i=0; i\u0026lt;K; i++) { cin\u0026gt;\u0026gt;a\u0026gt;\u0026gt;b; if(ma1.count(a)==0\u0026amp;\u0026amp;b!=0) { ma1[a]=b; } else { ma1[a]+=b; } } cin\u0026gt;\u0026gt;K; for(int i=0; i\u0026lt;K; i++) { cin\u0026gt;\u0026gt;a\u0026gt;\u0026gt;b; if(ma2.count(a)==0\u0026amp;\u0026amp;b!=0) { ma2[a]=b; } else { ma2[a]+=b; } } for(map\u0026lt;int,double\u0026gt;::iterator it1=ma1.begin(); (it1)!=ma1.end(); it1++) { for(map\u0026lt;int,double\u0026gt;::iterator it2=ma2.begin(); (it2)!=ma2.end(); it2++) { a=it1-\u0026gt;first+it2-\u0026gt;first; b=it1-\u0026gt;second*it2-\u0026gt;second; if(ma.count(a)==0\u0026amp;\u0026amp;b!=0) { ma[a]=b; } else { ma[a]+=b; } } } for(map\u0026lt;int,double\u0026gt;::iterator it=ma.begin(); (it)!=ma.end();) { if(it-\u0026gt;second==0) { ma.erase(it++); } else { ++it; } } if(ma.empty()) { cout\u0026lt;\u0026lt;0\u0026lt;\u0026lt;endl; } else { cout\u0026lt;\u0026lt;ma.size(); for(map\u0026lt;int,double\u0026gt;::iterator it=ma.begin(); (it)!=ma.end(); it++) { cout\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;it-\u0026gt;first\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;setiosflags(ios::fixed)\u0026lt;\u0026lt;setprecision(1)\u0026lt;\u0026lt;it-\u0026gt;second; } cout\u0026lt;\u0026lt;endl; } return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1009productofpolynomials%E6%A8%A1%E6%8B%9F/","section":"博客","summary":"1009 Product of Polynomials # 0、题目 # This time, you are supposed to find $A×B$ where $A$ and $B$ are two polynomials.","title":"1009ProductofPolynomials模拟"},{"content":"1010 Radix # Given a pair of positive integers, for example, 6 and 110, can this equation 6 = 110 be true? The answer is yes, if 6 is a decimal number and 110 is a binary number.\nNow for any pair of positive integers $N_1$ and $N_2$, your task is to find the radix of one number while that of the other is given.\nInput Specification: # Each input file contains one test case. Each case occupies a line which contains 4 positive integers:\nN1 N2 tag radix Here N1 and N2 each has no more than 10 digits. A digit is less than its radix and is chosen from the set { 0-9, a-z } where 0-9 represent the decimal numbers 0-9, and a-z represent the decimal numbers 10-35. The last number radix is the radix of N1 if tag is 1, or of N2 if tag is 2.\nOutput Specification: # For each test case, print in one line the radix of the other number so that the equation N1 = N2 is true. If the equation is impossible, print Impossible. If the solution is not unique, output the smallest possible radix.\nSample Input 1: # 6 110 1 10 Sample Output 1: # 2 Sample Input 2: # 1 ab 1 2 Sample Output 2: # Impossible 1、大致题意 # 给出两个数 $N1$ 和 $N2$ ，这两个数的长度不超过10位。+ 再给出一个标志位，\n如果是1，则后面的数是 $N1$ 的进制+ 如果是2，则后面的数是 $N2$ 的进制\n要求求出另外一个数为多少进制时跟所给出的数相等，并且输出该进制，如果不存在相等，则输出Impossible。\n2、基本思路 # 用两个字符串来接收 $N1$ 和 $N2$ ，便于进行处理。用两个整形来接收标志位和进制位，将所给出进制的数转化成十进制，未给出进制位的数，使用二分法，找出最小的进制位。\n3、解题过程 # 3.1 初次提交 （18/25） # 3.1.1 基数下界 # 基数下界这个问题其实很容易想到，比如说下面这个用例：\n输入：1 1z1 1 10 输出：Impossible z 不可能出现在10进制中。所以我在 judge 中加入了 3.2 中提到的部分来加以判断。\n有了上面所述的思路，马上写出了第一份代码\n#include\u0026lt;iostream\u0026gt; #include\u0026lt;math.h\u0026gt; using namespace std; string n1,n2; //两个数用string读入 int tag,radix; int size1,size2; //两个数的位数，便于使用pow函数计算 int num1,num2; //两个数的实际大小 int l,r,ans; //二分法求解 int todigit(char a) { //从char得出每一位数 if(a\u0026lt;=\u0026#39;9\u0026#39;\u0026amp;\u0026amp;a\u0026gt;=\u0026#39;0\u0026#39;) { return a-\u0026#39;0\u0026#39;; } else if(a\u0026lt;=\u0026#39;z\u0026#39;\u0026amp;\u0026amp;a\u0026gt;=\u0026#39;a\u0026#39;) { return a-\u0026#39;a\u0026#39;+10; } } int judge(int radix) { //判断二分是否正确 int num=0; if(tag==1) {// tag==1时候 for(int i=0; i\u0026lt;size2; i++) { if(todigit(n2[i])\u0026gt;radix) {// 此处有问题，当有数大于radix时，肯定不对 return -1; } num+=(todigit(n2[i])*pow(radix,size2-i-1)); //计算数 } if(num==num1) { return 0; } else if(num\u0026gt;num1) { return 1; } else { return -1; } } else { for(int i=0; i\u0026lt;size1; i++) { if(todigit(n1[i])\u0026gt;radix) { return -1; } num+=(todigit(n1[i])*pow(radix,size1-i-1)); } if(num==num2) { return 0; } else if(num\u0026gt;num2) { return 1; } else { return -1; } } return 0; } int main() { cin\u0026gt;\u0026gt;n1\u0026gt;\u0026gt;n2\u0026gt;\u0026gt;tag\u0026gt;\u0026gt;radix; size1=n1.size(); size2=n2.size(); num1=0,num2=0; // 计算已经给出基数的数的十进制值 if(tag==1) { for(int i=0; i\u0026lt;size1; i++) { num1+=(todigit(n1[i])*pow(radix,size1-i-1)); } } else { for(int i=0; i\u0026lt;size2; i++) { num2+=(todigit(n2[i])*pow(radix,size2-i-1)); } } //二分法求解 l=2; r=100; ans=-1; while(l\u0026lt;=r) { int mid=(l+r)/2; int k=judge(mid); if(k==0) { ans=mid; r=mid-1; } else if(k==-1) { l=mid+1; } else { r=mid-1; } } if(ans==-1) { cout\u0026lt;\u0026lt;\u0026#34;Impossible\u0026#34;; } else { cout\u0026lt;\u0026lt;ans; } return 0; } 很荣幸地没有过（18/25）。\n3.2 找到最小进制问题（提高1分，19/25） # 思考了一会以后，想到了一点，虽然在第一份代码中，我已经考虑了求到最小的进制，但是 0-8 的进制应该是 9 而不是 8\n输入：8 8 1 10 正确输出：9 第一份代码输出：8 所以应该修改下面的地方为 \u0026gt;=\nif(todigit(n2[i])\u0026gt;=radix) { return -1; } 和 if(todigit(n1[i])\u0026gt;=radix) { return -1; } 由于就修改了这个地方就去提交了。\n#include\u0026lt;iostream\u0026gt; #include\u0026lt;math.h\u0026gt; using namespace std; string n1,n2; //两个数用string读入 int tag,radix; int size1,size2; //两个数的位数，便于使用pow函数计算 int num1,num2; //两个数的实际大小 int l,r,ans; //二分法求解 int todigit(char a) { //从char得出每一位数 if(a\u0026lt;=\u0026#39;9\u0026#39;\u0026amp;\u0026amp;a\u0026gt;=\u0026#39;0\u0026#39;) { return a-\u0026#39;0\u0026#39;; } else if(a\u0026lt;=\u0026#39;z\u0026#39;\u0026amp;\u0026amp;a\u0026gt;=\u0026#39;a\u0026#39;) { return a-\u0026#39;a\u0026#39;+10; } } int judge(int radix) { //判断二分是否正确 int num=0; if(tag==1) {// tag==1时候 for(int i=0; i\u0026lt;size2; i++) { if(todigit(n2[i])\u0026gt;=radix) {// 此处有问题，当有数大于radix时，肯定不对 return -1; } num+=(todigit(n2[i])*pow(radix,size2-i-1)); //计算数 } if(num==num1) { return 0; } else if(num\u0026gt;num1) { return 1; } else { return -1; } } else { for(int i=0; i\u0026lt;size1; i++) { if(todigit(n1[i])\u0026gt;=radix) { return -1; } num+=(todigit(n1[i])*pow(radix,size1-i-1)); } if(num==num2) { return 0; } else if(num\u0026gt;num2) { return 1; } else { return -1; } } return 0; } int main() { cin\u0026gt;\u0026gt;n1\u0026gt;\u0026gt;n2\u0026gt;\u0026gt;tag\u0026gt;\u0026gt;radix; size1=n1.size(); size2=n2.size(); num1=0,num2=0; // 计算已经给出基数的数的十进制值 if(tag==1) { for(int i=0; i\u0026lt;size1; i++) { num1+=(todigit(n1[i])*pow(radix,size1-i-1)); } } else { for(int i=0; i\u0026lt;size2; i++) { num2+=(todigit(n2[i])*pow(radix,size2-i-1)); } } //二分法求解 l=2; r=100; ans=-1; while(l\u0026lt;=r) { int mid=(l+r)/2; int k=judge(mid); if(k==0) { ans=mid; r=mid-1; } else if(k==-1) { l=mid+1; } else { r=mid-1; } } if(ans==-1) { cout\u0026lt;\u0026lt;\u0026#34;Impossible\u0026#34;; } else { cout\u0026lt;\u0026lt;ans; } return 0; } 很高兴多对了一分。\n3.3 变量问题 # 既然这道题的基数可以很大，那么int类型势必是不太够的，本来考虑到既然 pow()函数返回值类型是 double，那我也用 double不就行了（毕竟 double 超级大啊！！！！），但是用 double 在暴力遍历法中用是可以的，但是在使用二分法的时候明显会出现小数部分，这就会给最后的判断带来麻烦。\n这个时候只能退而求其次，用 long long ，说是退而求其次，但事实证明，long long 在这道题中是够用的，而且后来在查阅资料的时候发现，大家遇事不决都会用 long long 这个整型数扛把子 数据类型取值范围int2147483648～2147483647long2147483648～2147483647long long-9223372036854775808 ~ 9223372036854775807\n3.4 数值溢出（24/25） # 当然这里的数值溢出指的不是指已知基数的字符串的数值会溢出，而是指在二分搜索过程中未知基数的字符串的值会在数制转换之后溢出为负数。这样一来，就会导致原本应该是当前基数过大，应该使基数上界下移，但变为负数之后，判断条件以为基数过小了，就会调整使基数下界上移，这与我们二分搜索的初衷背道而驰。\n注意啊，我下面这份代码里面，二分的上界 r 开了100，拿了（24/25）\n#include\u0026lt;iostream\u0026gt; #include\u0026lt;math.h\u0026gt; using namespace std; string n1,n2; //两个数用string读入 int tag; long long radix; int size1,size2; //两个数的位数，便于使用pow函数计算 long long num1,num2; //两个数的实际大小 long long l,r,ans; //二分法求解 int todigit(char a) { //从char得出每一位数 if(a\u0026lt;=\u0026#39;9\u0026#39;\u0026amp;\u0026amp;a\u0026gt;=\u0026#39;0\u0026#39;) { return a-\u0026#39;0\u0026#39;; } else if(a\u0026lt;=\u0026#39;z\u0026#39;\u0026amp;\u0026amp;a\u0026gt;=\u0026#39;a\u0026#39;) { return a-\u0026#39;a\u0026#39;+10; } } int judge(long long radix) { //判断二分是否正确 long long num=0; if(tag==1) {// tag==1时候 for(int i=0; i\u0026lt;size2; i++) { if(todigit(n2[i])\u0026gt;=radix) {// 此处有问题，当有数大于radix时，肯定不对 return -1; } num+=(todigit(n2[i])*pow(radix,size2-i-1)); //计算数 } if(num==num1) { return 0; } else if(num\u0026gt;num1||num\u0026lt;0) { return 1; } else { return -1; } } else { for(int i=0; i\u0026lt;size1; i++) { if(todigit(n1[i])\u0026gt;=radix) { return -1; } num+=(todigit(n1[i])*pow(radix,size1-i-1)); } if(num==num2) { return 0; } else if(num\u0026gt;num2||num\u0026lt;0) { return 1; } else { return -1; } } return 0; } int main() { cin\u0026gt;\u0026gt;n1\u0026gt;\u0026gt;n2\u0026gt;\u0026gt;tag\u0026gt;\u0026gt;radix; size1=n1.size(); size2=n2.size(); num1=0,num2=0; // 计算已经给出基数的数的十进制值 if(tag==1) { for(int i=0; i\u0026lt;size1; i++) { num1+=(todigit(n1[i])*pow(radix,size1-i-1)); } } else { for(int i=0; i\u0026lt;size2; i++) { num2+=(todigit(n2[i])*pow(radix,size2-i-1)); } } //二分法求解 l=2; r=100; ans=-1; while(l\u0026lt;=r) { long long mid=(l+r)/2; int k=judge(mid); if(k==0) { ans=mid; r=mid-1; } else if(k==-1) { l=mid+1; } else { r=mid-1; } } if(ans==-1) { cout\u0026lt;\u0026lt;\u0026#34;Impossible\u0026#34;; } else { cout\u0026lt;\u0026lt;ans; } return 0; } 3.5 二分法的上下限 # 3.5.1 基数上界 # 原本我以为进制的范围是 2 ~ 36，毕竟用来代替数字的字符也就指定了 36 个，所以我在二分的上界 r 开了100 ，当时感觉自己已经保守了。但是当很多测试用例跑不通时，我开始思考基数的上界是不是不止 36 ，比如下面这个用例：\n输入：42949672 10 1 10 输出：42949672 3.5.2 第一份AC代码 # 二分的上界 r 开了10000000000 就AC了。\n#include\u0026lt;iostream\u0026gt; #include\u0026lt;math.h\u0026gt; using namespace std; string n1,n2; //两个数用string读入 int tag; long long radix; long long num1,num2; //两个数的实际大小 long long l,r,mid,ans; //二分法求解 long long todigit(char a) { //从char得出每一位数 if(a\u0026lt;=\u0026#39;9\u0026#39;\u0026amp;\u0026amp;a\u0026gt;=\u0026#39;0\u0026#39;) { return a-\u0026#39;0\u0026#39;; } else if(a\u0026lt;=\u0026#39;z\u0026#39;\u0026amp;\u0026amp;a\u0026gt;=\u0026#39;a\u0026#39;) { return a-\u0026#39;a\u0026#39;+10; } else { return -1; } } long long strTo(string str,long long radix) { int size=str.size(); long long num,cur; num=0; for(int i=0; i\u0026lt;size; i++) { cur=todigit(str[i]); if(cur\u0026gt;=0\u0026amp;\u0026amp;cur\u0026lt;radix) { num+=(cur*pow(radix,size-i-1)); } else { return -1; } } return num; } int judge(long long radix) { //判断二分是否正确 long long num=0; if(tag==1) {// tag==1时候 num=strTo(n2, radix); if(num==num1) { return 0; } else if(num\u0026gt;num1||num\u0026lt;-1) { return 1; } else { return -1; } } else { num=strTo(n1, radix); if(num==num2) { return 0; } else if(num\u0026gt;num2||num\u0026lt;-1) { return 1; } else { return -1; } } return 0; } int main() { cin\u0026gt;\u0026gt;n1\u0026gt;\u0026gt;n2\u0026gt;\u0026gt;tag\u0026gt;\u0026gt;radix; num1=0,num2=0; // 计算已经给出基数的数的十进制值 if(tag==1) { num1=strTo(n1,radix); } else { num2=strTo(n2,radix); } //二分法求解 l=2; r=10000000000; ans=-1; while(l\u0026lt;=r) { mid=(l+r)/2; int k=judge(mid); if(k==0) { ans=mid; r=mid-1; } else if(k==1) { r=mid-1; } else { l=mid+1; } } if(ans==-1) { cout\u0026lt;\u0026lt;\u0026#34;Impossible\u0026#34;; } else { cout\u0026lt;\u0026lt;ans; } return 0; } 3.6 第二份AC代码 # 考虑到第一份AC代码重复的部分太多，耦合度实在太高（虽然感觉ACM的代码不讲究美观），重新修改了下面这一份代码，和上面的第一份AC代码没有本质区别。\n#include\u0026lt;iostream\u0026gt; #include\u0026lt;math.h\u0026gt; using namespace std; string n1,n2; //两个数用string读入 int tag; long long radix; long long num1,num2; //两个数的实际大小 long long l,r,mid,ans; //二分法求解 long long todigit(char a) { //从char得出每一位数 if(a\u0026lt;=\u0026#39;9\u0026#39;\u0026amp;\u0026amp;a\u0026gt;=\u0026#39;0\u0026#39;) { return a-\u0026#39;0\u0026#39;; } else if(a\u0026lt;=\u0026#39;z\u0026#39;\u0026amp;\u0026amp;a\u0026gt;=\u0026#39;a\u0026#39;) { return a-\u0026#39;a\u0026#39;+10; } else { return -1; } } long long strTo(string str,long long radix) { int size=str.size(); long long num,cur; num=0; for(int i=0; i\u0026lt;size; i++) { cur=todigit(str[i]); if(cur\u0026gt;=0\u0026amp;\u0026amp;cur\u0026lt;radix) { num+=(cur*pow(radix,size-i-1)); } else { return -1; } } return num; } int judge(long long radix) { //判断二分是否正确 long long num=0; if(tag==1) {// tag==1时候 num=strTo(n2, radix); if(num==num1) { return 0; } else if(num\u0026gt;num1||num\u0026lt;-1) { return 1; } else { return -1; } } else { num=strTo(n1, radix); if(num==num2) { return 0; } else if(num\u0026gt;num2||num\u0026lt;-1) { return 1; } else { return -1; } } return 0; } int main() { cin\u0026gt;\u0026gt;n1\u0026gt;\u0026gt;n2\u0026gt;\u0026gt;tag\u0026gt;\u0026gt;radix; num1=0,num2=0; // 计算已经给出基数的数的十进制值 if(tag==1) { num1=strTo(n1,radix); } else { num2=strTo(n2,radix); } //二分法求解 l=2; r=10000000000; ans=-1; while(l\u0026lt;=r) { mid=(l+r)/2; int k=judge(mid); if(k==0) { ans=mid; r=mid-1; } else if(k==1) { r=mid-1; } else { l=mid+1; } } if(ans==-1) { cout\u0026lt;\u0026lt;\u0026#34;Impossible\u0026#34;; } else { cout\u0026lt;\u0026lt;ans; } return 0; } 3.7 网上优秀代码 # 下面这份代码是搜索下来思路雷同的，而且代码写的比我优美的多，给这个博主一个大大的赞👍。\n附上链接\n#include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; long long MAP(long long i){ if(i\u0026gt;=\u0026#39;0\u0026#39;\u0026amp;\u0026amp;i\u0026lt;=\u0026#39;9\u0026#39;) return i-48; else if (i\u0026gt;=\u0026#39;a\u0026#39;\u0026amp;\u0026amp;i\u0026lt;=\u0026#39;z\u0026#39;) return i-87; else return -1; } long long ToD(string s,long long radix){ long long current, exp, ans=0; for(int i=s.size()-1;i\u0026gt;=0;i--){ current=MAP(s[i]); exp=s.size()-i-1; if(current\u0026gt;=0\u0026amp;\u0026amp;current\u0026lt;radix){ ans+=current*pow(radix,exp); }else return -1; } return ans; } long long upperBound(string s, string s2, long long radix){ long long current, high, max=-1; for(int i=0;i\u0026lt;s2.size();i++){ current=MAP(s2[i]); if(max\u0026lt;current) max=current; } max++; high=max\u0026gt;ToD(s,radix) ? max : ToD(s,radix); return (high+1); } int main(){ string input[2]; long long value[2]; int tag=0; long long radix;\tcin\u0026gt;\u0026gt;input[0]\u0026gt;\u0026gt;input[1]\u0026gt;\u0026gt;tag\u0026gt;\u0026gt;radix; value[tag-1]=ToD(input[tag-1],radix); //此处，tag-1代表被指定进制的数,2/tag-1代表的是进制待定的数 //接下来是二分法 //下界：因为我的进制转换函数以及二分过程做了修改，所以我的下界不需要指定，都从2开始即可 //上界： int low=2; long long m, curVal, high=upperBound(input[tag-1],input[2/tag-1], radix); while(low\u0026lt;=high){ m=(low+high)/2; curVal=ToD(input[2/tag-1],m);//进制待定那个数的当前值 if(curVal==value[tag-1]){ cout\u0026lt;\u0026lt;m; break; }else if(curVal\u0026gt;value[tag-1]||curVal\u0026lt;-1)\thigh=m-1; else\tlow=m+1; } if(low\u0026gt;high) cout\u0026lt;\u0026lt;\u0026#34;Impossible\u0026#34;; return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1010radix%E4%BA%8C%E5%88%86/","section":"博客","summary":"1010 Radix # Given a pair of positive integers, for example, 6 and 110, can this equation 6 = 110 be true?","title":"1010Radix二分"},{"content":"1011 World Cup Betting # 0、题目 # With the 2010 FIFA World Cup running, football fans the world over were becoming increasingly excited as the best players from the best teams doing battles for the World Cup trophy in South Africa. Similarly, football betting fans were putting their money where their mouths were, by laying all manner of World Cup bets.\nChinese Football Lottery provided a “Triple Winning” game. The rule of winning was simple: first select any three of the games. Then for each selected game, bet on one of the three possible results – namely W for win, T for tie, and L for lose. There was an odd assigned to each result. The winner’s odd would be the product of the three odds times 65%.\nFor example, 3 games’ odds are given as the following:\nW T L 1.1 2.5 1.7 1.2 3.1 1.6 4.1 1.2 1.1 To obtain the maximum profit, one must buy W for the 3rd game, T for the 2nd game, and T for the 1st game. If each bet takes 2 yuans, then the maximum profit would be (4.1×3.1×2.5×65%−1)×2=39.31 yuans (accurate up to 2 decimal places).\nInput Specification: # Each input file contains one test case. Each case contains the betting information of 3 games. Each game occupies a line with three distinct odds corresponding to W, T and L.\nOutput Specification: # For each test case, print in one line the best bet of each game, and the maximum profit accurate up to 2 decimal places. The characters and the number must be separated by one space.\nSample Input: # 1.1 2.5 1.7 1.2 3.1 1.6 4.1 1.2 1.1 Sample Output: # T T W 39.31 1、大致题意 # 就是有3场比赛，每一场从W、T、L选一个值，使得$(abc*0.65-1)*2$最大。\n2、基本思路 # 简单题\n3、AC代码 # #include\u0026lt;iostream\u0026gt; #include\u0026lt;iomanip\u0026gt; #include\u0026lt;cmath\u0026gt; using namespace std; double w,t,l,ans,tmp; int main(){ ans=1; for(int i=0;i\u0026lt;3;i++){ scanf(\u0026#34;%lf%lf%lf\u0026#34;,\u0026amp;w,\u0026amp;t,\u0026amp;l); if(w\u0026gt;=t\u0026amp;\u0026amp;w\u0026gt;=l){ cout\u0026lt;\u0026lt;\u0026#34;W\u0026#34;\u0026lt;\u0026lt;\u0026#34; \u0026#34;; tmp=w; }else if(t\u0026gt;=w\u0026amp;\u0026amp;t\u0026gt;=l){ cout\u0026lt;\u0026lt;\u0026#34;T\u0026#34;\u0026lt;\u0026lt;\u0026#34; \u0026#34;; tmp=t; }else{ cout\u0026lt;\u0026lt;\u0026#34;L\u0026#34;\u0026lt;\u0026lt;\u0026#34; \u0026#34;; tmp=l; } ans*=tmp; } ans=2*(ans*0.65-1); cout\u0026lt;\u0026lt;setiosflags(ios::fixed)\u0026lt;\u0026lt;setprecision(2)\u0026lt;\u0026lt;ans; return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1011worldcupbetting/","section":"博客","summary":"1011 World Cup Betting # 0、题目 # With the 2010 FIFA World Cup running, football fans the world over were becoming increasingly excited as the best players from the best teams doing battles for the World Cup trophy in South Africa.","title":"1011WorldCupBetting"},{"content":"1012 The Best Rank # 0、题目 # To evaluate the performance of our first year CS majored students, we consider their grades of three courses only: C - C Programming Language, M - Mathematics (Calculus or Linear Algrbra), and E - English. At the mean time, we encourage students by emphasizing on their best ranks – that is, among the four ranks with respect to the three courses and the average grade, we print the best rank for each student.\nFor example, The grades of C, M, E and A - Average of 4 students are given as the following:\nStudentID C M E A 310101 98 85 88 90 310102 70 95 88 84 310103 82 87 94 88 310104 91 91 91 91 Then the best ranks for all the students are No.1 since the 1st one has done the best in C Programming Language, while the 2nd one in Mathematics, the 3rd one in English, and the last one in average.\nInput Specification: # Each input file contains one test case. Each case starts with a line containing 2 numbers N and M (≤2000), which are the total number of students, and the number of students who would check their ranks, respectively. Then N lines follow, each contains a student ID which is a string of 6 digits, followed by the three integer grades (in the range of [0, 100]) of that student in the order of C, M and E. Then there are M lines, each containing a student ID.\nOutput Specification: # For each of the M students, print in one line the best rank for him/her, and the symbol of the corresponding rank, separated by a space.\nThe priorities of the ranking methods are ordered as A \u0026gt; C \u0026gt; M \u0026gt; E. Hence if there are two or more ways for a student to obtain the same best rank, output the one with the highest priority.\nIf a student is not on the grading list, simply output N/A.\nSample Input: # 5 6 310101 98 85 88 310102 70 95 88 310103 82 87 94 310104 91 91 91 310105 85 90 90 310101 310102 310103 310104 310105 999999 Sample Output: # 1 C 1 M 1 E 1 A 3 A N/A 1、大致题意 # 把考生C、M、E和A（平均分）分别排序，并输出最好的名次；如果名次相同，按照A\u0026gt;C\u0026gt;M\u0026gt;E的顺序输出；如果当前id不存在，输出N/A。\n2、基本思路 # 用结构体存储学生的 id、四门成绩、四门排名。然后 sort 大法出击，计算出每科排名。最后遍历得到最后结果。\n2.1 坑点 - 并列排名 # 并列排名应该是 1、1、3、4、5，而不不是 1、1、2、3、4，否则会有一个测试点不过\n2.2 四舍五入平均分 # 平均分是四舍五入的，所以需要按照 +0.5 后取整，保证是四舍五入的（听说不不四舍五入也能通过…）。\na[i].scord[0] = (a[i].scord[1]+a[i].scord[2]+a[i].scord[3])/3.0 + 0.5; //scord是int类型 当然为了方便也可以直接使用 double ，省时省力。\n3、AC代码 # #include\u0026lt;bits/stdc++.h\u0026gt; using namespace std; struct stu { int id; double score[5]; int Rank[5]; }; stu student[2005]; int now; bool cmp(stu a,stu b) { //降序排列 return a.score[now]\u0026gt;b.score[now]; } int rank1[1000000][5]; char sco[4]= {\u0026#39;C\u0026#39;,\u0026#39;M\u0026#39;,\u0026#39;E\u0026#39;,\u0026#39;A\u0026#39;}; int main() { int n,m; cin\u0026gt;\u0026gt;n\u0026gt;\u0026gt;m; for(int i=0; i\u0026lt;n; i++) { cin\u0026gt;\u0026gt;student[i].id; double sum=0; for(int j=0; j\u0026lt;3; j++) { cin\u0026gt;\u0026gt;student[i].score[j]; sum+=student[i].score[j]; } student[i].score[3]=sum/3; } for(now =0; now\u0026lt;4; now++) { sort(student,student+n,cmp); rank1[student[0].id][now]=1; for(int i=1; i\u0026lt;n; i++) { //计算出排名 if(student[i].score[now]==student[i-1].score[now]) { //并列第一 rank1[student[i].id][now]=rank1[student[i-1].id][now]; } else { rank1[student[i].id][now]=i+1; } } } while(m--) { int id; cin\u0026gt;\u0026gt;id; if(!rank1[id][0]) { cout\u0026lt;\u0026lt;\u0026#34;N/A\u0026#34;\u0026lt;\u0026lt;endl; } else { int k=3; //A \u0026gt; C \u0026gt; M \u0026gt; E for(int i=0; i\u0026lt;4; i++) { if(rank1[id][i]\u0026lt;rank1[id][k]) { k=i; } } cout\u0026lt;\u0026lt;rank1[id][k]\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;sco[k]\u0026lt;\u0026lt;endl; } } return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1012thebestrank%E6%8E%92%E5%BA%8F/","section":"博客","summary":"1012 The Best Rank # 0、题目 # To evaluate the performance of our first year CS majored students, we consider their grades of three courses only: C - C Programming Language, M - Mathematics (Calculus or Linear Algrbra), and E - English.","title":"1012TheBestRank排序"},{"content":"1013 Battle Over Cities # 0、题目 # It is vitally important to have all the cities connected by highways in a war. If a city is occupied by the enemy, all the highways from/toward that city are closed. We must know immediately if we need to repair any other highways to keep the rest of the cities connected. Given the map of cities which have all the remaining highways marked, you are supposed to tell the number of highways need to be repaired, quickly.\nFor example, if we have 3 cities and 2 highways connecting city1-city2 and city1-city3. Then if city1 is occupied by the enemy, we must have 1 highway repaired, that is the highway city2-city3.\nInput Specification: # Each input file contains one test case. Each case starts with a line containing 3 numbers N (\u0026lt;1000), M and K, which are the total number of cities, the number of remaining highways, and the number of cities to be checked, respectively. Then M lines follow, each describes a highway by 2 integers, which are the numbers of the cities the highway connects. The cities are numbered from 1 to N. Finally there is a line containing K numbers, which represent the cities we concern.\nOutput Specification: # For each of the K cities, output in a line the number of highways need to be repaired if that city is lost.\nSample Input: # 3 2 3 1 2 1 3 1 2 3 Sample Output: # 1 0 0 1、大致题意 # 给定一张城市地图，要求求出其中的一个城市被占领后要修的最少的道路，使剩余城市之间仍然连通。\n2、基本思路 # 图论的题目，可以等价于 $求连通子集的个数-1$ ，那么就很自然地想到了并查集，毕竟一个集合中的就是一个连通子集。\n设置一个邻接表，对于每个占领城市的输入，绕开它把其他存在边的结点合并，再数集合元素的个数，最后减一。\n3、解题过程 # 3.1 并查集知识点 # 首先是复习并查集的相关知识点。\n3.1.1 初始化 # void init(){ for(int i=1;i\u0026lt;=n;i++){ fa[i]=i;//把结点i的集合号初始化为其自身编号 } } 3.1.2 查找 # 3.1.2.1 版本一\nint find(int i){ while(i!=fa[i]){ i=fa[i]=fa[fa[i]]; } return fa[i]; } 3.1.2.2 版本二\nint find(int x) { if(x != fa[x]){//当x不等于它的爸爸时(当它是祖先时，它没有爸爸) fa[x] = find(fa[x]);//继续找他的爸爸的爸爸 } return fa[x];//返回祖先 }//查找 3.1.3 合并 # void unity(int x, int y){ int r1 = find(x);//找到x的祖先 int r2 = find(y);//找到y的祖先 if(r1!=r2){ fa[r1] = r2;//祖先和祖先结为父子(谁是父亲谁是儿子都可以) } }//合并 3.2 第一次提交（7/25） # 将邻接表和并查集相结合，就得到了第一稿的代码\n#include\u0026lt;iostream\u0026gt; #include\u0026lt;vector\u0026gt; #include\u0026lt;set\u0026gt; #include\u0026lt;cstring\u0026gt; using namespace std; int n,m,k; int fa[101000]; int head[101000],vis[101000],dis[101000],cnt; struct Edge { int v,w,next; } edge[501000],tmp; void addEdge(int u, int v,int w) { edge[++cnt].v=v; edge[cnt].w=w; edge[cnt].next=head[u]; head[u]=cnt; } void init() { //并查集初始化 for(int i=1; i\u0026lt;=n; i++) { fa[i]=i; } } int find(int x) { //并查集查找 if(x != fa[x]) { //当x不等于它的爸爸时(当它是祖先时，它没有爸爸) fa[x] = find(fa[x]);//继续找他的爸爸的爸爸 } return fa[x];//返回祖先 }//查找 void unity(int x, int y) { //并查集合并 int r1 = find(x);//找到x的祖先 int r2 = find(y);//找到y的祖先 if(r1!=r2) { fa[r1] = r2;//祖先和祖先结为父子(谁是父亲谁是儿子都可以) } }//合并 int main() { cin\u0026gt;\u0026gt;n\u0026gt;\u0026gt;m\u0026gt;\u0026gt;k; cnt=0; int a,b; for(int i=0; i\u0026lt;m; i++) { cin\u0026gt;\u0026gt;a\u0026gt;\u0026gt;b; addEdge(a,b,1); } set\u0026lt;int\u0026gt; con; for(int i=0; i\u0026lt;k; i++) { cin\u0026gt;\u0026gt;a; init(); for(int j=1; j\u0026lt;=n; j++) { if(j==a) { continue; } else { tmp=edge[head[j]]; while(tmp.next!=0) { unity(j,tmp.v); tmp=edge[tmp.next]; } unity(j,tmp.v); } } b=0; con.clear(); for(int j=1; j\u0026lt;=n; j++) { b=find(j); con.insert(b); } cout\u0026lt;\u0026lt;con.size()-1\u0026lt;\u0026lt;endl; } } 3.3 无向图问题（22/25） # 首先，这题是个无向图，所以在存储的时候，需要将一条边存两次（两个顶点后面链表中都有）\nfor(int i=0; i\u0026lt;m; i++) { cin\u0026gt;\u0026gt;a\u0026gt;\u0026gt;b; addEdge(a,b,1); addEdge(b,a,1); } 然后，变成无向图后，在并查集unity的过程中，不仅要防止 u-\u0026gt; 的，还要防止 -\u0026gt;u 的部分。\n#include\u0026lt;iostream\u0026gt; #include\u0026lt;vector\u0026gt; #include\u0026lt;set\u0026gt; #include\u0026lt;cstring\u0026gt; using namespace std; int n,m,k; int fa[101000]; int head[101000],vis[101000],dis[101000],cnt; set\u0026lt;int\u0026gt; con; struct Edge { int v,w,next; } edge[501000],tmp; void addEdge(int u, int v,int w) { edge[++cnt].v=v; edge[cnt].w=w; edge[cnt].next=head[u]; head[u]=cnt; } void init() { //并查集初始化 for(int i=1; i\u0026lt;=n; i++) { fa[i]=i; } } int find(int x) { //并查集查找 if(x != fa[x]) { //当x不等于它的爸爸时(当它是祖先时，它没有爸爸) fa[x] = find(fa[x]);//继续找他的爸爸的爸爸 } return fa[x];//返回祖先 }//查找 void unity(int x, int y) { //并查集合并 int r1 = find(x);//找到x的祖先 int r2 = find(y);//找到y的祖先 if(r1!=r2) { fa[r1] = r2;//祖先和祖先结为父子(谁是父亲谁是儿子都可以) } }//合并 int main() { cin\u0026gt;\u0026gt;n\u0026gt;\u0026gt;m\u0026gt;\u0026gt;k; cnt=0; int a,b; for(int i=0; i\u0026lt;m; i++) { cin\u0026gt;\u0026gt;a\u0026gt;\u0026gt;b; addEdge(a,b,1); addEdge(b,a,1); } for(int i=0; i\u0026lt;k; i++) { con.clear(); b=0; cin\u0026gt;\u0026gt;a; init(); for(int j=1; j\u0026lt;=n; j++) { if(j==a) { continue; } else { tmp=edge[head[j]]; while(tmp.next!=0) { if(tmp.v!=a) { unity(j,tmp.v); } tmp=edge[tmp.next]; } if(tmp.v!=a) { unity(j,tmp.v); } } } for(int j=1; j\u0026lt;=n; j++) { if(j!=a) { b=find(j); con.insert(b); } } cout\u0026lt;\u0026lt;con.size()-1\u0026lt;\u0026lt;endl; } } 出现段错误，很显然有数组越界了，思考了一下，可能是将边存储了两遍，使得下标越界了。修改 edge[501000] 为 edge[10001000]。\n#include\u0026lt;iostream\u0026gt; #include\u0026lt;vector\u0026gt; #include\u0026lt;set\u0026gt; #include\u0026lt;cstring\u0026gt; using namespace std; int n,m,k; int fa[101000]; int head[101000],vis[101000],dis[101000],cnt; set\u0026lt;int\u0026gt; con; struct Edge { int v,w,next; } edge[10001000],tmp; void addEdge(int u, int v,int w) { edge[++cnt].v=v; edge[cnt].w=w; edge[cnt].next=head[u]; head[u]=cnt; } void init() { //并查集初始化 for(int i=1; i\u0026lt;=n; i++) { fa[i]=i; } } int find(int x) { //并查集查找 if(x != fa[x]) { //当x不等于它的爸爸时(当它是祖先时，它没有爸爸) fa[x] = find(fa[x]);//继续找他的爸爸的爸爸 } return fa[x];//返回祖先 }//查找 void unity(int x, int y) { //并查集合并 int r1 = find(x);//找到x的祖先 int r2 = find(y);//找到y的祖先 if(r1!=r2) { fa[r1] = r2;//祖先和祖先结为父子(谁是父亲谁是儿子都可以) } }//合并 int main() { cin\u0026gt;\u0026gt;n\u0026gt;\u0026gt;m\u0026gt;\u0026gt;k; cnt=0; int a,b; for(int i=0; i\u0026lt;m; i++) { cin\u0026gt;\u0026gt;a\u0026gt;\u0026gt;b; addEdge(a,b,1); addEdge(b,a,1); } for(int i=0; i\u0026lt;k; i++) { con.clear(); b=0; cin\u0026gt;\u0026gt;a; init(); for(int j=1; j\u0026lt;=n; j++) { if(j==a) { continue; } else { tmp=edge[head[j]]; while(tmp.next!=0) { if(tmp.v!=a) { unity(j,tmp.v); } tmp=edge[tmp.next]; } if(tmp.v!=a) { unity(j,tmp.v); } } } for(int j=1; j\u0026lt;=n; j++) { if(j!=a) { b=find(j); con.insert(b); } } cout\u0026lt;\u0026lt;con.size()-1\u0026lt;\u0026lt;endl; } } 成功拿下第四个用例（22/25）\n3.4 最后一城 - 邻接表中的链表判断结尾（22/25） # 在一次一次的测试后，想到了一种极端情况，就是下面这种用例\n输入： 3 0 1 1 输出：1 PAT 中的 测试点2 应该就是这种形式的。\n因为没有边，所以在邻接表往并查集unity的过程中，在判断链表到达结尾时，会有问题。可能会在最后一个结点（tmp.next==0）时，加上一个 j-\u0026gt;0 的边，然而很显然 0 这个结点是不存在的。\n所以要在判断的时候要加上 tmp.v\u0026gt;0\ncin\u0026gt;\u0026gt;a; init(); for(int j=1; j\u0026lt;=n; j++) { if(j==a) { continue; } else { tmp=edge[head[j]]; while(tmp.next!=0) { if(tmp.v!=a\u0026amp;\u0026amp;tmp.v\u0026gt;0) { unity(j,tmp.v); } tmp=edge[tmp.next]; } if(tmp.v!=a\u0026amp;\u0026amp;tmp.v\u0026gt;0) { unity(j,tmp.v); } } } 3.5 邻接表版本 - AC 代码 # #include\u0026lt;iostream\u0026gt; #include\u0026lt;vector\u0026gt; #include\u0026lt;set\u0026gt; #include\u0026lt;cstring\u0026gt; using namespace std; int n,m,k; int fa[101000]; int head[101000],vis[101000],dis[101000],cnt; set\u0026lt;int\u0026gt; con; struct Edge { int v,w,next; } edge[10001000],tmp; void addEdge(int u, int v,int w) { edge[++cnt].v=v; edge[cnt].w=w; edge[cnt].next=head[u]; head[u]=cnt; } void init() { //并查集初始化 for(int i=1; i\u0026lt;=n; i++) { fa[i]=i; } } int find(int x) { //并查集查找 if(x != fa[x]) { //当x不等于它的爸爸时(当它是祖先时，它没有爸爸) fa[x] = find(fa[x]);//继续找他的爸爸的爸爸 } return fa[x];//返回祖先 }//查找 void unity(int x, int y) { //并查集合并 int r1 = find(x);//找到x的祖先 int r2 = find(y);//找到y的祖先 if(r1!=r2) { fa[r1] = r2;//祖先和祖先结为父子(谁是父亲谁是儿子都可以) } }//合并 int main() { cin\u0026gt;\u0026gt;n\u0026gt;\u0026gt;m\u0026gt;\u0026gt;k; cnt=0; int a,b; for(int i=0; i\u0026lt;m; i++) { cin\u0026gt;\u0026gt;a\u0026gt;\u0026gt;b; addEdge(a,b,1); addEdge(b,a,1); } for(int i=0; i\u0026lt;k; i++) { cin\u0026gt;\u0026gt;a; init(); for(int j=1; j\u0026lt;=n; j++) { if(j==a) { continue; } else { tmp=edge[head[j]]; while(tmp.next!=0) { if(tmp.v!=a\u0026amp;\u0026amp;tmp.v\u0026gt;0) { unity(j,tmp.v); } tmp=edge[tmp.next]; } if(tmp.v!=a\u0026amp;\u0026amp;tmp.v\u0026gt;0) { unity(j,tmp.v); } } } con.clear(); for(int j=1; j\u0026lt;=n; j++) { if(j!=a) { b=find(j); con.insert(b); } } cout\u0026lt;\u0026lt;con.size()-1\u0026lt;\u0026lt;endl; } } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1013battleovercities%E5%9B%BE%E5%B9%B6%E6%9F%A5%E9%9B%86%E6%B1%82%E8%BF%9E%E9%80%9A%E5%AD%90%E9%9B%86/","section":"博客","summary":"1013 Battle Over Cities # 0、题目 # It is vitally important to have all the cities connected by highways in a war.","title":"1013BattleOverCities图并查集求连通子集"},{"content":"1014 Waiting in Line # 0、题目 # Suppose a bank has N windows open for service. There is a yellow line in front of the windows which devides the waiting area into two parts. The rules for the customers to wait in line are:\nThe space inside the yellow line in front of each window is enough to contain a line with M customers. Hence when all the N lines are full, all the customers after (and including) the (NM+1)st one will have to wait in a line behind the yellow line.+ Each customer will choose the shortest line to wait in when crossing the yellow line. If there are two or more lines with the same length, the customer will always choose the window with the smallest number.+ Customeri will take Ti minutes to have his/her transaction processed.+ The first N customers are assumed to be served at 8:00am. Now given the processing time of each customer, you are supposed to tell the exact time at which a customer has his/her business done.\nFor example, suppose that a bank has 2 windows and each window may have 2 customers waiting inside the yellow line. There are 5 customers waiting with transactions taking 1, 2, 6, 4 and 3 minutes, respectively. At 08:00 in the morning, customer1 is served at window1 while customer2 is served at window2. Customer3 will wait in front of window1 and customer4 will wait in front of window2. Customer5 will wait behind the yellow line.\nAt 08:01, customer1 is done and customer5 enters the line in front of window1 since that line seems shorter now. Customer2 will leave at 08:02, customer4 at 08:06, customer3 at 08:07, and finally customer5 at 08:10.\nInput Specification: # Each input file contains one test case. Each case starts with a line containing 4 positive integers: N (≤20, number of windows), M (≤10, the maximum capacity of each line inside the yellow line), K (≤1000, number of customers), and Q (≤1000, number of customer queries).\nThe next line contains K positive integers, which are the processing time of the K customers.\nThe last line contains Q positive integers, which represent the customers who are asking about the time they can have their transactions done. The customers are numbered from 1 to K.\nOutput Specification: # For each of the Q customers, print in one line the time at which his/her transaction is finished, in the format HH:MM where HH is in [08, 17] and MM is in [00, 59]. Note that since the bank is closed everyday after 17:00, for those customers who cannot be served before 17:00, you must output Sorry instead.\nSample Input: # 2 2 7 5 1 2 6 4 3 534 2 3 4 5 6 7 Sample Output: # 08:07 08:06 08:10 17:00 Sorry 1、大致题意 # 店铺早上 8:00~17:00 营业，有 k 个顾客，按照一定规则排队，输出业务办理完时间。具体规则如下：\n每个窗口前面的黄线内的空间只能容纳 m 个客户。因此，当所有 n 行都满时，(n*m+1) 后的所有客户（包括）都必须在黄线后面排队等候。+ 每一位顾客在过黄线时都会选择最短的排队等候。如果有两条或两条以上相同长度的队，顾客总是会选择编号最小的窗口。+ 第 i 位顾客需要花费 $T_i$ 分钟办理他的业务 2、基本思路 # 整体思路就是使用优先队列来解题。\n首先构建一个顾客的结构体 customer，具体如下：\nstruct customer { int windows; int time; //办理业务所需时间 int start; int end; bool operator \u0026gt;(const customer \u0026amp;a)const { if(end!=a.end) { return end \u0026gt; a.end; } else { return windows \u0026gt; a.windows; } } } 可以看到，我在上面结构体里加入了 \u0026gt; 的复写，便于使用优先队列。然后便是模拟业务办理的规则。\n3、解题过程 # 3.1 优先队列相关知识 # 3.1.1 基本操作 # 具体写法如下：\n//升序队列，小根堆 priority_queue \u0026lt;int,vector\u0026lt;int\u0026gt;,greater\u0026lt;int\u0026gt; \u0026gt; q; //降序队列，大顶堆 priority_queue \u0026lt;int,vector\u0026lt;int\u0026gt;,less\u0026lt;int\u0026gt; \u0026gt;q; //greater和less是std实现的两个仿函数（就是使一个类的使用看上去像一个函数。其实现就是类中实现一个operator()，这个类就有了类似函数的行为，就是一个仿函数类了） 和队列基本操作相同： 操作函数解释top()访问队头元素empty()队列是否为空size()返回队列内元素个数push()插入元素到队尾（并排序）emplace()原地构造一个元素并插入队列pop()弹出队头元素swap()交换内容\n3.1.2 和 struct 双剑合璧 # STL和 struct 一起操作起来是真的骚 yyds 。但是如果你想双剑合璧，还得学一份武林秘籍 —— 重载运算符。\ngreater\u0026lt;\u0026gt; 要求重载 operator \u0026gt;+ less\u0026lt;int\u0026gt; 要求重载 operator \u0026lt; struct node { int l,r; bool operator \u0026lt;(const node \u0026amp;a)const{ return r \u0026lt; a.r; } } a[maxn]; 3.2 第一份提交代码（19/30） # 有了上面的思路，马上就有了第一份提交代码\n#include\u0026lt;iostream\u0026gt; #include\u0026lt;queue\u0026gt; using namespace std; struct customer { int windows; int time; //办理业务所需时间 int end; bool operator \u0026gt;(const customer \u0026amp;a)const { return end \u0026gt; a.end; } } cus[1005],tmp; int n,m,k,q; int wins[25]; int num; priority_queue\u0026lt;customer,vector\u0026lt;customer\u0026gt;,greater\u0026lt;customer\u0026gt; \u0026gt;q1; //小顶堆 int main() { cin\u0026gt;\u0026gt;n\u0026gt;\u0026gt;m\u0026gt;\u0026gt;k\u0026gt;\u0026gt;q; for(int i=1; i\u0026lt;=n; i++) { wins[i]=0; } for(int i=1; i\u0026lt;=k; i++) { cin\u0026gt;\u0026gt;cus[i].time; } for(int i=1; i\u0026lt;=m; i++) { for(int j=1; j\u0026lt;=n; j++) { num=(i-1)*n+j; //(i-1)*n+j为cus的编号 if(num\u0026lt;=k) { cus[num].windows=j; cus[num].end=wins[j]+cus[num].time; wins[j]=cus[num].end; q1.push(cus[num]); } else { break; } } if(num\u0026gt;k) { break; } } while(!q1.empty()\u0026amp;\u0026amp;num\u0026lt;=k) { tmp=q1.top(); q1.pop(); num++; int now=tmp.windows; cus[num].windows=now; cus[num].end=cus[num].time+wins[now]; wins[now]=cus[num].end; } int ques; for(int i=1; i\u0026lt;=q; i++) { cin\u0026gt;\u0026gt;ques; if(cus[ques].end\u0026gt;540){ cout\u0026lt;\u0026lt;\u0026#34;Sorry\u0026#34;\u0026lt;\u0026lt;endl; }else{ int HH=cus[ques].end/60+8; int MM=cus[ques].end%60; printf(\u0026#34;%02d:%02d\\n\u0026#34;,HH,MM); } } return 0; } 3.3 测试点 2，4 - 第二份提交代码（26/30） # 对于 2，4 的情况：一个人如果开始时间在540之前（不包括540）即使结束时间超过540，依旧要服务，例：一个人16:49 进入1号窗口办理业务，要办理 120 分钟，则他的完成时间为 18:49。比如下面这个用例：\n输入： 2 2 3 3 539 539 6 1 2 3 输出： 16:59 16:59 17:05 这点其实也很容易做到，就是在 customer 中加入一个新元素 start 就行了\n#include\u0026lt;iostream\u0026gt; #include\u0026lt;queue\u0026gt; #include\u0026lt;cmath\u0026gt; using namespace std; struct customer { int windows; int time; //办理业务所需时间 int start; int end; bool operator \u0026gt;(const customer \u0026amp;a)const { if(end!=a.end) { return end \u0026gt; a.end; } else { return windows \u0026gt; a.windows; } } } cus[1005],tmp; int n,m,k,q; int wins[25]; int num; priority_queue\u0026lt;customer,vector\u0026lt;customer\u0026gt;,greater\u0026lt;customer\u0026gt; \u0026gt;q1; //小顶堆 int toTime(int t){ //防止上溢 return min(1000,t); } int main() { cin\u0026gt;\u0026gt;n\u0026gt;\u0026gt;m\u0026gt;\u0026gt;k\u0026gt;\u0026gt;q; for(int i=1; i\u0026lt;=n; i++) { wins[i]=0; } for(int i=1; i\u0026lt;=k; i++) { cin\u0026gt;\u0026gt;cus[i].time; } for(int i=1; i\u0026lt;=m; i++) { for(int j=1; j\u0026lt;=n; j++) { num=(i-1)*n+j; //(i-1)*n+j为cus的编号 if(num\u0026lt;=k) { cus[num].windows=j; cus[num].start=wins[j]; cus[num].end=toTime(wins[j]+cus[num].time); wins[j]=cus[num].end; q1.push(cus[num]); } else { break; } } if(num\u0026gt;k) { break; } } while(!q1.empty()\u0026amp;\u0026amp;num\u0026lt;=k) { tmp=q1.top(); q1.pop(); num++; int now=tmp.windows; cus[num].windows=now; cus[num].start=wins[now]; cus[num].end=toTime(cus[num].time+wins[now]); wins[now]=cus[num].end; } int ques; for(int i=1; i\u0026lt;=q; i++) { cin\u0026gt;\u0026gt;ques; if(cus[ques].start\u0026gt;=540) { cout\u0026lt;\u0026lt;\u0026#34;Sorry\u0026#34;\u0026lt;\u0026lt;endl; } else { int HH=cus[ques].end/60+8; int MM=cus[ques].end%60; printf(\u0026#34;%02d:%02d\\n\u0026#34;,HH,MM); } } return 0; } 3.4 测试点 5 - 第三份提交代码 # 还有个测试点5，真的自己测试了很久，找到了一个用例\n输入： 1 1 3 3 539 100 6 1 2 3 正确输入： 16:59 18:39 Sorry 第二份提交代码： 16:59 18:39 08:00 问题出在哪里呢，调试了一遍后找到了，下面的代码中，少了注释的那一行。不然，当 q1 很少时，你不加入新的 customer ，甚至你都遍历不完所有的。\nwhile(!q1.empty()\u0026amp;\u0026amp;num\u0026lt;=k) { tmp=q1.top(); q1.pop(); num++; int now=tmp.windows; cus[num].windows=now; cus[num].start=wins[now]; cus[num].end=cus[num].time+wins[now]; wins[now]=cus[num].end; //\tq1.push(cus[num]); } 加上注释的那一行之后，终于 AC 。\n3.5 奇葩想法 # 当然用我这种方法有个问题，就是下面这种用例的时候。\n输入： 0 0 3 3 539 539 6 1 2 3 正确输出： Sorry Sorry Sorry 第二份代码输出： 08:00 08:00 08:00 这个错误找了好久，n、m 的取值为 0 一开始真是没有想到。\n3.6 AC代码 # #include\u0026lt;iostream\u0026gt; #include\u0026lt;queue\u0026gt; #include\u0026lt;cmath\u0026gt; using namespace std; struct customer { int windows; int time; //办理业务所需时间 int start; int end; bool operator \u0026gt;(const customer \u0026amp;a)const { if(end!=a.end) { return end \u0026gt; a.end; } else { return windows \u0026gt; a.windows; } } } cus[1005],tmp; int n,m,k,q; int wins[25]; int num; priority_queue\u0026lt;customer,vector\u0026lt;customer\u0026gt;,greater\u0026lt;customer\u0026gt; \u0026gt;q1; //小顶堆 int toTime(int t) { //防止上溢 return min(1000,t); } int main() { cin\u0026gt;\u0026gt;n\u0026gt;\u0026gt;m\u0026gt;\u0026gt;k\u0026gt;\u0026gt;q; for(int i=1; i\u0026lt;=n; i++) { wins[i]=0; } for(int i=1; i\u0026lt;=k; i++) { cin\u0026gt;\u0026gt;cus[i].time; } for(int i=1; i\u0026lt;=m; i++) { for(int j=1; j\u0026lt;=n; j++) { num=(i-1)*n+j; //(i-1)*n+j为cus的编号 if(num\u0026lt;=k) { cus[num].windows=j; cus[num].start=wins[j]; cus[num].end=wins[j]+cus[num].time; wins[j]=cus[num].end; q1.push(cus[num]); } else { break; } } if(num\u0026gt;k) { break; } } while(!q1.empty()\u0026amp;\u0026amp;num\u0026lt;=k) { tmp=q1.top(); q1.pop(); num++; int now=tmp.windows; cus[num].windows=now; cus[num].start=wins[now]; cus[num].end=cus[num].time+wins[now]; wins[now]=cus[num].end; q1.push(cus[num]); } int ques; for(int i=1; i\u0026lt;=q; i++) { cin\u0026gt;\u0026gt;ques; if(cus[ques].start\u0026gt;=540||n==0||m==0) { cout\u0026lt;\u0026lt;\u0026#34;Sorry\u0026#34;\u0026lt;\u0026lt;endl; } else { int HH=cus[ques].end/60+8; int MM=cus[ques].end%60; printf(\u0026#34;%02d:%02d\\n\u0026#34;,HH,MM); } } return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1014waitinginline%E6%A8%A1%E6%8B%9F/","section":"博客","summary":"1014 Waiting in Line # 0、题目 # Suppose a bank has N windows open for service.","title":"1014WaitinginLine模拟"},{"content":"1015 Reversible Primes # 0、题目 # A reversible prime in any number system is a prime whose “reverse” in that number system is also a prime. For example in the decimal system 73 is a reversible prime because its reverse 37 is also a prime.\nNow given any two positive integers N (\u0026lt;105) and D (1\u0026lt;D≤10), you are supposed to tell if N is a reversible prime with radix D.\nInput Specification: # The input file consists of several test cases. Each case occupies a line which contains two integers N and D. The input is finished by a negative N.\nOutput Specification: # For each test case, print in one line Yes if N is a reversible prime with radix D, or No if not.\nSample Input: # 73 10 23 2 23 10 -2 Sample Output: # Yes Yes No 英语词汇 # reversible prime 可逆素数 decimal system 十进位制；十进位系统 number system 数字系统；进制 radix 【数】基数；根值 1、大致题意 # 给出两个数 N 和 D ，判断 N 是否为素数，并以 D 为基数倒转该数后，判断转化为 10进制 后还是不是素数\n2、基本思路 # 素数判断函数+ 进制转换实现+ 素数判断并打印输出结果 3、解题思路 # 在写这道题前，我回顾了 质数相关知识。\n3.1 质数相关知识 # 3.1.1 奇技淫巧-欧几里得算法 # 欧几里得算法又称辗转相除法，是指用于计算两个非负整数a，b的最大公约数。应用领域有数学和计算机两个方面。计算公式 $gcd(a,b) = gcd(b,a mod b)$ 。\ninline int gcd(int x,int y) { if(y==0) return x; return gcd(y,x%y); } 3.1.2 判断质数方法 # 3.1.2.1 一个比较普通的判定方法\nbool IsPrime(int n) { for(int i=2; i\u0026lt;=sqrt(n); i++) { if(n%i==0) return false; } return true; } 3.1.2.2 Eratosthenes筛选法（质数的倍数一定不是质数）\n同时对于每个x，把大于等于x的平方的x的倍数标记为合数。\nvoid IsPrime(int n) { ///筛选1-n的素数 memset(vis,0,sizeof(vis)); int m = sqrt(n + 0.5); for(int i = 2; i \u0026lt;= m; i++) { if(!vis[i]) { for(int j = i * i; j \u0026lt;= n; j += i) vis[j] = 1; } } } 3.1.2.3 线性筛法\n每个合数只会被它的最小质因子筛一次，时间复杂度 $O(n)$\nint v[maxn],prime[maxn]; void primes(int n) { memset(v,0,sizeof(v));///最小质因子 m=0; for(i=2; i\u0026lt;=n; i++) { if(v[i]==0) { ///i为质数 v[i]=i; prime[++m]=i; } ///给当前的数i乘上一个质因子 for(j=1; j\u0026lt;=m; j++) { ///i有比prime[i]更小的质因子，或者超出n的范围 if(prime[j]\u0026gt;v[i]||prime[j]\u0026gt;n/i) break; v[i*prime[j]]=prime[j]; } } for(i=1; i\u0026lt;=m; i++) cout\u0026lt;\u0026lt;prime[i]\u0026lt;\u0026lt;endl; } 3.1.2.4 最快模板\n还有一个在网上看到的一个判断素数的方法，这是当时 ccpc 的时候也用到过的一个模板。\nbool isPrime( int num ) { //两个较小数另外处理 if(num ==2|| num==3 ) return 1 ; //不在6的倍数两侧的一定不是质数 if(num %6!= 1\u0026amp;\u0026amp;num %6!= 5) return 0 ; int tmp =sqrt( num); //在6的倍数两侧的也可能不是质数 for(int i= 5; i \u0026lt;=tmp; i+=6 ) if(num %i== 0||num %(i+ 2)==0 ) return 0 ; //排除所有，剩余的是质数 return 1 ; } 3.2 AC代码 # #include\u0026lt;cstdio\u0026gt; #include\u0026lt;cmath\u0026gt; #include\u0026lt;iostream\u0026gt; using namespace std; bool isPrime(int n) { //判断n是否为素数 if(n\u0026lt;=1) return false; for(int i=2; i*i\u0026lt;=n; i++) if(n%i == 0) return false; return true; } int d[111]; int main() { int n, radix; while(scanf(\u0026#34;%d\u0026#34;, \u0026amp;n) != EOF) { if(n\u0026lt;0) break; //当n为负数时，退出循环 scanf(\u0026#34;%d\u0026#34;, \u0026amp;radix); if(isPrime(n) == false) { //n不是素数，输出No，结束算法 printf(\u0026#34;No\\n\u0026#34;); } else { //n是素数，判断n在radix进制下的逆序是否是素数 int len = 0; do { //进制转换 d[len++] = n%radix; n/=radix; } while(n!=0); for(int i=0; i\u0026lt;len; i++) { //逆序转换进制 n = n*radix + d[i]; } if(isPrime(n) == true) { //逆序是素数 printf(\u0026#34;Yes\\n\u0026#34;); } else { printf(\u0026#34;No\\n\u0026#34;); } } } return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1015reversibleprimes%E8%B4%A8%E6%95%B0/","section":"博客","summary":"1015 Reversible Primes # 0、题目 # A reversible prime in any number system is a prime whose “reverse” in that number system is also a prime.","title":"1015ReversiblePrimes质数"},{"content":"1016 Phone Bills # 0、题目 # A long-distance telephone company charges its customers by the following rules:\nMaking a long-distance call costs a certain amount per minute, depending on the time of day when the call is made. When a customer starts connecting a long-distance call, the time will be recorded, and so will be the time when the customer hangs up the phone. Every calendar month, a bill is sent to the customer for each minute called (at a rate determined by the time of day). Your job is to prepare the bills for each month, given a set of phone call records.\nInput Specification: # Each input file contains one test case. Each case has two parts: the rate structure, and the phone call records.\nThe rate structure consists of a line with 24 non-negative integers denoting the toll (cents/minute) from 00:00 - 01:00, the toll from 01:00 - 02:00, and so on for each hour in the day.\nThe next line contains a positive number N (≤1000), followed by N lines of records. Each phone call record consists of the name of the customer (string of up to 20 characters without space), the time and date (MM:dd:HH:mm), and the word on-line or off-line.\nFor each test case, all dates will be within a single month. Each on-line record is paired with the chronologically next record for the same customer provided it is an off-line record. Any on-line records that are not paired with an off-line record are ignored, as are off-line records not paired with an on-line record. It is guaranteed that at least one call is well paired in the input. You may assume that no two records for the same customer have the same time. Times are recorded using a 24-hour clock.\nOutput Specification: # For each test case, you must print a phone bill for each customer.\nBills must be printed in alphabetical order of customers’ names. For each customer, first print in a line the name of the customer and the month of the bill in the format shown by the sample. Then for each time period of a call, print in one line the beginning and ending time and date (dd:HH:mm), the lasting time (in minute) and the charge of the call. The calls must be listed in chronological order. Finally, print the total charge for the month in the format shown by the sample.\nSample Input: # 10 10 10 10 10 10 20 20 20 15 15 15 15 15 15 15 20 30 20 15 15 10 10 10 10 CYLL 01:01:06:01 on-line CYLL 01:28:16:05 off-line CYJJ 01:01:07:00 off-line CYLL 01:01:08:03 off-line CYJJ 01:01:05:59 on-line aaa 01:01:01:03 on-line aaa 01:02:00:01 on-line CYLL 01:28:15:41 on-line aaa 01:05:02:24 on-line aaa 01:04:23:59 off-line Sample Output: # CYJJ 01 01:05:59 01:07:00 61 $12.10 Total amount: $12.10 CYLL 01 01:06:01 01:08:03 122 $24.40 28:15:41 28:16:05 24 $3.85 Total amount: $28.25 aaa 01 02:00:01 04:23:59 4318 $638.80 Total amount: $638.80 1、大致题意 # 给出24小时的电话费，并给出后台记录的电话记录，给出合理的电话报表\n2、基本思路 # 这道题目主要可以分解为三个问题：\n电话信息 on 和 off 的匹配，这个只要按照名字从小到大，再按照时间从小到大排序，最后前后两个比较是不是一个 on 一个 off 即可。因为off时间早于 on 的一定是无效的。+ 钱的计算问题。有两个方法，一个是以 on 的时间为起点，一分钟一分钟地加到off的时间，每个小时都要加个if；另一个是从柳神那里学来的，就是把所有计费都从月初的 00:00:00:00 开始，客户的实际消费就是 off 的时间减去 on 的时间。+ 第一次信息输出时需要带上名字和月份，最后一次输出要有 Total amount。所以这里需要有一个判断。 3、解题过程 # 0 、1测试点：没有消费的用户不能输出\n输入： CYY 01：02：01：01 on-line CYY 01：02：01：01 off-line 输出： CYY 01 01：02：01：01 01：02：01：01 0 $0 Total amount: $0 2测试点：on和off日期不在同一天却在同一个小时\n3测试点：on和off日期同一天同一个小时\n#include\u0026lt;bits/stdc++.h\u0026gt; using namespace std; int rate[25]; //存储24小时内的电话消费，rate【24】存储一天的消费 int N; typedef struct Phone { string name; int MM,DD,HH,mm,time; //time用来把所有时间换算成分钟 int turn; }; bool cmp(Phone a,Phone b) { //名字从小到大排序，名字相同按时间从小到大 if(a.name!=b.name) return a.name\u0026lt;b.name; return a.time\u0026lt;b.time; } double billfromzero(Phone call) { //从月初开始计费，开始与结束时间的话费差就是需要的费用 double total = rate[call.HH] * call.mm + rate[24] * 60 * call.DD; for (int i = 0; i \u0026lt; call.HH; i++) { total += rate[i] * 60; } total /= 100.0; return total; } int main() { for(int i=0; i\u0026lt;24; i++) { cin\u0026gt;\u0026gt;rate[i]; rate[24]+=rate[i]; } cin\u0026gt;\u0026gt;N; vector\u0026lt;Phone\u0026gt; P(N); for(int i=0; i\u0026lt;N; i++) { cin\u0026gt;\u0026gt;P[i].name; scanf(\u0026#34;%d:%d:%d:%d\u0026#34;,\u0026amp;P[i].MM,\u0026amp;P[i].DD,\u0026amp;P[i].HH,\u0026amp;P[i].mm); string a; cin\u0026gt;\u0026gt;a; P[i].turn = (a==\u0026#34;on-line\u0026#34;) ? 1:0;//“on-line”为1，“off-line”为0 P[i].time = P[i].DD*24*60+P[i].HH*60+P[i].mm;//换算成min为单位 } sort(P.begin(),P.end(),cmp); map\u0026lt;string,vector\u0026lt;Phone\u0026gt; \u0026gt; m; for(int i=1; i\u0026lt;N; i++) { if(P[i].name == P[i-1].name\u0026amp;\u0026amp;P[i-1].turn==1\u0026amp;\u0026amp;P[i].turn==0) { m[P[i-1].name].push_back(P[i-1]); m[P[i-1].name].push_back(P[i]); } } for(auto it : m) { vector\u0026lt;Phone\u0026gt; temp = it.second; cout\u0026lt;\u0026lt;it.first; printf(\u0026#34; %02d\\n\u0026#34;,temp[0].MM); double tol=0; for(int i=1; i\u0026lt;temp.size(); i+=2) { double money = billfromzero(temp[i])-billfromzero(temp[i-1]); printf(\u0026#34;%02d:%02d:%02d %02d:%02d:%02d %d $%.2f\\n\u0026#34;,temp[i-1].DD,temp[i-1].HH,temp[i-1].mm,temp[i].DD,temp[i].HH,temp[i].mm,temp[i].time-temp[i-1].time,money); tol+=money; } printf(\u0026#34;Total amount: $%.2f\\n\u0026#34;,tol); } return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1016phonebills%E6%A8%A1%E6%8B%9F/","section":"博客","summary":"1016 Phone Bills # 0、题目 # A long-distance telephone company charges its customers by the following rules:","title":"1016PhoneBills模拟"},{"content":"1017 Queueing at Bank # 0、题目 # Suppose a bank has K windows open for service. There is a yellow line in front of the windows which devides the waiting area into two parts. All the customers have to wait in line behind the yellow line, until it is his/her turn to be served and there is a window available. It is assumed that no window can be occupied by a single customer for more than 1 hour.\nNow given the arriving time T and the processing time P of each customer, you are supposed to tell the average waiting time of all the customers.\nInput Specification: # Each input file contains one test case. For each case, the first line contains 2 numbers: N (≤104) - the total number of customers, and K (≤100) - the number of windows. Then N lines follow, each contains 2 times: HH:MM:SS - the arriving time, and P - the processing time in minutes of a customer. Here HH is in the range [00, 23], MM and SS are both in [00, 59]. It is assumed that no two customers arrives at the same time.\nNotice that the bank opens from 08:00 to 17:00. Anyone arrives early will have to wait in line till 08:00, and anyone comes too late (at or after 17:00:01) will not be served nor counted into the average.\nOutput Specification: # For each test case, print in one line the average waiting time of all the customers, in minutes and accurate up to 1 decimal place.\nSample Input: # 7 3 07:55:00 16 17:00:01 2 07:59:59 15 08:01:00 60 08:00:00 30 08:00:02 2 08:03:00 10 Sample Output: # 8.2 1、大致题意 # 类似于 1014 Waiting in Line ，给出银行窗口数和办理业务人头数，并给出这些人到达的时间，计算最后人们的平均等待时间。\n1.1 测试点5 # 有了 1014 Waiting in Line 作为铺垫，基本上坑点都一模一样，就是 17：00 及以前来的那批到 17:00 时还没被服务的人，哪怕过了 17:00 也会被继续服务，哪怕银行加班到凌晨也要处理这批人的业务（诶，这真的反常识啊）。\n这个坑点被设计为 测试点5\n2、基本思路 # 设计结构体 customer 和 counter ，然后 sort 大法排序顾客到达店的顺序。用一个 priority_queue 模拟柜台。\nstruct customer { int hh,mm,ss,time; //time为转化为秒以后的时间 int cost; //办理业务所需时间 int start; //开始办理时间 bool operator \u0026gt;(const customer \u0026amp;a)const { if(time!=a.time) { return time \u0026lt; a.time; } else { return cost \u0026lt; a.cost; } } }; struct counter { int id; int end; bool operator \u0026gt;(const counter \u0026amp;a)const { if(end!=a.end) { return end \u0026gt; a.end; } else { return id \u0026gt; a.id; } } }; 3、AC代码 # #include\u0026lt;iostream\u0026gt; #include\u0026lt;algorithm\u0026gt; #include\u0026lt;queue\u0026gt; using namespace std; struct customer { int hh,mm,ss,time; //time为转化为秒以后的时间 int cost; //办理业务所需时间 int start; //开始办理时间 bool operator \u0026gt;(const customer \u0026amp;a)const { if(time!=a.time) { return time \u0026lt; a.time; } else { return cost \u0026lt; a.cost; } } } cus[10005]; struct counter { int id; int end; bool operator \u0026gt;(const counter \u0026amp;a)const { if(end!=a.end) { return end \u0026gt; a.end; } else { return id \u0026gt; a.id; } } } cou[105],tmp; int n,k,ans; priority_queue\u0026lt;counter,vector\u0026lt;counter\u0026gt;,greater\u0026lt;counter\u0026gt; \u0026gt;q1; int toTime(int hh,int mm,int ss) { return hh*60*60+mm*60+ss; } int main() { cin\u0026gt;\u0026gt;n\u0026gt;\u0026gt;k; for(int i=1; i\u0026lt;=n; i++) { scanf(\u0026#34;%d:%d:%d %d\u0026#34;,\u0026amp;cus[i].hh,\u0026amp;cus[i].mm,\u0026amp;cus[i].ss,\u0026amp;cus[i].cost); cus[i].cost*=60; cus[i].time=toTime(cus[i].hh,cus[i].mm,cus[i].ss); } sort(cus+1,cus+n+1,greater\u0026lt;customer\u0026gt;()); //\tcout\u0026lt;\u0026lt;cus[1].hh\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;cus[1].mm\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;cus[1].ss; ans=1; for(int i=1; i\u0026lt;=k; i++) { if(ans\u0026gt;n) { break; } else { tmp.id=i; if(cus[ans].time\u0026lt;=8*60*60) { cus[ans].start=8*60*60; } else { cus[ans].start=cus[ans].time; } tmp.end=cus[ans].start+cus[ans].cost; q1.push(tmp); ans++; } } while(!q1.empty()\u0026amp;\u0026amp;ans\u0026lt;=n) { tmp=q1.top(); q1.pop(); if(cus[ans].time\u0026gt;=tmp.end){ cus[ans].start=cus[ans].time; }else{ cus[ans].start=tmp.end; } tmp.end=cus[ans].start+cus[ans].cost; q1.push(tmp); ans++; } int summ=0,num=0; for(int i=1; i\u0026lt;=n; i++) { if(cus[i].time\u0026gt;17*60*60) { //坑点！！17:00到的不接待 continue; } else { num++; summ+=(cus[i].start-cus[i].time); } } printf(\u0026#34;%.1f\\n\u0026#34;,summ/num/60.0); return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1017queueingatbank%E6%A8%A1%E6%8B%9F/","section":"博客","summary":"1017 Queueing at Bank # 0、题目 # Suppose a bank has K windows open for service.","title":"1017QueueingatBank模拟"},{"content":"1018 Public Bike Management # 0、题目 # There is a public bike service in Hangzhou City which provides great convenience to the tourists from all over the world. One may rent a bike at any station and return it to any other stations in the city.\nThe Public Bike Management Center (PBMC) keeps monitoring the real-time capacity of all the stations. A station is said to be in perfect condition if it is exactly half-full. If a station is full or empty, PBMC will collect or send bikes to adjust the condition of that station to perfect. And more, all the stations on the way will be adjusted as well.\nWhen a problem station is reported, PBMC will always choose the shortest path to reach that station. If there are more than one shortest path, the one that requires the least number of bikes sent from PBMC will be chosen.\nThe above figure illustrates an example. The stations are represented by vertices and the roads correspond to the edges. The number on an edge is the time taken to reach one end station from another. The number written inside a vertex $S$ is the current number of bikes stored at S. Given that the maximum capacity of each station is 10. To solve the problem at $S_3$, we have 2 different shortest paths:\n$PBMC$ -\u0026gt; $S_1$ -\u0026gt; $S_3$. In this case, 4 bikes must be sent from PBMC, because we can collect 1 bike from $S_1$ and then take 5 bikes to $S_3$, so that both stations will be in perfect conditions.+ $PBMC$ -\u0026gt; $S_2$ -\u0026gt; $S_3$. This path requires the same time as path 1, but only 3 bikes sent from PBMC and hence is the one that will be chosen. Input Specification: # Each input file contains one test case. For each case, the first line contains 4 numbers: $C_{max}$ ($≤100$), always an even number, is the maximum capacity of each station; $N$ ($≤500$), the total number of stations; Sp, the index of the problem station (the stations are numbered from $1$ to $N$, and PBMC is represented by the vertex 0); and M, the number of roads. The second line contains N non-negative numbers $C_i$ (i=1,⋯,N) where each $C_i$ is the current number of bikes at $S_i$ respectively. Then $M$ lines follow, each contains 3 numbers: $S_i$, $S_j$, and $T_{ij}$ which describe the time $T_{ij}$ taken to move between stations Si and Sj. All the numbers in a line are separated by a space.\nOutput Specification: # For each test case, print your results in one line. First output the number of bikes that PBMC must send. Then after one space, output the path in the format: $0$−\u0026gt;$S_1$−\u0026gt;⋯−\u0026gt;$S_p$. Finally after another space, output the number of bikes that we must take back to PBMC after the condition of $S_p$ is adjusted to perfect.\nNote that if such a path is not unique, output the one that requires minimum number of bikes that we must take back to PBMC. The judge’s data guarantee that such a path is unique.\nSample Input: # 10 3 3 5 6 7 0 0 1 1 0 2 1 0 3 3 1 3 1 2 3 1 Sample Output: # 3 0-\u0026gt;2-\u0026gt;3 0 1、大致题意 # 从 0 点开始，找到 $S_p$ 的最短路，并使得路径上所有站点的自行车数量为 $\\frac{C_{max}}{2}$。\n若不唯一，找其中需要送出自行车最少的路径；+ 若还不唯一，找其中需要回收自行车最少的路径； 2、基本思路 # dijkstra 求单源最短路，在求最短路的同时，记录所有的最短路径。然后按照上面的两个条件进行比较。\n关于最短路的思路，在 1003 Emergency有写过，并且我也做过相关的总结 图论-最短路径问题。\n3、解题过程 # 安利一个 在线代码比较网站。\n3.1 第一份代码（22/30） # 有了上面的思路，很快写出了第一份代码\n#include\u0026lt;iostream\u0026gt; #include\u0026lt;algorithm\u0026gt; #include\u0026lt;queue\u0026gt; #include\u0026lt;vector\u0026gt; using namespace std; const int inf=0x3f3f3f3f; const int maxn=505; const int maxm=501000; int cmax,n,m,sp,c[maxn]; int head[maxn],vis[maxn],dis[maxn],num_edge; int pre[maxn]; int num_ans; vector\u0026lt;int\u0026gt;ans[maxn]; struct Edge { int to,wight,next; } edge[maxm]; void addEdge(int from, int to,int wight) { //头插法 edge[++num_edge].to=to; edge[num_edge].wight=wight; edge[num_edge].next=head[from]; head[from]=num_edge; } struct node { //到固定结点的距离 int dis; int pos; bool operator \u0026lt;( const node \u0026amp;x )const { return x.dis \u0026lt; dis; } }; priority_queue\u0026lt;node\u0026gt;q; void dijkstra(int sp) { for(int i=1; i\u0026lt;=n; i++) { dis[i]=inf; vis[i]=0; pre[i]=-1; } //从0点开始 q.push((node) { 0,0 }); dis[0]=0; pre[0]=0; while(!q.empty()) { node u=q.top(); q.pop(); int pos=u.pos; if(vis[pos]) { continue; } vis[pos]=1; for(int i=head[pos]; i; i=edge[i].next) { int to=edge[i].to; if(to==sp) { if(dis[to]\u0026gt;=dis[pos]+edge[i].wight) { if(dis[to]\u0026gt;dis[pos]+edge[i].wight) { num_ans=0; dis[to]=dis[pos]+edge[i].wight; pre[to]=pos; if(!vis[to]) { q.push((node) { dis[to],to }); } } ans[num_ans].clear(); for(int j=to; j; j=pre[j]) { //保存路径 ans[num_ans].push_back(j); } ans[num_ans].push_back(0); num_ans++; } } else { if(dis[to]\u0026gt;dis[pos]+edge[i].wight) { dis[to]=dis[pos]+edge[i].wight; pre[to]=pos; if(!vis[to]) { q.push((node) { dis[to],to }); } } } } } } int main() { cin\u0026gt;\u0026gt;cmax\u0026gt;\u0026gt;n\u0026gt;\u0026gt;sp\u0026gt;\u0026gt;m; for(int i=1; i\u0026lt;=n; i++) { cin\u0026gt;\u0026gt;c[i]; } int from,to,wight; num_edge=0; for(int i=0; i\u0026lt;m; i++) { cin\u0026gt;\u0026gt;from\u0026gt;\u0026gt;to\u0026gt;\u0026gt;wight; addEdge(from,to,wight); } num_ans=0; dijkstra(sp); //\tfor(int i=1; i\u0026lt;=n; i++) { //\tcout\u0026lt;\u0026lt;dis[i]\u0026lt;\u0026lt;\u0026#34; \u0026#34;; //\t} int send=0,end_send=inf; int back=0,end_back=inf; int tmp=0,end_n; int size; if(num_ans==0) { cout\u0026lt;\u0026lt;\u0026#34;no way!\u0026#34;; } else { for(int i=0; i\u0026lt;num_ans; i++) { size=ans[i].size(); for(int j=size-2; j\u0026gt;=0; j--) { int index=ans[i][j]; if(c[index]\u0026lt;cmax/2) { if(tmp\u0026gt;=cmax/2-c[index]) { tmp-=(cmax/2-c[index]); } else { send+=(cmax/2-c[index]-tmp); tmp=0; } } else if(c[index]\u0026gt;cmax/2) { tmp+=c[index]-cmax/2; } } back=tmp; if(end_send\u0026gt;=send) { if(end_send==send) { if(end_back\u0026gt;back) { end_n=i; end_send=send; end_back=back; } } else { end_n=i; end_send=send; end_back=back; } } } } cout\u0026lt;\u0026lt;end_send\u0026lt;\u0026lt;\u0026#34; \u0026#34;; size=ans[end_n].size(); for(int i=size-1; i\u0026gt;=0; i--) { cout\u0026lt;\u0026lt;ans[end_n][i]; if(i!=0) { cout\u0026lt;\u0026lt;\u0026#34;-\u0026gt;\u0026#34;; } } cout\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;end_back; return 0; } 3.2 无向图的邻接表表示法的初始化（13/30） # 这个在 1021 Deepest Root遇到过这个问题，就是在初始化的时候，需要把边的两个方向都存储。\nfor(int i=0; i\u0026lt;m; i++) { cin\u0026gt;\u0026gt;from\u0026gt;\u0026gt;to\u0026gt;\u0026gt;wight; addEdge(from,to,wight); addEdge(to,from,wight); } 但是当改了这个地方，反而分更低了（13/30）\n#include\u0026lt;iostream\u0026gt; #include\u0026lt;algorithm\u0026gt; #include\u0026lt;queue\u0026gt; #include\u0026lt;vector\u0026gt; using namespace std; const int inf=0x3f3f3f3f; const int maxn=5005; const int maxm=5010000; int cmax,n,m,sp,c[maxn]; int head[maxn],vis[maxn],dis[maxn],num_edge; int pre[maxn]; int num_ans; vector\u0026lt;int\u0026gt;ans[maxn]; struct Edge { int to,wight,next; } edge[maxm]; void addEdge(int from, int to,int wight) { //头插法 edge[++num_edge].to=to; edge[num_edge].wight=wight; edge[num_edge].next=head[from]; head[from]=num_edge; } struct node { //到固定结点的距离 int dis; int pos; bool operator \u0026lt;( const node \u0026amp;x )const { return x.dis \u0026lt; dis; } }; priority_queue\u0026lt;node\u0026gt;q; void dijkstra(int sp) { for(int i=1; i\u0026lt;=n; i++) { dis[i]=inf; vis[i]=0; pre[i]=-1; } //从0点开始 q.push((node) { 0,0 }); dis[0]=0; pre[0]=0; while(!q.empty()) { node u=q.top(); q.pop(); int pos=u.pos; if(vis[pos]) { continue; } vis[pos]=1; for(int i=head[pos]; i; i=edge[i].next) { int to=edge[i].to; if(to==sp) { //当结点为sp if(dis[to]\u0026gt;=dis[pos]+edge[i].wight) { //注意为 \u0026gt;= if(dis[to]\u0026gt;dis[pos]+edge[i].wight) { //\u0026gt;时，直接更新所有 num_ans=0; //更新所有就是将num_ans重新记录 dis[to]=dis[pos]+edge[i].wight; pre[to]=pos; if(!vis[to]) { q.push((node) { dis[to],to }); } } ans[num_ans].clear(); for(int j=to; j; j=pre[j]) { //保存路径 ans[num_ans].push_back(j); } ans[num_ans].push_back(0); //最后的0结点别忘了 num_ans++; } } else { //当结点不是sp if(dis[to]\u0026gt;dis[pos]+edge[i].wight) { dis[to]=dis[pos]+edge[i].wight; pre[to]=pos; if(!vis[to]) { q.push((node) { dis[to],to }); } } } } } } int main() { cin\u0026gt;\u0026gt;cmax\u0026gt;\u0026gt;n\u0026gt;\u0026gt;sp\u0026gt;\u0026gt;m; for(int i=1; i\u0026lt;=n; i++) { cin\u0026gt;\u0026gt;c[i]; } int from,to,wight; num_edge=0; for(int i=0; i\u0026lt;m; i++) { cin\u0026gt;\u0026gt;from\u0026gt;\u0026gt;to\u0026gt;\u0026gt;wight; addEdge(from,to,wight); addEdge(to,from,wight); } num_ans=0; dijkstra(sp); int send=0,end_send=inf; int back=0,end_back=inf; int tmp=0,end_n; int size; if(num_ans==0) { // cout\u0026lt;\u0026lt;\u0026#34;no way!\u0026#34;; } else { for(int i=0; i\u0026lt;num_ans; i++) { size=ans[i].size(); for(int j=size-2; j\u0026gt;=0; j--) { int index=ans[i][j]; if(c[index]\u0026lt;cmax/2) { if(tmp\u0026gt;=cmax/2-c[index]) { tmp-=(cmax/2-c[index]); } else { send+=(cmax/2-c[index]-tmp); tmp=0; } } else if(c[index]\u0026gt;cmax/2) { tmp+=c[index]-cmax/2; } } back=tmp; if(end_send\u0026gt;=send) { if(end_send==send) { if(end_back\u0026gt;back) { end_n=i; end_send=send; end_back=back; } } else { end_n=i; end_send=send; end_back=back; } } } } cout\u0026lt;\u0026lt;end_send\u0026lt;\u0026lt;\u0026#34; \u0026#34;; size=ans[end_n].size(); for(int i=size-1; i\u0026gt;=0; i--) { cout\u0026lt;\u0026lt;ans[end_n][i]; if(i!=0) { cout\u0026lt;\u0026lt;\u0026#34;-\u0026gt;\u0026#34;; } } cout\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;end_back; return 0; } 3.3 pre[] 数组的更新（27/30） # 直到上面的错误的检查以后，我陷入了很长时间的沉思。最后还是用的白盒测试法，主要就是下面的这些个用例：\n3.3.1 用例1 # 输入： 10 3 3 5 7 0 6 0 1 1 0 2 1 0 3 3 1 3 1 2 3 1 输出： 0 0-\u0026gt;1-\u0026gt;3 3 3.3.2 用例2 # 输入： 10 3 3 5 6 0 6 0 1 1 0 2 1 0 3 2 1 3 1 2 3 1 输出： 0 0-\u0026gt;3 1 3.3.3 用例3 # 输入： 10 2 2 2 2 10 0 1 1 1 2 1 输出： 3 0-\u0026gt;1-\u0026gt;2 5 3.3.4 用例4 # 输入： 10 3 3 3 11 0 10 0 1 1 1 2 1 2 3 1 输出： 0 0-\u0026gt;1-\u0026gt;2-\u0026gt;3 6 3.3.5 用例5 # 输入： 10 4 4 5 6 7 5 0 0 1 1 0 2 1 1 3 1 2 3 1 3 4 1 输出： 3 0-\u0026gt;2-\u0026gt;3-\u0026gt;4 0 3.3.6 用例6 # 输入： 10 4 4 4 6 0 11 0 0 1 1 1 2 1 2 3 1 3 4 1 输出： 4 0-\u0026gt;1-\u0026gt;2-\u0026gt;3-\u0026gt;4 1 3.3.7 用例7 # 输入： 10 4 4 8 0 11 3 0 0 1 2 0 2 3 0 3 2 1 2 1 2 3 1 1 4 2 2 4 1 3 4 2 输出： 0 0-\u0026gt;2-\u0026gt;4 1 3.3.8 修改过程（27/30） # 我们知道，在dijkstra的板子中，基本上所有的 pre 更新是在 dis[to]\u0026gt;dis[pos]+edge[i].wight 的条件下，但是在这道题中，对于$S_p$ 结点，条件应该变成dis[to]\u0026gt;=dis[pos]+edge[i].wight，并且对 \u0026gt; 和 = 讨论。这时就扯出来 pre[] 的修改时机问题。\nvoid dijkstra(int sp) { for(int i=1; i\u0026lt;=n; i++) { dis[i]=inf; vis[i]=0; pre[i]=-1; } //从0点开始 q.push((node) { 0,0 }); dis[0]=0; pre[0]=0; while(!q.empty()) { node u=q.top(); q.pop(); int pos=u.pos; if(vis[pos]) { continue; } vis[pos]=1; for(int i=head[pos]; i; i=edge[i].next) { int to=edge[i].to; if(to==sp) { //当结点为sp if(dis[to]\u0026gt;=dis[pos]+edge[i].wight) { //注意为 \u0026gt;= if(dis[to]\u0026gt;dis[pos]+edge[i].wight) { //\u0026gt;时，直接更新所有 num_ans=0; //更新所有就是将num_ans重新记录 dis[to]=dis[pos]+edge[i].wight; if(!vis[to]) { q.push((node) { dis[to],to }); } } pre[to]=pos; //这句话的位置很重要 ans[num_ans].clear(); for(int j=to; j; j=pre[j]) { //保存路径 ans[num_ans].push_back(j); } ans[num_ans].push_back(0); //最后的0结点别忘了 num_ans++; } } else { //当结点不是sp if(dis[to]\u0026gt;dis[pos]+edge[i].wight) { dis[to]=dis[pos]+edge[i].wight; pre[to]=pos; if(!vis[to]) { q.push((node) { dis[to],to }); } } } } } } 同时，我还发现了一个错误，就是在最后比较 send、back 等变量时，我没有对所有的变量初始化为0。\nsend=0,back=0,tmp=0; //初始化很重要 修改了这些以后，27/30，剩下了测试点7。\n#include\u0026lt;iostream\u0026gt; #include\u0026lt;algorithm\u0026gt; #include\u0026lt;queue\u0026gt; #include\u0026lt;vector\u0026gt; using namespace std; const int inf=0x3fffffff; const int maxn=5005; const int maxm=100010000; int cmax,n,m,sp,c[maxn]; int head[maxn],vis[maxn],dis[maxn],num_edge; int pre[maxn]; int num_ans; vector\u0026lt;int\u0026gt;ans[maxn]; struct Edge { int to,wight,next; } edge[maxm]; void addEdge(int from, int to,int wight) { //头插法 edge[++num_edge].to=to; edge[num_edge].wight=wight; edge[num_edge].next=head[from]; head[from]=num_edge; } struct node { //到固定结点的距离 int dis; int pos; bool operator \u0026lt;( const node \u0026amp;x )const { return x.dis \u0026lt; dis; } }; priority_queue\u0026lt;node\u0026gt;q; void dijkstra(int sp) { for(int i=1; i\u0026lt;=n; i++) { dis[i]=inf; vis[i]=0; pre[i]=-1; } //从0点开始 q.push((node) { 0,0 }); dis[0]=0; pre[0]=0; while(!q.empty()) { node u=q.top(); q.pop(); int pos=u.pos; if(vis[pos]) { continue; } vis[pos]=1; for(int i=head[pos]; i; i=edge[i].next) { int to=edge[i].to; if(to==sp) { //当结点为sp if(dis[to]\u0026gt;=dis[pos]+edge[i].wight) { //注意为 \u0026gt;= if(dis[to]\u0026gt;dis[pos]+edge[i].wight) { //\u0026gt;时，直接更新所有 num_ans=0; //更新所有就是将num_ans重新记录 dis[to]=dis[pos]+edge[i].wight; if(!vis[to]) { q.push((node) { dis[to],to }); } } pre[to]=pos; //这句话的位置很重要 ans[num_ans].clear(); for(int j=to; j; j=pre[j]) { //保存路径 ans[num_ans].push_back(j); } ans[num_ans].push_back(0); //最后的0结点别忘了 num_ans++; } } else { //当结点不是sp if(dis[to]\u0026gt;dis[pos]+edge[i].wight) { dis[to]=dis[pos]+edge[i].wight; pre[to]=pos; if(!vis[to]) { q.push((node) { dis[to],to }); } } } } } } int main() { cin\u0026gt;\u0026gt;cmax\u0026gt;\u0026gt;n\u0026gt;\u0026gt;sp\u0026gt;\u0026gt;m; for(int i=1; i\u0026lt;=n; i++) { cin\u0026gt;\u0026gt;c[i]; } int from,to,wight; num_edge=0; for(int i=0; i\u0026lt;m; i++) { cin\u0026gt;\u0026gt;from\u0026gt;\u0026gt;to\u0026gt;\u0026gt;wight; addEdge(from,to,wight); addEdge(to,from,wight); } num_ans=0; dijkstra(sp); int send=0,end_send=inf; int back=0,end_back=inf; int tmp=0,end_n; int size; if(num_ans==0) { // cout\u0026lt;\u0026lt;\u0026#34;no way!\u0026#34;; } else { for(int i=0; i\u0026lt;num_ans; i++) { send=0,back=0,tmp=0; //初始化很重要 size=ans[i].size(); for(int j=size-2; j\u0026gt;=0; j--) { int index=ans[i][j]; if(c[index]\u0026lt;cmax/2) { if(tmp\u0026gt;=cmax/2-c[index]) { tmp-=(cmax/2-c[index]); } else { send+=(cmax/2-c[index]-tmp); tmp=0; } } else if(c[index]\u0026gt;cmax/2) { tmp+=c[index]-cmax/2; } } back=tmp; if(end_send\u0026gt;send) { end_n=i; end_send=send; end_back=back; } else if(end_send==send) { if(end_back\u0026gt;=back) { end_n=i; end_send=send; end_back=back; } } } } cout\u0026lt;\u0026lt;end_send\u0026lt;\u0026lt;\u0026#34; \u0026#34;; size=ans[end_n].size(); for(int i=size-1; i\u0026gt;=0; i--) { cout\u0026lt;\u0026lt;ans[end_n][i]; if(i!=0) { cout\u0026lt;\u0026lt;\u0026#34;-\u0026gt;\u0026#34;; } } cout\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;end_back; return 0; } 3.4 测试点 7 # 这道题在牛客上也有， 牛客题目链接。牛客网的所有测试案例可以通过，但是可能会卡在PAT的第7个测试点。因为Note that if such a path is not unique, output the one that requires minimum number of bikes that we must take back to PBMC.即相同路径长度排序应该是先比较send较小的情况，其次在比较back较小的情况。\n但是我寻思我也考虑到了，但 测试点7 还是卡着。\n3.4.1 问题本质 # 后来我终于找到了问题。其实，我们不可以用一维数组 pre[] 直接记录所有路径，因为求解最短路可能会有多个前驱。如果你理解不了，可以看下面的这个用例：\n输入： 10 6 6 8 8 9 4 6 7 2 0 1 1 0 2 1 1 3 1 2 3 1 3 4 1 3 5 2 4 6 1 5 6 1 输出： 0 0-\u0026gt;1-\u0026gt;3-\u0026gt;4-\u0026gt;6 0 对于上面我的代码，每次都有更新，运行到点3时， pre[3] 可以为两个值 1 或者 2 ，但是由于运行时会更新 pre[] ，而 点1 的路径被覆盖了，所以最后的路径少记录了一条，最后比较的时候当然也会出错。\n3.4.2 修改方法 # 将 int pre[maxn]; 修改为 vector\u0026lt;int\u0026gt; pre[maxn]; ，然后将所有的前驱记录。当然修改这个点之后，就不能用简单的 for 循环遍历了，需要用到 dfs 。 在 dfs 时，保存所有路径，这个路径的保存可以参考二叉树的遍历和路径保存。具体代码如下：\n#include\u0026lt;iostream\u0026gt; #include\u0026lt;algorithm\u0026gt; #include\u0026lt;queue\u0026gt; #include\u0026lt;vector\u0026gt; using namespace std; const int inf=0x3fffffff; const int maxn=505; const int maxm=100010000; int cmax,n,m,sp,c[maxn]; int head[maxn],vis[maxn],dis[maxn],num_edge; vector\u0026lt;int\u0026gt; pre[maxn]; int num_ans; vector\u0026lt;int\u0026gt;ans[maxn]; struct Edge { int to,wight,next; } edge[maxm]; void addEdge(int from, int to,int wight) { //头插法 edge[++num_edge].to=to; edge[num_edge].wight=wight; edge[num_edge].next=head[from]; head[from]=num_edge; } struct node { //到固定结点的距离 int dis; int pos; bool operator \u0026lt;( const node \u0026amp;x )const { return x.dis \u0026lt; dis; } }; priority_queue\u0026lt;node\u0026gt;q; void dijkstra(int sp) { for(int i=1; i\u0026lt;=n; i++) { dis[i]=inf; vis[i]=0; pre[i].clear(); } //从0点开始 q.push((node) { 0,0 }); dis[0]=0; pre[0].push_back(0); while(!q.empty()) { node u=q.top(); q.pop(); int pos=u.pos; if(vis[pos]) { continue; } vis[pos]=1; for(int i=head[pos]; i; i=edge[i].next) { int to=edge[i].to; if(dis[to]\u0026gt;=dis[pos]+edge[i].wight) { //注意为 \u0026gt;= if(dis[to]\u0026gt;dis[pos]+edge[i].wight) { //\u0026gt;时，直接更新所有 pre[to].clear(); dis[to]=dis[pos]+edge[i].wight; if(!vis[to]) { q.push((node) { dis[to],to }); } } pre[to].push_back(pos); } } } } void dfs(int v,vector\u0026lt;int\u0026gt; a) { a.push_back(v); if(v==0) { ans[num_ans++]=a; a.clear(); return; } for(int i=0; i\u0026lt;pre[v].size(); i++) { dfs(pre[v][i],a); } } int main() { cin\u0026gt;\u0026gt;cmax\u0026gt;\u0026gt;n\u0026gt;\u0026gt;sp\u0026gt;\u0026gt;m; for(int i=1; i\u0026lt;=n; i++) { cin\u0026gt;\u0026gt;c[i]; } int from,to,wight; num_edge=0; for(int i=0; i\u0026lt;m; i++) { cin\u0026gt;\u0026gt;from\u0026gt;\u0026gt;to\u0026gt;\u0026gt;wight; addEdge(from,to,wight); addEdge(to,from,wight); } num_ans=0; dijkstra(sp); vector\u0026lt;int\u0026gt; a; dfs(sp,a); int send=0,end_send=inf; int back=0,end_back=inf; int tmp=0,end_n; int size; if(num_ans==0) { // cout\u0026lt;\u0026lt;\u0026#34;no way!\u0026#34;; } else { for(int i=0; i\u0026lt;num_ans; i++) { send=0,back=0,tmp=0; //初始化很重要 size=ans[i].size(); for(int j=size-2; j\u0026gt;=0; j--) { int index=ans[i][j]; if(c[index]\u0026lt;cmax/2) { if(tmp\u0026gt;=cmax/2-c[index]) { tmp-=(cmax/2-c[index]); } else { send+=(cmax/2-c[index]-tmp); tmp=0; } } else if(c[index]\u0026gt;cmax/2) { tmp+=c[index]-cmax/2; } } back=tmp; //\tcout\u0026lt;\u0026lt;send\u0026lt;\u0026lt;\u0026#34; \u0026#34;; //\tsize=ans[i].size(); //\tfor(int j=size-1; j\u0026gt;=0; j--) { //\tcout\u0026lt;\u0026lt;ans[i][j]; //\tif(j!=0) { //\tcout\u0026lt;\u0026lt;\u0026#34;-\u0026gt;\u0026#34;; //\t} //\t} //\tcout\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;back\u0026lt;\u0026lt;endl; if(end_send\u0026gt;=send) { if(end_send==send) { if(end_back\u0026gt;=back) { end_n=i; end_send=send; end_back=back; } } else if(end_send\u0026gt;send) { end_n=i; end_send=send; end_back=back; } } } } cout\u0026lt;\u0026lt;end_send\u0026lt;\u0026lt;\u0026#34; \u0026#34;; size=ans[end_n].size(); for(int i=size-1; i\u0026gt;=0; i--) { cout\u0026lt;\u0026lt;ans[end_n][i]; if(i!=0) { cout\u0026lt;\u0026lt;\u0026#34;-\u0026gt;\u0026#34;; } } cout\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;end_back; return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1018publicbikemanagementdijkstradfs/","section":"博客","summary":"1018 Public Bike Management # 0、题目 # There is a public bike service in Hangzhou City which provides great convenience to the tourists from all over the world.","title":"1018PublicBikeManagementDijkstraDFS"},{"content":"1019 General Palindromic Number # 0、题目 # A number that will be the same when it is written forwards or backwards is known as a Palindromic Number. For example, 1234321 is a palindromic number. All single digit numbers are palindromic numbers.\nAlthough palindromic numbers are most often considered in the decimal system, the concept of palindromicity can be applied to the natural numbers in any numeral system. Consider a number $N\u0026gt;0$ in base $b≥2$, where it is written in standard notation with $k+1$ digits $a_i$ as $∑{i=0}^k(a_ib_i)$. Here, as usual, $0≤ai\u0026lt;b$ for all $i$ and $a_k$ is non-zero. Then $N$ is palindromic if and only if $a_i=a{k−i}$ for all $i$. Zero is written 0 in any base and is also palindromic by definition.\nGiven any positive decimal integer $N$ and a base $b$, you are supposed to tell if $N$ is a palindromic number in base $b$.\nInput Specification: # Each input file contains one test case. Each case consists of two positive numbers N and b, where $0\u0026lt;N≤10^9$ is the decimal number and $2≤b≤10^9$ is the base. The numbers are separated by a space.\nOutput Specification: # For each test case, first print in one line Yes if N is a palindromic number in base b, or No if not. Then in the next line, print N as the number in base b in the form “$a_k a_{k−1} \u0026hellip; a_0$”. Notice that there must be no extra space at the end of output.\nSample Input 1: # 27 2 Sample Output 1: # Yes 1 1 0 1 1 Sample Input 2: # 121 5 Sample Output 2: # No 4 4 1 1、大致题意 # 给出一个 10进制数 和 另一个基数b，问这个 10进制 数转化为 基数b 的数值是不是回文数\n2、基本思路 # 简单题\n3、AC代码 # #include\u0026lt;iostream\u0026gt; #include\u0026lt;vector\u0026gt; using namespace std; int N,b; vector\u0026lt;int\u0026gt; a; void init(int N) { int n=N,k; while(n!=0) { k=n%b; n/=b; a.push_back(k); } } int is_Palindromic() { int size=a.size(); for(int i=0; i\u0026lt;size/2; i++) { if(a[i]!=a[size-i-1]) { return -1; } } return 1; } void print() { int size=a.size(); cout\u0026lt;\u0026lt;a[size-1]; for(int i=size-2; i\u0026gt;=0; i--) { cout\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;a[i]; } } int main() { cin\u0026gt;\u0026gt;N\u0026gt;\u0026gt;b; init(N); if(is_Palindromic()==1) { cout\u0026lt;\u0026lt;\u0026#34;Yes\u0026#34;\u0026lt;\u0026lt;endl; print(); } else { cout\u0026lt;\u0026lt;\u0026#34;No\u0026#34;\u0026lt;\u0026lt;endl; print(); } return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1019generalpalindromicnumber%E8%BF%9B%E5%88%B6%E8%BD%AC%E5%8C%96%E5%9B%9E%E6%96%87%E6%95%B0/","section":"博客","summary":"1019 General Palindromic Number # 0、题目 # A number that will be the same when it is written forwards or backwards is known as a Palindromic Number.","title":"1019GeneralPalindromicNumber进制转化回文数"},{"content":"1020 Tree Traversals # 0、题目 # Suppose that all the keys in a binary tree are distinct positive integers. Given the postorder and inorder traversal sequences, you are supposed to output the level order traversal sequence of the corresponding binary tree.\nInput Specification: # Each input file contains one test case. For each case, the first line gives a positive integer $N$ ($≤30$), the total number of nodes in the binary tree. The second line gives the postorder sequence and the third line gives the inorder sequence. All the numbers in a line are separated by a space.\nOutput Specification: # For each test case, print in one line the level order traversal sequence of the corresponding binary tree. All the numbers in a line must be separated by exactly one space, and there must be no extra space at the end of the line.\nSample Input: # 7 2 3 1 5 7 6 4 1 2 3 4 5 6 7 Sample Output: # 4 1 6 3 5 7 2 1、大致题意 # 给出二叉树的先序和中序排列，求层次排列。\n2、基本思路 # 序号0123456postorder2315764inorder1234567\n可以拿上面那个模拟一下，树长这样\n3、解题过程 # 这里给大家安利一个 二叉树和哈夫曼树在线绘制网站。如果感兴趣，可以看附件的 html 文件。\n3.1 AC代码 # #include\u0026lt;string\u0026gt; #include\u0026lt;queue\u0026gt; #include\u0026lt;algorithm\u0026gt; using namespace std; const int maxn = 50; struct node { int data; node* lchild; node* rchild; }; int n; int pre[maxn], in[maxn], post[maxn]; node* create(int postL, int postR, int inL, int inR) { if(postL \u0026gt; postR) return NULL; node* root = new node; root-\u0026gt;data = post[postR]; int k; for(k=inL; k\u0026lt;=inR; k++) { if(in[k] == post[postR]) { break; } } int numLeft = k -inL; root-\u0026gt;lchild = create(postL, postL + numLeft -1, inL, k-1); root-\u0026gt;rchild = create(postL + numLeft, postR-1, k+1, inR); return root; } void BFS(node* root) { queue\u0026lt;node*\u0026gt; q; q.push(root); int num = 0; while(!q.empty()) { node* now = q.front(); q.pop(); printf(\u0026#34;%d\u0026#34;, now-\u0026gt;data); num++; if(num \u0026lt; n) printf(\u0026#34; \u0026#34;); if(now-\u0026gt;lchild != NULL) q.push(now-\u0026gt;lchild); if(now-\u0026gt;rchild != NULL) q.push(now-\u0026gt;rchild); } } int main() { scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); for(int i=0; i\u0026lt;n; i++) { scanf(\u0026#34;%d\u0026#34;,\u0026amp;post[i]); } for(int i=0; i\u0026lt;n; i++) { scanf(\u0026#34;%d\u0026#34;,\u0026amp;in[i]); } node* root = create(0, n-1, 0, n-1); BFS(root); return 0; } 3.2 附件 - 在线绘制二叉树 # \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;!-- saved from url=(0039)http://www.easycode.top/binarytree.html --\u0026gt; \u0026lt;html lang=\u0026#34;zh\u0026#34;\u0026gt;\u0026lt;head\u0026gt;\u0026lt;meta http-equiv=\u0026#34;Content-Type\u0026#34; content=\u0026#34;text/html; charset=UTF-8\u0026#34;\u0026gt;\u0026lt;script charset=\u0026#34;utf-8\u0026#34; src=\u0026#34;./二叉树_files/UrlChangeTracker.js.下载\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script src=\u0026#34;./二叉树_files/f.txt\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script src=\u0026#34;./二叉树_files/f(1).txt\u0026#34; id=\u0026#34;google_shimpl\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script src=\u0026#34;./二叉树_files/hm.js.下载\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script\u0026gt; var _hmt = _hmt || []; (function() { var hm = document.createElement(\u0026#34;script\u0026#34;); hm.src = \u0026#34;https://hm.baidu.com/hm.js?50dd33792a91ed9832a6345b5162f0a6\u0026#34;; var s = document.getElementsByTagName(\u0026#34;script\u0026#34;)[0]; s.parentNode.insertBefore(hm, s); })(); \u0026lt;/script\u0026gt; \u0026lt;script async=\u0026#34;\u0026#34; src=\u0026#34;./二叉树_files/f(2).txt\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; data-checked-head=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;meta name=\u0026#34;baidu_union_verify\u0026#34; content=\u0026#34;1fa00acc28e7b488e6c9d6b1ad72b157\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;二叉树\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;renderer\u0026#34; content=\u0026#34;webkit|ie-comp|ie-stand\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;keywords\u0026#34; content=\u0026#34;二叉树,在线二叉树转换,前序,后序,中序,求后序,求前序,构建二叉树\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;heaboy作为一个在线工具提供者,立志于提供更多方便的在线工具\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;shortcut icon\u0026#34; type=\u0026#34;image/x-icon\u0026#34; href=\u0026#34;http://www.easycode.top/static/favicon.ico\u0026#34;\u0026gt; \u0026lt;!-- 最新版本的 Bootstrap 核心 CSS 文件 --\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;./二叉树_files/bootstrap.min.css\u0026#34; integrity=\u0026#34;sha384-HSMxcRTRxnN+Bdg0JdbxYKrThecOKuH5zCYotlSAcp1+c8xmyTe9GYg1l9a69psu\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt; \u0026lt;meta http-equiv=\u0026#34;origin-trial\u0026#34; content=\u0026#34;AzoawhTRDevLR66Y6MROu167EDncFPBvcKOaQispTo9ouEt5LvcBjnRFqiAByRT+2cDHG1Yj4dXwpLeIhc98/gIAAACFeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjYxMjk5MTk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ==\u0026#34;\u0026gt;\u0026lt;meta http-equiv=\u0026#34;origin-trial\u0026#34; content=\u0026#34;A6+nc62kbJgC46ypOwRsNW6RkDn2x7tgRh0wp7jb3DtFF7oEhu1hhm4rdZHZ6zXvnKZLlYcBlQUImC4d3kKihAcAAACLeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjYxMjk5MTk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ==\u0026#34;\u0026gt;\u0026lt;meta http-equiv=\u0026#34;origin-trial\u0026#34; content=\u0026#34;A/9La288e7MDEU2ifusFnMg1C2Ij6uoa/Z/ylwJIXSsWfK37oESIPbxbt4IU86OGqDEPnNVruUiMjfKo65H/CQwAAACLeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXRhZ3NlcnZpY2VzLmNvbTo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjYxMjk5MTk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ==\u0026#34;\u0026gt;\u0026lt;meta http-equiv=\u0026#34;origin-trial\u0026#34; content=\u0026#34;AzoawhTRDevLR66Y6MROu167EDncFPBvcKOaQispTo9ouEt5LvcBjnRFqiAByRT+2cDHG1Yj4dXwpLeIhc98/gIAAACFeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjYxMjk5MTk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ==\u0026#34;\u0026gt;\u0026lt;meta http-equiv=\u0026#34;origin-trial\u0026#34; content=\u0026#34;A6+nc62kbJgC46ypOwRsNW6RkDn2x7tgRh0wp7jb3DtFF7oEhu1hhm4rdZHZ6zXvnKZLlYcBlQUImC4d3kKihAcAAACLeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjYxMjk5MTk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ==\u0026#34;\u0026gt;\u0026lt;meta http-equiv=\u0026#34;origin-trial\u0026#34; content=\u0026#34;A/9La288e7MDEU2ifusFnMg1C2Ij6uoa/Z/ylwJIXSsWfK37oESIPbxbt4IU86OGqDEPnNVruUiMjfKo65H/CQwAAACLeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXRhZ3NlcnZpY2VzLmNvbTo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjYxMjk5MTk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ==\u0026#34;\u0026gt;\u0026lt;link rel=\u0026#34;preload\u0026#34; href=\u0026#34;./二叉树_files/f(3).txt\u0026#34; as=\u0026#34;script\u0026#34;\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;./二叉树_files/f(3).txt\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script src=\u0026#34;chrome-extension://mooikfkahbdckldjjndioackbalphokd/assets/prompt.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;header\u0026gt; \u0026lt;div class=\u0026#34;head\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;ul class=\u0026#34;nav navbar-nav\u0026#34;\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;http://www.easycode.top/binarytree.html\u0026#34;\u0026gt;二叉树\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;http://www.easycode.top/huffman.html\u0026#34;\u0026gt;哈夫曼树\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/header\u0026gt; \u0026lt;div class=\u0026#34;container-fluid\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;col-md-4\u0026#34;\u0026gt; \u0026lt;blockquote\u0026gt; \u0026lt;div\u0026gt;\u0026lt;strong\u0026gt;中序是必须的,\u0026lt;/strong\u0026gt;只有前序和后序，树基本上不唯一\u0026lt;/div\u0026gt; \u0026lt;/blockquote\u0026gt; \u0026lt;div class=\u0026#34;form form-horizontal\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;form-group\u0026#34;\u0026gt; \u0026lt;label for=\u0026#34;pre\u0026#34; class=\u0026#34;col-sm-2 control-label\u0026#34;\u0026gt;前序\u0026lt;/label\u0026gt; \u0026lt;div class=\u0026#34;col-sm-10\u0026#34;\u0026gt; \u0026lt;input class=\u0026#34;form-control\u0026#34; type=\u0026#34;text\u0026#34; name=\u0026#34;pre\u0026#34; id=\u0026#34;pre\u0026#34; value=\u0026#34;\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;form-group\u0026#34;\u0026gt; \u0026lt;label for=\u0026#34;mid\u0026#34; class=\u0026#34;col-sm-2 control-label\u0026#34;\u0026gt;中序\u0026lt;/label\u0026gt; \u0026lt;div class=\u0026#34;col-sm-10\u0026#34;\u0026gt; \u0026lt;input class=\u0026#34;form-control\u0026#34; type=\u0026#34;text\u0026#34; name=\u0026#34;mid\u0026#34; id=\u0026#34;mid\u0026#34; value=\u0026#34;HDIBEJAFKCG\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;form-group\u0026#34;\u0026gt; \u0026lt;label for=\u0026#34;back\u0026#34; class=\u0026#34;col-sm-2 control-label\u0026#34;\u0026gt;后序\u0026lt;/label\u0026gt; \u0026lt;div class=\u0026#34;col-sm-10\u0026#34;\u0026gt; \u0026lt;input class=\u0026#34;form-control\u0026#34; type=\u0026#34;text\u0026#34; name=\u0026#34;back\u0026#34; id=\u0026#34;back\u0026#34; value=\u0026#34;HIDJEBKFGCA\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;form-group\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;col-sm-offset-10 col-sm2\u0026#34;\u0026gt; \u0026lt;button class=\u0026#34;btn btn-primary btn-lg\u0026#34; onclick=\u0026#34;compute()\u0026#34;\u0026gt;走你\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;col-md-8 text-center\u0026#34;\u0026gt; \u0026lt;canvas id=\u0026#34;canvas\u0026#34; width=\u0026#34;600\u0026#34; height=\u0026#34;600\u0026#34;\u0026gt; 您的浏览器不支持图形,请升级浏览器 \u0026lt;/canvas\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script async=\u0026#34;\u0026#34; src=\u0026#34;./二叉树_files/f(2).txt\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; data-checked-head=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- 头广告 --\u0026gt; \u0026lt;ins class=\u0026#34;adsbygoogle\u0026#34; style=\u0026#34;display: block; height: 280px;\u0026#34; data-ad-client=\u0026#34;ca-pub-2380949900461835\u0026#34; data-ad-slot=\u0026#34;5565476897\u0026#34; data-ad-format=\u0026#34;auto\u0026#34; data-full-width-responsive=\u0026#34;true\u0026#34; data-adsbygoogle-status=\u0026#34;done\u0026#34; data-ad-status=\u0026#34;unfilled\u0026#34;\u0026gt;\u0026lt;div id=\u0026#34;aswift_1_host\u0026#34; tabindex=\u0026#34;0\u0026#34; title=\u0026#34;Advertisement\u0026#34; aria-label=\u0026#34;Advertisement\u0026#34; style=\u0026#34;border: none; height: 280px; width: 1200px; margin: 0px; padding: 0px; position: relative; visibility: visible; background-color: transparent; display: inline-block; overflow: visible;\u0026#34;\u0026gt;\u0026lt;iframe id=\u0026#34;aswift_1\u0026#34; name=\u0026#34;aswift_1\u0026#34; style=\u0026#34;left:0;position:absolute;top:0;border:0;width:1200px;height:280px;\u0026#34; sandbox=\u0026#34;allow-forms allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-top-navigation-by-user-activation\u0026#34; width=\u0026#34;1200\u0026#34; height=\u0026#34;280\u0026#34; frameborder=\u0026#34;0\u0026#34; marginwidth=\u0026#34;0\u0026#34; marginheight=\u0026#34;0\u0026#34; vspace=\u0026#34;0\u0026#34; hspace=\u0026#34;0\u0026#34; allowtransparency=\u0026#34;true\u0026#34; scrolling=\u0026#34;no\u0026#34; src=\u0026#34;./二叉树_files/ads.html\u0026#34; data-google-container-id=\u0026#34;a!2\u0026#34; data-load-complete=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/ins\u0026gt; \u0026lt;script\u0026gt; (adsbygoogle = window.adsbygoogle || []).push({}); \u0026lt;/script\u0026gt; \u0026lt;footer\u0026gt; \u0026lt;div id=\u0026#34;beian\u0026#34; class=\u0026#34;text-center\u0026#34; style=\u0026#34;margin-top: 60px;\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;https://beian.miit.gov.cn/#/Integrated/recordQuery\u0026#34; target=\u0026#34;_black\u0026#34;\u0026gt;冀ICP备2021023452号-1\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/footer\u0026gt; \u0026lt;script src=\u0026#34;./二叉树_files/binarytree.js.下载\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; var compute = function () { let pre = document.getElementById(\u0026#34;pre\u0026#34;).value; let mid = document.getElementById(\u0026#34;mid\u0026#34;).value; let back = document.getElementById(\u0026#34;back\u0026#34;).value; let preArray = new Array(); preArray = pre.split(\u0026#34;\u0026#34;); let midArray = new Array(); midArray = mid.split(\u0026#34;\u0026#34;); let backArray = new Array(); backArray = back.split(\u0026#34;\u0026#34;); console.log(preArray); if (midArray.length == 0) { console.log(\u0026#34;中序为空\u0026#34;); alert(\u0026#34;中序不得为空\u0026#34;); return; } if (preArray.length == 0) { console.log(\u0026#34;前序为空\u0026#34;); let root = backMid(midArray, backArray); draw(root, 0, 600, 25); let prearr = preorder(root); document.getElementById(\u0026#34;pre\u0026#34;).value = prearr.join(\u0026#34;\u0026#34;); } else if (backArray.length == 0) { console.log(\u0026#34;后序为空\u0026#34;); let root = preMid(preArray, midArray); draw(root, 0, 600, 25); let post = postorder(root); document.getElementById(\u0026#34;back\u0026#34;).value = post.join(\u0026#34;\u0026#34;); } } var backMid = function (midArr, backArr) { if (!midArr || midArr.length === 0 || !backArr || backArr.length === 0 || midArr === backArr) return null; // 获取后序的最后一个值为根节点, 获取中序的根节点的位置，分割成左右子树; 获取中序的左子树， 右子树， 获取后序的左子树和右子树 let rootNode = new BinTree(backArr[backArr.length - 1]); let index = midArr.indexOf(backArr[backArr.length - 1]); let midArrLeft = midArr.slice(0, index); let midArrRight = midArr.slice(index + 1, midArr.length); let backArrLeft = backArr.slice(0, index); let backArrRight = backArr.slice(index, backArr.length - 1); rootNode.left = backMid(midArrLeft, backArrLeft); rootNode.right = backMid(midArrRight, backArrRight); return rootNode; } var preMid = function (preArr, midArr) { if (!preArr || preArr.length === 0 || !midArr || midArr.length === 0 || preArr === midArr) return null; // 获取前序遍历的跟节点, 左子树的长度, 前序左子树，前序右子树， 中序左子树， 中序右子树 let rootNode = new BinTree(preArr[0]); let index = midArr.indexOf(preArr[0]); let preArrLeft = preArr.slice(1, index + 1); let preArrRight = preArr.slice(index + 1); let midArrLeft = midArr.slice(0, index); let midArrRight = midArr.slice(index + 1); rootNode.left = preMid(preArrLeft, midArrLeft); rootNode.right = preMid(preArrRight, midArrRight); return rootNode; } \u0026lt;/script\u0026gt; \u0026lt;ins class=\u0026#34;adsbygoogle adsbygoogle-noablate\u0026#34; data-adsbygoogle-status=\u0026#34;done\u0026#34; style=\u0026#34;display: none !important;\u0026#34; data-ad-status=\u0026#34;unfilled\u0026#34;\u0026gt;\u0026lt;div id=\u0026#34;aswift_0_host\u0026#34; tabindex=\u0026#34;0\u0026#34; title=\u0026#34;Advertisement\u0026#34; aria-label=\u0026#34;Advertisement\u0026#34; style=\u0026#34;border: none; height: 0px; width: 0px; margin: 0px; padding: 0px; position: relative; visibility: visible; background-color: transparent; display: inline-block;\u0026#34;\u0026gt;\u0026lt;iframe id=\u0026#34;aswift_0\u0026#34; name=\u0026#34;aswift_0\u0026#34; style=\u0026#34;left:0;position:absolute;top:0;border:0;width:undefinedpx;height:undefinedpx;\u0026#34; sandbox=\u0026#34;allow-forms allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-top-navigation-by-user-activation\u0026#34; frameborder=\u0026#34;0\u0026#34; marginwidth=\u0026#34;0\u0026#34; marginheight=\u0026#34;0\u0026#34; vspace=\u0026#34;0\u0026#34; hspace=\u0026#34;0\u0026#34; allowtransparency=\u0026#34;true\u0026#34; scrolling=\u0026#34;no\u0026#34; src=\u0026#34;./二叉树_files/ads(1).html\u0026#34; data-google-container-id=\u0026#34;a!1\u0026#34; data-load-complete=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/ins\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;iframe id=\u0026#34;google_esf\u0026#34; name=\u0026#34;google_esf\u0026#34; src=\u0026#34;./二叉树_files/zrt_lookup.html\u0026#34; style=\u0026#34;display: none;\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt;\u0026lt;/html\u0026gt; ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1020treetraversals%E4%BA%8C%E5%8F%89%E6%A0%91dlrldr%E6%B1%82%E5%B1%82%E6%AC%A1%E6%8E%92%E5%88%97/","section":"博客","summary":"1020 Tree Traversals # 0、题目 # Suppose that all the keys in a binary tree are distinct positive integers.","title":"1020TreeTraversals二叉树DLRLDR求层次排列"},{"content":"1021 Deepest Root # 0、题目 # graph which is connected and acyclic can be considered a tree. The height of the tree depends on the selected root. Now you are supposed to find the root that results in a highest tree. Such a root is called the deepest root.\nInput Specification: # Each input file contains one test case. For each case, the first line contains a positive integer $N$ ($≤10^4$) which is the number of nodes, and hence the nodes are numbered from $1$ to $N$. Then $N−1$ lines follow, each describes an edge by given the two adjacent nodes’ numbers.\nOutput Specification: # For each test case, print each of the deepest roots in a line. If such a root is not unique, print them in increasing order of their numbers. In case that the given graph is not a tree, print Error: K components where K is the number of connected components in the graph.\nSample Input 1: # 5 1 2 1 3 1 4 2 5 Sample Output 1: # 3 4 5 Sample Input 2: # 5 1 3 1 4 2 5 3 4 Sample Output 2: # Error: 2 components 1、大致题意 # 给出无向图的顶点个数 $N$ ，和 $N-1$ 条边，判断这个无向图可不可以转化成树。\n如果可以，给出所有具有最大深度树的树根+ 如果不行，输出连通子集个数 2、基本思路 # 2.1 无向图转化为树的条件 # 首先一个问题，怎样可以判断一个无向图可以转化为树？\n图必须是无回路的连通图+ 当该图为有n-1条边的连通图，也为树 这边可以思考一下为什么满足以上两个条件只一就行。应该画个图就行。\n2.1 判断图连通分量的个数 # 有两种方法 ——dfs 和 并查集。\n在前面的 1013 Battle Over Cities 使用过使用并查集查找连通子集的个数，这边就使用 dfs 。\nvis[] 记录访问历史， 访问过该结点就跳过，没有访问过就 dfs(i) ，同时 连通分量数+1，即：如果图是连通的，dfs一次就可以访问到所有结点。+ 如果该图为树，让每个结点都做一次根，deep[] 记录每个结点为根时的树的深度，对每个结点 dfs_deep()，用 tmp 记录当前的深度，并与 max_deep 比较，如果有更大的就更新 vector\u0026lt;int\u0026gt;ans 。 3、解题过程 # 3.1 在线画图工具 # 首先，安利一个在线 画图工具\n例如：\nSample Output 1: 5 1 2 1 3 1 4 2 5 Sample Input 2: 5 1 3 1 4 2 5 3 4 3.2 AC代码 # #include\u0026lt;iostream\u0026gt; #include\u0026lt;cstring\u0026gt; #include\u0026lt;vector\u0026gt; using namespace std; const int maxn=10040; const int maxm=100050; struct Edge { int to,next; } edge[maxm]; int n,head[maxn],vis[maxn],num_edge; int num_components,max_deep,tmp; vector\u0026lt;int\u0026gt;ans; void addedge(int from,int to) { //头插法插入结点 edge[++num_edge].next=head[from]; edge[num_edge].to=to; head[from]=num_edge; } void dfs(int x) { int k=head[x]; while(k!=0) { if(vis[edge[k].to]==1) { k=edge[k].next; continue; } vis[edge[k].to]=1; dfs(edge[k].to); k=edge[k].next; } } void dfs_deep(int x,int deep) { //dfs查找最大深度 if(!vis[x]) { vis[x]=1; int k=head[x]; while(k!=0) { if(vis[edge[k].to]==1) { k=edge[k].next; continue; } dfs_deep(edge[k].to,deep+1); tmp=max(deep+1,tmp); k=edge[k].next; } } } int main() { cin\u0026gt;\u0026gt;n; int a,b; memset(edge,0,sizeof(edge)); memset(vis,0,sizeof(vis)); memset(head,0,sizeof(head)); for(int i=0; i\u0026lt;n-1; i++) { cin\u0026gt;\u0026gt;a\u0026gt;\u0026gt;b; addedge(a,b); addedge(b,a); } num_components=0; for(int i=1; i\u0026lt;=n; i++) { if(vis[i]==0) { dfs(i); num_components++; } } if(num_components!=1) { //图不能转化为树 cout\u0026lt;\u0026lt;\u0026#34;Error: \u0026#34;\u0026lt;\u0026lt;num_components\u0026lt;\u0026lt;\u0026#34; components\u0026#34;; } else { max_deep=-1; for(int i=1; i\u0026lt;=n; i++) { memset(vis,0,sizeof(vis)); tmp=0; dfs_deep(i,0); if(tmp\u0026gt;max_deep) { //当前深度大于最大深度 ans.clear(); ans.push_back(i); max_deep=tmp; } else if(tmp==max_deep) { //当前深度等于最大深度 ans.push_back(i); max_deep=tmp; } } cout\u0026lt;\u0026lt;ans[0]; for(int i=1; i\u0026lt;ans.size(); i++) { cout\u0026lt;\u0026lt;\u0026#34;\\n\u0026#34;\u0026lt;\u0026lt;ans[i]; } } return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1021deepestrootdfs%E5%9B%BE%E7%9A%84%E8%81%94%E9%80%9A%E5%AD%90%E9%9B%86%E4%B8%AA%E6%95%B0%E6%A0%91%E7%9A%84%E6%B7%B1%E5%BA%A6/","section":"博客","summary":"1021 Deepest Root # 0、题目 # graph which is connected and acyclic can be considered a tree.","title":"1021DeepestRootdfs图的联通子集个数树的深度"},{"content":"1022 Digital Library # 0、题目 # A Digital Library contains millions of books, stored according to their titles, authors, key words of their abstracts, publishers, and published years. Each book is assigned an unique 7-digit number as its ID. Given any query from a reader, you are supposed to output the resulting books, sorted in increasing order of their ID’s.\nInput Specification: # Each input file contains one test case. For each case, the first line contains a positive integer N (≤104) which is the total number of books. Then N blocks follow, each contains the information of a book in 6 lines:\nLine #1: the 7-digit ID number;+ Line #2: the book title – a string of no more than 80 characters;+ Line #3: the author – a string of no more than 80 characters;+ Line #4: the key words – each word is a string of no more than 10 characters without any white space, and the keywords are separated by exactly one space;+ Line #5: the publisher – a string of no more than 80 characters;+ Line #6: the published year – a 4-digit number which is in the range [1000, 3000]. It is assumed that each book belongs to one author only, and contains no more than 5 key words; there are no more than 1000 distinct key words in total; and there are no more than 1000 distinct publishers.\nAfter the book information, there is a line containing a positive integer M (≤1000) which is the number of user’s search queries. Then M lines follow, each in one of the formats shown below:\n1: a book title+ 2: name of an author+ 3: a key word+ 4: name of a publisher+ 5: a 4-digit number representing the year Output Specification: # For each query, first print the original query in a line, then output the resulting book ID’s in increasing order, each occupying a line. If no book is found, print Not Found instead.\nSample Input: # 3 1111111 The Testing Book Yue Chen test code debug sort keywords ZUCS Print 2011 3333333 Another Testing Book Yue Chen test code sort keywords ZUCS Print2 2012 2222222 The Testing Book CYLL keywords debug book ZUCS Print2 2011 6 1: The Testing Book 2: Yue Chen 3: keywords 4: ZUCS Print 5: 2011 3: blablabla Sample Output: # 1: The Testing Book 1111111 2222222 2: Yue Chen 1111111 3333333 3: keywords 1111111 2222222 3333333 4: ZUCS Print 1111111 5: 2011 1111111 2222222 3: blablabla Not Found 1、大致题意 # 输入一本书的各个数据，建立图书查询系统，最后对书目进行查询。\n2、基本思路 # 使用 map 来映射书和书的各个数据，用 set 来存储书的编号，set 可以自动去重并且按从小到大的顺序排列。\n在写这题前，我复习了 map 的相关知识\n3、解题过程 # 对除了 id 之外的其他信息都建立一个 map\u0026lt;string, set\u0026gt; ，把相应的id插入对应搜索词的 map 的集合里面，形成一个信息对应一个集合，集合里面是复合条件的书的 id+ 因为对于输入的关键词（可以重复，算是书本对应的tag标签吧～）没有给定关键词的个数，可以使用 while(cin \u0026gt;\u0026gt; s) 并且判断 c = getchar()，c 是否等于\\n，如果是再退出循环～+ 建立 query ，通过传参的形式可以将不同的 map 名称统一化，先要判断 map.find() 和 m.end() 是否相等，如果不等再去遍历整个 map ，输出所有满足条件的 id，如果相等就说明不存在这个搜索词对应的id，那么就要输出 Not Found～+ 传参一定要用引用，否则最后一组数据可能会超时～ #include \u0026lt;iostream\u0026gt; #include \u0026lt;map\u0026gt; #include \u0026lt;set\u0026gt; using namespace std; map\u0026lt;string, set\u0026lt;int\u0026gt; \u0026gt; title, author, key, pub, year; void query(map\u0026lt;string, set\u0026lt;int\u0026gt; \u0026gt; \u0026amp;m, string \u0026amp;str) { if(m.find(str) != m.end()) { for(auto it = m[str].begin(); it != m[str].end(); it++) printf(\u0026#34;%07d\\n\u0026#34;, *it); } else cout \u0026lt;\u0026lt; \u0026#34;Not Found\\n\u0026#34;; } int main() { int n, m, id, num; scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); string ttitle, tauthor, tkey, tpub, tyear; for(int i = 0; i \u0026lt; n; i++) { scanf(\u0026#34;%d\\n\u0026#34;, \u0026amp;id); getline(cin, ttitle); title[ttitle].insert(id); getline(cin, tauthor); author[tauthor].insert(id); while(cin \u0026gt;\u0026gt; tkey) { key[tkey].insert(id); char c = getchar(); if(c == \u0026#39;\\n\u0026#39;) break; } getline(cin, tpub); pub[tpub].insert(id); getline(cin, tyear); year[tyear].insert(id); } scanf(\u0026#34;%d\u0026#34;, \u0026amp;m); for(int i = 0; i \u0026lt; m; i++) { scanf(\u0026#34;%d: \u0026#34;, \u0026amp;num); string temp; getline(cin, temp); cout \u0026lt;\u0026lt; num \u0026lt;\u0026lt; \u0026#34;: \u0026#34; \u0026lt;\u0026lt; temp \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; if(num == 1) query(title, temp); else if(num == 2) query(author, temp); else if(num == 3) query(key, temp); else if(num == 4) query(pub,temp); else if(num ==5) query(year, temp); } return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1022digitallibrary%E6%90%9C%E7%B4%A2map%E5%92%8Cset/","section":"博客","summary":"1022 Digital Library # 0、题目 # A Digital Library contains millions of books, stored according to their titles, authors, key words of their abstracts, publishers, and published years.","title":"1022DigitalLibrary搜索map和set"},{"content":"1023 Have Fun with Numbers # 0、题目 # Notice that the number 123456789 is a 9-digit number consisting exactly the numbers from 1 to 9, with no duplication. Double it we will obtain 246913578, which happens to be another 9-digit number consisting exactly the numbers from 1 to 9, only in a different permutation. Check to see the result if we double it again!\nNow you are suppose to check if there are more numbers with this property. That is, double a given number with k digits, you are to tell if the resulting number consists of only a permutation of the digits in the original number.\nInput Specification: # Each input contains one test case. Each case contains one positive integer with no more than 20 digits.\nOutput Specification: # For each test case, first print in a line “Yes” if doubling the input number gives a number that consists of only a permutation of the digits in the original number, or “No” if not. Then in the next line, print the doubled number.\nSample Input: # 1234567899 Sample Output: # Yes 2469135798 1、大致题意 # 给出一个长度不超过20的整数，问这个整数两倍后的数位是否为原数位的一个排列。不管是yes还是no最后都要输出整数乘以2的结果\n2、基本思路 # 简单题\n3、AC代码 # #include \u0026lt;cstdio\u0026gt; #include \u0026lt;string.h\u0026gt; using namespace std; int book[10]; int main() { char num[22]; scanf(\u0026#34;%s\u0026#34;, num); int flag = 0, len = strlen(num); for(int i = len - 1; i \u0026gt;= 0; i--) { int temp = num[i] - \u0026#39;0\u0026#39;; book[temp]++; temp = temp * 2 + flag; flag = 0; if(temp \u0026gt;= 10) { temp = temp - 10; flag = 1; } num[i] = (temp + \u0026#39;0\u0026#39;); book[temp]--; } int flag1 = 0; for(int i = 0; i \u0026lt; 10; i++) { if(book[i] != 0) flag1 = 1; } printf(\u0026#34;%s\u0026#34;, (flag == 1 || flag1 == 1) ? \u0026#34;No\\n\u0026#34; : \u0026#34;Yes\\n\u0026#34;); if(flag == 1) printf(\u0026#34;1\u0026#34;); printf(\u0026#34;%s\u0026#34;, num); return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1023havefunwithnumbers/","section":"博客","summary":"1023 Have Fun with Numbers # 0、题目 # Notice that the number 123456789 is a 9-digit number consisting exactly the numbers from 1 to 9, with no duplication.","title":"1023HaveFunwithNumbers"},{"content":"1024 Palindromic Number # 0、题目 # A number that will be the same when it is written forwards or backwards is known as a Palindromic Number. For example, 1234321 is a palindromic number. All single digit numbers are palindromic numbers.\nNon-palindromic numbers can be paired with palindromic ones via a series of operations. First, the non-palindromic number is reversed and the result is added to the original number. If the result is not a palindromic number, this is repeated until it gives a palindromic number. For example, if we start from 67, we can obtain a palindromic number in 2 steps: 67 + 76 = 143, and 143 + 341 = 484.\nGiven any positive integer N, you are supposed to find its paired palindromic number and the number of steps taken to find it.\nInput Specification: # Each input file contains one test case. Each case consists of two positive numbers N and K, where N (≤1010) is the initial numer and K (≤100) is the maximum number of steps. The numbers are separated by a space.\nOutput Specification: # For each test case, output two numbers, one in each line. The first number is the paired palindromic number of N, and the second number is the number of steps taken to find the palindromic number. If the palindromic number is not found after K steps, just output the number obtained at the Kth step and K instead.\nSample Input 1: # 67 3 Sample Output 1: # 484 2 Sample Input 2: # 69 3 Sample Output 2: # 1353 3 1、大致题意 # 给定一个数字，和允许翻转后相加的次数cnt，求要多少次才能变成一个回文数字，输出那个回文数字和翻转相加了多少次，如果本身就是回文数字就输出0次，如果超过给定的次数cnt了，就输出那个不是回文的结果，并且输出给定的次数cnt\n2、基本思路 # 简单题\n3、解题过程 # 3.1 数组存法 # #include\u0026lt;iostream\u0026gt; #include\u0026lt;cmath\u0026gt; using namespace std; long long n,k; int a[105],num_a; void seta(long long k) { num_a=0; while(k!=0) { a[num_a]=k%10; k/=10; num_a++; } } int is_Palindromic(long long k) { seta(k); for(int i=0; i\u0026lt;num_a/2; i++) { if(a[i]!=a[num_a-i-1]) { return -1; } else { return 1; } } } int main() { scanf(\u0026#34;%lld%lld\u0026#34;,\u0026amp;n,\u0026amp;k); long long ans=0; int cnt=0; while(is_Palindromic(n)!=1\u0026amp;\u0026amp;cnt\u0026lt;k) { ans=0; num_a=0; seta(n); for(int i=num_a-1; i\u0026gt;=0; i--) { ans=ans+(long long)a[i]*pow(10,num_a-i-1); } n=n+ans; cnt++; } cout\u0026lt;\u0026lt;n\u0026lt;\u0026lt;endl\u0026lt;\u0026lt;cnt; return 0; } 3.1.1 一位数 # 当一位数的时候，会出错，例如下面的用例\n输入： 1 3 输出： 1 0 问题出在 c++的默认返回值\nint is_Palindromic(long long k) { seta(k); for(int i=0; i\u0026lt;num_a/2; i++) { if(a[i]!=a[num_a-i-1]) { return -1; } else { return 1; } } return 1;//这句一定要加 } 编译器会脑补默认的返回值——0。当然，成熟的编译器会给出一条警告，但不会认为这是个错误。\n#include\u0026lt;iostream\u0026gt; #include\u0026lt;cmath\u0026gt; using namespace std; long long n,k; int a[105],num_a; void seta(long long k) { num_a=0; while(k!=0) { a[num_a]=k%10; k/=10; num_a++; } } int is_Palindromic(long long k) { seta(k); for(int i=0; i\u0026lt;num_a/2; i++) { if(a[i]!=a[num_a-i-1]) { return -1; } else { return 1; } } return 1; } int main() { scanf(\u0026#34;%lld%lld\u0026#34;,\u0026amp;n,\u0026amp;k); long long ans=0; int cnt=0; while(is_Palindromic(n)!=1\u0026amp;\u0026amp;cnt\u0026lt;k) { ans=0; num_a=0; seta(n); for(int i=num_a-1; i\u0026gt;=0; i--) { ans=ans+(long long1)a[i]*pow(10,num_a-i-1); } n=n+ans; cnt++; } cout\u0026lt;\u0026lt;n\u0026lt;\u0026lt;endl\u0026lt;\u0026lt;cnt; return 0; } 3.1.2 大数 # 数字太大会出错\n输入： 5165454546454 4 输出： 1039973000379940 4 数据类型取值范围int2147483648～2147483647long2147483648～2147483647long long-9223372036854775808 ~ 9223372036854775807\n当 long long 吃不消时，就只能转用其他的办法\n3.2 AC代码 # #include \u0026lt;iostream\u0026gt; #include \u0026lt;algorithm\u0026gt; using namespace std; string s; void add(string t) { int len = s.length(), carry = 0; for(int i = 0; i \u0026lt; len; i++) { s[i] = s[i] + t[i] + carry - \u0026#39;0\u0026#39;; carry = 0; if(s[i] \u0026gt; \u0026#39;9\u0026#39;) { s[i] = s[i] - 10; carry = 1; } } if(carry) s += \u0026#39;1\u0026#39;; reverse(s.begin(), s.end()); } int main() { int cnt, i; cin \u0026gt;\u0026gt; s \u0026gt;\u0026gt; cnt; for(i = 0; i \u0026lt;= cnt; i++) { string t = s; reverse(t.begin(), t.end()); if(s == t || i == cnt) break; add(t); } cout \u0026lt;\u0026lt; s \u0026lt;\u0026lt; endl \u0026lt;\u0026lt; i; return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1024palindromicnumber%E5%9B%9E%E6%96%87%E6%95%B0/","section":"博客","summary":"1024 Palindromic Number # 0、题目 # A number that will be the same when it is written forwards or backwards is known as a Palindromic Number.","title":"1024PalindromicNumber回文数"},{"content":"1025 PAT Ranking # 0、题目 # Programming Ability Test (PAT) is organized by the College of Computer Science and Technology of Zhejiang University. Each test is supposed to run simultaneously in several places, and the ranklists will be merged immediately after the test. Now it is your job to write a program to correctly merge all the ranklists and generate the final rank.\nInput Specification: # Each input file contains one test case. For each case, the first line contains a positive number N (≤100), the number of test locations. Then N ranklists follow, each starts with a line containing a positive integer K (≤300), the number of testees, and then K lines containing the registration number (a 13-digit number) and the total score of each testee. All the numbers in a line are separated by a space.\nOutput Specification: # For each test case, first print in one line the total number of testees. Then print the final ranklist in the following format:\nregistration_number final_rank location_number local_rank The locations are numbered from 1 to N. The output must be sorted in nondecreasing order of the final ranks. The testees with the same score must have the same rank, and the output must be sorted in nondecreasing order of their registration numbers.\n1、大致题意 # 有 n 个考场，每个考场有若干数量的学生，给出每个考场中考生的编号和分数，要求算排名，输出所有考生的编号、排名、考场号、考场内排名\n2、基本思路 # 简单题\n3、解题过程 # 3.1 坑点 - 测试点4（22/25） # ID有类似 000003 这种形式的，如果是数字类型就容易输出错误，前导零别忘了。\n#include\u0026lt;iostream\u0026gt; #include\u0026lt;algorithm\u0026gt; int n,k[105],cnt; struct Student { long long num; int score; int final_rank,location_number,local_rank; bool operator \u0026gt;(const Student \u0026amp;a)const { if(score!=a.score) { return score\u0026gt;a.score; } else { return num\u0026lt;a.num; } }; } stu[10000005]; using namespace std; int main() { cin\u0026gt;\u0026gt;n; cnt=0; for(int i=1; i\u0026lt;=n; i++) { cin\u0026gt;\u0026gt;k[i]; for(int j=1; j\u0026lt;=k[i]; j++) { cin\u0026gt;\u0026gt;stu[cnt+j].num\u0026gt;\u0026gt;stu[cnt+j].score; } sort(stu+cnt+1,stu+cnt+k[i]+1,greater\u0026lt;Student\u0026gt;()); stu[cnt+1].location_number=i; stu[cnt+1].local_rank=1; for(int j=2; j\u0026lt;=k[i]; j++) { if(stu[cnt+j].score==stu[cnt+j-1].score) { stu[cnt+j].local_rank=stu[cnt+j-1].local_rank; } else { stu[cnt+j].local_rank=j; } stu[cnt+j].location_number=i; } cnt+=k[i]; } sort(stu+1,stu+cnt+1,greater\u0026lt;Student\u0026gt;()); stu[1].final_rank=1; for(int i=2; i\u0026lt;=cnt; i++) { if(stu[i].score==stu[i-1].score) { stu[i].final_rank=stu[i-1].final_rank; } else { stu[i].final_rank=i; } } cout\u0026lt;\u0026lt;cnt\u0026lt;\u0026lt;endl; //\t1234567890005 1 1 1 for(int i=1; i\u0026lt;=cnt; i++) { cout\u0026lt;\u0026lt;stu[i].num\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;stu[i].final_rank\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;stu[i].location_number\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;stu[i].local_rank\u0026lt;\u0026lt;endl; } return 0; } 3.2 AC代码 # #include\u0026lt;iostream\u0026gt; #include\u0026lt;algorithm\u0026gt; #include\u0026lt;iomanip\u0026gt; int n,k[105],cnt; struct Student { long long num; int score; int final_rank,location_number,local_rank; bool operator \u0026gt;(const Student \u0026amp;a)const { if(score!=a.score) { return score\u0026gt;a.score; } else { return num\u0026lt;a.num; } }; } stu[10000005]; using namespace std; int main() { cin\u0026gt;\u0026gt;n; cnt=0; for(int i=1; i\u0026lt;=n; i++) { cin\u0026gt;\u0026gt;k[i]; for(int j=1; j\u0026lt;=k[i]; j++) { cin\u0026gt;\u0026gt;stu[cnt+j].num\u0026gt;\u0026gt;stu[cnt+j].score; } sort(stu+cnt+1,stu+cnt+k[i]+1,greater\u0026lt;Student\u0026gt;()); stu[cnt+1].location_number=i; stu[cnt+1].local_rank=1; for(int j=2; j\u0026lt;=k[i]; j++) { if(stu[cnt+j].score==stu[cnt+j-1].score) { stu[cnt+j].local_rank=stu[cnt+j-1].local_rank; } else { stu[cnt+j].local_rank=j; } stu[cnt+j].location_number=i; } cnt+=k[i]; } sort(stu+1,stu+cnt+1,greater\u0026lt;Student\u0026gt;()); stu[1].final_rank=1; for(int i=2; i\u0026lt;=cnt; i++) { if(stu[i].score==stu[i-1].score) { stu[i].final_rank=stu[i-1].final_rank; } else { stu[i].final_rank=i; } } cout\u0026lt;\u0026lt;cnt\u0026lt;\u0026lt;endl; for(int i=1; i\u0026lt;=cnt; i++) { cout\u0026lt;\u0026lt;setfill(\u0026#39;0\u0026#39;)\u0026lt;\u0026lt;setw(13)\u0026lt;\u0026lt;stu[i].num\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;stu[i].final_rank\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;stu[i].location_number\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;stu[i].local_rank\u0026lt;\u0026lt;endl; } return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1025patranking%E6%8E%92%E5%BA%8F/","section":"博客","summary":"1025 PAT Ranking # 0、题目 # Programming Ability Test (PAT) is organized by the College of Computer Science and Technology of Zhejiang University.","title":"1025PATRanking排序"},{"content":"1027 Colors in Mars # 0、题目 # People in Mars represent the colors in their computers in a similar way as the Earth people. That is, a color is represented by a 6-digit number, where the first 2 digits are for Red, the middle 2 digits for Green, and the last 2 digits for Blue. The only difference is that they use radix 13 (0-9 and A-C) instead of 16. Now given a color in three decimal numbers (each between 0 and 168), you are supposed to output their Mars RGB values.\nInput Specification: # Each input file contains one test case which occupies a line containing the three decimal color values.\nOutput Specification: # For each test case you should output the Mars RGB value in the following format: first output #, then followed by a 6-digit number where all the English characters must be upper-cased. If a single color is only 1-digit long, you must print a 0 to its left.\nSample Input: # 15 43 71 Sample Output: # #123456 1、大致题意 # 给三个十进制的数，把它们转换为十三进制的数输出。要求在前面加上一个\u0026quot;#\u0026ldquo;号\n2、基本思路 # 因为 0~168 的十进制转换为 13进制 不会超过两位数，所以这个两位数为 $(num / 13)$ 、 $(num % 13)$ 构成的数字\n3、AC代码 # #include \u0026lt;cstdio\u0026gt; using namespace std; int main() { char c[14] = {\u0026#34;0123456789ABC\u0026#34;}; printf(\u0026#34;#\u0026#34;); for(int i = 0; i \u0026lt; 3; i++) { int num; scanf(\u0026#34;%d\u0026#34;, \u0026amp;num); printf(\u0026#34;%c%c\u0026#34;, c[num/13], c[num%13]); } return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1027colorsinmars/","section":"博客","summary":"1027 Colors in Mars # 0、题目 # People in Mars represent the colors in their computers in a similar way as the Earth people.","title":"1027ColorsinMars"},{"content":"1028 List Sorting # 0、题目 # Excel can sort records according to any column. Now you are supposed to imitate this function.\nInput Specification: # Each input file contains one test case. For each case, the first line contains two integers N (≤105) and C, where N is the number of records and C is the column that you are supposed to sort the records with. Then N lines follow, each contains a record of a student. A student’s record consists of his or her distinct ID (a 6-digit number), name (a string with no more than 8 characters without space), and grade (an integer between 0 and 100, inclusive).\nOutput Specification: # For each test case, output the sorting result in N lines. That is, if C = 1 then the records must be sorted in increasing order according to ID’s; if C = 2 then the records must be sorted in non-decreasing order according to names; and if C = 3 then the records must be sorted in non-decreasing order according to grades. If there are several students who have the same name or grade, they must be sorted according to their ID’s in increasing order.\nSample Input 1: # 3 1 000007 James 85 000010 Amy 90 000001 Zoe 60 Sample Output 1: # 000001 Zoe 60 000007 James 85 000010 Amy 90 Sample Input 2: # 4 2 000007 James 85 000010 Amy 90 000001 Zoe 60 000002 James 98 Sample Output 2: # 000010 Amy 90 000002 James 98 000007 James 85 000001 Zoe 60 Sample Input 3: # 4 3 000007 James 85 000010 Amy 90 000001 Zoe 60 000002 James 9 Sample Output 3: # 000002 James 9 000001 Zoe 60 000007 James 85 000010 Amy 90 1、大致题意 # 给表格排序，根据c的值是1还是2还是3，对相应的列排序。第一列升序，第二列不降序，第三列不降序\n2、基本思路 # sort大法，简单题\n3、解题过程 # 3.1 char数组比较 # 注意，按照名称的不降序排序，因为 strcmp 比较的是 ACSII码 ，所以 A \u0026lt; Z 。写cmp函数 的时候 return strcmp(a.name, b.name) \u0026lt;= 0; return语句返回的是true或者false的值，所以要写 \u0026lt;= 0 这样的形式。比较 ACSII码 的大小，strcmp('a', 'z') 返回负值，因为 a\u0026lt;z a – z \u0026lt; 0 按照分数的不降序排序，a.score \u0026lt;= b.score\n3.2 AC代码 # #include \u0026lt;iostream\u0026gt; #include \u0026lt;algorithm\u0026gt; #include \u0026lt;string.h\u0026gt; using namespace std; const int maxn = 100001; struct NODE { int no, score; char name[10]; }node[maxn]; int c; int cmp1(NODE a, NODE b) { if(c == 1) { return a.no \u0026lt; b.no; } else if(c == 2) { if(strcmp(a.name, b.name) == 0) return a.no \u0026lt; b.no; return strcmp(a.name, b.name) \u0026lt;= 0; } else { if(a.score == b.score) return a.no \u0026lt; b.no; return a.score \u0026lt;= b.score; } } int main() { int n; scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;n, \u0026amp;c); for(int i = 0; i \u0026lt; n; i++) scanf(\u0026#34;%d %s %d\u0026#34;, \u0026amp;node[i].no, node[i].name, \u0026amp;node[i].score); sort(node, node + n, cmp1); for(int i = 0; i \u0026lt; n; i++) printf(\u0026#34;%06d %s %d\\n\u0026#34;, node[i].no, node[i].name, node[i].score); return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1028listsorting/","section":"博客","summary":"1028 List Sorting # 0、题目 # Excel can sort records according to any column.","title":"1028ListSorting"},{"content":"1029 Median # 0、题目 # Given an increasing sequence S of N integers, the median is the number at the middle position. For example, the median of S1 = { 11, 12, 13, 14 } is 12, and the median of S2 = { 9, 10, 15, 16, 17 } is 15. The median of two sequences is defined to be the median of the nondecreasing sequence which contains all the elements of both sequences. For example, the median of S1 and S2 is 13.\nGiven two increasing sequences of integers, you are asked to find their median.\nInput Specification: # Each input file contains one test case. Each case occupies 2 lines, each gives the information of a sequence. For each sequence, the first positive integer N (≤2×105) is the size of that sequence. Then N integers follow, separated by a space. It is guaranteed that all the integers are in the range of long int.\nOutput Specification: # For each test case you should output the median of the two given sequences in a line.\nSample Input: # 4 11 12 13 14 5 9 10 15 16 17 Sample Output: # 13 1、大致题意 # 给出两个已排序序列，求这两个序列合并后的中间数\n2、基本思路 # 其实可以存储两个数组，然后 sort 大法。\n但是我一开始想到的是 priority_queue ，那就两种都做一下吧。\n3、解题过程 # 3.1 priority_queue 取出指定下标的数 # 这边有个坑，就是因为你不能直接取出指定下标的数，所以你只能遍历 $\\frac{size}{2}$ 次，然后取出中位数。\n但是奇数和偶数的中位数的取法不一样，奇数是取到 i\u0026lt;=size/2 ；偶数是取到 i\u0026lt;size/2\n3.1.1 第一份代码（18/25） # 在第一份代码里面就是没有区分出来奇数和偶数，所以出错\n#include\u0026lt;queue\u0026gt; #include\u0026lt;iostream\u0026gt; using namespace std; priority_queue\u0026lt;int,vector\u0026lt;int\u0026gt;,greater\u0026lt;int\u0026gt; \u0026gt; q; int main() { int n,m; while(!q.empty()) { q.pop(); } scanf(\u0026#34;%d\u0026#34;,\u0026amp;n); for(int i=0; i\u0026lt;n; i++) { scanf(\u0026#34;%d\u0026#34;,\u0026amp;m); q.push(m); } scanf(\u0026#34;%d\u0026#34;,\u0026amp;n); for(int i=0; i\u0026lt;n; i++) { scanf(\u0026#34;%d\u0026#34;,\u0026amp;m); q.push(m); } int size=q.size(); size=size/2; while(size){ n=q.top(); q.pop(); size--; } n=q.top(); cout\u0026lt;\u0026lt;n; return 0; } 3.1.2 AC代码 # #include\u0026lt;queue\u0026gt; #include\u0026lt;iostream\u0026gt; using namespace std; priority_queue\u0026lt;int,vector\u0026lt;int\u0026gt;,greater\u0026lt;int\u0026gt; \u0026gt; q; int main() { int n,m; while(!q.empty()) { q.pop(); } scanf(\u0026#34;%d\u0026#34;,\u0026amp;n); for(int i=0; i\u0026lt;n; i++) { scanf(\u0026#34;%d\u0026#34;,\u0026amp;m); q.push(m); } scanf(\u0026#34;%d\u0026#34;,\u0026amp;n); for(int i=0; i\u0026lt;n; i++) { scanf(\u0026#34;%d\u0026#34;,\u0026amp;m); q.push(m); } int size=q.size(); if(size%2==0) { for(int i=0; i\u0026lt;size/2; i++) { n=q.top(); q.pop(); } } else { for(int i=0; i\u0026lt;=size/2; i++) { n=q.top(); q.pop(); } } cout\u0026lt;\u0026lt;n; return 0; } 3.2 第二种思路 # #include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; int a[400005]; int main(){ int n,m; scanf(\u0026#34;%d\u0026#34;,\u0026amp;n); for(int i=0;i\u0026lt;n;i++) scanf(\u0026#34;%d\u0026#34;,\u0026amp;a[i]); scanf(\u0026#34;%d\u0026#34;,\u0026amp;m); for(int i=0;i\u0026lt;m;i++) scanf(\u0026#34;%d\u0026#34;,\u0026amp;a[n+i]); sort(a,a+m+n); int mid=(m+n-1)/2; cout\u0026lt;\u0026lt;a[mid]\u0026lt;\u0026lt;endl; return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1029median%E6%8E%92%E5%BA%8F/","section":"博客","summary":"1029 Median # 0、题目 # Given an increasing sequence S of N integers, the median is the number at the middle position.","title":"1029Median排序"},{"content":"1030 Travel Plan（Dijksta +DFS） # 0、题目 # A traveler’s map gives the distances between cities along the highways, together with the cost of each highway. Now you are supposed to write a program to help a traveler to decide the shortest path between his/her starting city and the destination. If such a shortest path is not unique, you are supposed to output the one with the minimum cost, which is guaranteed to be unique.\nInput Specification: # Each input file contains one test case. Each case starts with a line containing 4 positive integers N, M, S, and D, where N (≤500) is the number of cities (and hence the cities are numbered from 0 to N−1); M is the number of highways; S and D are the starting and the destination cities, respectively. Then M lines follow, each provides the information of a highway, in the format:\nCity1 City2 Distance Cost where the numbers are all integers no more than 500, and are separated by a space.\nOutput Specification: # For each test case, print in one line the cities along the shortest path from the starting point to the destination, followed by the total distance and the total cost of the path. The numbers must be separated by a space and there must be no extra space at the end of output.\nSample Input: # 4 5 0 3 0 1 1 20 1 3 2 30 0 3 4 10 0 2 2 20 2 3 1 20 Sample Output: # 0 2 3 3 40 1、大致题意 # 求起点到终点的最短路径最短距离和花费，要求首先路径最短，其次花费最少，要输出完整路径\n2、基本思路 # Dijksta + DFS。 Dijkstra 记录路径 pre 数组，然后用 dfs 求最短的一条 $min_{cost}$ 以及它的路径 path ，最后输出 path 数组和 $min_{cost}$\n注意，路径 path 因为是从末端一直压入 push_back 到 path 里面的，所以要输出路径的时候倒着输出\n3、解题过程 # 3.1 第一份代码（18/30） # 写出第一份 Dijksta + DFS 的代码，很遗憾只对了第一个测试用例\n#include\u0026lt;iostream\u0026gt; #include\u0026lt;algorithm\u0026gt; #include\u0026lt;queue\u0026gt; #include\u0026lt;cstring\u0026gt; using namespace std; const long long inf=0x7fffffff; int n,m,s,e,cnt,end_cost; int head[10100],vis[10100],dis[10100]; vector\u0026lt;int\u0026gt; pre[10100]; vector\u0026lt;int\u0026gt; path; struct Edge { int v,w,cost,next; } edge[501000]; void addEdge(int u, int v,int w,int cost) { edge[++cnt].v=v; edge[cnt].w=w; edge[cnt].cost=cost; edge[cnt].next=head[u]; head[u]=cnt; } struct node { int dis; int pos; bool operator \u0026lt;( const node \u0026amp;x )const { return x.dis \u0026lt; dis; } }ntmp; priority_queue\u0026lt;node\u0026gt;q; void dijkstra(int s) { for(int i=1; i\u0026lt;=n; i++) { //这行有问题 dis[i]=inf; vis[i]=0; } ntmp.dis=0; ntmp.pos=s; q.push(ntmp); dis[s]=0; while(!q.empty()) { node u=q.top(); q.pop(); int pos=u.pos; if(vis[pos]) { continue; } vis[pos]=1; for(int i=head[pos]; i; i=edge[i].next) { int v=edge[i].v; if(dis[v]\u0026gt;=dis[pos]+edge[i].w) { if(dis[v]\u0026gt;dis[pos]+edge[i].w) { pre[v].clear(); dis[v]=dis[pos]+edge[i].w; } pre[v].push_back(pos); if(!vis[v]) { ntmp.dis=dis[v]; ntmp.pos=v; q.push((ntmp)); } } } } } void dfs(int pos,int cost,vector\u0026lt;int\u0026gt; p) { p.push_back(pos); if(pos==s) { if(end_cost\u0026gt;cost) { end_cost=cost; path=p; } return; } int tmp; for(int i=0; i\u0026lt;pre[pos].size(); i++) { for(int j=head[pos]; j; j=edge[j].next) { if(edge[j].v==pre[pos][i]) { tmp=edge[j].cost; break; } } cost+=tmp; dfs(pre[pos][i],cost,p); cost-=tmp; } return; } int main() { scanf(\u0026#34;%d%d%d%d\u0026#34;,\u0026amp;n,\u0026amp;m,\u0026amp;s,\u0026amp;e); memset(edge,0,sizeof(edge)); int u,v,w,cost; cnt=0; for(int i=0; i\u0026lt;m; i++) { scanf(\u0026#34;%d%d%d%d\u0026#34;,\u0026amp;u,\u0026amp;v,\u0026amp;w,\u0026amp;cost); addEdge(u,v,w,cost); addEdge(v,u,w,cost); } dijkstra(s); end_cost=inf; vector\u0026lt;int\u0026gt;p; dfs(e,0,p); for(int i=path.size()-1; i\u0026gt;=0; i--) { cout\u0026lt;\u0026lt;path[i]\u0026lt;\u0026lt;\u0026#34; \u0026#34;; } cout\u0026lt;\u0026lt;dis[e]\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;end_cost; return 0; } 3.2 预处理的错误 # 经过排查，找到了一个测试用例，如下：\n输入： 4 5 1 2 0 1 1 20 1 3 2 30 0 3 4 10 0 2 2 20 2 3 1 20 输出： 1 0 2 3 40 主要是在 点0 的位置没有初始化，然后出错。这个原因也很简单，就是因为我使用的Dijksta板子，基本上都是以 0 为起始位置的，所以不用考虑 点0 ，以后还是注意这些细节。\n3.3 AC代码 # #include\u0026lt;iostream\u0026gt; #include\u0026lt;algorithm\u0026gt; #include\u0026lt;queue\u0026gt; #include\u0026lt;cstring\u0026gt; using namespace std; const long long inf=0x7fffffff; int n,m,s,e,cnt,end_cost; int head[10100],vis[10100],dis[10100]; vector\u0026lt;int\u0026gt; pre[10100]; vector\u0026lt;int\u0026gt; path; struct Edge { int v,w,cost,next; } edge[501000]; void addEdge(int u, int v,int w,int cost) { edge[++cnt].v=v; edge[cnt].w=w; edge[cnt].cost=cost; edge[cnt].next=head[u]; head[u]=cnt; } struct node { int dis; int pos; bool operator \u0026lt;( const node \u0026amp;x )const { return x.dis \u0026lt; dis; } }ntmp; priority_queue\u0026lt;node\u0026gt;q; void dijkstra(int s) { for(int i=0; i\u0026lt;=n; i++) { //注意这行 dis[i]=inf; vis[i]=0; } ntmp.dis=0; ntmp.pos=s; q.push(ntmp); dis[s]=0; while(!q.empty()) { node u=q.top(); q.pop(); int pos=u.pos; if(vis[pos]) { continue; } vis[pos]=1; for(int i=head[pos]; i; i=edge[i].next) { int v=edge[i].v; if(dis[v]\u0026gt;=dis[pos]+edge[i].w) { if(dis[v]\u0026gt;dis[pos]+edge[i].w) { pre[v].clear(); dis[v]=dis[pos]+edge[i].w; } pre[v].push_back(pos); if(!vis[v]) { ntmp.dis=dis[v]; ntmp.pos=v; q.push((ntmp)); } } } } } void dfs(int pos,int cost,vector\u0026lt;int\u0026gt; p) { p.push_back(pos); if(pos==s) { if(end_cost\u0026gt;cost) { end_cost=cost; path=p; } return; } int tmp; for(int i=0; i\u0026lt;pre[pos].size(); i++) { for(int j=head[pos]; j; j=edge[j].next) { if(edge[j].v==pre[pos][i]) { tmp=edge[j].cost; break; } } dfs(pre[pos][i],cost+tmp,p); } return; } int main() { scanf(\u0026#34;%d%d%d%d\u0026#34;,\u0026amp;n,\u0026amp;m,\u0026amp;s,\u0026amp;e); memset(edge,0,sizeof(edge)); int u,v,w,cost; cnt=0; for(int i=0; i\u0026lt;m; i++) { scanf(\u0026#34;%d%d%d%d\u0026#34;,\u0026amp;u,\u0026amp;v,\u0026amp;w,\u0026amp;cost); addEdge(u,v,w,cost); addEdge(v,u,w,cost); } dijkstra(s); end_cost=inf; vector\u0026lt;int\u0026gt;p; dfs(e,0,p); for(int i=path.size()-1; i\u0026gt;=0; i--) { cout\u0026lt;\u0026lt;path[i]\u0026lt;\u0026lt;\u0026#34; \u0026#34;; } cout\u0026lt;\u0026lt;dis[e]\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;end_cost; return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1030travelplandijkstadfs/","section":"博客","summary":"1030 Travel Plan（Dijksta +DFS） # 0、题目 # A traveler’s map gives the distances between cities along the highways, together with the cost of each highway.","title":"1030TravelPlanDijkstaDFS"},{"content":"1031 Hello World for U # 0、题目 # Given any string of N (≥5) characters, you are asked to form the characters into the shape of U. For example, helloworld can be printed as:\nh d e l l r lowo That is, the characters must be printed in the original order, starting top-down from the left vertical line with n1 characters, then left to right along the bottom line with n2 characters, and finally bottom-up along the vertical line with n3 characters. And more, we would like U to be as squared as possible – that is, it must be satisfied that n1=n3=max { k | k≤n2 for all 3≤n2≤N } with n1+n2+n3−2=N.\nInput Specification: # Each input file contains one test case. Each case contains one string with no less than 5 and no more than 80 characters in a line. The string contains no white space.\nOutput Specification: # For each test case, print the input string in the shape of U as specified in the description.\nSample Input: # helloworld! Sample Output: # h ! e d l l lowor 1、大致题意 # 用所给字符串按U型输出。n1和n3是左右两条竖线从上到下的字符个数，n2是底部横线从左到右的字符个数。并满足一下要求：\nn1 == n3+ n2 \u0026gt;= n1+ n1为在满足上述条件的情况下的最大值 2、基本思路 # 分三种情况讨论：\n如果 $n % 3 == 0$ ，直接 $n1 = n2 = n3$+ 如果 $n % 3 == 1$，因为 $n2$ 要比 $n1$ 大，所以把多出来的那1个给 $n2$+ 如果 $n % 3 == 2$，就把多出来的那2个给$n2$ 所以得到公式：n1 = n / 3，n2 = n / 3 + n % 3\n把它们存储到二维字符数组中，一开始初始化字符数组为空格，然后按照u型填充进去，最后输出这个数组u。\n3、AC代码 # #include \u0026lt;iostream\u0026gt; #include \u0026lt;string.h\u0026gt; using namespace std; int main() { char c[81], u[30][30]; memset(u, \u0026#39; \u0026#39;, sizeof(u)); scanf(\u0026#34;%s\u0026#34;, c); int n = strlen(c) + 2; int n1 = n / 3, n2 = n / 3 + n % 3, index = 0; for(int i = 0; i \u0026lt; n1; i++) u[i][0] = c[index++]; for(int i = 1; i \u0026lt;= n2 - 2; i++) u[n1-1][i] = c[index++]; for(int i = n1 - 1; i \u0026gt;= 0; i--) u[i][n2-1] = c[index++]; for(int i = 0; i \u0026lt; n1; i++) { for(int j = 0; j \u0026lt; n2; j++) printf(\u0026#34;%c\u0026#34;, u[i][j]); printf(\u0026#34;\\n\u0026#34;); } return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1031helloworldforu/","section":"博客","summary":"1031 Hello World for U # 0、题目 # Given any string of N (≥5) characters, you are asked to form the characters into the shape of U.","title":"1031HelloWorldforU"},{"content":"1032 Sharing（链表） # 0、题目 # To store English words, one method is to use linked lists and store a word letter by letter. To save some space, we may let the words share the same sublist if they share the same suffix. For example, loading and being are stored as showed in Figure 1.\nFigure 1\nYou are supposed to find the starting position of the common suffix (e.g. the position of i in Figure 1).\nInput Specification: # Each input file contains one test case. For each case, the first line contains two addresses of nodes and a positive N (≤105), where the two addresses are the addresses of the first nodes of the two words, and N is the total number of nodes. The address of a node is a 5-digit positive integer, and NULL is represented by −1.\nThen N lines follow, each describes a node in the format:\nAddress Data Next whereAddress is the position of the node, Data is the letter contained by this node which is an English letter chosen from { a-z, A-Z }, and Next is the position of the next node.\nOutput Specification: # For each case, simply output the 5-digit starting position of the common suffix. If the two words have no common suffix, output -1 instead.\nSample Input 1: # 11111 22222 9 67890 i 00002 00010 a 12345 00003 g -1 12345 D 67890 00002 n 00003 22222 B 23456 11111 L 00001 23456 e 67890 00001 o 00010 Sample Output 1: # 67890 Sample Input 2: # 00001 00002 4 00001 a 10001 10001 s -1 00002 a 10002 10002 t -1 Sample Output 2: # -1 1、大致题意 # 求两个链表的首个共同结点的地址。如果没有，就输出-1\n2、大致思路 # 用结构体数组存储，node[i]表示地址为i的结点，key表示值，next为下一个结点的地址，flag表示第一条链表有没有该结点。遍历第一条链表，将访问过的结点的flag都标记为true，当遍历第二条结点的时候，如果遇到了true的结点就输出并结束程序，没有遇到就输出-1。\n3、AC代码 # #include \u0026lt;cstdio\u0026gt; using namespace std; struct NODE { char key; int next; bool flag; }node[100000]; int main() { int s1, s2, n, a, b; scanf(\u0026#34;%d%d%d\u0026#34;, \u0026amp;s1, \u0026amp;s2, \u0026amp;n); char data; for(int i = 0; i \u0026lt; n; i++) { scanf(\u0026#34;%d %c %d\u0026#34;, \u0026amp;a, \u0026amp;data, \u0026amp;b); node[a] = {data, b, false}; } for(int i = s1; i != -1; i = node[i].next) node[i].flag = true; for(int i = s2; i != -1; i = node[i].next) { if(node[i].flag == true) { printf(\u0026#34;%05d\u0026#34;, i); return 0; } } printf(\u0026#34;-1\u0026#34;); return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1032sharing%E9%93%BE%E8%A1%A8/","section":"博客","summary":"1032 Sharing（链表） # 0、题目 # To store English words, one method is to use linked lists and store a word letter by letter.","title":"1032Sharing链表"},{"content":"1035 Password # 0、题目 # To prepare for PAT, the judge sometimes has to generate random passwords for the users. The problem is that there are always some confusing passwords since it is hard to distinguish 1 (one) from l (L in lowercase), or 0 (zero) from O (o in uppercase). One solution is to replace 1 (one) by @, 0 (zero) by %, l by L, and O by o. Now it is your job to write a program to check the accounts generated by the judge, and to help the juge modify the confusing passwords.\nInput Specification: # Each input file contains one test case. Each case contains a positive integer N (≤1000), followed by N lines of accounts. Each account consists of a user name and a password, both are strings of no more than 10 characters with no space.\nOutput Specification: # For each test case, first print the number M of accounts that have been modified, then print in the following M lines the modified accounts info, that is, the user names and the corresponding modified passwords. The accounts must be printed in the same order as they are read in. If no account is modified, print in one line There are N accounts and no account is modified where N is the total number of accounts. However, if N is one, you must print There is 1 account and no account is modified instead.\nSample Input 1: # 3 Team000002 Rlsp0dfa Team000003 perfectpwd Team000001 R1spOdfa Sample Output 1: # 2 Team000002 RLsp%dfa Team000001 R@spodfa Sample Input 2: # 1 team110 abcdefg332 Sample Output 2: # There is 1 account and no account is modified Sample Input 3: # 2 team110 abcdefg222 team220 abcdefg333 Sample Output 3: # There are 2 accounts and no account is modified 1、大致题意 # 给定n个用户的姓名和密码，把密码中的\n1 改为 @+ 0 改为 %+ l 改为 L+ O 改为 o 如果不存在需要修改的密码，则输出 There are n accounts and no account is modified。注意单复数，如果只有一个账户，就输出 There is 1 account and no account is modified\n2、基本思路 # 简单题\n3、AC代码 # #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; using namespace std; int main() { int n; scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); vector\u0026lt;string\u0026gt; v; for(int i = 0; i \u0026lt; n; i++) { string name, s; cin \u0026gt;\u0026gt; name \u0026gt;\u0026gt; s; int len = s.length(), flag = 0; for(int j = 0; j \u0026lt; len; j++) { switch(s[j]) { case \u0026#39;1\u0026#39; : s[j] = \u0026#39;@\u0026#39;; flag = 1; break; case \u0026#39;0\u0026#39; : s[j] = \u0026#39;%\u0026#39;; flag = 1; break; case \u0026#39;l\u0026#39; : s[j] = \u0026#39;L\u0026#39;; flag = 1; break; case \u0026#39;O\u0026#39; : s[j] = \u0026#39;o\u0026#39;; flag = 1; break; } } if(flag) { string temp = name + \u0026#34; \u0026#34; + s; v.push_back(temp); } } int cnt = v.size(); if(cnt != 0) { printf(\u0026#34;%d\\n\u0026#34;, cnt); for(int i = 0; i \u0026lt; cnt; i++) cout \u0026lt;\u0026lt; v[i] \u0026lt;\u0026lt; endl; } else if(n == 1) { printf(\u0026#34;There is 1 account and no account is modified\u0026#34;); } else { printf(\u0026#34;There are %d accounts and no account is modified\u0026#34;, n); } return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1035password/","section":"博客","summary":"1035 Password # 0、题目 # To prepare for PAT, the judge sometimes has to generate random passwords for the users.","title":"1035Password"},{"content":"1036 Boys vs Girls（排序） # 0、题目 # This time you are asked to tell the difference between the lowest grade of all the male students and the highest grade of all the female students.\nInput Specification: # Each input file contains one test case. Each case contains a positive integer N, followed by N lines of student information. Each line contains a student’s name, gender, ID and grade, separated by a space, where name and ID are strings of no more than 10 characters with no space, gender is either F (female) or M (male), and grade is an integer between 0 and 100. It is guaranteed that all the grades are distinct.\nOutput Specification: # For each test case, output in 3 lines. The first line gives the name and ID of the female student with the highest grade, and the second line gives that of the male student with the lowest grade. The third line gives the difference grade**F−grade**M. If one such kind of student is missing, output Absent in the corresponding line, and output NA in the third line instead.\nSample Input 1: # 3 Joe M Math990112 89 Mike M CS991301 100 Mary F EE990830 95 Sample Output 1: # Mary EE990830 Joe Math990112 6 Sample Input 2: # 1 Jean M AA980920 60 Sample Output 2: # Absent Jean AA980920 NA 1、大致题意 # 给出 N 个同学的信息，输出女生中的最高分获得者的信息与男生中最低分获得者的信息，并输出他们的分数差。如果不存在女生或者男生，则对应获得者信息处输出Absent，而且差值处输出NA\n2、基本思路 # 设计结构体Student，然后sort大法\nstruct Student { string name; string ID; int grade; } 3、解题过程 # 3.1 operator\u0026gt; 的可疑问题（20/25） # 首先第一份代码，写的很快，但是测试点2没过，很疑惑，感觉所有的点都考虑到了。\n#include\u0026lt;vector\u0026gt; #include\u0026lt;iostream\u0026gt; #include\u0026lt;algorithm\u0026gt; using namespace std; struct Student { string name; string ID; int grade; bool operator\u0026gt;(const Student a) const { //这行有问题 if(grade!=a.grade) { //这行有问题 if(ID!=a.ID) { //这行有问题 return name\u0026gt;a.name; //这行有问题 } else //这行有问题 return ID\u0026gt;a.ID; //这行有问题 } else { //这行有问题 return grade\u0026gt;a.grade; //这行有问题 } } } tmp; vector\u0026lt;Student\u0026gt;boys,girls; int main() { int n; char sex; cin\u0026gt;\u0026gt;n; for(int i=0; i\u0026lt;n; i++) { cin\u0026gt;\u0026gt;tmp.name\u0026gt;\u0026gt;sex\u0026gt;\u0026gt;tmp.ID\u0026gt;\u0026gt;tmp.grade; if(sex==\u0026#39;M\u0026#39;) { boys.push_back(tmp); } else if(sex==\u0026#39;F\u0026#39;) { girls.push_back(tmp); } } sort(girls.begin(),girls.end(),greater\u0026lt;Student\u0026gt;()); sort(boys.begin(),boys.end(),greater\u0026lt;Student\u0026gt;()); if(!girls.empty()) { cout\u0026lt;\u0026lt;girls[0].name\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;girls[0].ID\u0026lt;\u0026lt;endl; } else { cout\u0026lt;\u0026lt;\u0026#34;Absent\u0026#34;\u0026lt;\u0026lt;endl; } if(!boys.empty()) { cout\u0026lt;\u0026lt;boys[boys.size()-1].name\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;boys[boys.size()-1].ID\u0026lt;\u0026lt;endl; } else { cout\u0026lt;\u0026lt;\u0026#34;Absent\u0026#34;\u0026lt;\u0026lt;endl; } if(!girls.empty()\u0026amp;\u0026amp;!boys.empty()) { int size=boys.size(); int gg=girls[0].grade-boys[size-1].grade; cout\u0026lt;\u0026lt;gg; } else { cout\u0026lt;\u0026lt;\u0026#34;NA\u0026#34;; } return 0; } 然后经过一系列的代码比对\n发现如果不用 operator\u0026gt; 的重写，用cmp可以过的很轻松\n#include\u0026lt;vector\u0026gt; #include\u0026lt;iostream\u0026gt; #include\u0026lt;algorithm\u0026gt; using namespace std; struct Student { string name; string ID; int grade; bool operator\u0026gt;(const Student a) const { //\tif(grade\u0026gt;a.grade){ //\treturn true; //\t}else{ //\treturn false; //\t} if(grade!=a.grade) { //这行有问题 if(ID!=a.ID) { //这行有问题 return name\u0026gt;a.name; //这行有问题 } else //这行有问题 return ID\u0026gt;a.ID; //这行有问题 } else { //这行有问题 return grade\u0026gt;a.grade; //这行有问题 } } } tmp; bool cmp1(Student s1,Student s2) { if(s1.grade\u0026lt;s2.grade) { return true; } else { return false; } } bool cmp2(Student s1,Student s2) { if(s1.grade\u0026gt;s2.grade) { return true; } else { return false; } } vector\u0026lt;Student\u0026gt;boys,girls; int main() { int n; char sex; cin\u0026gt;\u0026gt;n; for(int i=0; i\u0026lt;n; i++) { cin\u0026gt;\u0026gt;tmp.name\u0026gt;\u0026gt;sex\u0026gt;\u0026gt;tmp.ID\u0026gt;\u0026gt;tmp.grade; if(sex==\u0026#39;M\u0026#39;) { boys.push_back(tmp); } else if(sex==\u0026#39;F\u0026#39;) { girls.push_back(tmp); } } //\tsort(girls.begin(),girls.end(),greater\u0026lt;Student\u0026gt;()); //\tsort(boys.begin(),boys.end(),greater\u0026lt;Student\u0026gt;()); sort(girls.begin(),girls.end(),cmp2); sort(boys.begin(),boys.end(),cmp1); if(!girls.empty()) { cout\u0026lt;\u0026lt;girls[0].name\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;girls[0].ID\u0026lt;\u0026lt;endl; } else { cout\u0026lt;\u0026lt;\u0026#34;Absent\u0026#34;\u0026lt;\u0026lt;endl; } if(!boys.empty()) { cout\u0026lt;\u0026lt;boys[boys.size()-1].name\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;boys[boys.size()-1].ID\u0026lt;\u0026lt;endl; } else { cout\u0026lt;\u0026lt;\u0026#34;Absent\u0026#34;\u0026lt;\u0026lt;endl; } if(!girls.empty()\u0026amp;\u0026amp;!boys.empty()) { int size=boys.size(); int gg=girls[0].grade-boys[size-1].grade; cout\u0026lt;\u0026lt;gg; } else { cout\u0026lt;\u0026lt;\u0026#34;NA\u0026#34;; } return 0; } 直接AC，很奇怪，这两点有什么不同吗？经过排查发现下面的 operator\u0026gt; 就可以通过：\nstruct Student { string name; string ID; int grade; bool operator\u0026gt;(const Student a) const { if(grade\u0026gt;a.grade){ return true; }else{ return false; } //\tif(grade!=a.grade) { //这行有问题 //\tif(ID!=a.ID) { //这行有问题 //\treturn name\u0026gt;a.name; //这行有问题 //\t} else //这行有问题 //\treturn ID\u0026gt;a.ID; //这行有问题 //\t} else { //这行有问题 //\treturn grade\u0026gt;a.grade; //这行有问题 //\t} } } ; 就是不能写成注释的样子。可是题目中明确说明It is guaranteed that all the grades are distinct.\n最后发现，我的 operator\u0026gt; 写的确实有问题。\nstruct Student { string name; string ID; int grade; bool operator\u0026gt;(const Student a) const { if(grade==a.grade) { //注意这行 if(ID==a.ID) { return ID\u0026gt;a.ID; } else return name\u0026gt;a.name; } else { if(grade\u0026gt;a.grade) { return true; } else { return false; } } } } ; 3.2 AC代码 # #include\u0026lt;vector\u0026gt; #include\u0026lt;iostream\u0026gt; #include\u0026lt;algorithm\u0026gt; using namespace std; struct Student { string name; string ID; int grade; bool operator\u0026gt;(const Student a) const { if(grade==a.grade) { if(ID==a.ID) { return ID\u0026gt;a.ID; } else return name\u0026gt;a.name; } else { if(grade\u0026gt;a.grade) { return true; } else { return false; } } } } tmp; vector\u0026lt;Student\u0026gt;boys,girls; int main() { int n; char sex; cin\u0026gt;\u0026gt;n; for(int i=0; i\u0026lt;n; i++) { cin\u0026gt;\u0026gt;tmp.name\u0026gt;\u0026gt;sex\u0026gt;\u0026gt;tmp.ID\u0026gt;\u0026gt;tmp.grade; if(sex==\u0026#39;M\u0026#39;) { boys.push_back(tmp); } else if(sex==\u0026#39;F\u0026#39;) { girls.push_back(tmp); } } sort(girls.begin(),girls.end(),greater\u0026lt;Student\u0026gt;()); sort(boys.begin(),boys.end(),greater\u0026lt;Student\u0026gt;()); if(!girls.empty()) { cout\u0026lt;\u0026lt;girls[0].name\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;girls[0].ID\u0026lt;\u0026lt;endl; } else { cout\u0026lt;\u0026lt;\u0026#34;Absent\u0026#34;\u0026lt;\u0026lt;endl; } if(!boys.empty()) { cout\u0026lt;\u0026lt;boys[boys.size()-1].name\u0026lt;\u0026lt;\u0026#34; \u0026#34;\u0026lt;\u0026lt;boys[boys.size()-1].ID\u0026lt;\u0026lt;endl; } else { cout\u0026lt;\u0026lt;\u0026#34;Absent\u0026#34;\u0026lt;\u0026lt;endl; } if(!girls.empty()\u0026amp;\u0026amp;!boys.empty()) { int size=boys.size(); int gg=girls[0].grade-boys[size-1].grade; cout\u0026lt;\u0026lt;gg; } else { cout\u0026lt;\u0026lt;\u0026#34;NA\u0026#34;; } return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1036boysvsgirls%E6%8E%92%E5%BA%8F/","section":"博客","summary":"1036 Boys vs Girls（排序） # 0、题目 # This time you are asked to tell the difference between the lowest grade of all the male students and the highest grade of all the female students.","title":"1036BoysvsGirls排序"},{"content":"1037 Magic Coupon（贪心，排序） # 0、题目 # The magic shop in Mars is offering some magic coupons. Each coupon has an integer N printed on it, meaning that when you use this coupon with a product, you may get N times the value of that product back! What is more, the shop also offers some bonus product for free. However, if you apply a coupon with a positive N to this bonus product, you will have to pay the shop N times the value of the bonus product… but hey, magically, they have some coupons with negative N’s!\nFor example, given a set of coupons { 1 2 4 −1 }, and a set of product values { 7 6 −2 −3 } (in Mars dollars M$) where a negative value corresponds to a bonus product. You can apply coupon 3 (with N being 4) to product 1 (with value M$7) to get M$28 back; coupon 2 to product 2 to get M$12 back; and coupon 4 to product 4 to get M$3 back. On the other hand, if you apply coupon 3 to product 4, you will have to pay M$12 to the shop.\nEach coupon and each product may be selected at most once. Your task is to get as much money back as possible.\nInput Specification: # Each input file contains one test case. For each case, the first line contains the number of coupons N**C, followed by a line with N**C coupon integers. Then the next line contains the number of products N**P, followed by a line with N**P product values. Here 1≤N**C,N**P≤105, and it is guaranteed that all the numbers will not exceed 230.\nOutput Specification: # For each test case, simply print in a line the maximum amount of money you can get back.\nSample Input: # 4 1 2 4 -1 4 7 6 -2 -3 Sample Output: # 43 1、大致题意 # 给出两个数字序列，从这两个序列中分别选取相同数量的元素进行一对一相乘，问能得到的乘积之和最大为多少\n2、基本思路 # 把这两个序列在存储时，将正数和负数分开，并将正数从大到小排序，负数从小到大排序。将前面都是负数的数相乘求和，然后将后面都是正数的数相乘求和。\n3、AC代码 # #include\u0026lt;iostream\u0026gt; #include\u0026lt;algorithm\u0026gt; #include\u0026lt;vector\u0026gt; using namespace std; int n,m,tmp; vector\u0026lt;int\u0026gt;az,af,bz,bf; int main() { cin\u0026gt;\u0026gt;m; for(int i=0; i\u0026lt;m; i++) { cin\u0026gt;\u0026gt;tmp; if(tmp\u0026gt;0) { az.push_back(tmp); } else { af.push_back(tmp); } } sort(az.begin(),az.end(),greater\u0026lt;int\u0026gt;()); sort(af.begin(),af.end(),less\u0026lt;int\u0026gt;()); cin\u0026gt;\u0026gt;n; for(int i=0; i\u0026lt;n; i++) { cin\u0026gt;\u0026gt;tmp; if(tmp\u0026gt;0) { bz.push_back(tmp); } else { bf.push_back(tmp); } } sort(bz.begin(),bz.end(),greater\u0026lt;int\u0026gt;()); sort(bf.begin(),bf.end(),less\u0026lt;int\u0026gt;()); int ans=0; int size=min(az.size(),bz.size()); for(int i=0; i\u0026lt;size; i++) { ans+=az[i]*bz[i]; } size=min(af.size(),bf.size()); for(int i=0; i\u0026lt;size; i++) { ans+=af[i]*bf[i]; } cout\u0026lt;\u0026lt;ans; return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1037magiccoupon%E8%B4%AA%E5%BF%83%E6%8E%92%E5%BA%8F/","section":"博客","summary":"1037 Magic Coupon（贪心，排序） # 0、题目 # The magic shop in Mars is offering some magic coupons.","title":"1037MagicCoupon贪心排序"},{"content":"1038 Recover the Smallest Number（贪心） # 0、题目 # Given a collection of number segments, you are supposed to recover the smallest number from them. For example, given { 32, 321, 3214, 0229, 87 }, we can recover many numbers such like 32-321-3214-0229-87 or 0229-32-87-321-3214 with respect to different orders of combinations of these segments, and the smallest number is 0229-321-3214-32-87.\nInput Specification: # Each input file contains one test case. Each case gives a positive integer N (≤104) followed by N number segments. Each segment contains a non-negative integer of no more than 8 digits. All the numbers in a line are separated by a space.\nOutput Specification: # For each test case, print the smallest number in one line. Notice that the first digit must not be zero.\nSample Input: # 5 32 321 3214 0229 87 Sample Output: # 22932132143287 1、大致题意 # 给一些字符串，求它们拼接起来构成最小数字的方式\n2、基本思路 # 让我们一起来见证 cmp 函数的强大之处！！~不是按照字典序排列就可以的，必须保证两个字符串构成的数字是最小的才行，所以 cmp函数写成 return a + b \u0026lt; b + a; 的形式，保证它排列按照能够组成的最小数字的形式排列。\n因为字符串可能前面有0，这些要移除掉（用s.erase(s.begin())就可以了嗯 string如此神奇）。输出拼接后的字符串即可。\n注意：如果移出了0之后发现 s.length() == 0了，说明这个数是0，那么要特别地输出这个0，否则会什么都不输出\n3、解题思路 # 3.1 string 知识点 # 我在解题前，复习了 string 库的相关知识点，此题主要是 string 的 erase() 函数，这个函数有2种用法：\n删除单个元素+ 删除一个区间内的所有元素+ 时间复杂度均为O(N) #include\u0026lt;string\u0026gt; #include\u0026lt;iostream\u0026gt; using namespace std; int main() { // 删除单个元素 string ss = \u0026#34;12345\u0026#34;; cout \u0026lt;\u0026lt; \u0026#34;ss: \u0026#34; \u0026lt;\u0026lt; ss \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; endl; ss.erase(ss.begin()+4); cout \u0026lt;\u0026lt; \u0026#34;ss.erase(ss.begin()+24: \u0026#34; \u0026lt;\u0026lt; ss \u0026lt;\u0026lt; endl; // 删除1个区间内所有元素 cout \u0026lt;\u0026lt; endl; ss = \u0026#34;678910\u0026#34;; cout \u0026lt;\u0026lt; \u0026#34;ss: \u0026#34; \u0026lt;\u0026lt; ss \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; endl; ss.erase(ss.begin()+2, ss.end()-1); cout \u0026lt;\u0026lt; \u0026#34;ss.erase(ss.begin()+2, ss.end()-1): \u0026#34; \u0026lt;\u0026lt; ss \u0026lt;\u0026lt; endl; // 从起始位置，删除x个字符 cout \u0026lt;\u0026lt; endl; ss = \u0026#34;678910\u0026#34;; cout \u0026lt;\u0026lt; \u0026#34;ss: \u0026#34; \u0026lt;\u0026lt; ss \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; endl; ss.erase(3,3); cout \u0026lt;\u0026lt; \u0026#34;ss.erase(3,3): \u0026#34; \u0026lt;\u0026lt; ss; return 0; } 输出：\nss: 12345 ss.erase(ss.begin()+24: 1234 ss: 678910 ss.erase(ss.begin()+2, ss.end()-1): 670 ss: 678910 ss.erase(3,3): 678 Process returned 0 (0x0) execution time : 0.018 s Press any key to continue. 3.2 AC代码 # #include\u0026lt;vector\u0026gt; #include\u0026lt;iostream\u0026gt; #include\u0026lt;algorithm\u0026gt; using namespace std; int n; vector\u0026lt;string\u0026gt; str; bool cmp(string a, string b) { return a+b\u0026lt;b+a; } int main() { cin\u0026gt;\u0026gt;n; string s; for(int i=0; i\u0026lt;n; i++){ cin\u0026gt;\u0026gt;s; str.push_back(s); } sort(str.begin(), str.end(), cmp); string ans; for(int i=0; i\u0026lt;n; i++) ans+=str[i]; while(ans.size() != 0 \u0026amp;\u0026amp; ans[0] == \u0026#39;0\u0026#39;) ans.erase(ans.begin()); if(ans.size() == 0) cout \u0026lt;\u0026lt; 0; else cout \u0026lt;\u0026lt; ans; return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1038recoverthesmallestnumber%E8%B4%AA%E5%BF%83%E6%8E%92%E5%BA%8F/","section":"博客","summary":"1038 Recover the Smallest Number（贪心） # 0、题目 # Given a collection of number segments, you are supposed to recover the smallest number from them.","title":"1038RecovertheSmallestNumber贪心排序"},{"content":"1039 Course List for Student # 0、题目 # Zhejiang University has 40000 students and provides 2500 courses. Now given the student name lists of all the courses, you are supposed to output the registered course list for each student who comes for a query.\nInput Specification: # Each input file contains one test case. For each case, the first line contains 2 positive integers: N (≤40,000), the number of students who look for their course lists, and K (≤2,500), the total number of courses. Then the student name lists are given for the courses (numbered from 1 to K) in the following format: for each course i, first the course index i and the number of registered students N**i (≤200) are given in a line. Then in the next line, N**i student names are given. A student name consists of 3 capital English letters plus a one-digit number. Finally the last line contains the N names of students who come for a query. All the names and numbers in a line are separated by a space.\nOutput Specification: # For each test case, print your results in N lines. Each line corresponds to one student, in the following format: first print the student’s name, then the total number of registered courses of that student, and finally the indices of the courses in increasing order. The query results must be printed in the same order as input. All the data in a line must be separated by a space, with no extra space at the end of the line.\nSample Input: # 11 5 4 7 BOB5 DON2 FRA8 JAY9 KAT3 LOR6 ZOE1 1 4 ANN0 BOB5 JAY9 LOR6 2 7 ANN0 BOB5 FRA8 JAY9 JOE4 KAT3 LOR6 3 1 BOB5 5 9 AMY7 ANN0 BOB5 DON2 FRA8 JAY9 KAT3 LOR6 ZOE1 ZOE1 ANN0 BOB5 JOE4 JAY9 FRA8 DON2 AMY7 KAT3 LOR6 NON9 Sample Output: # ZOE1 2 4 5 ANN0 3 1 2 5 BOB5 5 1 2 3 4 5 JOE4 1 2 JAY9 4 1 2 4 5 FRA8 3 2 4 5 DON2 2 4 5 AMY7 1 5 KAT3 3 2 4 5 LOR6 4 1 2 4 5 NON9 0 1、大致题意 # 有N个学生，K门课，给出选择每门课的学生姓名，并在唱吧给出N个学生的姓名，要求按顺序给出每个学生的选课情况\n2、基本思路 # 简单题。\n考虑到string、cin、cout会超时，可以使用hash(262626*10+10)将学生姓名变为int型，然后存储在vector里面\n3、AC代码 # #include\u0026lt;cstdio\u0026gt; #include\u0026lt;vector\u0026gt; #include\u0026lt;algorithm\u0026gt; using namespace std; int getid(char *name) { int id = 0; for(int i = 0; i \u0026lt; 3; i++) id = 26 * id + (name[i] - \u0026#39;A\u0026#39;); id = id * 10 + (name[3] - \u0026#39;0\u0026#39;); return id; } const int maxn = 26 * 26 * 26 * 10 + 10; vector\u0026lt;int\u0026gt; v[maxn]; int main() { int n, k, no, num, id = 0; char name[5]; scanf(\u0026#34;%d %d\u0026#34;, \u0026amp;n, \u0026amp;k); for(int i = 0; i \u0026lt; k; i++) { scanf(\u0026#34;%d %d\u0026#34;, \u0026amp;no, \u0026amp;num); for(int j = 0; j \u0026lt; num; j++) { scanf(\u0026#34;%s\u0026#34;, name); id = getid(name); v[id].push_back(no); } } for(int i = 0; i \u0026lt; n; i++) { scanf(\u0026#34;%s\u0026#34;, name); id = getid(name); sort(v[id].begin(), v[id].end()); printf(\u0026#34;%s %lu\u0026#34;, name, v[id].size()); for(int j = 0; j \u0026lt; v[id].size(); j++) printf(\u0026#34; %d\u0026#34;, v[id][j]); printf(\u0026#34;\\n\u0026#34;); } return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1039courselistforstudent/","section":"博客","summary":"1039 Course List for Student # 0、题目 # Zhejiang University has 40000 students and provides 2500 courses.","title":"1039CourseListforStudent"},{"content":"1040 Longest Symmetric String # 0、题目 # Given a string, you are supposed to output the length of the longest symmetric sub-string. For example, given Is PAT\u0026amp;TAP symmetric?, the longest symmetric sub-string is s PAT\u0026amp;TAP s, hence you must output 11.\nInput Specification: # Each input file contains one test case which gives a non-empty string of length no more than 1000.\nOutput Specification: # For each test case, simply print the maximum length in a line.\nSample Input: # Is PAT\u0026amp;TAP symmetric? Sample Output: # 11 1、大致题意 # 给出一个字符串，要求出其中最长回文串的长度。\n2、基本思路 # 回文子串要么是奇数串要么是偶数串，从中间开始往两边遍历，计算长度即可。\n3、AC代码 # #include\u0026lt;iostream\u0026gt; using namespace std; int main() { string s; int n = 1; getline(cin,s); for(int i=0; i\u0026lt;s.length(); i++) { //奇数 int j=0; while((j+i\u0026lt;=s.length()-1)\u0026amp;\u0026amp;(i-j\u0026gt;=0)\u0026amp;\u0026amp;(s[i+j] == s[i-j])) j++; if(n\u0026lt;2*j-1) n=2*j-1; } for(int i=0; i\u0026lt;s.length()-1; i++) { if(s[i] == s[i+1]) { int j=0; while(((j+i+1)\u0026lt;=s.length()-1)\u0026amp;\u0026amp;(i-j\u0026gt;=0)\u0026amp;\u0026amp;(s[i+j+1]==s[i-j])) j++; if(n\u0026lt;(2*j)) n=2*j; } } printf(\u0026#34;%d\u0026#34;,n); return 0; } 4、DP做法及思路 # $dp[i][j]$表示 $s[i]$ 到 $s[j]$ 所表示的字串是否是回文字串。只有0和1+ 递推方程：\n当s[i] == s[j] : dp[i][j] = dp[i+1][j-1]+ 当s[i] != s[j] : dp[i][j] =0\n边界：dp[i][j] = 1, dp[i][i+1] = (s[i] == s[i+1]) ? 1 : 0+ 因为i、j如果从小到大的顺序来枚举的话，无法保证更新dp[i][j]的时候dp[i+1][j-1]已经被计算过。因此不妨考虑按照字串的长度和子串的初试位置进行枚举，即第一遍将长度为3的子串的dp的值全部求出，第二遍通过第一遍结果计算出长度为4的子串的dp的值…这样就可以避免状态无法转移的问题+ 首先初始化dp[i][i] = 1, dp[i][i+1]，把长度为1和2的都初始化好，然后从L = 3开始一直到 L \u0026lt;= len 根据动态规划的递归方程来判断\n#include \u0026lt;iostream\u0026gt; using namespace std; int dp[1010][1010]; int main() { string s; getline(cin, s); int len = s.length(), ans = 1; for(int i = 0; i \u0026lt; len; i++) { dp[i][i] = 1; if(i \u0026lt; len - 1 \u0026amp;\u0026amp; s[i] == s[i+1]) { dp[i][i+1] = 1; ans = 2; } } for(int L = 3; L \u0026lt;= len; L++) { for(int i = 0; i + L - 1 \u0026lt; len; i++) { int j = i + L -1; if(s[i] == s[j] \u0026amp;\u0026amp; dp[i+1][j-1] == 1) { dp[i][j] = 1; ans = L; } } } printf(\u0026#34;%d\u0026#34;, ans); return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1040longestsymmetricstring%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/","section":"博客","summary":"1040 Longest Symmetric String # 0、题目 # Given a string, you are supposed to output the length of the longest symmetric sub-string.","title":"1040LongestSymmetricString动态规划"},{"content":"1041 Be Unique # 0、题目 # Being unique is so important to people on Mars that even their lottery is designed in a unique way. The rule of winning is simple: one bets on a number chosen from [1,104]. The first one who bets on a unique number wins. For example, if there are 7 people betting on { 5 31 5 88 67 88 17 }, then the second one who bets on 31 wins.\nInput Specification: # Each input file contains one test case. Each case contains a line which begins with a positive integer N (≤105) and then followed by N bets. The numbers are separated by a space.\nOutput Specification: # For each test case, print the winning number in a line. If there is no winner, print None instead.\nSample Input 1: # 7 5 31 5 88 67 88 17 Sample Output 1: # 31 Sample Input 2: # 5 888 666 666 888 888 Sample Output 2: # None 1、大致题意 # 给n个数字，按照读入顺序，哪个数字是第一个在所有数字中只出现一次的数字。如果所有数字出现都超过了一次，则输出None\n2、基本思路 # 简单题。\n建立一个数组，存储每个数字出现的次数，然后遍历一遍输入的顺序看是否有出现次数为1的数字\n3、AC代码 # #include \u0026lt;cstdio\u0026gt; using namespace std; int a[100001], m[100000]; int main() { int n; scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); for(int i = 0; i \u0026lt; n; i++) { scanf(\u0026#34;%d\u0026#34;, \u0026amp;a[i]); m[a[i]]++; } for(int i = 0; i \u0026lt; n; i++) { if(m[a[i]] == 1) { printf(\u0026#34;%d\u0026#34;, a[i]); return 0; } } printf(\u0026#34;None\u0026#34;); return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1041beunique/","section":"博客","summary":"1041 Be Unique # 0、题目 # Being unique is so important to people on Mars that even their lottery is designed in a unique way.","title":"1041BeUnique"},{"content":"1042 Shuffling Machine（模拟） # 0、题目 # Shuffling is a procedure used to randomize a deck of playing cards. Because standard shuffling techniques are seen as weak, and in order to avoid “inside jobs” where employees collaborate with gamblers by performing inadequate shuffles, many casinos employ automatic shuffling machines. Your task is to simulate a shuffling machine.\nThe machine shuffles a deck of 54 cards according to a given random order and repeats for a given number of times. It is assumed that the initial status of a card deck is in the following order:\nS1, S2, ..., S13, H1, H2, ..., H13, C1, C2, ..., C13, D1, D2, ..., D13, J1, J2 where “S” stands for “Spade”, “H” for “Heart”, “C” for “Club”, “D” for “Diamond”, and “J” for “Joker”. A given order is a permutation of distinct integers in [1, 54]. If the number at the i-th position is j, it means to move the card from position i to position j. For example, suppose we only have 5 cards: S3, H5, C1, D13 and J2. Given a shuffling order {4, 2, 5, 3, 1}, the result will be: J2, H5, D13, S3, C1. If we are to repeat the shuffling again, the result will be: C1, H5, S3, J2, D13.\nInput Specification: # Each input file contains one test case. For each case, the first line contains a positive integer K (≤20) which is the number of repeat times. Then the next line contains the given order. All the numbers in a line are separated by a space.\nOutput Specification: # For each test case, print the shuffling results in one line. All the cards are separated by a space, and there must be no extra space at the end of the line.\nSample Input: # 2 36 52 37 38 3 39 40 53 54 41 11 12 13 42 43 44 2 4 23 24 25 26 27 6 7 8 48 49 50 51 9 10 14 15 16 5 17 18 19 1 20 21 22 28 29 30 31 32 33 34 35 45 46 47 Sample Output: # S7 C11 C10 C12 S1 H7 H8 H9 D8 D9 S11 S12 S13 D10 D11 D12 S3 S4 S6 S10 H1 H2 C13 D2 D3 D4 H6 H3 D13 J1 J2 C1 C2 C3 C4 D1 S5 H5 H11 H12 C6 C7 C8 C9 S2 S8 S9 H10 D5 D6 D7 H4 H13 C5 1、大致题意 # 按照给出的顺序洗牌\n2、基本思路 # 简单题，理解题目即可解答\n3、AC代码 # #include \u0026lt;cstdio\u0026gt; using namespace std; int main() { int cnt; scanf(\u0026#34;%d\u0026#34;, \u0026amp;cnt); int start[55], end[55], scan[55]; for(int i = 1; i \u0026lt; 55; i++) { scanf(\u0026#34;%d\u0026#34;, \u0026amp;scan[i]); end[i] = i; } for(int i = 0; i \u0026lt; cnt; i++) { for(int j = 1; j \u0026lt; 55; j++) start[j] = end[j]; for(int k = 1; k \u0026lt; 55; k++) end[scan[k]] = start[k]; } char c[6] = {\u0026#34;SHCDJ\u0026#34;}; for(int i = 1; i \u0026lt; 55; i++) { end[i] = end[i] - 1; printf(\u0026#34;%c%d\u0026#34;, c[end[i]/13], end[i]%13+1); if(i != 54) printf(\u0026#34; \u0026#34;); } return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1042shufflingmachine%E6%A8%A1%E6%8B%9F/","section":"博客","summary":"1042 Shuffling Machine（模拟） # 0、题目 # Shuffling is a procedure used to randomize a deck of playing cards.","title":"1042ShufflingMachine模拟"},{"content":"1043 Is It a Binary Search Tree # 0、题目 # A Binary Search Tree (BST) is recursively defined as a binary tree which has the following properties:\nThe left subtree of a node contains only nodes with keys less than the node’s key.+ The right subtree of a node contains only nodes with keys greater than or equal to the node’s key.+ Both the left and right subtrees must also be binary search trees. If we swap the left and right subtrees of every node, then the resulting tree is called the Mirror Image of a BST.\nNow given a sequence of integer keys, you are supposed to tell if it is the preorder traversal sequence of a BST or the mirror image of a BST.\nInput Specification: # Each input file contains one test case. For each case, the first line contains a positive integer N (≤1000). Then N integer keys are given in the next line. All the numbers in a line are separated by a space.\nOutput Specification: # For each test case, first print in a line YES if the sequence is the preorder traversal sequence of a BST or the mirror image of a BST, or NO if not. Then if the answer is YES, print in the next line the postorder traversal sequence of that tree. All the numbers in a line must be separated by a space, and there must be no extra space at the end of the line.\nSample Input 1: # 7 8 6 5 7 10 8 11 Sample Output 1: # YES 5 7 6 8 11 10 8 Sample Input 2: # 7 8 10 11 8 6 7 5 Sample Output 2: # YES 11 8 10 7 5 6 8 Sample Input 3: # 7 8 6 8 5 10 9 11 Sample Output 3: # NO 1、大致题意 # 给定一个整数键值序列，现请你编写程序，判断这是否是对一棵二叉搜索树或其镜像进行前序遍历的结果。\n2、基本思路 # 2.1 二叉排序树 # 二叉排序树有以下的性质：\n左边结点的值比其父节点的值小+ 右边结点的值大于或等于其父节点的值+ 左子树和右子树都必须是二叉树 2.1.1 二叉排序树的镜像 # 如果交换左右结点的值，那么形成的二叉排序树就是原二叉排序树的镜像。现给出一串整数序列，你需要判断该序列是否是一棵二叉排序树或二叉排序树的镜像的先序序列，并输出其后序序列。\n2.2 解题思路 # 首先边输入结点的值边插入形成一棵二叉排序树，见函数build()；\n接着对构建好的二叉排序树进行先序preOrder()、镜像先序mirrorPreOrder()和后序postOrder()、镜像后序mirrorPostOrder()遍历；\n最后判断题目给出的序列和先序遍历结果、镜像先序遍历结果是否一致，输出结果即可。\n先序遍历：根节点、左子树、右子树（中左右）+ 镜像先序遍历：根节点、右子树、左子树（中右左）+ 后序遍历：左子树、右子树、根节点（左右中）+ 镜像后序遍历：右子树、左子树、根节点（右左中） 2.3 补充-C++中如何判断两个vector是否相等？ # 直接使用==呀\n如果vector里面的元素类型是简单类型（内置类型），可以直接使用==或者!=进行比较 因为在STL里面，==和!=是可以直接使用的：\ntemplate\u0026lt; class T, class Alloc \u0026gt; bool operator==( const vector\u0026lt;T,Alloc\u0026gt;\u0026amp; lhs, const vector\u0026lt;T,Alloc\u0026gt;\u0026amp; rhs ); template\u0026lt; class T, class Alloc \u0026gt; bool operator!=( const vector\u0026lt;T,Alloc\u0026gt;\u0026amp; lhs, const vector\u0026lt;T,Alloc\u0026gt;\u0026amp; rhs ); 甚至可以使用\u0026lt;=、\u0026lt;、\u0026gt;=、\u0026gt;比较两个vector大小：按照字典序排列\ntemplate\u0026lt; class T, class Alloc \u0026gt; bool operator\u0026lt;( const vector\u0026lt;T,Alloc\u0026gt;\u0026amp; lhs, const vector\u0026lt;T,Alloc\u0026gt;\u0026amp; rhs ); template\u0026lt; class T, class Alloc \u0026gt; bool operator\u0026lt;=( const vector\u0026lt;T,Alloc\u0026gt;\u0026amp; lhs, const vector\u0026lt;T,Alloc\u0026gt;\u0026amp; rhs ); template\u0026lt; class T, class Alloc \u0026gt; bool operator\u0026gt;( const vector\u0026lt;T,Alloc\u0026gt;\u0026amp; lhs, const vector\u0026lt;T,Alloc\u0026gt;\u0026amp; rhs ); template\u0026lt; class T, class Alloc \u0026gt; bool operator\u0026gt;=( const vector\u0026lt;T,Alloc\u0026gt;\u0026amp; lhs, const vector\u0026lt;T,Alloc\u0026gt;\u0026amp; rhs ); 3、AC代码 # #include\u0026lt;iostream\u0026gt; #include\u0026lt;vector\u0026gt; using namespace std; struct Node{ int data; Node *left,*right; }; int N; vector\u0026lt;int\u0026gt; origin,pre,preMirror,post,postMirror; Node *tree = NULL; int sum = 0; //构建二叉排序树 void build(Node* \u0026amp;node,int data){ sum ++; if(node == NULL){ node = new Node; node-\u0026gt;data = data; node-\u0026gt;left = node-\u0026gt;right = NULL; return; } if(data \u0026lt; node-\u0026gt;data) build(node-\u0026gt;left,data); else build(node-\u0026gt;right,data); } //先序遍历 void preOrder(Node* tree){ if(tree == NULL) return; pre.push_back(tree-\u0026gt;data); preOrder(tree-\u0026gt;left); preOrder(tree-\u0026gt;right); } //先序遍历镜像 void mirrorPreOrder(Node *tree){ if(tree == NULL) return; preMirror.push_back(tree-\u0026gt;data); mirrorPreOrder(tree-\u0026gt;right); mirrorPreOrder(tree-\u0026gt;left); } //后序遍历 void postOrder(Node *tree){ if(tree == NULL) return; postOrder(tree-\u0026gt;left); postOrder(tree-\u0026gt;right); post.push_back(tree-\u0026gt;data); } //后序遍历镜像 void mirrorPostOrder(Node *tree){ if(tree == NULL) return; mirrorPostOrder(tree-\u0026gt;right); mirrorPostOrder(tree-\u0026gt;left); postMirror.push_back(tree-\u0026gt;data); } int main(){ scanf(\u0026#34;%d\u0026#34;,\u0026amp;N); for(int i = 0 ; i \u0026lt; N ; i ++){ int temp; scanf(\u0026#34;%d\u0026#34;,\u0026amp;temp); origin.push_back(temp); build(tree,origin[i]);//构建二叉排序树 } preOrder(tree); mirrorPreOrder(tree); postOrder(tree); mirrorPostOrder(tree); if(pre == origin){//初始序列等于先序序列 printf(\u0026#34;YES\\n\u0026#34;); for(int i = 0 ; i \u0026lt; post.size() ; i ++){ if(i \u0026gt; 0) printf(\u0026#34; \u0026#34;); printf(\u0026#34;%d\u0026#34;,post[i]); } }else if(preMirror == origin){//初始序列等于先序镜像序列 printf(\u0026#34;YES\\n\u0026#34;); for(int i = 0 ; i \u0026lt; postMirror.size() ; i ++){ if(i \u0026gt; 0) printf(\u0026#34; \u0026#34;); printf(\u0026#34;%d\u0026#34;,postMirror[i]); } }else{ printf(\u0026#34;NO\u0026#34;); } return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1043isitabinarysearchtree%E4%BA%8C%E5%8F%89%E6%9F%A5%E6%89%BE%E6%A0%91bst/","section":"博客","summary":"1043 Is It a Binary Search Tree # 0、题目 # A Binary Search Tree (BST) is recursively defined as a binary tree which has the following properties:","title":"1043IsItaBinarySearchTree二叉查找树BST"},{"content":"1044 Shopping in Mars # 0、题目 # Shopping in Mars is quite a different experience. The Mars people pay by chained diamonds. Each diamond has a value (in Mars dollars M$). When making the payment, the chain can be cut at any position for only once and some of the diamonds are taken off the chain one by one. Once a diamond is off the chain, it cannot be taken back. For example, if we have a chain of 8 diamonds with values M$3, 2, 1, 5, 4, 6, 8, 7, and we must pay M$15. We may have 3 options:\nCut the chain between 4 and 6, and take off the diamonds from the position 1 to 5 (with values 3+2+1+5+4=15).+ Cut before 5 or after 6, and take off the diamonds from the position 4 to 6 (with values 5+4+6=15).+ Cut before 8, and take off the diamonds from the position 7 to 8 (with values 8+7=15). Now given the chain of diamond values and the amount that a customer has to pay, you are supposed to list all the paying options for the customer.\nIf it is impossible to pay the exact amount, you must suggest solutions with minimum lost.\nInput Specification: # Each input file contains one test case. For each case, the first line contains 2 numbers: N (≤105), the total number of diamonds on the chain, and M (≤108), the amount that the customer has to pay. Then the next line contains N positive numbers D1⋯D**N (D**i≤103 for all i=1,⋯,N) which are the values of the diamonds. All the numbers in a line are separated by a space.\nOutput Specification: # For each test case, print i-j in a line for each pair of i ≤ j such that Di + … + Dj = M. Note that if there are more than one solution, all the solutions must be printed in increasing order of i.\nIf there is no solution, output i-j for pairs of i ≤ j such that Di + … + Dj \u0026gt;M with (Di + … + Dj −M) minimized. Again all the solutions must be printed in increasing order of i.\nIt is guaranteed that the total value of diamonds is sufficient to pay the given amount.\nSample Input 1: # 16 15 3 2 1 5 4 6 8 7 16 10 15 11 9 12 14 13 Sample Output 1: # 1-5 4-6 7-8 11-11 Sample Input 2: # 5 13 2 4 5 7 9 Sample Output 2: # 2-4 4-5 1、大致题意 # 求一串的数字中连续的一段，使得这个连续的段内数字的和恰好等于所期望的值m。如果不能找到恰好等于，就找让自己付出最少的价格（总和必须大于等于所给值）的那段区间，求所有可能的结果\n2、基本思路 # 首先二分查找合适的价格，然后遍历一遍即可。\n3、AC代码 # #include\u0026lt;iostream\u0026gt; using namespace std; int n,m; int a[1000005]; bool judge(int ans) { int summ=0,pre=1; for(int i=1; i\u0026lt;=n; i++) { summ+=a[i]; while(summ\u0026gt;ans) { summ-=a[pre]; pre++; } if(summ\u0026gt;=m\u0026amp;\u0026amp;summ\u0026lt;=ans) { return true; } } return false; } int main() { cin\u0026gt;\u0026gt;n\u0026gt;\u0026gt;m; for(int i=1; i\u0026lt;=n; i++) { cin\u0026gt;\u0026gt;a[i]; } //二分 int l=0,r=2000000008,mid,ans; while(l\u0026lt;=r) { int mid=(l+r)/2; if(judge(mid)) { ans=mid; //记录可行解 r=mid-1; } else { l=mid+1; } } //\tcout\u0026lt;\u0026lt;ans\u0026lt;\u0026lt;endl; //查找 int summ=0,pre=1; for(int i=1; i\u0026lt;=n; i++) { summ+=a[i]; while(summ\u0026gt;ans) { summ-=a[pre]; pre++; } if(summ==ans) { cout\u0026lt;\u0026lt;pre\u0026lt;\u0026lt;\u0026#34;-\u0026#34;\u0026lt;\u0026lt;i\u0026lt;\u0026lt;endl; } else if(summ\u0026lt;ans) { continue; } } return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1044shoppinginmars%E4%BA%8C%E5%88%86/","section":"博客","summary":"1044 Shopping in Mars # 0、题目 # Shopping in Mars is quite a different experience.","title":"1044ShoppinginMars二分"},{"content":"1045 Favorite Color Stripe（动态规划） # 0、题目 # Eva is trying to make her own color stripe out of a given one. She would like to keep only her favorite colors in her favorite order by cutting off those unwanted pieces and sewing the remaining parts together to form her favorite color stripe.\nIt is said that a normal human eye can distinguish about less than 200 different colors, so Eva’s favorite colors are limited. However the original stripe could be very long, and Eva would like to have the remaining favorite stripe with the maximum length. So she needs your help to find her the best result.\nNote that the solution might not be unique, but you only have to tell her the maximum length. For example, given a stripe of colors {2 2 4 1 5 5 6 3 1 1 5 6}. If Eva’s favorite colors are given in her favorite order as {2 3 1 5 6}, then she has 4 possible best solutions {2 2 1 1 1 5 6}, {2 2 1 5 5 5 6}, {2 2 1 5 5 6 6}, and {2 2 3 1 1 5 6}.\nInput Specification: # Each input file contains one test case. For each case, the first line contains a positive integer N (≤200) which is the total number of colors involved (and hence the colors are numbered from 1 to N). Then the next line starts with a positive integer M (≤200) followed by M Eva’s favorite color numbers given in her favorite order. Finally the third line starts with a positive integer L (≤104) which is the length of the given stripe, followed by L colors on the stripe. All the numbers in a line a separated by a space.\nOutput Specification: # For each test case, simply print in a line the maximum length of Eva’s favorite stripe.\nSample Input: # 6 5 2 3 1 5 6 12 2 2 4 1 5 5 6 3 1 1 5 6 Sample Output: # 7 1、大致题意 # 给出m中颜色作为喜欢的颜色（同时也给出顺序），然后给出一串长度为L的颜色序列，现在要去掉这个序列中的不喜欢的颜色，然后求剩下序列的一个子序列，使得这个子序列表示的颜色顺序符合自己喜欢的颜色的顺序，不一定要所有喜欢的颜色都出现\n2、基本思路 # 因为喜欢的颜色是不重复的，把喜欢的颜色的序列按照存储到数组中，book[i] = j表示i颜色的下标为j。先在输入的时候剔除不在喜欢的序列中的元素，然后把剩余的保存在数组a中。按照最长不下降子序列的方式做，对于从前到后的每一个i，如果它前面的所有的j，一下子找到了一个j的下标book[j]比book[i]小，此时就更新dp[i]使它 = max(dp[i], dp[j] + 1);并且同时再每一次遍历完成一次j后更新maxn的值为长度的最大值，最后输出maxn\n测试用例\n输入： 4 3 1 2 3 10 4 2 4 1 1 2 1 1 1 4 输出： 5 3、AC代码 # #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; using namespace std; int book[201], a[10001], dp[10001]; int main() { int n, m, x, l, num = 0, maxn = 0; scanf(\u0026#34;%d %d\u0026#34;, \u0026amp;n, \u0026amp;m); for(int i = 1; i \u0026lt;= m; i++) { scanf(\u0026#34;%d\u0026#34;, \u0026amp;x); book[x] = i; } scanf(\u0026#34;%d\u0026#34;, \u0026amp;l); for(int i = 0; i \u0026lt; l; i++) { scanf(\u0026#34;%d\u0026#34;, \u0026amp;x); if(book[x] \u0026gt;= 1) a[num++] = book[x]; } for(int i = 0; i \u0026lt; num; i++) { dp[i] = 1; for(int j = 0; j \u0026lt; i; j++) if(a[i] \u0026gt;= a[j]) dp[i] = max(dp[i], dp[j] + 1); maxn = max(dp[i], maxn); } printf(\u0026#34;%d\u0026#34;, maxn); return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1045favoritecolorstripe%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/","section":"博客","summary":"1045 Favorite Color Stripe（动态规划） # 0、题目 # Eva is trying to make her own color stripe out of a given one.","title":"1045FavoriteColorStripe动态规划"},{"content":"1046 Shortest Distance（前缀和） # 0、题目 # The task is really simple: given N exits on a highway which forms a simple cycle, you are supposed to tell the shortest distance between any pair of exits.\nInput Specification: # Each input file contains one test case. For each case, the first line contains an integer N (in [3,105]), followed by N integer distances D1 D2 ⋯ D**N, where D**i is the distance between the i-th and the (i+1)-st exits, and D**N is between the N-th and the 1st exits. All the numbers in a line are separated by a space. The second line gives a positive integer M (≤104), with M lines follow, each contains a pair of exit numbers, provided that the exits are numbered from 1 to N. It is guaranteed that the total round trip distance is no more than 107.\nOutput Specification: # For each test case, print your results in M lines, each contains the shortest distance between the corresponding given pair of exits.\nSample Input: # 5 1 2 4 14 9 3 1 3 2 5 4 1 Sample Output: # 3 10 7 1、大致题意 # 给一个环上边的距离，求指定点之间的最短距离。\n2、基本思路 # 简单模拟。所有结点连起来会形成一个环形。dis[i]存储第1个结点到第i个结点的下一个结点的距离。sum保存整个路径一圈的总和值。求得结果就是dis[right – 1] – dis[left – 1]和 sum – dis[right – 1] – dis[left – 1]中较小的那一个\n注意：可能left和right的顺序颠倒了，这时候要把left和right的值交换～\n3、AC代码 # #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; using namespace std; int main() { int n; scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); vector\u0026lt;int\u0026gt; dis(n + 1); int sum = 0, left, right, cnt; for(int i = 1; i \u0026lt;= n; i++) { int temp; scanf(\u0026#34;%d\u0026#34;, \u0026amp;temp); sum += temp; dis[i] = sum; } scanf(\u0026#34;%d\u0026#34;, \u0026amp;cnt); for(int i = 0; i \u0026lt; cnt; i++) { scanf(\u0026#34;%d %d\u0026#34;, \u0026amp;left, \u0026amp;right); if(left \u0026gt; right) swap(left, right); int temp = dis[right - 1] - dis[left - 1]; printf(\u0026#34;%d\\n\u0026#34;, min(temp, sum - temp)); } return 0; } ","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/1046shortestdistance%E5%89%8D%E7%BC%80%E5%92%8C/","section":"博客","summary":"1046 Shortest Distance（前缀和） # 0、题目 # The task is really simple: given N exits on a highway which forms a simple cycle, you are supposed to tell the shortest distance between any pair of exits.","title":"1046ShortestDistance前缀和"},{"content":"","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/","section":"博客","summary":"PAT甲级知识点与计算机考研的数据结构知识点高度吻合，准备与参加PAT考试不仅提高了我的编程能力，还巩固了相关知识，对我帮助很大。","title":"PAT甲级刷题笔记"},{"content":"PAT 甲级题目分布 # 最短路径： # 1003、1018、1030、1072、1087、1111\n深度优先搜索DFS： # 1013、1021、1034、1103、1130、1131、1134\n广度优先搜索BFS： # 1076、1091\n树： # 1004、1053、1079、1090、1094、1102、1106\n二叉树： # 1020、1043、1064、1066、1086、1099、1110、1115、1119、1127、1135、1147、1151、1155\n栈： # 1051\n堆： # 1098\n并查集： # 1107、1114、1118\n队列： # 1014、1056\n排序： # 1012、1016、1025、1028、1055、1062、1075、1080、1083、1113、1125、1141\n链表： # 1032、1052、1074、1097、1133\n其他的图论： # 1123、1126、1142\n拓扑排序： # 1146\n字符串处理： # 1001、1005、1023、1024、1035、1060、1061、1073、1077、1082、1108、1140、1150、1152\n记忆化搜索： # 1007、1040、1045、1068、1101\n二分查找： # 1010、1044、1085\n查找元素： # 1006、1011、1036\n贪心： # 1033、1037、1038、1067、1070\n树状数组： # 1057\n分数模拟： # 1081、1088\n模拟： # 1002、1009、1017、1026、1042、1046、1065、1105、1153\n","date":"1 September 2023","permalink":"/posts/reviews/pat%E7%94%B2%E7%BA%A7/pat%E7%94%B2%E7%BA%A7%E9%A2%98%E7%9B%AE%E5%88%86%E5%B8%83/","section":"博客","summary":"PAT 甲级题目分布 # 最短路径： # 1003、1018、1030、1072、1087、1111","title":"PAT甲级题目分布"},{"content":"","date":"1 September 2023","permalink":"/tags/pta/","section":"Tags","summary":"","title":"Pta"},{"content":" WFUing A graduate who loves coding. Interests # Distributed computing platform（for cloud computing, big data and deep learning） Domain-Specific Language Proficient in spring, springboot, redis, Message Queue, mysql, etc Education # Bachelor: Department of Computer Science and Technology, Zhejiang Sci-Tech University Master: Department of Software Engineering, Nanjing University Warning! Don\u0026rsquo;t forget to follow me on Github. ","date":"1 September 2023","permalink":"/","section":"WFUing","summary":"WFUing A graduate who loves coding.","title":"WFUing"},{"content":"","date":"1 January 0001","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"1 January 0001","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"1 January 0001","permalink":"/series/","section":"Series","summary":"","title":"Series"}]